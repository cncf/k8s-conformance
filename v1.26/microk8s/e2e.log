I0118 21:00:35.479096      20 e2e.go:126] Starting e2e run "de0ec6bb-1b40-473e-879d-9346f84b1ea8" on Ginkgo node 1
Jan 18 21:00:35.498: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674075635 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 18 21:00:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:00:35.682: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0118 21:00:35.682284      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 18 21:00:35.696: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 21:00:35.715: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 21:00:35.715: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jan 18 21:00:35.716: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 21:00:35.722: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 18 21:00:35.722: INFO: e2e test version: v1.26.0
Jan 18 21:00:35.723: INFO: kube-apiserver version: v1.26.0
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan 18 21:00:35.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:00:35.728: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.048 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 18 21:00:35.680: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:00:35.682: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0118 21:00:35.682284      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 18 21:00:35.696: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 18 21:00:35.715: INFO: 4 / 4 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 18 21:00:35.715: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
    Jan 18 21:00:35.716: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 18 21:00:35.722: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 18 21:00:35.722: INFO: e2e test version: v1.26.0
    Jan 18 21:00:35.723: INFO: kube-apiserver version: v1.26.0
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan 18 21:00:35.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:00:35.728: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:00:35.758
Jan 18 21:00:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:00:35.759
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:35.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:35.805
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-3497/configmap-test-5a0cea12-5171-4c42-8de9-6e9a0c99e0bb 01/18/23 21:00:35.808
STEP: Creating a pod to test consume configMaps 01/18/23 21:00:35.82
Jan 18 21:00:35.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8" in namespace "configmap-3497" to be "Succeeded or Failed"
Jan 18 21:00:35.841: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076357ms
Jan 18 21:00:37.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008970144s
Jan 18 21:00:39.847: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010258583s
Jan 18 21:00:41.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Running", Reason="", readiness=true. Elapsed: 6.008796883s
Jan 18 21:00:43.846: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Running", Reason="", readiness=false. Elapsed: 8.009848218s
Jan 18 21:00:45.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.008766004s
STEP: Saw pod success 01/18/23 21:00:45.845
Jan 18 21:00:45.846: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8" satisfied condition "Succeeded or Failed"
Jan 18 21:00:45.849: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 container env-test: <nil>
STEP: delete the pod 01/18/23 21:00:45.881
Jan 18 21:00:45.908: INFO: Waiting for pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 to disappear
Jan 18 21:00:45.915: INFO: Pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:00:45.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3497" for this suite. 01/18/23 21:00:45.92
------------------------------
• [SLOW TEST] [10.178 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:00:35.758
    Jan 18 21:00:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:00:35.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:35.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:35.805
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-3497/configmap-test-5a0cea12-5171-4c42-8de9-6e9a0c99e0bb 01/18/23 21:00:35.808
    STEP: Creating a pod to test consume configMaps 01/18/23 21:00:35.82
    Jan 18 21:00:35.836: INFO: Waiting up to 5m0s for pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8" in namespace "configmap-3497" to be "Succeeded or Failed"
    Jan 18 21:00:35.841: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076357ms
    Jan 18 21:00:37.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008970144s
    Jan 18 21:00:39.847: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010258583s
    Jan 18 21:00:41.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Running", Reason="", readiness=true. Elapsed: 6.008796883s
    Jan 18 21:00:43.846: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Running", Reason="", readiness=false. Elapsed: 8.009848218s
    Jan 18 21:00:45.845: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.008766004s
    STEP: Saw pod success 01/18/23 21:00:45.845
    Jan 18 21:00:45.846: INFO: Pod "pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8" satisfied condition "Succeeded or Failed"
    Jan 18 21:00:45.849: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 container env-test: <nil>
    STEP: delete the pod 01/18/23 21:00:45.881
    Jan 18 21:00:45.908: INFO: Waiting for pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 to disappear
    Jan 18 21:00:45.915: INFO: Pod pod-configmaps-83716730-44ab-49a9-8a94-41d4c0bd7ee8 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:00:45.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3497" for this suite. 01/18/23 21:00:45.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSE0118 21:00:45.937603      20 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:00:45.942
Jan 18 21:00:45.943: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 21:00:45.944
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:45.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:45.987
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/18/23 21:00:45.995
STEP: delete the rc 01/18/23 21:00:51.05
STEP: wait for the rc to be deleted 01/18/23 21:00:51.232
Jan 18 21:00:52.299: INFO: 98 pods remaining
Jan 18 21:00:52.301: INFO: 90 pods has nil DeletionTimestamp
Jan 18 21:00:52.301: INFO: 
Jan 18 21:00:53.274: INFO: 70 pods remaining
Jan 18 21:00:53.275: INFO: 69 pods has nil DeletionTimestamp
Jan 18 21:00:53.278: INFO: 
Jan 18 21:00:54.294: INFO: 62 pods remaining
Jan 18 21:00:54.294: INFO: 60 pods has nil DeletionTimestamp
Jan 18 21:00:54.294: INFO: 
Jan 18 21:00:55.267: INFO: 59 pods remaining
Jan 18 21:00:55.267: INFO: 50 pods has nil DeletionTimestamp
Jan 18 21:00:55.267: INFO: 
Jan 18 21:00:56.272: INFO: 44 pods remaining
Jan 18 21:00:56.272: INFO: 35 pods has nil DeletionTimestamp
Jan 18 21:00:56.272: INFO: 
Jan 18 21:00:57.278: INFO: 18 pods remaining
Jan 18 21:00:57.278: INFO: 16 pods has nil DeletionTimestamp
Jan 18 21:00:57.278: INFO: 
Jan 18 21:00:58.321: INFO: 0 pods remaining
Jan 18 21:00:58.321: INFO: 0 pods has nil DeletionTimestamp
Jan 18 21:00:58.321: INFO: 
STEP: Gathering metrics 01/18/23 21:00:59.313
W0118 21:00:59.332929      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 21:00:59.333: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 21:00:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6860" for this suite. 01/18/23 21:00:59.356
------------------------------
• [SLOW TEST] [13.475 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:00:45.942
    Jan 18 21:00:45.943: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 21:00:45.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:45.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:45.987
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/18/23 21:00:45.995
    STEP: delete the rc 01/18/23 21:00:51.05
    STEP: wait for the rc to be deleted 01/18/23 21:00:51.232
    Jan 18 21:00:52.299: INFO: 98 pods remaining
    Jan 18 21:00:52.301: INFO: 90 pods has nil DeletionTimestamp
    Jan 18 21:00:52.301: INFO: 
    Jan 18 21:00:53.274: INFO: 70 pods remaining
    Jan 18 21:00:53.275: INFO: 69 pods has nil DeletionTimestamp
    Jan 18 21:00:53.278: INFO: 
    Jan 18 21:00:54.294: INFO: 62 pods remaining
    Jan 18 21:00:54.294: INFO: 60 pods has nil DeletionTimestamp
    Jan 18 21:00:54.294: INFO: 
    Jan 18 21:00:55.267: INFO: 59 pods remaining
    Jan 18 21:00:55.267: INFO: 50 pods has nil DeletionTimestamp
    Jan 18 21:00:55.267: INFO: 
    Jan 18 21:00:56.272: INFO: 44 pods remaining
    Jan 18 21:00:56.272: INFO: 35 pods has nil DeletionTimestamp
    Jan 18 21:00:56.272: INFO: 
    Jan 18 21:00:57.278: INFO: 18 pods remaining
    Jan 18 21:00:57.278: INFO: 16 pods has nil DeletionTimestamp
    Jan 18 21:00:57.278: INFO: 
    Jan 18 21:00:58.321: INFO: 0 pods remaining
    Jan 18 21:00:58.321: INFO: 0 pods has nil DeletionTimestamp
    Jan 18 21:00:58.321: INFO: 
    STEP: Gathering metrics 01/18/23 21:00:59.313
    W0118 21:00:59.332929      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 21:00:59.333: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:00:59.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6860" for this suite. 01/18/23 21:00:59.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:00:59.436
Jan 18 21:00:59.436: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:00:59.438
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:59.526
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:59.53
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-9805 01/18/23 21:00:59.547
STEP: creating replication controller nodeport-test in namespace services-9805 01/18/23 21:00:59.627
I0118 21:00:59.720125      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9805, replica count: 2
I0118 21:01:02.774416      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:01:05.776127      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:01:08.778408      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:01:11.782408      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:01:14.782716      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:01:17.783087      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:01:17.783: INFO: Creating new exec pod
Jan 18 21:01:17.797: INFO: Waiting up to 5m0s for pod "execpodsndtk" in namespace "services-9805" to be "running"
Jan 18 21:01:17.805: INFO: Pod "execpodsndtk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.719707ms
Jan 18 21:01:19.810: INFO: Pod "execpodsndtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01314234s
Jan 18 21:01:19.810: INFO: Pod "execpodsndtk" satisfied condition "running"
Jan 18 21:01:20.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan 18 21:01:21.037: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 21:01:21.037: INFO: stdout: ""
Jan 18 21:01:21.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.152.183.183 80'
Jan 18 21:01:21.209: INFO: stderr: "+ nc -v -z -w 2 10.152.183.183 80\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
Jan 18 21:01:21.209: INFO: stdout: ""
Jan 18 21:01:21.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 32106'
Jan 18 21:01:21.381: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 32106\nConnection to 10.0.0.5 32106 port [tcp/*] succeeded!\n"
Jan 18 21:01:21.381: INFO: stdout: ""
Jan 18 21:01:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32106'
Jan 18 21:01:21.551: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32106\nConnection to 10.0.0.4 32106 port [tcp/*] succeeded!\n"
Jan 18 21:01:21.551: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:01:21.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9805" for this suite. 01/18/23 21:01:21.556
------------------------------
• [SLOW TEST] [22.134 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:00:59.436
    Jan 18 21:00:59.436: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:00:59.438
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:00:59.526
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:00:59.53
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-9805 01/18/23 21:00:59.547
    STEP: creating replication controller nodeport-test in namespace services-9805 01/18/23 21:00:59.627
    I0118 21:00:59.720125      20 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9805, replica count: 2
    I0118 21:01:02.774416      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:01:05.776127      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:01:08.778408      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:01:11.782408      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:01:14.782716      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:01:17.783087      20 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:01:17.783: INFO: Creating new exec pod
    Jan 18 21:01:17.797: INFO: Waiting up to 5m0s for pod "execpodsndtk" in namespace "services-9805" to be "running"
    Jan 18 21:01:17.805: INFO: Pod "execpodsndtk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.719707ms
    Jan 18 21:01:19.810: INFO: Pod "execpodsndtk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01314234s
    Jan 18 21:01:19.810: INFO: Pod "execpodsndtk" satisfied condition "running"
    Jan 18 21:01:20.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan 18 21:01:21.037: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 21:01:21.037: INFO: stdout: ""
    Jan 18 21:01:21.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.152.183.183 80'
    Jan 18 21:01:21.209: INFO: stderr: "+ nc -v -z -w 2 10.152.183.183 80\nConnection to 10.152.183.183 80 port [tcp/http] succeeded!\n"
    Jan 18 21:01:21.209: INFO: stdout: ""
    Jan 18 21:01:21.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 32106'
    Jan 18 21:01:21.381: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 32106\nConnection to 10.0.0.5 32106 port [tcp/*] succeeded!\n"
    Jan 18 21:01:21.381: INFO: stdout: ""
    Jan 18 21:01:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9805 exec execpodsndtk -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32106'
    Jan 18 21:01:21.551: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32106\nConnection to 10.0.0.4 32106 port [tcp/*] succeeded!\n"
    Jan 18 21:01:21.551: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:01:21.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9805" for this suite. 01/18/23 21:01:21.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:01:21.571
Jan 18 21:01:21.571: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:01:21.572
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:01:21.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:01:21.611
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:01:21.614
Jan 18 21:01:21.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434" in namespace "projected-480" to be "Succeeded or Failed"
Jan 18 21:01:21.634: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564547ms
Jan 18 21:01:23.640: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008922243s
Jan 18 21:01:25.641: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Running", Reason="", readiness=false. Elapsed: 4.009820378s
Jan 18 21:01:27.642: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01099015s
STEP: Saw pod success 01/18/23 21:01:27.642
Jan 18 21:01:27.642: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434" satisfied condition "Succeeded or Failed"
Jan 18 21:01:27.646: INFO: Trying to get logs from node test-vm-2 pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 container client-container: <nil>
STEP: delete the pod 01/18/23 21:01:27.682
Jan 18 21:01:28.276: INFO: Waiting for pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 to disappear
Jan 18 21:01:28.310: INFO: Pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:01:28.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-480" for this suite. 01/18/23 21:01:28.318
------------------------------
• [SLOW TEST] [6.766 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:01:21.571
    Jan 18 21:01:21.571: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:01:21.572
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:01:21.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:01:21.611
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:01:21.614
    Jan 18 21:01:21.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434" in namespace "projected-480" to be "Succeeded or Failed"
    Jan 18 21:01:21.634: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Pending", Reason="", readiness=false. Elapsed: 3.564547ms
    Jan 18 21:01:23.640: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008922243s
    Jan 18 21:01:25.641: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Running", Reason="", readiness=false. Elapsed: 4.009820378s
    Jan 18 21:01:27.642: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01099015s
    STEP: Saw pod success 01/18/23 21:01:27.642
    Jan 18 21:01:27.642: INFO: Pod "downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434" satisfied condition "Succeeded or Failed"
    Jan 18 21:01:27.646: INFO: Trying to get logs from node test-vm-2 pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:01:27.682
    Jan 18 21:01:28.276: INFO: Waiting for pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 to disappear
    Jan 18 21:01:28.310: INFO: Pod downwardapi-volume-4e27ca2c-9348-4bd1-9d04-00eae7826434 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:01:28.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-480" for this suite. 01/18/23 21:01:28.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:01:28.338
Jan 18 21:01:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 21:01:28.34
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:01:28.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:01:28.447
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 in namespace container-probe-914 01/18/23 21:01:28.45
Jan 18 21:01:28.506: INFO: Waiting up to 5m0s for pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70" in namespace "container-probe-914" to be "not pending"
Jan 18 21:01:28.514: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70": Phase="Pending", Reason="", readiness=false. Elapsed: 7.548597ms
Jan 18 21:01:30.520: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70": Phase="Running", Reason="", readiness=true. Elapsed: 2.01360989s
Jan 18 21:01:30.520: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70" satisfied condition "not pending"
Jan 18 21:01:30.520: INFO: Started pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 in namespace container-probe-914
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:01:30.52
Jan 18 21:01:30.524: INFO: Initial restart count of pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is 0
Jan 18 21:01:50.584: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 1 (20.060099358s elapsed)
Jan 18 21:02:10.644: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 2 (40.120366443s elapsed)
Jan 18 21:02:30.731: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 3 (1m0.207414538s elapsed)
Jan 18 21:02:50.788: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 4 (1m20.264679191s elapsed)
Jan 18 21:03:50.986: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 5 (2m20.461933603s elapsed)
STEP: deleting the pod 01/18/23 21:03:50.986
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 21:03:51.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-914" for this suite. 01/18/23 21:03:51.022
------------------------------
• [SLOW TEST] [142.704 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:01:28.338
    Jan 18 21:01:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 21:01:28.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:01:28.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:01:28.447
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 in namespace container-probe-914 01/18/23 21:01:28.45
    Jan 18 21:01:28.506: INFO: Waiting up to 5m0s for pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70" in namespace "container-probe-914" to be "not pending"
    Jan 18 21:01:28.514: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70": Phase="Pending", Reason="", readiness=false. Elapsed: 7.548597ms
    Jan 18 21:01:30.520: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70": Phase="Running", Reason="", readiness=true. Elapsed: 2.01360989s
    Jan 18 21:01:30.520: INFO: Pod "liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70" satisfied condition "not pending"
    Jan 18 21:01:30.520: INFO: Started pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 in namespace container-probe-914
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:01:30.52
    Jan 18 21:01:30.524: INFO: Initial restart count of pod liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is 0
    Jan 18 21:01:50.584: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 1 (20.060099358s elapsed)
    Jan 18 21:02:10.644: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 2 (40.120366443s elapsed)
    Jan 18 21:02:30.731: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 3 (1m0.207414538s elapsed)
    Jan 18 21:02:50.788: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 4 (1m20.264679191s elapsed)
    Jan 18 21:03:50.986: INFO: Restart count of pod container-probe-914/liveness-fb56b25e-7455-4ef6-83e7-2fff31f25c70 is now 5 (2m20.461933603s elapsed)
    STEP: deleting the pod 01/18/23 21:03:50.986
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:03:51.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-914" for this suite. 01/18/23 21:03:51.022
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:03:51.042
Jan 18 21:03:51.042: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:03:51.043
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:03:51.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:03:51.08
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8949 01/18/23 21:03:51.084
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-8949 01/18/23 21:03:51.094
Jan 18 21:03:51.139: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:04:01.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/18/23 21:04:01.152
STEP: updating a scale subresource 01/18/23 21:04:01.155
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 21:04:01.171
STEP: Patch a scale subresource 01/18/23 21:04:01.175
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 21:04:01.192
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:04:01.195: INFO: Deleting all statefulset in ns statefulset-8949
Jan 18 21:04:01.199: INFO: Scaling statefulset ss to 0
Jan 18 21:04:11.227: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:04:11.231: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:04:11.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8949" for this suite. 01/18/23 21:04:11.265
------------------------------
• [SLOW TEST] [20.237 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:03:51.042
    Jan 18 21:03:51.042: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:03:51.043
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:03:51.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:03:51.08
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8949 01/18/23 21:03:51.084
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-8949 01/18/23 21:03:51.094
    Jan 18 21:03:51.139: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:04:01.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/18/23 21:04:01.152
    STEP: updating a scale subresource 01/18/23 21:04:01.155
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 21:04:01.171
    STEP: Patch a scale subresource 01/18/23 21:04:01.175
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 21:04:01.192
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:04:01.195: INFO: Deleting all statefulset in ns statefulset-8949
    Jan 18 21:04:01.199: INFO: Scaling statefulset ss to 0
    Jan 18 21:04:11.227: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:04:11.231: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:04:11.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8949" for this suite. 01/18/23 21:04:11.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:04:11.281
Jan 18 21:04:11.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:04:11.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:04:11.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:04:11.324
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:04:11.327
Jan 18 21:04:11.344: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854" in namespace "downward-api-2650" to be "Succeeded or Failed"
Jan 18 21:04:11.348: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Pending", Reason="", readiness=false. Elapsed: 3.816445ms
Jan 18 21:04:13.354: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009419731s
Jan 18 21:04:15.355: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010222539s
STEP: Saw pod success 01/18/23 21:04:15.355
Jan 18 21:04:15.355: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854" satisfied condition "Succeeded or Failed"
Jan 18 21:04:15.358: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 container client-container: <nil>
STEP: delete the pod 01/18/23 21:04:15.379
Jan 18 21:04:15.405: INFO: Waiting for pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 to disappear
Jan 18 21:04:15.411: INFO: Pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:04:15.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2650" for this suite. 01/18/23 21:04:15.415
------------------------------
• [4.146 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:04:11.281
    Jan 18 21:04:11.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:04:11.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:04:11.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:04:11.324
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:04:11.327
    Jan 18 21:04:11.344: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854" in namespace "downward-api-2650" to be "Succeeded or Failed"
    Jan 18 21:04:11.348: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Pending", Reason="", readiness=false. Elapsed: 3.816445ms
    Jan 18 21:04:13.354: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009419731s
    Jan 18 21:04:15.355: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010222539s
    STEP: Saw pod success 01/18/23 21:04:15.355
    Jan 18 21:04:15.355: INFO: Pod "downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854" satisfied condition "Succeeded or Failed"
    Jan 18 21:04:15.358: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:04:15.379
    Jan 18 21:04:15.405: INFO: Waiting for pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 to disappear
    Jan 18 21:04:15.411: INFO: Pod downwardapi-volume-ab2be863-0706-47f6-b2cf-cdd6e2df2854 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:04:15.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2650" for this suite. 01/18/23 21:04:15.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:04:15.428
Jan 18 21:04:15.429: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:04:15.43
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:04:15.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:04:15.467
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-d7t89" 01/18/23 21:04:15.476
Jan 18 21:04:15.490: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard cpu limit of 500m
Jan 18 21:04:15.490: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.49
STEP: Confirm /status for "e2e-rq-status-d7t89" resourceQuota via watch 01/18/23 21:04:15.532
Jan 18 21:04:15.533: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList(nil)
Jan 18 21:04:15.533: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 18 21:04:15.534: INFO: ResourceQuota "e2e-rq-status-d7t89" /status was updated
STEP: Patching hard spec values for cpu & memory 01/18/23 21:04:15.537
Jan 18 21:04:15.549: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard cpu limit of 1
Jan 18 21:04:15.549: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.549
STEP: Confirm /status for "e2e-rq-status-d7t89" resourceQuota via watch 01/18/23 21:04:15.561
Jan 18 21:04:15.562: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan 18 21:04:15.562: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 18 21:04:15.563: INFO: ResourceQuota "e2e-rq-status-d7t89" /status was patched
STEP: Get "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.563
Jan 18 21:04:15.567: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard cpu of 1
Jan 18 21:04:15.567: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-d7t89" /status before checking Spec is unchanged 01/18/23 21:04:15.57
Jan 18 21:04:15.592: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard cpu of 2
Jan 18 21:04:15.592: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard memory of 2Gi
Jan 18 21:04:15.593: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan 18 21:04:15.593: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan 18 21:05:25.602: INFO: ResourceQuota "e2e-rq-status-d7t89" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:05:25.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5319" for this suite. 01/18/23 21:05:25.608
------------------------------
• [SLOW TEST] [70.196 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:04:15.428
    Jan 18 21:04:15.429: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:04:15.43
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:04:15.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:04:15.467
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-d7t89" 01/18/23 21:04:15.476
    Jan 18 21:04:15.490: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard cpu limit of 500m
    Jan 18 21:04:15.490: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.49
    STEP: Confirm /status for "e2e-rq-status-d7t89" resourceQuota via watch 01/18/23 21:04:15.532
    Jan 18 21:04:15.533: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList(nil)
    Jan 18 21:04:15.533: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 18 21:04:15.534: INFO: ResourceQuota "e2e-rq-status-d7t89" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/18/23 21:04:15.537
    Jan 18 21:04:15.549: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard cpu limit of 1
    Jan 18 21:04:15.549: INFO: Resource quota "e2e-rq-status-d7t89" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.549
    STEP: Confirm /status for "e2e-rq-status-d7t89" resourceQuota via watch 01/18/23 21:04:15.561
    Jan 18 21:04:15.562: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan 18 21:04:15.562: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 18 21:04:15.563: INFO: ResourceQuota "e2e-rq-status-d7t89" /status was patched
    STEP: Get "e2e-rq-status-d7t89" /status 01/18/23 21:04:15.563
    Jan 18 21:04:15.567: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard cpu of 1
    Jan 18 21:04:15.567: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-d7t89" /status before checking Spec is unchanged 01/18/23 21:04:15.57
    Jan 18 21:04:15.592: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard cpu of 2
    Jan 18 21:04:15.592: INFO: Resourcequota "e2e-rq-status-d7t89" reports status: hard memory of 2Gi
    Jan 18 21:04:15.593: INFO: observed resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan 18 21:04:15.593: INFO: Found resourceQuota "e2e-rq-status-d7t89" in namespace "resourcequota-5319" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan 18 21:05:25.602: INFO: ResourceQuota "e2e-rq-status-d7t89" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:05:25.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5319" for this suite. 01/18/23 21:05:25.608
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:05:25.625
Jan 18 21:05:25.625: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:05:25.627
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:05:25.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:05:25.667
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-2103 01/18/23 21:05:25.67
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[] 01/18/23 21:05:25.69
Jan 18 21:05:25.697: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 18 21:05:26.707: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2103 01/18/23 21:05:26.707
Jan 18 21:05:26.726: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2103" to be "running and ready"
Jan 18 21:05:26.730: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815943ms
Jan 18 21:05:26.730: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:05:28.736: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009741809s
Jan 18 21:05:28.736: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 21:05:28.736: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod1:[80]] 01/18/23 21:05:28.739
Jan 18 21:05:28.752: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/18/23 21:05:28.752
Jan 18 21:05:28.752: INFO: Creating new exec pod
Jan 18 21:05:28.764: INFO: Waiting up to 5m0s for pod "execpodxvk6h" in namespace "services-2103" to be "running"
Jan 18 21:05:28.768: INFO: Pod "execpodxvk6h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56744ms
Jan 18 21:05:30.772: INFO: Pod "execpodxvk6h": Phase="Running", Reason="", readiness=true. Elapsed: 2.007901289s
Jan 18 21:05:30.772: INFO: Pod "execpodxvk6h" satisfied condition "running"
Jan 18 21:05:31.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 21:05:31.943: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:31.943: INFO: stdout: ""
Jan 18 21:05:31.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
Jan 18 21:05:32.119: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:32.119: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-2103 01/18/23 21:05:32.119
Jan 18 21:05:32.131: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2103" to be "running and ready"
Jan 18 21:05:32.135: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961444ms
Jan 18 21:05:32.135: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:05:34.139: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008427594s
Jan 18 21:05:34.139: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:05:36.139: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008339005s
Jan 18 21:05:36.139: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 21:05:36.139: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 21:05:36.144
Jan 18 21:05:36.162: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 21:05:36.162
Jan 18 21:05:37.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 21:05:37.348: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:37.348: INFO: stdout: ""
Jan 18 21:05:37.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
Jan 18 21:05:37.579: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:37.579: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2103 01/18/23 21:05:37.579
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod2:[80]] 01/18/23 21:05:37.607
Jan 18 21:05:38.624: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/18/23 21:05:38.624
Jan 18 21:05:39.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan 18 21:05:39.808: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:39.808: INFO: stdout: ""
Jan 18 21:05:39.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
Jan 18 21:05:39.972: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
Jan 18 21:05:39.972: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-2103 01/18/23 21:05:39.972
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[] 01/18/23 21:05:40.042
Jan 18 21:05:40.062: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:05:40.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2103" for this suite. 01/18/23 21:05:40.122
------------------------------
• [SLOW TEST] [14.516 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:05:25.625
    Jan 18 21:05:25.625: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:05:25.627
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:05:25.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:05:25.667
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-2103 01/18/23 21:05:25.67
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[] 01/18/23 21:05:25.69
    Jan 18 21:05:25.697: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 18 21:05:26.707: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2103 01/18/23 21:05:26.707
    Jan 18 21:05:26.726: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2103" to be "running and ready"
    Jan 18 21:05:26.730: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.815943ms
    Jan 18 21:05:26.730: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:05:28.736: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009741809s
    Jan 18 21:05:28.736: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 21:05:28.736: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod1:[80]] 01/18/23 21:05:28.739
    Jan 18 21:05:28.752: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/18/23 21:05:28.752
    Jan 18 21:05:28.752: INFO: Creating new exec pod
    Jan 18 21:05:28.764: INFO: Waiting up to 5m0s for pod "execpodxvk6h" in namespace "services-2103" to be "running"
    Jan 18 21:05:28.768: INFO: Pod "execpodxvk6h": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56744ms
    Jan 18 21:05:30.772: INFO: Pod "execpodxvk6h": Phase="Running", Reason="", readiness=true. Elapsed: 2.007901289s
    Jan 18 21:05:30.772: INFO: Pod "execpodxvk6h" satisfied condition "running"
    Jan 18 21:05:31.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 21:05:31.943: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:31.943: INFO: stdout: ""
    Jan 18 21:05:31.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
    Jan 18 21:05:32.119: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:32.119: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-2103 01/18/23 21:05:32.119
    Jan 18 21:05:32.131: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2103" to be "running and ready"
    Jan 18 21:05:32.135: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.961444ms
    Jan 18 21:05:32.135: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:05:34.139: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008427594s
    Jan 18 21:05:34.139: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:05:36.139: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008339005s
    Jan 18 21:05:36.139: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 21:05:36.139: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 21:05:36.144
    Jan 18 21:05:36.162: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 21:05:36.162
    Jan 18 21:05:37.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 21:05:37.348: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:37.348: INFO: stdout: ""
    Jan 18 21:05:37.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
    Jan 18 21:05:37.579: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:37.579: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2103 01/18/23 21:05:37.579
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[pod2:[80]] 01/18/23 21:05:37.607
    Jan 18 21:05:38.624: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/18/23 21:05:38.624
    Jan 18 21:05:39.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan 18 21:05:39.808: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:39.808: INFO: stdout: ""
    Jan 18 21:05:39.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2103 exec execpodxvk6h -- /bin/sh -x -c nc -v -z -w 2 10.152.183.93 80'
    Jan 18 21:05:39.972: INFO: stderr: "+ nc -v -z -w 2 10.152.183.93 80\nConnection to 10.152.183.93 80 port [tcp/http] succeeded!\n"
    Jan 18 21:05:39.972: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-2103 01/18/23 21:05:39.972
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2103 to expose endpoints map[] 01/18/23 21:05:40.042
    Jan 18 21:05:40.062: INFO: successfully validated that service endpoint-test2 in namespace services-2103 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:05:40.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2103" for this suite. 01/18/23 21:05:40.122
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:05:40.142
Jan 18 21:05:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:05:40.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:05:40.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:05:40.185
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:05:40.221
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:05:40.606
STEP: Deploying the webhook pod 01/18/23 21:05:40.627
STEP: Wait for the deployment to be ready 01/18/23 21:05:40.649
Jan 18 21:05:40.661: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:05:42.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:05:44.681
STEP: Verifying the service has paired with the endpoint 01/18/23 21:05:44.702
Jan 18 21:05:45.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 21:05:45.707
STEP: create a pod that should be denied by the webhook 01/18/23 21:05:46.767
STEP: create a pod that causes the webhook to hang 01/18/23 21:05:47.793
STEP: create a configmap that should be denied by the webhook 01/18/23 21:05:57.802
STEP: create a configmap that should be admitted by the webhook 01/18/23 21:05:58.847
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 21:05:59.896
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 21:05:59.908
STEP: create a namespace that bypass the webhook 01/18/23 21:05:59.915
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 21:05:59.931
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:00.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4495" for this suite. 01/18/23 21:06:01.093
STEP: Destroying namespace "webhook-4495-markers" for this suite. 01/18/23 21:06:01.112
------------------------------
• [SLOW TEST] [20.986 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:05:40.142
    Jan 18 21:05:40.142: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:05:40.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:05:40.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:05:40.185
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:05:40.221
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:05:40.606
    STEP: Deploying the webhook pod 01/18/23 21:05:40.627
    STEP: Wait for the deployment to be ready 01/18/23 21:05:40.649
    Jan 18 21:05:40.661: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:05:42.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 5, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:05:44.681
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:05:44.702
    Jan 18 21:05:45.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 21:05:45.707
    STEP: create a pod that should be denied by the webhook 01/18/23 21:05:46.767
    STEP: create a pod that causes the webhook to hang 01/18/23 21:05:47.793
    STEP: create a configmap that should be denied by the webhook 01/18/23 21:05:57.802
    STEP: create a configmap that should be admitted by the webhook 01/18/23 21:05:58.847
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 21:05:59.896
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 21:05:59.908
    STEP: create a namespace that bypass the webhook 01/18/23 21:05:59.915
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 21:05:59.931
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:00.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4495" for this suite. 01/18/23 21:06:01.093
    STEP: Destroying namespace "webhook-4495-markers" for this suite. 01/18/23 21:06:01.112
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:01.129
Jan 18 21:06:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:06:01.131
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:01.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:01.172
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/18/23 21:06:01.178
Jan 18 21:06:01.178: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8741 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/18/23 21:06:01.272
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:01.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8741" for this suite. 01/18/23 21:06:01.297
------------------------------
• [0.182 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:01.129
    Jan 18 21:06:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:06:01.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:01.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:01.172
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/18/23 21:06:01.178
    Jan 18 21:06:01.178: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8741 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/18/23 21:06:01.272
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:01.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8741" for this suite. 01/18/23 21:06:01.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:01.313
Jan 18 21:06:01.313: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:06:01.314
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:01.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:01.361
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:06:01.364
Jan 18 21:06:01.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 21:06:01.483: INFO: stderr: ""
Jan 18 21:06:01.483: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/18/23 21:06:01.483
Jan 18 21:06:01.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan 18 21:06:02.720: INFO: stderr: ""
Jan 18 21:06:02.720: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:06:02.72
Jan 18 21:06:02.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 delete pods e2e-test-httpd-pod'
Jan 18 21:06:05.478: INFO: stderr: ""
Jan 18 21:06:05.478: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5720" for this suite. 01/18/23 21:06:05.489
------------------------------
• [4.194 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:01.313
    Jan 18 21:06:01.313: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:06:01.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:01.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:01.361
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:06:01.364
    Jan 18 21:06:01.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 21:06:01.483: INFO: stderr: ""
    Jan 18 21:06:01.483: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/18/23 21:06:01.483
    Jan 18 21:06:01.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan 18 21:06:02.720: INFO: stderr: ""
    Jan 18 21:06:02.720: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:06:02.72
    Jan 18 21:06:02.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-5720 delete pods e2e-test-httpd-pod'
    Jan 18 21:06:05.478: INFO: stderr: ""
    Jan 18 21:06:05.478: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:05.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5720" for this suite. 01/18/23 21:06:05.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:05.508
Jan 18 21:06:05.508: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:06:05.508
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:05.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:05.557
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 18 21:06:05.592: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 21:06:10.597: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:06:10.597
Jan 18 21:06:10.597: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 21:06:12.603: INFO: Creating deployment "test-rollover-deployment"
Jan 18 21:06:12.623: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 21:06:14.633: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 21:06:14.643: INFO: Ensure that both replica sets have 1 created replica
Jan 18 21:06:14.653: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 21:06:14.676: INFO: Updating deployment test-rollover-deployment
Jan 18 21:06:14.676: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 21:06:16.685: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 21:06:16.694: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 21:06:16.705: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 21:06:16.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:18.715: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 21:06:18.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:20.716: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 21:06:20.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:22.714: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 21:06:22.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:24.715: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 21:06:24.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:26.724: INFO: 
Jan 18 21:06:26.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:06:28.716: INFO: 
Jan 18 21:06:28.716: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:06:28.727: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9921  32685b8b-be55-4d09-9011-a164cf8829a7 3587 2 2023-01-18 21:06:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:06:12 +0000 UTC,LastTransitionTime:2023-01-18 21:06:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-18 21:06:26 +0000 UTC,LastTransitionTime:2023-01-18 21:06:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 21:06:28.732: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9921  314a8202-bad2-417c-a6dc-ef1a0e300e0c 3577 2 2023-01-18 21:06:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce6d7 0xc0035ce6d8}] [] [{kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:06:28.732: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 21:06:28.732: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9921  bc0aa6d8-c3ef-4b21-9bcf-243ecd7b0eb7 3586 2 2023-01-18 21:06:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce7df 0xc0035ce7f0}] [] [{e2e.test Update apps/v1 2023-01-18 21:06:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035ce8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:06:28.732: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9921  6753f7fc-51a8-450a-ac5e-112ca82941fe 3536 2 2023-01-18 21:06:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce5b7 0xc0035ce5b8}] [] [{kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:06:28.737: INFO: Pod "test-rollover-deployment-6c6df9974f-nqcxr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-nqcxr test-rollover-deployment-6c6df9974f- deployment-9921  f54c0ffe-9f82-4835-a289-0f2fbeb1ddc6 3554 0 2023-01-18 21:06:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:fbd223ec144171ac542719c01007357a770091c0265f80df4836e9d8b83a6ca7 cni.projectcalico.org/podIP:10.1.192.55/32 cni.projectcalico.org/podIPs:10.1.192.55/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 314a8202-bad2-417c-a6dc-ef1a0e300e0c 0xc003669087 0xc003669088}] [] [{kubelite Update v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314a8202-bad2-417c-a6dc-ef1a0e300e0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:06:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:06:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z6ccq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6ccq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.55,StartTime:2023-01-18 21:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:06:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9909b5b1823a9cbb0e7dbff4cebc1046f38c8dffc9ab2960b50c577d708a0033,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:28.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9921" for this suite. 01/18/23 21:06:28.742
------------------------------
• [SLOW TEST] [23.248 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:05.508
    Jan 18 21:06:05.508: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:06:05.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:05.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:05.557
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 18 21:06:05.592: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 18 21:06:10.597: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:06:10.597
    Jan 18 21:06:10.597: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 18 21:06:12.603: INFO: Creating deployment "test-rollover-deployment"
    Jan 18 21:06:12.623: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 18 21:06:14.633: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 18 21:06:14.643: INFO: Ensure that both replica sets have 1 created replica
    Jan 18 21:06:14.653: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 18 21:06:14.676: INFO: Updating deployment test-rollover-deployment
    Jan 18 21:06:14.676: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 18 21:06:16.685: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 18 21:06:16.694: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 18 21:06:16.705: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 21:06:16.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:18.715: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 21:06:18.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:20.716: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 21:06:20.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:22.714: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 21:06:22.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:24.715: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 21:06:24.715: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:26.724: INFO: 
    Jan 18 21:06:26.724: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 6, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 6, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:06:28.716: INFO: 
    Jan 18 21:06:28.716: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:06:28.727: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9921  32685b8b-be55-4d09-9011-a164cf8829a7 3587 2 2023-01-18 21:06:12 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:06:12 +0000 UTC,LastTransitionTime:2023-01-18 21:06:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-18 21:06:26 +0000 UTC,LastTransitionTime:2023-01-18 21:06:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 21:06:28.732: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9921  314a8202-bad2-417c-a6dc-ef1a0e300e0c 3577 2 2023-01-18 21:06:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce6d7 0xc0035ce6d8}] [] [{kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:06:28.732: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 18 21:06:28.732: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9921  bc0aa6d8-c3ef-4b21-9bcf-243ecd7b0eb7 3586 2 2023-01-18 21:06:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce7df 0xc0035ce7f0}] [] [{e2e.test Update apps/v1 2023-01-18 21:06:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-01-18 21:06:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035ce8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:06:28.732: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9921  6753f7fc-51a8-450a-ac5e-112ca82941fe 3536 2 2023-01-18 21:06:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 32685b8b-be55-4d09-9011-a164cf8829a7 0xc0035ce5b7 0xc0035ce5b8}] [] [{kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"32685b8b-be55-4d09-9011-a164cf8829a7\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035ce678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:06:28.737: INFO: Pod "test-rollover-deployment-6c6df9974f-nqcxr" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-nqcxr test-rollover-deployment-6c6df9974f- deployment-9921  f54c0ffe-9f82-4835-a289-0f2fbeb1ddc6 3554 0 2023-01-18 21:06:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:fbd223ec144171ac542719c01007357a770091c0265f80df4836e9d8b83a6ca7 cni.projectcalico.org/podIP:10.1.192.55/32 cni.projectcalico.org/podIPs:10.1.192.55/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 314a8202-bad2-417c-a6dc-ef1a0e300e0c 0xc003669087 0xc003669088}] [] [{kubelite Update v1 2023-01-18 21:06:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314a8202-bad2-417c-a6dc-ef1a0e300e0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:06:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:06:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.55\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z6ccq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6ccq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:06:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.55,StartTime:2023-01-18 21:06:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:06:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://9909b5b1823a9cbb0e7dbff4cebc1046f38c8dffc9ab2960b50c577d708a0033,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:28.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9921" for this suite. 01/18/23 21:06:28.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:28.758
Jan 18 21:06:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:06:28.76
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:28.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:28.795
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:06:28.799
Jan 18 21:06:28.820: INFO: Waiting up to 5m0s for pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088" in namespace "emptydir-435" to be "Succeeded or Failed"
Jan 18 21:06:28.824: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648846ms
Jan 18 21:06:30.829: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009441007s
Jan 18 21:06:32.830: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010328661s
STEP: Saw pod success 01/18/23 21:06:32.83
Jan 18 21:06:32.831: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088" satisfied condition "Succeeded or Failed"
Jan 18 21:06:32.835: INFO: Trying to get logs from node test-vm-2 pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 container test-container: <nil>
STEP: delete the pod 01/18/23 21:06:32.854
Jan 18 21:06:32.890: INFO: Waiting for pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 to disappear
Jan 18 21:06:32.897: INFO: Pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:32.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-435" for this suite. 01/18/23 21:06:32.902
------------------------------
• [4.162 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:28.758
    Jan 18 21:06:28.758: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:06:28.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:28.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:28.795
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:06:28.799
    Jan 18 21:06:28.820: INFO: Waiting up to 5m0s for pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088" in namespace "emptydir-435" to be "Succeeded or Failed"
    Jan 18 21:06:28.824: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648846ms
    Jan 18 21:06:30.829: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009441007s
    Jan 18 21:06:32.830: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010328661s
    STEP: Saw pod success 01/18/23 21:06:32.83
    Jan 18 21:06:32.831: INFO: Pod "pod-3fa9a996-8007-4bc6-a929-4500abee5088" satisfied condition "Succeeded or Failed"
    Jan 18 21:06:32.835: INFO: Trying to get logs from node test-vm-2 pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:06:32.854
    Jan 18 21:06:32.890: INFO: Waiting for pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 to disappear
    Jan 18 21:06:32.897: INFO: Pod pod-3fa9a996-8007-4bc6-a929-4500abee5088 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:32.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-435" for this suite. 01/18/23 21:06:32.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:32.921
Jan 18 21:06:32.921: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:06:32.922
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:32.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:32.959
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 18 21:06:32.962: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:39.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8867" for this suite. 01/18/23 21:06:39.432
------------------------------
• [SLOW TEST] [6.525 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:32.921
    Jan 18 21:06:32.921: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:06:32.922
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:32.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:32.959
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 18 21:06:32.962: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:39.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8867" for this suite. 01/18/23 21:06:39.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:39.446
Jan 18 21:06:39.446: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:06:39.449
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:39.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:39.488
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/18/23 21:06:39.491
Jan 18 21:06:39.509: INFO: Waiting up to 5m0s for pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf" in namespace "downward-api-3349" to be "running and ready"
Jan 18 21:06:39.517: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261597ms
Jan 18 21:06:39.517: INFO: The phase of Pod labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:06:41.524: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014900743s
Jan 18 21:06:41.524: INFO: The phase of Pod labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf is Running (Ready = true)
Jan 18 21:06:41.524: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf" satisfied condition "running and ready"
Jan 18 21:06:42.069: INFO: Successfully updated pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:46.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3349" for this suite. 01/18/23 21:06:46.106
------------------------------
• [SLOW TEST] [6.673 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:39.446
    Jan 18 21:06:39.446: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:06:39.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:39.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:39.488
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/18/23 21:06:39.491
    Jan 18 21:06:39.509: INFO: Waiting up to 5m0s for pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf" in namespace "downward-api-3349" to be "running and ready"
    Jan 18 21:06:39.517: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261597ms
    Jan 18 21:06:39.517: INFO: The phase of Pod labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:06:41.524: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014900743s
    Jan 18 21:06:41.524: INFO: The phase of Pod labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf is Running (Ready = true)
    Jan 18 21:06:41.524: INFO: Pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf" satisfied condition "running and ready"
    Jan 18 21:06:42.069: INFO: Successfully updated pod "labelsupdateb08db422-0afd-4cd4-96c1-9ec0bdcb7daf"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:46.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3349" for this suite. 01/18/23 21:06:46.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:46.12
Jan 18 21:06:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 21:06:46.122
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:46.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:46.192
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/18/23 21:06:46.194
STEP: Creating hostNetwork=false pod 01/18/23 21:06:46.195
Jan 18 21:06:46.221: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5632" to be "running and ready"
Jan 18 21:06:46.233: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.227543ms
Jan 18 21:06:46.234: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:06:48.240: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018328696s
Jan 18 21:06:48.240: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:06:50.239: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018153864s
Jan 18 21:06:50.239: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 18 21:06:50.239: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/18/23 21:06:50.243
Jan 18 21:06:50.256: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5632" to be "running and ready"
Jan 18 21:06:50.260: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286152ms
Jan 18 21:06:50.260: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:06:52.267: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010940989s
Jan 18 21:06:52.267: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 18 21:06:52.267: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/18/23 21:06:52.271
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 21:06:52.271
Jan 18 21:06:52.271: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.272: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.272: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 21:06:52.412: INFO: Exec stderr: ""
Jan 18 21:06:52.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.412: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.413: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.413: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 21:06:52.539: INFO: Exec stderr: ""
Jan 18 21:06:52.539: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.539: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.540: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.540: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 21:06:52.624: INFO: Exec stderr: ""
Jan 18 21:06:52.625: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.625: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.625: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 21:06:52.723: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 21:06:52.723
Jan 18 21:06:52.724: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.724: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.724: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 21:06:52.818: INFO: Exec stderr: ""
Jan 18 21:06:52.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.818: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.819: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.819: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 21:06:52.913: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 21:06:52.913
Jan 18 21:06:52.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:52.914: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:52.914: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 21:06:53.003: INFO: Exec stderr: ""
Jan 18 21:06:53.003: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:53.004: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:53.004: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:53.004: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 21:06:53.092: INFO: Exec stderr: ""
Jan 18 21:06:53.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:53.093: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:53.093: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 21:06:53.189: INFO: Exec stderr: ""
Jan 18 21:06:53.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:06:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:06:53.189: INFO: ExecWithOptions: Clientset creation
Jan 18 21:06:53.189: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 21:06:53.284: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan 18 21:06:53.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5632" for this suite. 01/18/23 21:06:53.29
------------------------------
• [SLOW TEST] [7.183 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:46.12
    Jan 18 21:06:46.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 21:06:46.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:46.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:46.192
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/18/23 21:06:46.194
    STEP: Creating hostNetwork=false pod 01/18/23 21:06:46.195
    Jan 18 21:06:46.221: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-5632" to be "running and ready"
    Jan 18 21:06:46.233: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.227543ms
    Jan 18 21:06:46.234: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:06:48.240: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018328696s
    Jan 18 21:06:48.240: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:06:50.239: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018153864s
    Jan 18 21:06:50.239: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 18 21:06:50.239: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/18/23 21:06:50.243
    Jan 18 21:06:50.256: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-5632" to be "running and ready"
    Jan 18 21:06:50.260: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.286152ms
    Jan 18 21:06:50.260: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:06:52.267: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010940989s
    Jan 18 21:06:52.267: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 18 21:06:52.267: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/18/23 21:06:52.271
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 21:06:52.271
    Jan 18 21:06:52.271: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.272: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.272: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 21:06:52.412: INFO: Exec stderr: ""
    Jan 18 21:06:52.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.412: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.413: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.413: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 21:06:52.539: INFO: Exec stderr: ""
    Jan 18 21:06:52.539: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.539: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.540: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.540: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 21:06:52.624: INFO: Exec stderr: ""
    Jan 18 21:06:52.625: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.625: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.625: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.625: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 21:06:52.723: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 21:06:52.723
    Jan 18 21:06:52.724: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.724: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.724: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 21:06:52.818: INFO: Exec stderr: ""
    Jan 18 21:06:52.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.818: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.819: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.819: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 21:06:52.913: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 21:06:52.913
    Jan 18 21:06:52.913: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:52.914: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:52.914: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 21:06:53.003: INFO: Exec stderr: ""
    Jan 18 21:06:53.003: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:53.004: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:53.004: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:53.004: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 21:06:53.092: INFO: Exec stderr: ""
    Jan 18 21:06:53.093: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:53.093: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:53.093: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:53.093: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 21:06:53.189: INFO: Exec stderr: ""
    Jan 18 21:06:53.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5632 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:06:53.189: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:06:53.189: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:06:53.189: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5632/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 21:06:53.284: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:06:53.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-5632" for this suite. 01/18/23 21:06:53.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:06:53.305
Jan 18 21:06:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 21:06:53.306
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:53.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:53.343
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/18/23 21:06:53.351
STEP: delete the rc 01/18/23 21:06:58.375
STEP: wait for the rc to be deleted 01/18/23 21:06:58.481
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 21:07:03.561
STEP: Gathering metrics 01/18/23 21:07:33.592
W0118 21:07:33.601856      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 21:07:33.601: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 21:07:33.601: INFO: Deleting pod "simpletest.rc-q6q58" in namespace "gc-9938"
Jan 18 21:07:33.655: INFO: Deleting pod "simpletest.rc-t7dht" in namespace "gc-9938"
Jan 18 21:07:33.744: INFO: Deleting pod "simpletest.rc-knw5q" in namespace "gc-9938"
Jan 18 21:07:33.853: INFO: Deleting pod "simpletest.rc-8dlwq" in namespace "gc-9938"
Jan 18 21:07:33.913: INFO: Deleting pod "simpletest.rc-b7t4v" in namespace "gc-9938"
Jan 18 21:07:33.964: INFO: Deleting pod "simpletest.rc-rphp6" in namespace "gc-9938"
Jan 18 21:07:34.048: INFO: Deleting pod "simpletest.rc-lgv6l" in namespace "gc-9938"
Jan 18 21:07:34.410: INFO: Deleting pod "simpletest.rc-rt254" in namespace "gc-9938"
Jan 18 21:07:34.654: INFO: Deleting pod "simpletest.rc-tn726" in namespace "gc-9938"
Jan 18 21:07:34.735: INFO: Deleting pod "simpletest.rc-zx68c" in namespace "gc-9938"
Jan 18 21:07:34.798: INFO: Deleting pod "simpletest.rc-brhz4" in namespace "gc-9938"
Jan 18 21:07:35.044: INFO: Deleting pod "simpletest.rc-bpjjc" in namespace "gc-9938"
Jan 18 21:07:35.199: INFO: Deleting pod "simpletest.rc-9wzk7" in namespace "gc-9938"
Jan 18 21:07:35.286: INFO: Deleting pod "simpletest.rc-mcspj" in namespace "gc-9938"
Jan 18 21:07:35.432: INFO: Deleting pod "simpletest.rc-kscxv" in namespace "gc-9938"
Jan 18 21:07:35.664: INFO: Deleting pod "simpletest.rc-9k7hb" in namespace "gc-9938"
Jan 18 21:07:36.152: INFO: Deleting pod "simpletest.rc-2wjgp" in namespace "gc-9938"
Jan 18 21:07:36.475: INFO: Deleting pod "simpletest.rc-rqxtz" in namespace "gc-9938"
Jan 18 21:07:36.806: INFO: Deleting pod "simpletest.rc-b4577" in namespace "gc-9938"
Jan 18 21:07:36.997: INFO: Deleting pod "simpletest.rc-lcbbw" in namespace "gc-9938"
Jan 18 21:07:37.251: INFO: Deleting pod "simpletest.rc-dxt4b" in namespace "gc-9938"
Jan 18 21:07:37.436: INFO: Deleting pod "simpletest.rc-7mgj5" in namespace "gc-9938"
Jan 18 21:07:37.652: INFO: Deleting pod "simpletest.rc-8jgz5" in namespace "gc-9938"
Jan 18 21:07:37.803: INFO: Deleting pod "simpletest.rc-hwpm6" in namespace "gc-9938"
Jan 18 21:07:38.133: INFO: Deleting pod "simpletest.rc-4bwmj" in namespace "gc-9938"
Jan 18 21:07:38.296: INFO: Deleting pod "simpletest.rc-9fz28" in namespace "gc-9938"
Jan 18 21:07:38.691: INFO: Deleting pod "simpletest.rc-jqnqr" in namespace "gc-9938"
Jan 18 21:07:38.941: INFO: Deleting pod "simpletest.rc-zf997" in namespace "gc-9938"
Jan 18 21:07:39.146: INFO: Deleting pod "simpletest.rc-82c55" in namespace "gc-9938"
Jan 18 21:07:39.332: INFO: Deleting pod "simpletest.rc-wbw7j" in namespace "gc-9938"
Jan 18 21:07:39.566: INFO: Deleting pod "simpletest.rc-n7gqb" in namespace "gc-9938"
Jan 18 21:07:39.755: INFO: Deleting pod "simpletest.rc-pz8ff" in namespace "gc-9938"
Jan 18 21:07:40.046: INFO: Deleting pod "simpletest.rc-q9fkr" in namespace "gc-9938"
Jan 18 21:07:40.307: INFO: Deleting pod "simpletest.rc-st2cq" in namespace "gc-9938"
Jan 18 21:07:40.477: INFO: Deleting pod "simpletest.rc-xd9zf" in namespace "gc-9938"
Jan 18 21:07:40.869: INFO: Deleting pod "simpletest.rc-7q2bc" in namespace "gc-9938"
Jan 18 21:07:41.080: INFO: Deleting pod "simpletest.rc-qft5h" in namespace "gc-9938"
Jan 18 21:07:41.301: INFO: Deleting pod "simpletest.rc-k8mlx" in namespace "gc-9938"
Jan 18 21:07:41.460: INFO: Deleting pod "simpletest.rc-cgwl4" in namespace "gc-9938"
Jan 18 21:07:41.627: INFO: Deleting pod "simpletest.rc-xk4lg" in namespace "gc-9938"
Jan 18 21:07:41.762: INFO: Deleting pod "simpletest.rc-4mvpd" in namespace "gc-9938"
Jan 18 21:07:41.836: INFO: Deleting pod "simpletest.rc-gc8d5" in namespace "gc-9938"
Jan 18 21:07:42.110: INFO: Deleting pod "simpletest.rc-njt8n" in namespace "gc-9938"
Jan 18 21:07:42.229: INFO: Deleting pod "simpletest.rc-k4lpb" in namespace "gc-9938"
Jan 18 21:07:42.327: INFO: Deleting pod "simpletest.rc-lsjhd" in namespace "gc-9938"
Jan 18 21:07:42.569: INFO: Deleting pod "simpletest.rc-vjz89" in namespace "gc-9938"
Jan 18 21:07:42.809: INFO: Deleting pod "simpletest.rc-b9q2h" in namespace "gc-9938"
Jan 18 21:07:43.045: INFO: Deleting pod "simpletest.rc-446tr" in namespace "gc-9938"
Jan 18 21:07:43.251: INFO: Deleting pod "simpletest.rc-mqq7d" in namespace "gc-9938"
Jan 18 21:07:43.396: INFO: Deleting pod "simpletest.rc-mtwg6" in namespace "gc-9938"
Jan 18 21:07:43.630: INFO: Deleting pod "simpletest.rc-5l5r2" in namespace "gc-9938"
Jan 18 21:07:43.801: INFO: Deleting pod "simpletest.rc-9bfqc" in namespace "gc-9938"
Jan 18 21:07:43.970: INFO: Deleting pod "simpletest.rc-wh9lw" in namespace "gc-9938"
Jan 18 21:07:44.077: INFO: Deleting pod "simpletest.rc-gzbnh" in namespace "gc-9938"
Jan 18 21:07:44.221: INFO: Deleting pod "simpletest.rc-wnvwh" in namespace "gc-9938"
Jan 18 21:07:44.494: INFO: Deleting pod "simpletest.rc-zvdjp" in namespace "gc-9938"
Jan 18 21:07:44.697: INFO: Deleting pod "simpletest.rc-n6xgg" in namespace "gc-9938"
Jan 18 21:07:44.947: INFO: Deleting pod "simpletest.rc-4m775" in namespace "gc-9938"
Jan 18 21:07:45.180: INFO: Deleting pod "simpletest.rc-gw2vr" in namespace "gc-9938"
Jan 18 21:07:45.318: INFO: Deleting pod "simpletest.rc-2tc4f" in namespace "gc-9938"
Jan 18 21:07:45.593: INFO: Deleting pod "simpletest.rc-xll8g" in namespace "gc-9938"
Jan 18 21:07:45.800: INFO: Deleting pod "simpletest.rc-prphb" in namespace "gc-9938"
Jan 18 21:07:45.898: INFO: Deleting pod "simpletest.rc-zwh7m" in namespace "gc-9938"
Jan 18 21:07:45.989: INFO: Deleting pod "simpletest.rc-ddgws" in namespace "gc-9938"
Jan 18 21:07:46.134: INFO: Deleting pod "simpletest.rc-dk9b2" in namespace "gc-9938"
Jan 18 21:07:46.286: INFO: Deleting pod "simpletest.rc-cwbdt" in namespace "gc-9938"
Jan 18 21:07:46.374: INFO: Deleting pod "simpletest.rc-7xkmt" in namespace "gc-9938"
Jan 18 21:07:46.537: INFO: Deleting pod "simpletest.rc-m6qnr" in namespace "gc-9938"
Jan 18 21:07:46.734: INFO: Deleting pod "simpletest.rc-9npg5" in namespace "gc-9938"
Jan 18 21:07:46.914: INFO: Deleting pod "simpletest.rc-qvm5l" in namespace "gc-9938"
Jan 18 21:07:47.181: INFO: Deleting pod "simpletest.rc-ssvns" in namespace "gc-9938"
Jan 18 21:07:47.388: INFO: Deleting pod "simpletest.rc-wblpd" in namespace "gc-9938"
Jan 18 21:07:47.527: INFO: Deleting pod "simpletest.rc-txv98" in namespace "gc-9938"
Jan 18 21:07:47.676: INFO: Deleting pod "simpletest.rc-2ds6t" in namespace "gc-9938"
Jan 18 21:07:47.883: INFO: Deleting pod "simpletest.rc-v5rdx" in namespace "gc-9938"
Jan 18 21:07:48.043: INFO: Deleting pod "simpletest.rc-7sppx" in namespace "gc-9938"
Jan 18 21:07:48.319: INFO: Deleting pod "simpletest.rc-f899v" in namespace "gc-9938"
Jan 18 21:07:48.505: INFO: Deleting pod "simpletest.rc-g9mmd" in namespace "gc-9938"
Jan 18 21:07:48.763: INFO: Deleting pod "simpletest.rc-496bh" in namespace "gc-9938"
Jan 18 21:07:48.947: INFO: Deleting pod "simpletest.rc-74p74" in namespace "gc-9938"
Jan 18 21:07:49.125: INFO: Deleting pod "simpletest.rc-skvrd" in namespace "gc-9938"
Jan 18 21:07:49.320: INFO: Deleting pod "simpletest.rc-bqq2w" in namespace "gc-9938"
Jan 18 21:07:49.468: INFO: Deleting pod "simpletest.rc-gqd29" in namespace "gc-9938"
Jan 18 21:07:49.677: INFO: Deleting pod "simpletest.rc-rcxjt" in namespace "gc-9938"
Jan 18 21:07:49.874: INFO: Deleting pod "simpletest.rc-xtwmm" in namespace "gc-9938"
Jan 18 21:07:50.000: INFO: Deleting pod "simpletest.rc-dpsm4" in namespace "gc-9938"
Jan 18 21:07:50.167: INFO: Deleting pod "simpletest.rc-bvgt4" in namespace "gc-9938"
Jan 18 21:07:50.443: INFO: Deleting pod "simpletest.rc-4mhnw" in namespace "gc-9938"
Jan 18 21:07:50.615: INFO: Deleting pod "simpletest.rc-sqlrv" in namespace "gc-9938"
Jan 18 21:07:50.695: INFO: Deleting pod "simpletest.rc-mw7fc" in namespace "gc-9938"
Jan 18 21:07:50.814: INFO: Deleting pod "simpletest.rc-mnnkd" in namespace "gc-9938"
Jan 18 21:07:50.990: INFO: Deleting pod "simpletest.rc-7qrjb" in namespace "gc-9938"
Jan 18 21:07:51.134: INFO: Deleting pod "simpletest.rc-2xwg6" in namespace "gc-9938"
Jan 18 21:07:51.356: INFO: Deleting pod "simpletest.rc-8wg5d" in namespace "gc-9938"
Jan 18 21:07:51.540: INFO: Deleting pod "simpletest.rc-lwbtf" in namespace "gc-9938"
Jan 18 21:07:51.719: INFO: Deleting pod "simpletest.rc-x7r78" in namespace "gc-9938"
Jan 18 21:07:51.910: INFO: Deleting pod "simpletest.rc-4pgcx" in namespace "gc-9938"
Jan 18 21:07:52.054: INFO: Deleting pod "simpletest.rc-k78dh" in namespace "gc-9938"
Jan 18 21:07:52.192: INFO: Deleting pod "simpletest.rc-lrjq8" in namespace "gc-9938"
Jan 18 21:07:52.377: INFO: Deleting pod "simpletest.rc-nkvq4" in namespace "gc-9938"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 21:07:52.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9938" for this suite. 01/18/23 21:07:52.532
------------------------------
• [SLOW TEST] [59.333 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:06:53.305
    Jan 18 21:06:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 21:06:53.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:06:53.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:06:53.343
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/18/23 21:06:53.351
    STEP: delete the rc 01/18/23 21:06:58.375
    STEP: wait for the rc to be deleted 01/18/23 21:06:58.481
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 21:07:03.561
    STEP: Gathering metrics 01/18/23 21:07:33.592
    W0118 21:07:33.601856      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 21:07:33.601: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 21:07:33.601: INFO: Deleting pod "simpletest.rc-q6q58" in namespace "gc-9938"
    Jan 18 21:07:33.655: INFO: Deleting pod "simpletest.rc-t7dht" in namespace "gc-9938"
    Jan 18 21:07:33.744: INFO: Deleting pod "simpletest.rc-knw5q" in namespace "gc-9938"
    Jan 18 21:07:33.853: INFO: Deleting pod "simpletest.rc-8dlwq" in namespace "gc-9938"
    Jan 18 21:07:33.913: INFO: Deleting pod "simpletest.rc-b7t4v" in namespace "gc-9938"
    Jan 18 21:07:33.964: INFO: Deleting pod "simpletest.rc-rphp6" in namespace "gc-9938"
    Jan 18 21:07:34.048: INFO: Deleting pod "simpletest.rc-lgv6l" in namespace "gc-9938"
    Jan 18 21:07:34.410: INFO: Deleting pod "simpletest.rc-rt254" in namespace "gc-9938"
    Jan 18 21:07:34.654: INFO: Deleting pod "simpletest.rc-tn726" in namespace "gc-9938"
    Jan 18 21:07:34.735: INFO: Deleting pod "simpletest.rc-zx68c" in namespace "gc-9938"
    Jan 18 21:07:34.798: INFO: Deleting pod "simpletest.rc-brhz4" in namespace "gc-9938"
    Jan 18 21:07:35.044: INFO: Deleting pod "simpletest.rc-bpjjc" in namespace "gc-9938"
    Jan 18 21:07:35.199: INFO: Deleting pod "simpletest.rc-9wzk7" in namespace "gc-9938"
    Jan 18 21:07:35.286: INFO: Deleting pod "simpletest.rc-mcspj" in namespace "gc-9938"
    Jan 18 21:07:35.432: INFO: Deleting pod "simpletest.rc-kscxv" in namespace "gc-9938"
    Jan 18 21:07:35.664: INFO: Deleting pod "simpletest.rc-9k7hb" in namespace "gc-9938"
    Jan 18 21:07:36.152: INFO: Deleting pod "simpletest.rc-2wjgp" in namespace "gc-9938"
    Jan 18 21:07:36.475: INFO: Deleting pod "simpletest.rc-rqxtz" in namespace "gc-9938"
    Jan 18 21:07:36.806: INFO: Deleting pod "simpletest.rc-b4577" in namespace "gc-9938"
    Jan 18 21:07:36.997: INFO: Deleting pod "simpletest.rc-lcbbw" in namespace "gc-9938"
    Jan 18 21:07:37.251: INFO: Deleting pod "simpletest.rc-dxt4b" in namespace "gc-9938"
    Jan 18 21:07:37.436: INFO: Deleting pod "simpletest.rc-7mgj5" in namespace "gc-9938"
    Jan 18 21:07:37.652: INFO: Deleting pod "simpletest.rc-8jgz5" in namespace "gc-9938"
    Jan 18 21:07:37.803: INFO: Deleting pod "simpletest.rc-hwpm6" in namespace "gc-9938"
    Jan 18 21:07:38.133: INFO: Deleting pod "simpletest.rc-4bwmj" in namespace "gc-9938"
    Jan 18 21:07:38.296: INFO: Deleting pod "simpletest.rc-9fz28" in namespace "gc-9938"
    Jan 18 21:07:38.691: INFO: Deleting pod "simpletest.rc-jqnqr" in namespace "gc-9938"
    Jan 18 21:07:38.941: INFO: Deleting pod "simpletest.rc-zf997" in namespace "gc-9938"
    Jan 18 21:07:39.146: INFO: Deleting pod "simpletest.rc-82c55" in namespace "gc-9938"
    Jan 18 21:07:39.332: INFO: Deleting pod "simpletest.rc-wbw7j" in namespace "gc-9938"
    Jan 18 21:07:39.566: INFO: Deleting pod "simpletest.rc-n7gqb" in namespace "gc-9938"
    Jan 18 21:07:39.755: INFO: Deleting pod "simpletest.rc-pz8ff" in namespace "gc-9938"
    Jan 18 21:07:40.046: INFO: Deleting pod "simpletest.rc-q9fkr" in namespace "gc-9938"
    Jan 18 21:07:40.307: INFO: Deleting pod "simpletest.rc-st2cq" in namespace "gc-9938"
    Jan 18 21:07:40.477: INFO: Deleting pod "simpletest.rc-xd9zf" in namespace "gc-9938"
    Jan 18 21:07:40.869: INFO: Deleting pod "simpletest.rc-7q2bc" in namespace "gc-9938"
    Jan 18 21:07:41.080: INFO: Deleting pod "simpletest.rc-qft5h" in namespace "gc-9938"
    Jan 18 21:07:41.301: INFO: Deleting pod "simpletest.rc-k8mlx" in namespace "gc-9938"
    Jan 18 21:07:41.460: INFO: Deleting pod "simpletest.rc-cgwl4" in namespace "gc-9938"
    Jan 18 21:07:41.627: INFO: Deleting pod "simpletest.rc-xk4lg" in namespace "gc-9938"
    Jan 18 21:07:41.762: INFO: Deleting pod "simpletest.rc-4mvpd" in namespace "gc-9938"
    Jan 18 21:07:41.836: INFO: Deleting pod "simpletest.rc-gc8d5" in namespace "gc-9938"
    Jan 18 21:07:42.110: INFO: Deleting pod "simpletest.rc-njt8n" in namespace "gc-9938"
    Jan 18 21:07:42.229: INFO: Deleting pod "simpletest.rc-k4lpb" in namespace "gc-9938"
    Jan 18 21:07:42.327: INFO: Deleting pod "simpletest.rc-lsjhd" in namespace "gc-9938"
    Jan 18 21:07:42.569: INFO: Deleting pod "simpletest.rc-vjz89" in namespace "gc-9938"
    Jan 18 21:07:42.809: INFO: Deleting pod "simpletest.rc-b9q2h" in namespace "gc-9938"
    Jan 18 21:07:43.045: INFO: Deleting pod "simpletest.rc-446tr" in namespace "gc-9938"
    Jan 18 21:07:43.251: INFO: Deleting pod "simpletest.rc-mqq7d" in namespace "gc-9938"
    Jan 18 21:07:43.396: INFO: Deleting pod "simpletest.rc-mtwg6" in namespace "gc-9938"
    Jan 18 21:07:43.630: INFO: Deleting pod "simpletest.rc-5l5r2" in namespace "gc-9938"
    Jan 18 21:07:43.801: INFO: Deleting pod "simpletest.rc-9bfqc" in namespace "gc-9938"
    Jan 18 21:07:43.970: INFO: Deleting pod "simpletest.rc-wh9lw" in namespace "gc-9938"
    Jan 18 21:07:44.077: INFO: Deleting pod "simpletest.rc-gzbnh" in namespace "gc-9938"
    Jan 18 21:07:44.221: INFO: Deleting pod "simpletest.rc-wnvwh" in namespace "gc-9938"
    Jan 18 21:07:44.494: INFO: Deleting pod "simpletest.rc-zvdjp" in namespace "gc-9938"
    Jan 18 21:07:44.697: INFO: Deleting pod "simpletest.rc-n6xgg" in namespace "gc-9938"
    Jan 18 21:07:44.947: INFO: Deleting pod "simpletest.rc-4m775" in namespace "gc-9938"
    Jan 18 21:07:45.180: INFO: Deleting pod "simpletest.rc-gw2vr" in namespace "gc-9938"
    Jan 18 21:07:45.318: INFO: Deleting pod "simpletest.rc-2tc4f" in namespace "gc-9938"
    Jan 18 21:07:45.593: INFO: Deleting pod "simpletest.rc-xll8g" in namespace "gc-9938"
    Jan 18 21:07:45.800: INFO: Deleting pod "simpletest.rc-prphb" in namespace "gc-9938"
    Jan 18 21:07:45.898: INFO: Deleting pod "simpletest.rc-zwh7m" in namespace "gc-9938"
    Jan 18 21:07:45.989: INFO: Deleting pod "simpletest.rc-ddgws" in namespace "gc-9938"
    Jan 18 21:07:46.134: INFO: Deleting pod "simpletest.rc-dk9b2" in namespace "gc-9938"
    Jan 18 21:07:46.286: INFO: Deleting pod "simpletest.rc-cwbdt" in namespace "gc-9938"
    Jan 18 21:07:46.374: INFO: Deleting pod "simpletest.rc-7xkmt" in namespace "gc-9938"
    Jan 18 21:07:46.537: INFO: Deleting pod "simpletest.rc-m6qnr" in namespace "gc-9938"
    Jan 18 21:07:46.734: INFO: Deleting pod "simpletest.rc-9npg5" in namespace "gc-9938"
    Jan 18 21:07:46.914: INFO: Deleting pod "simpletest.rc-qvm5l" in namespace "gc-9938"
    Jan 18 21:07:47.181: INFO: Deleting pod "simpletest.rc-ssvns" in namespace "gc-9938"
    Jan 18 21:07:47.388: INFO: Deleting pod "simpletest.rc-wblpd" in namespace "gc-9938"
    Jan 18 21:07:47.527: INFO: Deleting pod "simpletest.rc-txv98" in namespace "gc-9938"
    Jan 18 21:07:47.676: INFO: Deleting pod "simpletest.rc-2ds6t" in namespace "gc-9938"
    Jan 18 21:07:47.883: INFO: Deleting pod "simpletest.rc-v5rdx" in namespace "gc-9938"
    Jan 18 21:07:48.043: INFO: Deleting pod "simpletest.rc-7sppx" in namespace "gc-9938"
    Jan 18 21:07:48.319: INFO: Deleting pod "simpletest.rc-f899v" in namespace "gc-9938"
    Jan 18 21:07:48.505: INFO: Deleting pod "simpletest.rc-g9mmd" in namespace "gc-9938"
    Jan 18 21:07:48.763: INFO: Deleting pod "simpletest.rc-496bh" in namespace "gc-9938"
    Jan 18 21:07:48.947: INFO: Deleting pod "simpletest.rc-74p74" in namespace "gc-9938"
    Jan 18 21:07:49.125: INFO: Deleting pod "simpletest.rc-skvrd" in namespace "gc-9938"
    Jan 18 21:07:49.320: INFO: Deleting pod "simpletest.rc-bqq2w" in namespace "gc-9938"
    Jan 18 21:07:49.468: INFO: Deleting pod "simpletest.rc-gqd29" in namespace "gc-9938"
    Jan 18 21:07:49.677: INFO: Deleting pod "simpletest.rc-rcxjt" in namespace "gc-9938"
    Jan 18 21:07:49.874: INFO: Deleting pod "simpletest.rc-xtwmm" in namespace "gc-9938"
    Jan 18 21:07:50.000: INFO: Deleting pod "simpletest.rc-dpsm4" in namespace "gc-9938"
    Jan 18 21:07:50.167: INFO: Deleting pod "simpletest.rc-bvgt4" in namespace "gc-9938"
    Jan 18 21:07:50.443: INFO: Deleting pod "simpletest.rc-4mhnw" in namespace "gc-9938"
    Jan 18 21:07:50.615: INFO: Deleting pod "simpletest.rc-sqlrv" in namespace "gc-9938"
    Jan 18 21:07:50.695: INFO: Deleting pod "simpletest.rc-mw7fc" in namespace "gc-9938"
    Jan 18 21:07:50.814: INFO: Deleting pod "simpletest.rc-mnnkd" in namespace "gc-9938"
    Jan 18 21:07:50.990: INFO: Deleting pod "simpletest.rc-7qrjb" in namespace "gc-9938"
    Jan 18 21:07:51.134: INFO: Deleting pod "simpletest.rc-2xwg6" in namespace "gc-9938"
    Jan 18 21:07:51.356: INFO: Deleting pod "simpletest.rc-8wg5d" in namespace "gc-9938"
    Jan 18 21:07:51.540: INFO: Deleting pod "simpletest.rc-lwbtf" in namespace "gc-9938"
    Jan 18 21:07:51.719: INFO: Deleting pod "simpletest.rc-x7r78" in namespace "gc-9938"
    Jan 18 21:07:51.910: INFO: Deleting pod "simpletest.rc-4pgcx" in namespace "gc-9938"
    Jan 18 21:07:52.054: INFO: Deleting pod "simpletest.rc-k78dh" in namespace "gc-9938"
    Jan 18 21:07:52.192: INFO: Deleting pod "simpletest.rc-lrjq8" in namespace "gc-9938"
    Jan 18 21:07:52.377: INFO: Deleting pod "simpletest.rc-nkvq4" in namespace "gc-9938"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:07:52.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9938" for this suite. 01/18/23 21:07:52.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:07:52.639
Jan 18 21:07:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:07:52.64
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:52.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:52.775
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:07:52.778
Jan 18 21:07:52.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02" in namespace "projected-4894" to be "Succeeded or Failed"
Jan 18 21:07:52.828: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 7.528698ms
Jan 18 21:07:54.839: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018987712s
Jan 18 21:07:56.835: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014578918s
Jan 18 21:07:58.833: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013042041s
STEP: Saw pod success 01/18/23 21:07:58.833
Jan 18 21:07:58.833: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02" satisfied condition "Succeeded or Failed"
Jan 18 21:07:58.838: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 container client-container: <nil>
STEP: delete the pod 01/18/23 21:07:58.847
Jan 18 21:07:58.883: INFO: Waiting for pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 to disappear
Jan 18 21:07:58.893: INFO: Pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:07:58.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4894" for this suite. 01/18/23 21:07:58.899
------------------------------
• [SLOW TEST] [6.278 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:07:52.639
    Jan 18 21:07:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:07:52.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:52.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:52.775
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:07:52.778
    Jan 18 21:07:52.820: INFO: Waiting up to 5m0s for pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02" in namespace "projected-4894" to be "Succeeded or Failed"
    Jan 18 21:07:52.828: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 7.528698ms
    Jan 18 21:07:54.839: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018987712s
    Jan 18 21:07:56.835: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014578918s
    Jan 18 21:07:58.833: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013042041s
    STEP: Saw pod success 01/18/23 21:07:58.833
    Jan 18 21:07:58.833: INFO: Pod "downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02" satisfied condition "Succeeded or Failed"
    Jan 18 21:07:58.838: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:07:58.847
    Jan 18 21:07:58.883: INFO: Waiting for pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 to disappear
    Jan 18 21:07:58.893: INFO: Pod downwardapi-volume-09198237-252b-4f62-bcfb-a2564d293e02 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:07:58.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4894" for this suite. 01/18/23 21:07:58.899
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:07:58.918
Jan 18 21:07:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:07:58.919
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:58.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:58.975
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/18/23 21:07:58.978
STEP: submitting the pod to kubernetes 01/18/23 21:07:58.978
STEP: verifying QOS class is set on the pod 01/18/23 21:07:58.999
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan 18 21:07:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2172" for this suite. 01/18/23 21:07:59.013
------------------------------
• [0.110 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:07:58.918
    Jan 18 21:07:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:07:58.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:58.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:58.975
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/18/23 21:07:58.978
    STEP: submitting the pod to kubernetes 01/18/23 21:07:58.978
    STEP: verifying QOS class is set on the pod 01/18/23 21:07:58.999
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:07:59.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2172" for this suite. 01/18/23 21:07:59.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:07:59.033
Jan 18 21:07:59.033: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename cronjob 01/18/23 21:07:59.035
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:59.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:59.107
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/18/23 21:07:59.112
STEP: Ensuring a job is scheduled 01/18/23 21:07:59.134
STEP: Ensuring exactly one is scheduled 01/18/23 21:08:01.144
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 21:08:01.15
STEP: Ensuring no more jobs are scheduled 01/18/23 21:08:01.157
STEP: Removing cronjob 01/18/23 21:13:01.166
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:01.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8536" for this suite. 01/18/23 21:13:01.188
------------------------------
• [SLOW TEST] [302.168 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:07:59.033
    Jan 18 21:07:59.033: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:07:59.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:07:59.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:07:59.107
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/18/23 21:07:59.112
    STEP: Ensuring a job is scheduled 01/18/23 21:07:59.134
    STEP: Ensuring exactly one is scheduled 01/18/23 21:08:01.144
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 21:08:01.15
    STEP: Ensuring no more jobs are scheduled 01/18/23 21:08:01.157
    STEP: Removing cronjob 01/18/23 21:13:01.166
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:01.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8536" for this suite. 01/18/23 21:13:01.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:01.202
Jan 18 21:13:01.202: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:13:01.203
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:01.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:01.321
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 21:13:01.324
Jan 18 21:13:01.324: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:13:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9849" for this suite. 01/18/23 21:13:12.464
------------------------------
• [SLOW TEST] [11.279 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:01.202
    Jan 18 21:13:01.202: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:13:01.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:01.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:01.321
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 21:13:01.324
    Jan 18 21:13:01.324: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:13:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:12.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9849" for this suite. 01/18/23 21:13:12.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:12.482
Jan 18 21:13:12.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:13:12.483
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:12.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:12.519
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-22183d0f-389e-4b41-9505-f1694eaffb14 01/18/23 21:13:12.522
STEP: Creating a pod to test consume configMaps 01/18/23 21:13:12.532
Jan 18 21:13:12.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36" in namespace "projected-2076" to be "Succeeded or Failed"
Jan 18 21:13:12.560: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368375ms
Jan 18 21:13:14.565: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011630801s
Jan 18 21:13:16.565: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012280273s
STEP: Saw pod success 01/18/23 21:13:16.566
Jan 18 21:13:16.566: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36" satisfied condition "Succeeded or Failed"
Jan 18 21:13:16.572: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:13:16.604
Jan 18 21:13:16.639: INFO: Waiting for pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 to disappear
Jan 18 21:13:16.651: INFO: Pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:16.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2076" for this suite. 01/18/23 21:13:16.655
------------------------------
• [4.204 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:12.482
    Jan 18 21:13:12.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:13:12.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:12.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:12.519
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-22183d0f-389e-4b41-9505-f1694eaffb14 01/18/23 21:13:12.522
    STEP: Creating a pod to test consume configMaps 01/18/23 21:13:12.532
    Jan 18 21:13:12.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36" in namespace "projected-2076" to be "Succeeded or Failed"
    Jan 18 21:13:12.560: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368375ms
    Jan 18 21:13:14.565: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011630801s
    Jan 18 21:13:16.565: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012280273s
    STEP: Saw pod success 01/18/23 21:13:16.566
    Jan 18 21:13:16.566: INFO: Pod "pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36" satisfied condition "Succeeded or Failed"
    Jan 18 21:13:16.572: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:13:16.604
    Jan 18 21:13:16.639: INFO: Waiting for pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 to disappear
    Jan 18 21:13:16.651: INFO: Pod pod-projected-configmaps-9b8c95f4-4137-4b8d-8184-088d294cdf36 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:16.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2076" for this suite. 01/18/23 21:13:16.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:16.687
Jan 18 21:13:16.687: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename podtemplate 01/18/23 21:13:16.689
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:16.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:16.805
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/18/23 21:13:16.807
Jan 18 21:13:16.847: INFO: created test-podtemplate-1
Jan 18 21:13:16.900: INFO: created test-podtemplate-2
Jan 18 21:13:16.917: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/18/23 21:13:16.917
STEP: delete collection of pod templates 01/18/23 21:13:16.921
Jan 18 21:13:16.921: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/18/23 21:13:16.991
Jan 18 21:13:16.991: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7253" for this suite. 01/18/23 21:13:17.006
------------------------------
• [0.339 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:16.687
    Jan 18 21:13:16.687: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename podtemplate 01/18/23 21:13:16.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:16.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:16.805
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/18/23 21:13:16.807
    Jan 18 21:13:16.847: INFO: created test-podtemplate-1
    Jan 18 21:13:16.900: INFO: created test-podtemplate-2
    Jan 18 21:13:16.917: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/18/23 21:13:16.917
    STEP: delete collection of pod templates 01/18/23 21:13:16.921
    Jan 18 21:13:16.921: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/18/23 21:13:16.991
    Jan 18 21:13:16.991: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:17.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7253" for this suite. 01/18/23 21:13:17.006
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:17.027
Jan 18 21:13:17.027: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:13:17.028
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:17.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:17.104
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-d3fb0116-77ec-4b9d-ab23-8828c2ceed50 01/18/23 21:13:17.106
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7041" for this suite. 01/18/23 21:13:17.113
------------------------------
• [0.106 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:17.027
    Jan 18 21:13:17.027: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:13:17.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:17.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:17.104
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-d3fb0116-77ec-4b9d-ab23-8828c2ceed50 01/18/23 21:13:17.106
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7041" for this suite. 01/18/23 21:13:17.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:17.136
Jan 18 21:13:17.136: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:13:17.137
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:17.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:17.21
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan 18 21:13:17.245: INFO: created pod
Jan 18 21:13:17.245: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2402" to be "Succeeded or Failed"
Jan 18 21:13:17.249: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924347ms
Jan 18 21:13:19.256: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011005348s
Jan 18 21:13:21.253: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008333238s
STEP: Saw pod success 01/18/23 21:13:21.253
Jan 18 21:13:21.253: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 21:13:51.253: INFO: polling logs
Jan 18 21:13:51.263: INFO: Pod logs: 
I0118 21:13:18.394200       1 log.go:198] OK: Got token
I0118 21:13:18.394243       1 log.go:198] validating with in-cluster discovery
I0118 21:13:18.394721       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0118 21:13:18.394756       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2402:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674076997, NotBefore:1674076397, IssuedAt:1674076397, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2402", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b8e44244-6b38-4e01-a9db-8bf8a1b76876"}}}
I0118 21:13:18.407848       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0118 21:13:18.413956       1 log.go:198] OK: Validated signature on JWT
I0118 21:13:18.414087       1 log.go:198] OK: Got valid claims from token!
I0118 21:13:18.414130       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2402:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674076997, NotBefore:1674076397, IssuedAt:1674076397, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2402", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b8e44244-6b38-4e01-a9db-8bf8a1b76876"}}}

Jan 18 21:13:51.263: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2402" for this suite. 01/18/23 21:13:51.291
------------------------------
• [SLOW TEST] [34.174 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:17.136
    Jan 18 21:13:17.136: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:13:17.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:17.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:17.21
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan 18 21:13:17.245: INFO: created pod
    Jan 18 21:13:17.245: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2402" to be "Succeeded or Failed"
    Jan 18 21:13:17.249: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924347ms
    Jan 18 21:13:19.256: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011005348s
    Jan 18 21:13:21.253: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008333238s
    STEP: Saw pod success 01/18/23 21:13:21.253
    Jan 18 21:13:21.253: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 18 21:13:51.253: INFO: polling logs
    Jan 18 21:13:51.263: INFO: Pod logs: 
    I0118 21:13:18.394200       1 log.go:198] OK: Got token
    I0118 21:13:18.394243       1 log.go:198] validating with in-cluster discovery
    I0118 21:13:18.394721       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0118 21:13:18.394756       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2402:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674076997, NotBefore:1674076397, IssuedAt:1674076397, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2402", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b8e44244-6b38-4e01-a9db-8bf8a1b76876"}}}
    I0118 21:13:18.407848       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0118 21:13:18.413956       1 log.go:198] OK: Validated signature on JWT
    I0118 21:13:18.414087       1 log.go:198] OK: Got valid claims from token!
    I0118 21:13:18.414130       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2402:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674076997, NotBefore:1674076397, IssuedAt:1674076397, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2402", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"b8e44244-6b38-4e01-a9db-8bf8a1b76876"}}}

    Jan 18 21:13:51.263: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:51.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2402" for this suite. 01/18/23 21:13:51.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:51.313
Jan 18 21:13:51.313: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:13:51.314
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:51.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:51.348
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 21:13:51.352
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:13:51.66
STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:13:51.685
STEP: Wait for the deployment to be ready 01/18/23 21:13:51.71
Jan 18 21:13:51.721: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:13:53.734
STEP: Verifying the service has paired with the endpoint 01/18/23 21:13:53.754
Jan 18 21:13:54.754: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 18 21:13:54.759: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Creating a v1 custom resource 01/18/23 21:13:58.35
STEP: Create a v2 custom resource 01/18/23 21:13:58.379
STEP: List CRs in v1 01/18/23 21:13:58.391
STEP: List CRs in v2 01/18/23 21:13:58.428
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:13:58.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9967" for this suite. 01/18/23 21:13:59.079
------------------------------
• [SLOW TEST] [7.779 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:51.313
    Jan 18 21:13:51.313: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:13:51.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:51.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:51.348
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 21:13:51.352
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:13:51.66
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:13:51.685
    STEP: Wait for the deployment to be ready 01/18/23 21:13:51.71
    Jan 18 21:13:51.721: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:13:53.734
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:13:53.754
    Jan 18 21:13:54.754: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 18 21:13:54.759: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Creating a v1 custom resource 01/18/23 21:13:58.35
    STEP: Create a v2 custom resource 01/18/23 21:13:58.379
    STEP: List CRs in v1 01/18/23 21:13:58.391
    STEP: List CRs in v2 01/18/23 21:13:58.428
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:13:58.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9967" for this suite. 01/18/23 21:13:59.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:13:59.094
Jan 18 21:13:59.094: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:13:59.095
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:59.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:59.144
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-74fde8a9-f5d9-457d-bec0-f92906f05df4 01/18/23 21:13:59.148
STEP: Creating a pod to test consume secrets 01/18/23 21:13:59.158
Jan 18 21:13:59.174: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f" in namespace "projected-2057" to be "Succeeded or Failed"
Jan 18 21:13:59.181: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107157ms
Jan 18 21:14:01.188: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013305048s
Jan 18 21:14:03.187: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012838767s
STEP: Saw pod success 01/18/23 21:14:03.187
Jan 18 21:14:03.188: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f" satisfied condition "Succeeded or Failed"
Jan 18 21:14:03.191: INFO: Trying to get logs from node test-vm-2 pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:14:03.211
Jan 18 21:14:03.238: INFO: Waiting for pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f to disappear
Jan 18 21:14:03.244: INFO: Pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:03.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2057" for this suite. 01/18/23 21:14:03.248
------------------------------
• [4.166 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:13:59.094
    Jan 18 21:13:59.094: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:13:59.095
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:13:59.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:13:59.144
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-74fde8a9-f5d9-457d-bec0-f92906f05df4 01/18/23 21:13:59.148
    STEP: Creating a pod to test consume secrets 01/18/23 21:13:59.158
    Jan 18 21:13:59.174: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f" in namespace "projected-2057" to be "Succeeded or Failed"
    Jan 18 21:13:59.181: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.107157ms
    Jan 18 21:14:01.188: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013305048s
    Jan 18 21:14:03.187: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012838767s
    STEP: Saw pod success 01/18/23 21:14:03.187
    Jan 18 21:14:03.188: INFO: Pod "pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f" satisfied condition "Succeeded or Failed"
    Jan 18 21:14:03.191: INFO: Trying to get logs from node test-vm-2 pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:14:03.211
    Jan 18 21:14:03.238: INFO: Waiting for pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f to disappear
    Jan 18 21:14:03.244: INFO: Pod pod-projected-secrets-88de8efa-08ed-4d4b-9638-6ba56d3fed1f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:03.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2057" for this suite. 01/18/23 21:14:03.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:03.262
Jan 18 21:14:03.262: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svc-latency 01/18/23 21:14:03.263
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:03.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:03.303
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 18 21:14:03.306: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2123 01/18/23 21:14:03.307
I0118 21:14:03.321689      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2123, replica count: 1
I0118 21:14:04.373335      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:14:05.373550      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:14:06.373720      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:14:06.499: INFO: Created: latency-svc-njvkw
Jan 18 21:14:06.516: INFO: Got endpoints: latency-svc-njvkw [42.448998ms]
Jan 18 21:14:06.543: INFO: Created: latency-svc-ck5g7
Jan 18 21:14:06.571: INFO: Got endpoints: latency-svc-ck5g7 [54.385909ms]
Jan 18 21:14:06.576: INFO: Created: latency-svc-nhssd
Jan 18 21:14:06.590: INFO: Got endpoints: latency-svc-nhssd [73.058083ms]
Jan 18 21:14:06.600: INFO: Created: latency-svc-fbgdp
Jan 18 21:14:06.619: INFO: Got endpoints: latency-svc-fbgdp [101.787153ms]
Jan 18 21:14:06.624: INFO: Created: latency-svc-k78t2
Jan 18 21:14:06.638: INFO: Got endpoints: latency-svc-k78t2 [121.605739ms]
Jan 18 21:14:06.651: INFO: Created: latency-svc-v7k7t
Jan 18 21:14:06.675: INFO: Got endpoints: latency-svc-v7k7t [158.532084ms]
Jan 18 21:14:06.680: INFO: Created: latency-svc-zchpr
Jan 18 21:14:06.693: INFO: Got endpoints: latency-svc-zchpr [176.24675ms]
Jan 18 21:14:06.704: INFO: Created: latency-svc-zn8tz
Jan 18 21:14:06.719: INFO: Got endpoints: latency-svc-zn8tz [202.155192ms]
Jan 18 21:14:06.730: INFO: Created: latency-svc-gt8x6
Jan 18 21:14:06.751: INFO: Got endpoints: latency-svc-gt8x6 [234.504496ms]
Jan 18 21:14:06.756: INFO: Created: latency-svc-l6rlh
Jan 18 21:14:06.773: INFO: Got endpoints: latency-svc-l6rlh [256.3763ms]
Jan 18 21:14:06.786: INFO: Created: latency-svc-dzcdr
Jan 18 21:14:06.800: INFO: Got endpoints: latency-svc-dzcdr [283.238452ms]
Jan 18 21:14:06.812: INFO: Created: latency-svc-rcb57
Jan 18 21:14:06.833: INFO: Got endpoints: latency-svc-rcb57 [315.633354ms]
Jan 18 21:14:06.837: INFO: Created: latency-svc-cv4kz
Jan 18 21:14:06.855: INFO: Got endpoints: latency-svc-cv4kz [337.596861ms]
Jan 18 21:14:06.866: INFO: Created: latency-svc-xhjbk
Jan 18 21:14:06.897: INFO: Got endpoints: latency-svc-xhjbk [380.077958ms]
Jan 18 21:14:06.902: INFO: Created: latency-svc-rqwwd
Jan 18 21:14:06.916: INFO: Got endpoints: latency-svc-rqwwd [399.803643ms]
Jan 18 21:14:06.928: INFO: Created: latency-svc-8rnlt
Jan 18 21:14:06.953: INFO: Got endpoints: latency-svc-8rnlt [435.648379ms]
Jan 18 21:14:06.962: INFO: Created: latency-svc-t7rq5
Jan 18 21:14:06.982: INFO: Got endpoints: latency-svc-t7rq5 [410.956647ms]
Jan 18 21:14:06.991: INFO: Created: latency-svc-kn8lw
Jan 18 21:14:07.010: INFO: Got endpoints: latency-svc-kn8lw [419.81923ms]
Jan 18 21:14:07.022: INFO: Created: latency-svc-vnh2v
Jan 18 21:14:07.041: INFO: Got endpoints: latency-svc-vnh2v [422.477755ms]
Jan 18 21:14:07.046: INFO: Created: latency-svc-2hj5k
Jan 18 21:14:07.073: INFO: Got endpoints: latency-svc-2hj5k [434.254206ms]
Jan 18 21:14:07.087: INFO: Created: latency-svc-h2mr2
Jan 18 21:14:07.110: INFO: Got endpoints: latency-svc-h2mr2 [434.395058ms]
Jan 18 21:14:07.123: INFO: Created: latency-svc-2lcj4
Jan 18 21:14:07.138: INFO: Got endpoints: latency-svc-2lcj4 [445.274199ms]
Jan 18 21:14:07.149: INFO: Created: latency-svc-p2sr4
Jan 18 21:14:07.176: INFO: Got endpoints: latency-svc-p2sr4 [457.182161ms]
Jan 18 21:14:07.187: INFO: Created: latency-svc-87pr8
Jan 18 21:14:07.211: INFO: Got endpoints: latency-svc-87pr8 [459.761833ms]
Jan 18 21:14:07.216: INFO: Created: latency-svc-q88hx
Jan 18 21:14:07.229: INFO: Got endpoints: latency-svc-q88hx [455.229815ms]
Jan 18 21:14:07.241: INFO: Created: latency-svc-rqwrb
Jan 18 21:14:07.260: INFO: Got endpoints: latency-svc-rqwrb [459.681398ms]
Jan 18 21:14:07.268: INFO: Created: latency-svc-pw52p
Jan 18 21:14:07.285: INFO: Got endpoints: latency-svc-pw52p [451.800258ms]
Jan 18 21:14:07.306: INFO: Created: latency-svc-lwd9h
Jan 18 21:14:07.336: INFO: Got endpoints: latency-svc-lwd9h [481.501506ms]
Jan 18 21:14:07.342: INFO: Created: latency-svc-27mtm
Jan 18 21:14:07.371: INFO: Got endpoints: latency-svc-27mtm [473.565178ms]
Jan 18 21:14:07.379: INFO: Created: latency-svc-z2tpr
Jan 18 21:14:07.396: INFO: Got endpoints: latency-svc-z2tpr [479.398667ms]
Jan 18 21:14:07.409: INFO: Created: latency-svc-zbkwx
Jan 18 21:14:07.429: INFO: Got endpoints: latency-svc-zbkwx [476.075881ms]
Jan 18 21:14:07.434: INFO: Created: latency-svc-ndcjf
Jan 18 21:14:07.451: INFO: Got endpoints: latency-svc-ndcjf [468.916544ms]
Jan 18 21:14:07.467: INFO: Created: latency-svc-hcv68
Jan 18 21:14:07.510: INFO: Got endpoints: latency-svc-hcv68 [500.350918ms]
Jan 18 21:14:07.516: INFO: Created: latency-svc-62jrb
Jan 18 21:14:07.538: INFO: Got endpoints: latency-svc-62jrb [496.61252ms]
Jan 18 21:14:07.545: INFO: Created: latency-svc-mhzk6
Jan 18 21:14:07.565: INFO: Got endpoints: latency-svc-mhzk6 [491.951772ms]
Jan 18 21:14:07.578: INFO: Created: latency-svc-xqk8t
Jan 18 21:14:07.595: INFO: Got endpoints: latency-svc-xqk8t [484.819796ms]
Jan 18 21:14:07.607: INFO: Created: latency-svc-h9s2d
Jan 18 21:14:07.623: INFO: Got endpoints: latency-svc-h9s2d [484.26339ms]
Jan 18 21:14:07.633: INFO: Created: latency-svc-j58sb
Jan 18 21:14:07.647: INFO: Got endpoints: latency-svc-j58sb [470.274039ms]
Jan 18 21:14:07.660: INFO: Created: latency-svc-wj5p6
Jan 18 21:14:07.674: INFO: Got endpoints: latency-svc-wj5p6 [462.782259ms]
Jan 18 21:14:07.687: INFO: Created: latency-svc-mrzqd
Jan 18 21:14:07.702: INFO: Got endpoints: latency-svc-mrzqd [473.533474ms]
Jan 18 21:14:07.714: INFO: Created: latency-svc-rdqpp
Jan 18 21:14:07.729: INFO: Got endpoints: latency-svc-rdqpp [468.758424ms]
Jan 18 21:14:07.740: INFO: Created: latency-svc-cqldz
Jan 18 21:14:07.759: INFO: Got endpoints: latency-svc-cqldz [474.754287ms]
Jan 18 21:14:07.769: INFO: Created: latency-svc-smlww
Jan 18 21:14:07.787: INFO: Got endpoints: latency-svc-smlww [450.539329ms]
Jan 18 21:14:07.803: INFO: Created: latency-svc-f2f97
Jan 18 21:14:07.817: INFO: Got endpoints: latency-svc-f2f97 [446.411684ms]
Jan 18 21:14:07.829: INFO: Created: latency-svc-dbzbm
Jan 18 21:14:07.844: INFO: Got endpoints: latency-svc-dbzbm [448.225003ms]
Jan 18 21:14:07.855: INFO: Created: latency-svc-vfc2l
Jan 18 21:14:07.867: INFO: Got endpoints: latency-svc-vfc2l [438.052694ms]
Jan 18 21:14:07.879: INFO: Created: latency-svc-8wxmr
Jan 18 21:14:07.898: INFO: Got endpoints: latency-svc-8wxmr [446.785988ms]
Jan 18 21:14:07.910: INFO: Created: latency-svc-9hdwd
Jan 18 21:14:07.923: INFO: Got endpoints: latency-svc-9hdwd [413.32183ms]
Jan 18 21:14:07.936: INFO: Created: latency-svc-npsnp
Jan 18 21:14:07.958: INFO: Got endpoints: latency-svc-npsnp [419.732098ms]
Jan 18 21:14:07.962: INFO: Created: latency-svc-4pf8m
Jan 18 21:14:07.977: INFO: Got endpoints: latency-svc-4pf8m [412.386119ms]
Jan 18 21:14:07.988: INFO: Created: latency-svc-5j79p
Jan 18 21:14:08.002: INFO: Got endpoints: latency-svc-5j79p [407.913571ms]
Jan 18 21:14:08.013: INFO: Created: latency-svc-vvwlf
Jan 18 21:14:08.030: INFO: Got endpoints: latency-svc-vvwlf [406.742159ms]
Jan 18 21:14:08.040: INFO: Created: latency-svc-t852p
Jan 18 21:14:08.057: INFO: Got endpoints: latency-svc-t852p [410.756801ms]
Jan 18 21:14:08.068: INFO: Created: latency-svc-2rt7b
Jan 18 21:14:08.088: INFO: Got endpoints: latency-svc-2rt7b [414.246239ms]
Jan 18 21:14:08.102: INFO: Created: latency-svc-krdqf
Jan 18 21:14:08.125: INFO: Got endpoints: latency-svc-krdqf [423.157635ms]
Jan 18 21:14:08.135: INFO: Created: latency-svc-69qpl
Jan 18 21:14:08.152: INFO: Got endpoints: latency-svc-69qpl [423.60174ms]
Jan 18 21:14:08.179: INFO: Created: latency-svc-pkn9l
Jan 18 21:14:08.209: INFO: Got endpoints: latency-svc-pkn9l [449.209014ms]
Jan 18 21:14:08.220: INFO: Created: latency-svc-dvbqt
Jan 18 21:14:08.240: INFO: Got endpoints: latency-svc-dvbqt [453.230757ms]
Jan 18 21:14:08.247: INFO: Created: latency-svc-ggb56
Jan 18 21:14:08.267: INFO: Got endpoints: latency-svc-ggb56 [449.944622ms]
Jan 18 21:14:08.279: INFO: Created: latency-svc-bc52v
Jan 18 21:14:08.296: INFO: Got endpoints: latency-svc-bc52v [452.448048ms]
Jan 18 21:14:08.314: INFO: Created: latency-svc-7ptfp
Jan 18 21:14:08.332: INFO: Got endpoints: latency-svc-7ptfp [464.69848ms]
Jan 18 21:14:08.345: INFO: Created: latency-svc-rt6pc
Jan 18 21:14:08.363: INFO: Got endpoints: latency-svc-rt6pc [465.441388ms]
Jan 18 21:14:08.376: INFO: Created: latency-svc-nb9vr
Jan 18 21:14:08.398: INFO: Got endpoints: latency-svc-nb9vr [474.453584ms]
Jan 18 21:14:08.408: INFO: Created: latency-svc-fs9df
Jan 18 21:14:08.436: INFO: Got endpoints: latency-svc-fs9df [477.579718ms]
Jan 18 21:14:08.440: INFO: Created: latency-svc-pzxqs
Jan 18 21:14:08.465: INFO: Got endpoints: latency-svc-pzxqs [488.05213ms]
Jan 18 21:14:08.470: INFO: Created: latency-svc-xwltw
Jan 18 21:14:08.495: INFO: Got endpoints: latency-svc-xwltw [492.143174ms]
Jan 18 21:14:08.506: INFO: Created: latency-svc-7vfpl
Jan 18 21:14:08.527: INFO: Got endpoints: latency-svc-7vfpl [497.012426ms]
Jan 18 21:14:08.546: INFO: Created: latency-svc-49pdz
Jan 18 21:14:08.569: INFO: Got endpoints: latency-svc-49pdz [511.738384ms]
Jan 18 21:14:08.584: INFO: Created: latency-svc-9zt2b
Jan 18 21:14:08.602: INFO: Got endpoints: latency-svc-9zt2b [513.835607ms]
Jan 18 21:14:08.614: INFO: Created: latency-svc-gjmfh
Jan 18 21:14:08.631: INFO: Got endpoints: latency-svc-gjmfh [505.76322ms]
Jan 18 21:14:08.647: INFO: Created: latency-svc-49fdv
Jan 18 21:14:08.665: INFO: Got endpoints: latency-svc-49fdv [512.354191ms]
Jan 18 21:14:08.679: INFO: Created: latency-svc-jlnxq
Jan 18 21:14:08.697: INFO: Got endpoints: latency-svc-jlnxq [488.097031ms]
Jan 18 21:14:08.709: INFO: Created: latency-svc-lptv4
Jan 18 21:14:08.729: INFO: Got endpoints: latency-svc-lptv4 [488.440434ms]
Jan 18 21:14:08.742: INFO: Created: latency-svc-2hqz6
Jan 18 21:14:08.756: INFO: Got endpoints: latency-svc-2hqz6 [489.014241ms]
Jan 18 21:14:08.767: INFO: Created: latency-svc-6g5jt
Jan 18 21:14:08.782: INFO: Got endpoints: latency-svc-6g5jt [485.637604ms]
Jan 18 21:14:08.801: INFO: Created: latency-svc-5bdqt
Jan 18 21:14:08.819: INFO: Got endpoints: latency-svc-5bdqt [487.050619ms]
Jan 18 21:14:08.832: INFO: Created: latency-svc-bnkq6
Jan 18 21:14:08.890: INFO: Created: latency-svc-7fn6c
Jan 18 21:14:08.919: INFO: Got endpoints: latency-svc-bnkq6 [556.11936ms]
Jan 18 21:14:08.932: INFO: Got endpoints: latency-svc-7fn6c [533.76522ms]
Jan 18 21:14:08.944: INFO: Created: latency-svc-c5rwn
Jan 18 21:14:08.968: INFO: Got endpoints: latency-svc-c5rwn [532.024501ms]
Jan 18 21:14:08.974: INFO: Created: latency-svc-86kvt
Jan 18 21:14:09.000: INFO: Created: latency-svc-c5dk5
Jan 18 21:14:09.033: INFO: Got endpoints: latency-svc-86kvt [568.042687ms]
Jan 18 21:14:09.038: INFO: Created: latency-svc-cpcs7
Jan 18 21:14:09.055: INFO: Created: latency-svc-49smq
Jan 18 21:14:09.071: INFO: Got endpoints: latency-svc-c5dk5 [576.383577ms]
Jan 18 21:14:09.102: INFO: Created: latency-svc-hlg4d
Jan 18 21:14:09.110: INFO: Created: latency-svc-h7fwj
Jan 18 21:14:09.124: INFO: Got endpoints: latency-svc-cpcs7 [597.034898ms]
Jan 18 21:14:09.135: INFO: Created: latency-svc-s4frd
Jan 18 21:14:09.154: INFO: Created: latency-svc-czlnr
Jan 18 21:14:09.168: INFO: Got endpoints: latency-svc-49smq [597.647305ms]
Jan 18 21:14:09.179: INFO: Created: latency-svc-zp8c2
Jan 18 21:14:09.197: INFO: Created: latency-svc-tgs7j
Jan 18 21:14:09.214: INFO: Got endpoints: latency-svc-hlg4d [612.17426ms]
Jan 18 21:14:09.225: INFO: Created: latency-svc-djj67
Jan 18 21:14:09.242: INFO: Created: latency-svc-shw87
Jan 18 21:14:09.263: INFO: Created: latency-svc-scsgg
Jan 18 21:14:09.280: INFO: Got endpoints: latency-svc-h7fwj [649.198857ms]
Jan 18 21:14:09.286: INFO: Created: latency-svc-6jsnn
Jan 18 21:14:09.304: INFO: Created: latency-svc-rzdmq
Jan 18 21:14:09.317: INFO: Got endpoints: latency-svc-s4frd [652.327791ms]
Jan 18 21:14:09.331: INFO: Created: latency-svc-mxllm
Jan 18 21:14:09.347: INFO: Created: latency-svc-r95pk
Jan 18 21:14:09.360: INFO: Got endpoints: latency-svc-czlnr [663.234308ms]
Jan 18 21:14:09.374: INFO: Created: latency-svc-8h78r
Jan 18 21:14:09.392: INFO: Created: latency-svc-h4xfl
Jan 18 21:14:09.427: INFO: Created: latency-svc-zg88j
Jan 18 21:14:09.436: INFO: Got endpoints: latency-svc-zp8c2 [707.647684ms]
Jan 18 21:14:09.464: INFO: Created: latency-svc-7hbzq
Jan 18 21:14:10.008: INFO: Got endpoints: latency-svc-tgs7j [1.251762314s]
Jan 18 21:14:10.028: INFO: Got endpoints: latency-svc-djj67 [1.245688949s]
Jan 18 21:14:10.059: INFO: Got endpoints: latency-svc-shw87 [1.239924788s]
Jan 18 21:14:10.084: INFO: Got endpoints: latency-svc-scsgg [1.164682081s]
Jan 18 21:14:10.089: INFO: Got endpoints: latency-svc-6jsnn [1.157713907s]
Jan 18 21:14:10.102: INFO: Got endpoints: latency-svc-rzdmq [1.133919352s]
Jan 18 21:14:10.112: INFO: Created: latency-svc-94zd5
Jan 18 21:14:10.125: INFO: Got endpoints: latency-svc-mxllm [1.091680099s]
Jan 18 21:14:10.136: INFO: Got endpoints: latency-svc-r95pk [1.064483007s]
Jan 18 21:14:10.154: INFO: Got endpoints: latency-svc-8h78r [1.029748935s]
Jan 18 21:14:10.159: INFO: Got endpoints: latency-svc-h4xfl [991.112221ms]
Jan 18 21:14:10.169: INFO: Got endpoints: latency-svc-zg88j [955.158335ms]
Jan 18 21:14:10.181: INFO: Created: latency-svc-v2s28
Jan 18 21:14:10.197: INFO: Got endpoints: latency-svc-7hbzq [916.629623ms]
Jan 18 21:14:10.211: INFO: Got endpoints: latency-svc-94zd5 [893.466275ms]
Jan 18 21:14:10.222: INFO: Got endpoints: latency-svc-v2s28 [862.453943ms]
Jan 18 21:14:10.255: INFO: Created: latency-svc-8rhtb
Jan 18 21:14:10.276: INFO: Got endpoints: latency-svc-8rhtb [839.589997ms]
Jan 18 21:14:10.290: INFO: Created: latency-svc-58m6s
Jan 18 21:14:10.304: INFO: Got endpoints: latency-svc-58m6s [295.659368ms]
Jan 18 21:14:10.315: INFO: Created: latency-svc-lr4x8
Jan 18 21:14:10.336: INFO: Got endpoints: latency-svc-lr4x8 [308.125602ms]
Jan 18 21:14:10.347: INFO: Created: latency-svc-nbvz2
Jan 18 21:14:10.362: INFO: Got endpoints: latency-svc-nbvz2 [303.420052ms]
Jan 18 21:14:10.375: INFO: Created: latency-svc-jpj5l
Jan 18 21:14:10.398: INFO: Got endpoints: latency-svc-jpj5l [314.032666ms]
Jan 18 21:14:10.407: INFO: Created: latency-svc-46ctm
Jan 18 21:14:10.421: INFO: Got endpoints: latency-svc-46ctm [331.360051ms]
Jan 18 21:14:10.433: INFO: Created: latency-svc-82fdp
Jan 18 21:14:10.448: INFO: Created: latency-svc-tnscv
Jan 18 21:14:10.474: INFO: Got endpoints: latency-svc-82fdp [371.988387ms]
Jan 18 21:14:10.479: INFO: Created: latency-svc-ndbdw
Jan 18 21:14:10.497: INFO: Created: latency-svc-5pxmm
Jan 18 21:14:10.511: INFO: Got endpoints: latency-svc-tnscv [385.871335ms]
Jan 18 21:14:10.522: INFO: Created: latency-svc-fv86d
Jan 18 21:14:10.538: INFO: Created: latency-svc-2b6nb
Jan 18 21:14:10.563: INFO: Created: latency-svc-df4g8
Jan 18 21:14:10.578: INFO: Got endpoints: latency-svc-ndbdw [442.32674ms]
Jan 18 21:14:10.595: INFO: Created: latency-svc-2g9sm
Jan 18 21:14:10.615: INFO: Created: latency-svc-7vsw8
Jan 18 21:14:10.630: INFO: Got endpoints: latency-svc-5pxmm [475.8919ms]
Jan 18 21:14:10.643: INFO: Created: latency-svc-xc8cm
Jan 18 21:14:10.664: INFO: Got endpoints: latency-svc-fv86d [505.497617ms]
Jan 18 21:14:10.681: INFO: Created: latency-svc-2tj5f
Jan 18 21:14:10.692: INFO: Created: latency-svc-q9ntz
Jan 18 21:14:10.713: INFO: Created: latency-svc-fbrv6
Jan 18 21:14:10.728: INFO: Got endpoints: latency-svc-2b6nb [558.312483ms]
Jan 18 21:14:10.739: INFO: Created: latency-svc-wrj4g
Jan 18 21:14:10.759: INFO: Created: latency-svc-wdsjq
Jan 18 21:14:10.782: INFO: Got endpoints: latency-svc-df4g8 [585.223872ms]
Jan 18 21:14:10.793: INFO: Created: latency-svc-bglrq
Jan 18 21:14:10.814: INFO: Got endpoints: latency-svc-2g9sm [603.416167ms]
Jan 18 21:14:10.819: INFO: Created: latency-svc-9k4g2
Jan 18 21:14:10.846: INFO: Created: latency-svc-tgw7s
Jan 18 21:14:10.861: INFO: Created: latency-svc-whggq
Jan 18 21:14:10.868: INFO: Got endpoints: latency-svc-7vsw8 [645.331616ms]
Jan 18 21:14:10.887: INFO: Created: latency-svc-j7xxr
Jan 18 21:14:10.905: INFO: Created: latency-svc-48gcn
Jan 18 21:14:10.921: INFO: Got endpoints: latency-svc-xc8cm [645.076813ms]
Jan 18 21:14:10.933: INFO: Created: latency-svc-m7ws7
Jan 18 21:14:10.950: INFO: Created: latency-svc-gdh7f
Jan 18 21:14:10.967: INFO: Got endpoints: latency-svc-2tj5f [663.294408ms]
Jan 18 21:14:10.983: INFO: Created: latency-svc-zhhrb
Jan 18 21:14:11.000: INFO: Created: latency-svc-p4ffw
Jan 18 21:14:11.015: INFO: Got endpoints: latency-svc-q9ntz [678.806775ms]
Jan 18 21:14:11.028: INFO: Created: latency-svc-88bw5
Jan 18 21:14:11.042: INFO: Created: latency-svc-xkcqq
Jan 18 21:14:11.065: INFO: Got endpoints: latency-svc-fbrv6 [702.67483ms]
Jan 18 21:14:11.087: INFO: Created: latency-svc-9gvcq
Jan 18 21:14:11.141: INFO: Got endpoints: latency-svc-wrj4g [742.351155ms]
Jan 18 21:14:11.170: INFO: Got endpoints: latency-svc-wdsjq [749.334531ms]
Jan 18 21:14:11.192: INFO: Created: latency-svc-p26f9
Jan 18 21:14:11.208: INFO: Created: latency-svc-s56cs
Jan 18 21:14:11.219: INFO: Got endpoints: latency-svc-bglrq [745.168086ms]
Jan 18 21:14:11.243: INFO: Created: latency-svc-vsjn4
Jan 18 21:14:11.260: INFO: Got endpoints: latency-svc-9k4g2 [749.428932ms]
Jan 18 21:14:11.285: INFO: Created: latency-svc-gcltj
Jan 18 21:14:11.313: INFO: Got endpoints: latency-svc-tgw7s [735.035777ms]
Jan 18 21:14:11.334: INFO: Created: latency-svc-wx88v
Jan 18 21:14:11.366: INFO: Got endpoints: latency-svc-whggq [736.859596ms]
Jan 18 21:14:11.389: INFO: Created: latency-svc-7hdp4
Jan 18 21:14:11.411: INFO: Got endpoints: latency-svc-j7xxr [746.923104ms]
Jan 18 21:14:11.436: INFO: Created: latency-svc-b5dsp
Jan 18 21:14:11.466: INFO: Got endpoints: latency-svc-48gcn [737.961908ms]
Jan 18 21:14:11.488: INFO: Created: latency-svc-bq6kn
Jan 18 21:14:11.519: INFO: Got endpoints: latency-svc-m7ws7 [736.213989ms]
Jan 18 21:14:11.542: INFO: Created: latency-svc-fl9fm
Jan 18 21:14:11.568: INFO: Got endpoints: latency-svc-gdh7f [754.137682ms]
Jan 18 21:14:11.588: INFO: Created: latency-svc-b6hqh
Jan 18 21:14:11.611: INFO: Got endpoints: latency-svc-zhhrb [743.082063ms]
Jan 18 21:14:11.633: INFO: Created: latency-svc-npg2w
Jan 18 21:14:11.660: INFO: Got endpoints: latency-svc-p4ffw [738.553515ms]
Jan 18 21:14:11.681: INFO: Created: latency-svc-cmfjh
Jan 18 21:14:11.709: INFO: Got endpoints: latency-svc-88bw5 [742.017352ms]
Jan 18 21:14:11.729: INFO: Created: latency-svc-td969
Jan 18 21:14:11.761: INFO: Got endpoints: latency-svc-xkcqq [745.498089ms]
Jan 18 21:14:11.781: INFO: Created: latency-svc-bzt4d
Jan 18 21:14:11.811: INFO: Got endpoints: latency-svc-9gvcq [745.813092ms]
Jan 18 21:14:11.845: INFO: Created: latency-svc-5cc6k
Jan 18 21:14:11.861: INFO: Got endpoints: latency-svc-p26f9 [720.250118ms]
Jan 18 21:14:11.882: INFO: Created: latency-svc-6cf55
Jan 18 21:14:11.912: INFO: Got endpoints: latency-svc-s56cs [741.751849ms]
Jan 18 21:14:11.936: INFO: Created: latency-svc-s9hbd
Jan 18 21:14:11.962: INFO: Got endpoints: latency-svc-vsjn4 [742.74566ms]
Jan 18 21:14:11.985: INFO: Created: latency-svc-hl5t5
Jan 18 21:14:12.011: INFO: Got endpoints: latency-svc-gcltj [750.22164ms]
Jan 18 21:14:12.032: INFO: Created: latency-svc-chk29
Jan 18 21:14:12.067: INFO: Got endpoints: latency-svc-wx88v [753.780378ms]
Jan 18 21:14:12.096: INFO: Created: latency-svc-52tbc
Jan 18 21:14:12.113: INFO: Got endpoints: latency-svc-7hdp4 [745.970994ms]
Jan 18 21:14:12.140: INFO: Created: latency-svc-45h5v
Jan 18 21:14:12.171: INFO: Got endpoints: latency-svc-b5dsp [759.64944ms]
Jan 18 21:14:12.193: INFO: Created: latency-svc-bgh6c
Jan 18 21:14:12.210: INFO: Got endpoints: latency-svc-bq6kn [744.223575ms]
Jan 18 21:14:12.230: INFO: Created: latency-svc-6l2sv
Jan 18 21:14:12.271: INFO: Got endpoints: latency-svc-fl9fm [752.485364ms]
Jan 18 21:14:12.292: INFO: Created: latency-svc-sj25h
Jan 18 21:14:12.316: INFO: Got endpoints: latency-svc-b6hqh [747.709813ms]
Jan 18 21:14:12.336: INFO: Created: latency-svc-hj87p
Jan 18 21:14:12.360: INFO: Got endpoints: latency-svc-npg2w [748.663123ms]
Jan 18 21:14:12.381: INFO: Created: latency-svc-pmgbr
Jan 18 21:14:12.418: INFO: Got endpoints: latency-svc-cmfjh [758.096124ms]
Jan 18 21:14:12.439: INFO: Created: latency-svc-mdj4s
Jan 18 21:14:12.462: INFO: Got endpoints: latency-svc-td969 [752.709667ms]
Jan 18 21:14:12.484: INFO: Created: latency-svc-6nskx
Jan 18 21:14:12.523: INFO: Got endpoints: latency-svc-bzt4d [762.44767ms]
Jan 18 21:14:12.552: INFO: Created: latency-svc-swwvk
Jan 18 21:14:12.563: INFO: Got endpoints: latency-svc-5cc6k [752.05206ms]
Jan 18 21:14:12.585: INFO: Created: latency-svc-dtq9d
Jan 18 21:14:12.613: INFO: Got endpoints: latency-svc-6cf55 [751.510454ms]
Jan 18 21:14:12.635: INFO: Created: latency-svc-mxgjp
Jan 18 21:14:12.668: INFO: Got endpoints: latency-svc-s9hbd [756.166203ms]
Jan 18 21:14:12.692: INFO: Created: latency-svc-m9fmc
Jan 18 21:14:12.721: INFO: Got endpoints: latency-svc-hl5t5 [759.583739ms]
Jan 18 21:14:12.748: INFO: Created: latency-svc-nqbp8
Jan 18 21:14:12.763: INFO: Got endpoints: latency-svc-chk29 [752.295262ms]
Jan 18 21:14:12.789: INFO: Created: latency-svc-28bm6
Jan 18 21:14:12.811: INFO: Got endpoints: latency-svc-52tbc [743.502168ms]
Jan 18 21:14:12.833: INFO: Created: latency-svc-fqpkn
Jan 18 21:14:12.865: INFO: Got endpoints: latency-svc-45h5v [752.08296ms]
Jan 18 21:14:12.902: INFO: Created: latency-svc-wcsm6
Jan 18 21:14:12.922: INFO: Got endpoints: latency-svc-bgh6c [750.619944ms]
Jan 18 21:14:12.950: INFO: Created: latency-svc-7ggts
Jan 18 21:14:12.964: INFO: Got endpoints: latency-svc-6l2sv [753.665377ms]
Jan 18 21:14:12.995: INFO: Created: latency-svc-8hw7v
Jan 18 21:14:13.023: INFO: Got endpoints: latency-svc-sj25h [751.948758ms]
Jan 18 21:14:13.048: INFO: Created: latency-svc-ld75m
Jan 18 21:14:13.085: INFO: Got endpoints: latency-svc-hj87p [768.325134ms]
Jan 18 21:14:13.106: INFO: Created: latency-svc-cfnzb
Jan 18 21:14:13.113: INFO: Got endpoints: latency-svc-pmgbr [752.917469ms]
Jan 18 21:14:13.133: INFO: Created: latency-svc-jzs99
Jan 18 21:14:13.161: INFO: Got endpoints: latency-svc-mdj4s [742.757159ms]
Jan 18 21:14:13.183: INFO: Created: latency-svc-7bhbp
Jan 18 21:14:13.211: INFO: Got endpoints: latency-svc-6nskx [749.37103ms]
Jan 18 21:14:13.232: INFO: Created: latency-svc-97sxz
Jan 18 21:14:13.261: INFO: Got endpoints: latency-svc-swwvk [737.671505ms]
Jan 18 21:14:13.282: INFO: Created: latency-svc-ttq6q
Jan 18 21:14:13.311: INFO: Got endpoints: latency-svc-dtq9d [748.070417ms]
Jan 18 21:14:13.331: INFO: Created: latency-svc-8xfk8
Jan 18 21:14:13.378: INFO: Got endpoints: latency-svc-mxgjp [765.436103ms]
Jan 18 21:14:13.402: INFO: Created: latency-svc-9lvjm
Jan 18 21:14:13.416: INFO: Got endpoints: latency-svc-m9fmc [747.922215ms]
Jan 18 21:14:13.442: INFO: Created: latency-svc-hlcvk
Jan 18 21:14:13.463: INFO: Got endpoints: latency-svc-nqbp8 [741.743849ms]
Jan 18 21:14:13.484: INFO: Created: latency-svc-6p7st
Jan 18 21:14:13.512: INFO: Got endpoints: latency-svc-28bm6 [748.952726ms]
Jan 18 21:14:13.533: INFO: Created: latency-svc-5prdn
Jan 18 21:14:13.562: INFO: Got endpoints: latency-svc-fqpkn [750.949847ms]
Jan 18 21:14:13.582: INFO: Created: latency-svc-f7cbc
Jan 18 21:14:13.618: INFO: Got endpoints: latency-svc-wcsm6 [753.445774ms]
Jan 18 21:14:13.652: INFO: Created: latency-svc-qvknc
Jan 18 21:14:13.663: INFO: Got endpoints: latency-svc-7ggts [741.224443ms]
Jan 18 21:14:13.689: INFO: Created: latency-svc-ps7l9
Jan 18 21:14:13.711: INFO: Got endpoints: latency-svc-8hw7v [747.39011ms]
Jan 18 21:14:13.736: INFO: Created: latency-svc-85f26
Jan 18 21:14:13.769: INFO: Got endpoints: latency-svc-ld75m [745.827093ms]
Jan 18 21:14:13.790: INFO: Created: latency-svc-nk24l
Jan 18 21:14:13.811: INFO: Got endpoints: latency-svc-cfnzb [726.117981ms]
Jan 18 21:14:13.835: INFO: Created: latency-svc-qv5pd
Jan 18 21:14:13.863: INFO: Got endpoints: latency-svc-jzs99 [749.750235ms]
Jan 18 21:14:13.888: INFO: Created: latency-svc-5b9g4
Jan 18 21:14:13.914: INFO: Got endpoints: latency-svc-7bhbp [752.851668ms]
Jan 18 21:14:13.937: INFO: Created: latency-svc-srpnh
Jan 18 21:14:13.968: INFO: Got endpoints: latency-svc-97sxz [756.522407ms]
Jan 18 21:14:13.990: INFO: Created: latency-svc-bv4x7
Jan 18 21:14:14.017: INFO: Got endpoints: latency-svc-ttq6q [756.224904ms]
Jan 18 21:14:14.040: INFO: Created: latency-svc-dm9fn
Jan 18 21:14:14.086: INFO: Got endpoints: latency-svc-8xfk8 [774.243597ms]
Jan 18 21:14:14.117: INFO: Got endpoints: latency-svc-9lvjm [739.173921ms]
Jan 18 21:14:14.128: INFO: Created: latency-svc-p2kjc
Jan 18 21:14:14.150: INFO: Created: latency-svc-sdzkg
Jan 18 21:14:14.168: INFO: Got endpoints: latency-svc-hlcvk [751.384952ms]
Jan 18 21:14:14.201: INFO: Created: latency-svc-p2hvd
Jan 18 21:14:14.213: INFO: Got endpoints: latency-svc-6p7st [749.610633ms]
Jan 18 21:14:14.240: INFO: Created: latency-svc-wmsgq
Jan 18 21:14:14.279: INFO: Got endpoints: latency-svc-5prdn [766.917418ms]
Jan 18 21:14:14.307: INFO: Created: latency-svc-l4qtt
Jan 18 21:14:14.318: INFO: Got endpoints: latency-svc-f7cbc [756.686309ms]
Jan 18 21:14:14.345: INFO: Created: latency-svc-mdfm9
Jan 18 21:14:14.361: INFO: Got endpoints: latency-svc-qvknc [742.729159ms]
Jan 18 21:14:14.411: INFO: Got endpoints: latency-svc-ps7l9 [748.36732ms]
Jan 18 21:14:14.462: INFO: Got endpoints: latency-svc-85f26 [750.24334ms]
Jan 18 21:14:14.519: INFO: Got endpoints: latency-svc-nk24l [749.388331ms]
Jan 18 21:14:14.568: INFO: Got endpoints: latency-svc-qv5pd [756.967511ms]
Jan 18 21:14:14.623: INFO: Got endpoints: latency-svc-5b9g4 [759.949844ms]
Jan 18 21:14:14.662: INFO: Got endpoints: latency-svc-srpnh [748.514722ms]
Jan 18 21:14:14.713: INFO: Got endpoints: latency-svc-bv4x7 [744.883182ms]
Jan 18 21:14:14.767: INFO: Got endpoints: latency-svc-dm9fn [750.33074ms]
Jan 18 21:14:14.812: INFO: Got endpoints: latency-svc-p2kjc [726.098981ms]
Jan 18 21:14:14.861: INFO: Got endpoints: latency-svc-sdzkg [743.766271ms]
Jan 18 21:14:14.913: INFO: Got endpoints: latency-svc-p2hvd [745.677291ms]
Jan 18 21:14:14.967: INFO: Got endpoints: latency-svc-wmsgq [754.02378ms]
Jan 18 21:14:15.013: INFO: Got endpoints: latency-svc-l4qtt [733.863964ms]
Jan 18 21:14:15.071: INFO: Got endpoints: latency-svc-mdfm9 [752.214879ms]
Jan 18 21:14:15.071: INFO: Latencies: [54.385909ms 73.058083ms 101.787153ms 121.605739ms 158.532084ms 176.24675ms 202.155192ms 234.504496ms 256.3763ms 283.238452ms 295.659368ms 303.420052ms 308.125602ms 314.032666ms 315.633354ms 331.360051ms 337.596861ms 371.988387ms 380.077958ms 385.871335ms 399.803643ms 406.742159ms 407.913571ms 410.756801ms 410.956647ms 412.386119ms 413.32183ms 414.246239ms 419.732098ms 419.81923ms 422.477755ms 423.157635ms 423.60174ms 434.254206ms 434.395058ms 435.648379ms 438.052694ms 442.32674ms 445.274199ms 446.411684ms 446.785988ms 448.225003ms 449.209014ms 449.944622ms 450.539329ms 451.800258ms 452.448048ms 453.230757ms 455.229815ms 457.182161ms 459.681398ms 459.761833ms 462.782259ms 464.69848ms 465.441388ms 468.758424ms 468.916544ms 470.274039ms 473.533474ms 473.565178ms 474.453584ms 474.754287ms 475.8919ms 476.075881ms 477.579718ms 479.398667ms 481.501506ms 484.26339ms 484.819796ms 485.637604ms 487.050619ms 488.05213ms 488.097031ms 488.440434ms 489.014241ms 491.951772ms 492.143174ms 496.61252ms 497.012426ms 500.350918ms 505.497617ms 505.76322ms 511.738384ms 512.354191ms 513.835607ms 532.024501ms 533.76522ms 556.11936ms 558.312483ms 568.042687ms 576.383577ms 585.223872ms 597.034898ms 597.647305ms 603.416167ms 612.17426ms 645.076813ms 645.331616ms 649.198857ms 652.327791ms 663.234308ms 663.294408ms 678.806775ms 702.67483ms 707.647684ms 720.250118ms 726.098981ms 726.117981ms 733.863964ms 735.035777ms 736.213989ms 736.859596ms 737.671505ms 737.961908ms 738.553515ms 739.173921ms 741.224443ms 741.743849ms 741.751849ms 742.017352ms 742.351155ms 742.729159ms 742.74566ms 742.757159ms 743.082063ms 743.502168ms 743.766271ms 744.223575ms 744.883182ms 745.168086ms 745.498089ms 745.677291ms 745.813092ms 745.827093ms 745.970994ms 746.923104ms 747.39011ms 747.709813ms 747.922215ms 748.070417ms 748.36732ms 748.514722ms 748.663123ms 748.952726ms 749.334531ms 749.37103ms 749.388331ms 749.428932ms 749.610633ms 749.750235ms 750.22164ms 750.24334ms 750.33074ms 750.619944ms 750.949847ms 751.384952ms 751.510454ms 751.948758ms 752.05206ms 752.08296ms 752.214879ms 752.295262ms 752.485364ms 752.709667ms 752.851668ms 752.917469ms 753.445774ms 753.665377ms 753.780378ms 754.02378ms 754.137682ms 756.166203ms 756.224904ms 756.522407ms 756.686309ms 756.967511ms 758.096124ms 759.583739ms 759.64944ms 759.949844ms 762.44767ms 765.436103ms 766.917418ms 768.325134ms 774.243597ms 839.589997ms 862.453943ms 893.466275ms 916.629623ms 955.158335ms 991.112221ms 1.029748935s 1.064483007s 1.091680099s 1.133919352s 1.157713907s 1.164682081s 1.239924788s 1.245688949s 1.251762314s]
Jan 18 21:14:15.071: INFO: 50 %ile: 663.234308ms
Jan 18 21:14:15.071: INFO: 90 %ile: 762.44767ms
Jan 18 21:14:15.071: INFO: 99 %ile: 1.245688949s
Jan 18 21:14:15.071: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:15.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-2123" for this suite. 01/18/23 21:14:15.082
------------------------------
• [SLOW TEST] [11.836 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:03.262
    Jan 18 21:14:03.262: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svc-latency 01/18/23 21:14:03.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:03.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:03.303
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 18 21:14:03.306: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2123 01/18/23 21:14:03.307
    I0118 21:14:03.321689      20 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2123, replica count: 1
    I0118 21:14:04.373335      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:14:05.373550      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:14:06.373720      20 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:14:06.499: INFO: Created: latency-svc-njvkw
    Jan 18 21:14:06.516: INFO: Got endpoints: latency-svc-njvkw [42.448998ms]
    Jan 18 21:14:06.543: INFO: Created: latency-svc-ck5g7
    Jan 18 21:14:06.571: INFO: Got endpoints: latency-svc-ck5g7 [54.385909ms]
    Jan 18 21:14:06.576: INFO: Created: latency-svc-nhssd
    Jan 18 21:14:06.590: INFO: Got endpoints: latency-svc-nhssd [73.058083ms]
    Jan 18 21:14:06.600: INFO: Created: latency-svc-fbgdp
    Jan 18 21:14:06.619: INFO: Got endpoints: latency-svc-fbgdp [101.787153ms]
    Jan 18 21:14:06.624: INFO: Created: latency-svc-k78t2
    Jan 18 21:14:06.638: INFO: Got endpoints: latency-svc-k78t2 [121.605739ms]
    Jan 18 21:14:06.651: INFO: Created: latency-svc-v7k7t
    Jan 18 21:14:06.675: INFO: Got endpoints: latency-svc-v7k7t [158.532084ms]
    Jan 18 21:14:06.680: INFO: Created: latency-svc-zchpr
    Jan 18 21:14:06.693: INFO: Got endpoints: latency-svc-zchpr [176.24675ms]
    Jan 18 21:14:06.704: INFO: Created: latency-svc-zn8tz
    Jan 18 21:14:06.719: INFO: Got endpoints: latency-svc-zn8tz [202.155192ms]
    Jan 18 21:14:06.730: INFO: Created: latency-svc-gt8x6
    Jan 18 21:14:06.751: INFO: Got endpoints: latency-svc-gt8x6 [234.504496ms]
    Jan 18 21:14:06.756: INFO: Created: latency-svc-l6rlh
    Jan 18 21:14:06.773: INFO: Got endpoints: latency-svc-l6rlh [256.3763ms]
    Jan 18 21:14:06.786: INFO: Created: latency-svc-dzcdr
    Jan 18 21:14:06.800: INFO: Got endpoints: latency-svc-dzcdr [283.238452ms]
    Jan 18 21:14:06.812: INFO: Created: latency-svc-rcb57
    Jan 18 21:14:06.833: INFO: Got endpoints: latency-svc-rcb57 [315.633354ms]
    Jan 18 21:14:06.837: INFO: Created: latency-svc-cv4kz
    Jan 18 21:14:06.855: INFO: Got endpoints: latency-svc-cv4kz [337.596861ms]
    Jan 18 21:14:06.866: INFO: Created: latency-svc-xhjbk
    Jan 18 21:14:06.897: INFO: Got endpoints: latency-svc-xhjbk [380.077958ms]
    Jan 18 21:14:06.902: INFO: Created: latency-svc-rqwwd
    Jan 18 21:14:06.916: INFO: Got endpoints: latency-svc-rqwwd [399.803643ms]
    Jan 18 21:14:06.928: INFO: Created: latency-svc-8rnlt
    Jan 18 21:14:06.953: INFO: Got endpoints: latency-svc-8rnlt [435.648379ms]
    Jan 18 21:14:06.962: INFO: Created: latency-svc-t7rq5
    Jan 18 21:14:06.982: INFO: Got endpoints: latency-svc-t7rq5 [410.956647ms]
    Jan 18 21:14:06.991: INFO: Created: latency-svc-kn8lw
    Jan 18 21:14:07.010: INFO: Got endpoints: latency-svc-kn8lw [419.81923ms]
    Jan 18 21:14:07.022: INFO: Created: latency-svc-vnh2v
    Jan 18 21:14:07.041: INFO: Got endpoints: latency-svc-vnh2v [422.477755ms]
    Jan 18 21:14:07.046: INFO: Created: latency-svc-2hj5k
    Jan 18 21:14:07.073: INFO: Got endpoints: latency-svc-2hj5k [434.254206ms]
    Jan 18 21:14:07.087: INFO: Created: latency-svc-h2mr2
    Jan 18 21:14:07.110: INFO: Got endpoints: latency-svc-h2mr2 [434.395058ms]
    Jan 18 21:14:07.123: INFO: Created: latency-svc-2lcj4
    Jan 18 21:14:07.138: INFO: Got endpoints: latency-svc-2lcj4 [445.274199ms]
    Jan 18 21:14:07.149: INFO: Created: latency-svc-p2sr4
    Jan 18 21:14:07.176: INFO: Got endpoints: latency-svc-p2sr4 [457.182161ms]
    Jan 18 21:14:07.187: INFO: Created: latency-svc-87pr8
    Jan 18 21:14:07.211: INFO: Got endpoints: latency-svc-87pr8 [459.761833ms]
    Jan 18 21:14:07.216: INFO: Created: latency-svc-q88hx
    Jan 18 21:14:07.229: INFO: Got endpoints: latency-svc-q88hx [455.229815ms]
    Jan 18 21:14:07.241: INFO: Created: latency-svc-rqwrb
    Jan 18 21:14:07.260: INFO: Got endpoints: latency-svc-rqwrb [459.681398ms]
    Jan 18 21:14:07.268: INFO: Created: latency-svc-pw52p
    Jan 18 21:14:07.285: INFO: Got endpoints: latency-svc-pw52p [451.800258ms]
    Jan 18 21:14:07.306: INFO: Created: latency-svc-lwd9h
    Jan 18 21:14:07.336: INFO: Got endpoints: latency-svc-lwd9h [481.501506ms]
    Jan 18 21:14:07.342: INFO: Created: latency-svc-27mtm
    Jan 18 21:14:07.371: INFO: Got endpoints: latency-svc-27mtm [473.565178ms]
    Jan 18 21:14:07.379: INFO: Created: latency-svc-z2tpr
    Jan 18 21:14:07.396: INFO: Got endpoints: latency-svc-z2tpr [479.398667ms]
    Jan 18 21:14:07.409: INFO: Created: latency-svc-zbkwx
    Jan 18 21:14:07.429: INFO: Got endpoints: latency-svc-zbkwx [476.075881ms]
    Jan 18 21:14:07.434: INFO: Created: latency-svc-ndcjf
    Jan 18 21:14:07.451: INFO: Got endpoints: latency-svc-ndcjf [468.916544ms]
    Jan 18 21:14:07.467: INFO: Created: latency-svc-hcv68
    Jan 18 21:14:07.510: INFO: Got endpoints: latency-svc-hcv68 [500.350918ms]
    Jan 18 21:14:07.516: INFO: Created: latency-svc-62jrb
    Jan 18 21:14:07.538: INFO: Got endpoints: latency-svc-62jrb [496.61252ms]
    Jan 18 21:14:07.545: INFO: Created: latency-svc-mhzk6
    Jan 18 21:14:07.565: INFO: Got endpoints: latency-svc-mhzk6 [491.951772ms]
    Jan 18 21:14:07.578: INFO: Created: latency-svc-xqk8t
    Jan 18 21:14:07.595: INFO: Got endpoints: latency-svc-xqk8t [484.819796ms]
    Jan 18 21:14:07.607: INFO: Created: latency-svc-h9s2d
    Jan 18 21:14:07.623: INFO: Got endpoints: latency-svc-h9s2d [484.26339ms]
    Jan 18 21:14:07.633: INFO: Created: latency-svc-j58sb
    Jan 18 21:14:07.647: INFO: Got endpoints: latency-svc-j58sb [470.274039ms]
    Jan 18 21:14:07.660: INFO: Created: latency-svc-wj5p6
    Jan 18 21:14:07.674: INFO: Got endpoints: latency-svc-wj5p6 [462.782259ms]
    Jan 18 21:14:07.687: INFO: Created: latency-svc-mrzqd
    Jan 18 21:14:07.702: INFO: Got endpoints: latency-svc-mrzqd [473.533474ms]
    Jan 18 21:14:07.714: INFO: Created: latency-svc-rdqpp
    Jan 18 21:14:07.729: INFO: Got endpoints: latency-svc-rdqpp [468.758424ms]
    Jan 18 21:14:07.740: INFO: Created: latency-svc-cqldz
    Jan 18 21:14:07.759: INFO: Got endpoints: latency-svc-cqldz [474.754287ms]
    Jan 18 21:14:07.769: INFO: Created: latency-svc-smlww
    Jan 18 21:14:07.787: INFO: Got endpoints: latency-svc-smlww [450.539329ms]
    Jan 18 21:14:07.803: INFO: Created: latency-svc-f2f97
    Jan 18 21:14:07.817: INFO: Got endpoints: latency-svc-f2f97 [446.411684ms]
    Jan 18 21:14:07.829: INFO: Created: latency-svc-dbzbm
    Jan 18 21:14:07.844: INFO: Got endpoints: latency-svc-dbzbm [448.225003ms]
    Jan 18 21:14:07.855: INFO: Created: latency-svc-vfc2l
    Jan 18 21:14:07.867: INFO: Got endpoints: latency-svc-vfc2l [438.052694ms]
    Jan 18 21:14:07.879: INFO: Created: latency-svc-8wxmr
    Jan 18 21:14:07.898: INFO: Got endpoints: latency-svc-8wxmr [446.785988ms]
    Jan 18 21:14:07.910: INFO: Created: latency-svc-9hdwd
    Jan 18 21:14:07.923: INFO: Got endpoints: latency-svc-9hdwd [413.32183ms]
    Jan 18 21:14:07.936: INFO: Created: latency-svc-npsnp
    Jan 18 21:14:07.958: INFO: Got endpoints: latency-svc-npsnp [419.732098ms]
    Jan 18 21:14:07.962: INFO: Created: latency-svc-4pf8m
    Jan 18 21:14:07.977: INFO: Got endpoints: latency-svc-4pf8m [412.386119ms]
    Jan 18 21:14:07.988: INFO: Created: latency-svc-5j79p
    Jan 18 21:14:08.002: INFO: Got endpoints: latency-svc-5j79p [407.913571ms]
    Jan 18 21:14:08.013: INFO: Created: latency-svc-vvwlf
    Jan 18 21:14:08.030: INFO: Got endpoints: latency-svc-vvwlf [406.742159ms]
    Jan 18 21:14:08.040: INFO: Created: latency-svc-t852p
    Jan 18 21:14:08.057: INFO: Got endpoints: latency-svc-t852p [410.756801ms]
    Jan 18 21:14:08.068: INFO: Created: latency-svc-2rt7b
    Jan 18 21:14:08.088: INFO: Got endpoints: latency-svc-2rt7b [414.246239ms]
    Jan 18 21:14:08.102: INFO: Created: latency-svc-krdqf
    Jan 18 21:14:08.125: INFO: Got endpoints: latency-svc-krdqf [423.157635ms]
    Jan 18 21:14:08.135: INFO: Created: latency-svc-69qpl
    Jan 18 21:14:08.152: INFO: Got endpoints: latency-svc-69qpl [423.60174ms]
    Jan 18 21:14:08.179: INFO: Created: latency-svc-pkn9l
    Jan 18 21:14:08.209: INFO: Got endpoints: latency-svc-pkn9l [449.209014ms]
    Jan 18 21:14:08.220: INFO: Created: latency-svc-dvbqt
    Jan 18 21:14:08.240: INFO: Got endpoints: latency-svc-dvbqt [453.230757ms]
    Jan 18 21:14:08.247: INFO: Created: latency-svc-ggb56
    Jan 18 21:14:08.267: INFO: Got endpoints: latency-svc-ggb56 [449.944622ms]
    Jan 18 21:14:08.279: INFO: Created: latency-svc-bc52v
    Jan 18 21:14:08.296: INFO: Got endpoints: latency-svc-bc52v [452.448048ms]
    Jan 18 21:14:08.314: INFO: Created: latency-svc-7ptfp
    Jan 18 21:14:08.332: INFO: Got endpoints: latency-svc-7ptfp [464.69848ms]
    Jan 18 21:14:08.345: INFO: Created: latency-svc-rt6pc
    Jan 18 21:14:08.363: INFO: Got endpoints: latency-svc-rt6pc [465.441388ms]
    Jan 18 21:14:08.376: INFO: Created: latency-svc-nb9vr
    Jan 18 21:14:08.398: INFO: Got endpoints: latency-svc-nb9vr [474.453584ms]
    Jan 18 21:14:08.408: INFO: Created: latency-svc-fs9df
    Jan 18 21:14:08.436: INFO: Got endpoints: latency-svc-fs9df [477.579718ms]
    Jan 18 21:14:08.440: INFO: Created: latency-svc-pzxqs
    Jan 18 21:14:08.465: INFO: Got endpoints: latency-svc-pzxqs [488.05213ms]
    Jan 18 21:14:08.470: INFO: Created: latency-svc-xwltw
    Jan 18 21:14:08.495: INFO: Got endpoints: latency-svc-xwltw [492.143174ms]
    Jan 18 21:14:08.506: INFO: Created: latency-svc-7vfpl
    Jan 18 21:14:08.527: INFO: Got endpoints: latency-svc-7vfpl [497.012426ms]
    Jan 18 21:14:08.546: INFO: Created: latency-svc-49pdz
    Jan 18 21:14:08.569: INFO: Got endpoints: latency-svc-49pdz [511.738384ms]
    Jan 18 21:14:08.584: INFO: Created: latency-svc-9zt2b
    Jan 18 21:14:08.602: INFO: Got endpoints: latency-svc-9zt2b [513.835607ms]
    Jan 18 21:14:08.614: INFO: Created: latency-svc-gjmfh
    Jan 18 21:14:08.631: INFO: Got endpoints: latency-svc-gjmfh [505.76322ms]
    Jan 18 21:14:08.647: INFO: Created: latency-svc-49fdv
    Jan 18 21:14:08.665: INFO: Got endpoints: latency-svc-49fdv [512.354191ms]
    Jan 18 21:14:08.679: INFO: Created: latency-svc-jlnxq
    Jan 18 21:14:08.697: INFO: Got endpoints: latency-svc-jlnxq [488.097031ms]
    Jan 18 21:14:08.709: INFO: Created: latency-svc-lptv4
    Jan 18 21:14:08.729: INFO: Got endpoints: latency-svc-lptv4 [488.440434ms]
    Jan 18 21:14:08.742: INFO: Created: latency-svc-2hqz6
    Jan 18 21:14:08.756: INFO: Got endpoints: latency-svc-2hqz6 [489.014241ms]
    Jan 18 21:14:08.767: INFO: Created: latency-svc-6g5jt
    Jan 18 21:14:08.782: INFO: Got endpoints: latency-svc-6g5jt [485.637604ms]
    Jan 18 21:14:08.801: INFO: Created: latency-svc-5bdqt
    Jan 18 21:14:08.819: INFO: Got endpoints: latency-svc-5bdqt [487.050619ms]
    Jan 18 21:14:08.832: INFO: Created: latency-svc-bnkq6
    Jan 18 21:14:08.890: INFO: Created: latency-svc-7fn6c
    Jan 18 21:14:08.919: INFO: Got endpoints: latency-svc-bnkq6 [556.11936ms]
    Jan 18 21:14:08.932: INFO: Got endpoints: latency-svc-7fn6c [533.76522ms]
    Jan 18 21:14:08.944: INFO: Created: latency-svc-c5rwn
    Jan 18 21:14:08.968: INFO: Got endpoints: latency-svc-c5rwn [532.024501ms]
    Jan 18 21:14:08.974: INFO: Created: latency-svc-86kvt
    Jan 18 21:14:09.000: INFO: Created: latency-svc-c5dk5
    Jan 18 21:14:09.033: INFO: Got endpoints: latency-svc-86kvt [568.042687ms]
    Jan 18 21:14:09.038: INFO: Created: latency-svc-cpcs7
    Jan 18 21:14:09.055: INFO: Created: latency-svc-49smq
    Jan 18 21:14:09.071: INFO: Got endpoints: latency-svc-c5dk5 [576.383577ms]
    Jan 18 21:14:09.102: INFO: Created: latency-svc-hlg4d
    Jan 18 21:14:09.110: INFO: Created: latency-svc-h7fwj
    Jan 18 21:14:09.124: INFO: Got endpoints: latency-svc-cpcs7 [597.034898ms]
    Jan 18 21:14:09.135: INFO: Created: latency-svc-s4frd
    Jan 18 21:14:09.154: INFO: Created: latency-svc-czlnr
    Jan 18 21:14:09.168: INFO: Got endpoints: latency-svc-49smq [597.647305ms]
    Jan 18 21:14:09.179: INFO: Created: latency-svc-zp8c2
    Jan 18 21:14:09.197: INFO: Created: latency-svc-tgs7j
    Jan 18 21:14:09.214: INFO: Got endpoints: latency-svc-hlg4d [612.17426ms]
    Jan 18 21:14:09.225: INFO: Created: latency-svc-djj67
    Jan 18 21:14:09.242: INFO: Created: latency-svc-shw87
    Jan 18 21:14:09.263: INFO: Created: latency-svc-scsgg
    Jan 18 21:14:09.280: INFO: Got endpoints: latency-svc-h7fwj [649.198857ms]
    Jan 18 21:14:09.286: INFO: Created: latency-svc-6jsnn
    Jan 18 21:14:09.304: INFO: Created: latency-svc-rzdmq
    Jan 18 21:14:09.317: INFO: Got endpoints: latency-svc-s4frd [652.327791ms]
    Jan 18 21:14:09.331: INFO: Created: latency-svc-mxllm
    Jan 18 21:14:09.347: INFO: Created: latency-svc-r95pk
    Jan 18 21:14:09.360: INFO: Got endpoints: latency-svc-czlnr [663.234308ms]
    Jan 18 21:14:09.374: INFO: Created: latency-svc-8h78r
    Jan 18 21:14:09.392: INFO: Created: latency-svc-h4xfl
    Jan 18 21:14:09.427: INFO: Created: latency-svc-zg88j
    Jan 18 21:14:09.436: INFO: Got endpoints: latency-svc-zp8c2 [707.647684ms]
    Jan 18 21:14:09.464: INFO: Created: latency-svc-7hbzq
    Jan 18 21:14:10.008: INFO: Got endpoints: latency-svc-tgs7j [1.251762314s]
    Jan 18 21:14:10.028: INFO: Got endpoints: latency-svc-djj67 [1.245688949s]
    Jan 18 21:14:10.059: INFO: Got endpoints: latency-svc-shw87 [1.239924788s]
    Jan 18 21:14:10.084: INFO: Got endpoints: latency-svc-scsgg [1.164682081s]
    Jan 18 21:14:10.089: INFO: Got endpoints: latency-svc-6jsnn [1.157713907s]
    Jan 18 21:14:10.102: INFO: Got endpoints: latency-svc-rzdmq [1.133919352s]
    Jan 18 21:14:10.112: INFO: Created: latency-svc-94zd5
    Jan 18 21:14:10.125: INFO: Got endpoints: latency-svc-mxllm [1.091680099s]
    Jan 18 21:14:10.136: INFO: Got endpoints: latency-svc-r95pk [1.064483007s]
    Jan 18 21:14:10.154: INFO: Got endpoints: latency-svc-8h78r [1.029748935s]
    Jan 18 21:14:10.159: INFO: Got endpoints: latency-svc-h4xfl [991.112221ms]
    Jan 18 21:14:10.169: INFO: Got endpoints: latency-svc-zg88j [955.158335ms]
    Jan 18 21:14:10.181: INFO: Created: latency-svc-v2s28
    Jan 18 21:14:10.197: INFO: Got endpoints: latency-svc-7hbzq [916.629623ms]
    Jan 18 21:14:10.211: INFO: Got endpoints: latency-svc-94zd5 [893.466275ms]
    Jan 18 21:14:10.222: INFO: Got endpoints: latency-svc-v2s28 [862.453943ms]
    Jan 18 21:14:10.255: INFO: Created: latency-svc-8rhtb
    Jan 18 21:14:10.276: INFO: Got endpoints: latency-svc-8rhtb [839.589997ms]
    Jan 18 21:14:10.290: INFO: Created: latency-svc-58m6s
    Jan 18 21:14:10.304: INFO: Got endpoints: latency-svc-58m6s [295.659368ms]
    Jan 18 21:14:10.315: INFO: Created: latency-svc-lr4x8
    Jan 18 21:14:10.336: INFO: Got endpoints: latency-svc-lr4x8 [308.125602ms]
    Jan 18 21:14:10.347: INFO: Created: latency-svc-nbvz2
    Jan 18 21:14:10.362: INFO: Got endpoints: latency-svc-nbvz2 [303.420052ms]
    Jan 18 21:14:10.375: INFO: Created: latency-svc-jpj5l
    Jan 18 21:14:10.398: INFO: Got endpoints: latency-svc-jpj5l [314.032666ms]
    Jan 18 21:14:10.407: INFO: Created: latency-svc-46ctm
    Jan 18 21:14:10.421: INFO: Got endpoints: latency-svc-46ctm [331.360051ms]
    Jan 18 21:14:10.433: INFO: Created: latency-svc-82fdp
    Jan 18 21:14:10.448: INFO: Created: latency-svc-tnscv
    Jan 18 21:14:10.474: INFO: Got endpoints: latency-svc-82fdp [371.988387ms]
    Jan 18 21:14:10.479: INFO: Created: latency-svc-ndbdw
    Jan 18 21:14:10.497: INFO: Created: latency-svc-5pxmm
    Jan 18 21:14:10.511: INFO: Got endpoints: latency-svc-tnscv [385.871335ms]
    Jan 18 21:14:10.522: INFO: Created: latency-svc-fv86d
    Jan 18 21:14:10.538: INFO: Created: latency-svc-2b6nb
    Jan 18 21:14:10.563: INFO: Created: latency-svc-df4g8
    Jan 18 21:14:10.578: INFO: Got endpoints: latency-svc-ndbdw [442.32674ms]
    Jan 18 21:14:10.595: INFO: Created: latency-svc-2g9sm
    Jan 18 21:14:10.615: INFO: Created: latency-svc-7vsw8
    Jan 18 21:14:10.630: INFO: Got endpoints: latency-svc-5pxmm [475.8919ms]
    Jan 18 21:14:10.643: INFO: Created: latency-svc-xc8cm
    Jan 18 21:14:10.664: INFO: Got endpoints: latency-svc-fv86d [505.497617ms]
    Jan 18 21:14:10.681: INFO: Created: latency-svc-2tj5f
    Jan 18 21:14:10.692: INFO: Created: latency-svc-q9ntz
    Jan 18 21:14:10.713: INFO: Created: latency-svc-fbrv6
    Jan 18 21:14:10.728: INFO: Got endpoints: latency-svc-2b6nb [558.312483ms]
    Jan 18 21:14:10.739: INFO: Created: latency-svc-wrj4g
    Jan 18 21:14:10.759: INFO: Created: latency-svc-wdsjq
    Jan 18 21:14:10.782: INFO: Got endpoints: latency-svc-df4g8 [585.223872ms]
    Jan 18 21:14:10.793: INFO: Created: latency-svc-bglrq
    Jan 18 21:14:10.814: INFO: Got endpoints: latency-svc-2g9sm [603.416167ms]
    Jan 18 21:14:10.819: INFO: Created: latency-svc-9k4g2
    Jan 18 21:14:10.846: INFO: Created: latency-svc-tgw7s
    Jan 18 21:14:10.861: INFO: Created: latency-svc-whggq
    Jan 18 21:14:10.868: INFO: Got endpoints: latency-svc-7vsw8 [645.331616ms]
    Jan 18 21:14:10.887: INFO: Created: latency-svc-j7xxr
    Jan 18 21:14:10.905: INFO: Created: latency-svc-48gcn
    Jan 18 21:14:10.921: INFO: Got endpoints: latency-svc-xc8cm [645.076813ms]
    Jan 18 21:14:10.933: INFO: Created: latency-svc-m7ws7
    Jan 18 21:14:10.950: INFO: Created: latency-svc-gdh7f
    Jan 18 21:14:10.967: INFO: Got endpoints: latency-svc-2tj5f [663.294408ms]
    Jan 18 21:14:10.983: INFO: Created: latency-svc-zhhrb
    Jan 18 21:14:11.000: INFO: Created: latency-svc-p4ffw
    Jan 18 21:14:11.015: INFO: Got endpoints: latency-svc-q9ntz [678.806775ms]
    Jan 18 21:14:11.028: INFO: Created: latency-svc-88bw5
    Jan 18 21:14:11.042: INFO: Created: latency-svc-xkcqq
    Jan 18 21:14:11.065: INFO: Got endpoints: latency-svc-fbrv6 [702.67483ms]
    Jan 18 21:14:11.087: INFO: Created: latency-svc-9gvcq
    Jan 18 21:14:11.141: INFO: Got endpoints: latency-svc-wrj4g [742.351155ms]
    Jan 18 21:14:11.170: INFO: Got endpoints: latency-svc-wdsjq [749.334531ms]
    Jan 18 21:14:11.192: INFO: Created: latency-svc-p26f9
    Jan 18 21:14:11.208: INFO: Created: latency-svc-s56cs
    Jan 18 21:14:11.219: INFO: Got endpoints: latency-svc-bglrq [745.168086ms]
    Jan 18 21:14:11.243: INFO: Created: latency-svc-vsjn4
    Jan 18 21:14:11.260: INFO: Got endpoints: latency-svc-9k4g2 [749.428932ms]
    Jan 18 21:14:11.285: INFO: Created: latency-svc-gcltj
    Jan 18 21:14:11.313: INFO: Got endpoints: latency-svc-tgw7s [735.035777ms]
    Jan 18 21:14:11.334: INFO: Created: latency-svc-wx88v
    Jan 18 21:14:11.366: INFO: Got endpoints: latency-svc-whggq [736.859596ms]
    Jan 18 21:14:11.389: INFO: Created: latency-svc-7hdp4
    Jan 18 21:14:11.411: INFO: Got endpoints: latency-svc-j7xxr [746.923104ms]
    Jan 18 21:14:11.436: INFO: Created: latency-svc-b5dsp
    Jan 18 21:14:11.466: INFO: Got endpoints: latency-svc-48gcn [737.961908ms]
    Jan 18 21:14:11.488: INFO: Created: latency-svc-bq6kn
    Jan 18 21:14:11.519: INFO: Got endpoints: latency-svc-m7ws7 [736.213989ms]
    Jan 18 21:14:11.542: INFO: Created: latency-svc-fl9fm
    Jan 18 21:14:11.568: INFO: Got endpoints: latency-svc-gdh7f [754.137682ms]
    Jan 18 21:14:11.588: INFO: Created: latency-svc-b6hqh
    Jan 18 21:14:11.611: INFO: Got endpoints: latency-svc-zhhrb [743.082063ms]
    Jan 18 21:14:11.633: INFO: Created: latency-svc-npg2w
    Jan 18 21:14:11.660: INFO: Got endpoints: latency-svc-p4ffw [738.553515ms]
    Jan 18 21:14:11.681: INFO: Created: latency-svc-cmfjh
    Jan 18 21:14:11.709: INFO: Got endpoints: latency-svc-88bw5 [742.017352ms]
    Jan 18 21:14:11.729: INFO: Created: latency-svc-td969
    Jan 18 21:14:11.761: INFO: Got endpoints: latency-svc-xkcqq [745.498089ms]
    Jan 18 21:14:11.781: INFO: Created: latency-svc-bzt4d
    Jan 18 21:14:11.811: INFO: Got endpoints: latency-svc-9gvcq [745.813092ms]
    Jan 18 21:14:11.845: INFO: Created: latency-svc-5cc6k
    Jan 18 21:14:11.861: INFO: Got endpoints: latency-svc-p26f9 [720.250118ms]
    Jan 18 21:14:11.882: INFO: Created: latency-svc-6cf55
    Jan 18 21:14:11.912: INFO: Got endpoints: latency-svc-s56cs [741.751849ms]
    Jan 18 21:14:11.936: INFO: Created: latency-svc-s9hbd
    Jan 18 21:14:11.962: INFO: Got endpoints: latency-svc-vsjn4 [742.74566ms]
    Jan 18 21:14:11.985: INFO: Created: latency-svc-hl5t5
    Jan 18 21:14:12.011: INFO: Got endpoints: latency-svc-gcltj [750.22164ms]
    Jan 18 21:14:12.032: INFO: Created: latency-svc-chk29
    Jan 18 21:14:12.067: INFO: Got endpoints: latency-svc-wx88v [753.780378ms]
    Jan 18 21:14:12.096: INFO: Created: latency-svc-52tbc
    Jan 18 21:14:12.113: INFO: Got endpoints: latency-svc-7hdp4 [745.970994ms]
    Jan 18 21:14:12.140: INFO: Created: latency-svc-45h5v
    Jan 18 21:14:12.171: INFO: Got endpoints: latency-svc-b5dsp [759.64944ms]
    Jan 18 21:14:12.193: INFO: Created: latency-svc-bgh6c
    Jan 18 21:14:12.210: INFO: Got endpoints: latency-svc-bq6kn [744.223575ms]
    Jan 18 21:14:12.230: INFO: Created: latency-svc-6l2sv
    Jan 18 21:14:12.271: INFO: Got endpoints: latency-svc-fl9fm [752.485364ms]
    Jan 18 21:14:12.292: INFO: Created: latency-svc-sj25h
    Jan 18 21:14:12.316: INFO: Got endpoints: latency-svc-b6hqh [747.709813ms]
    Jan 18 21:14:12.336: INFO: Created: latency-svc-hj87p
    Jan 18 21:14:12.360: INFO: Got endpoints: latency-svc-npg2w [748.663123ms]
    Jan 18 21:14:12.381: INFO: Created: latency-svc-pmgbr
    Jan 18 21:14:12.418: INFO: Got endpoints: latency-svc-cmfjh [758.096124ms]
    Jan 18 21:14:12.439: INFO: Created: latency-svc-mdj4s
    Jan 18 21:14:12.462: INFO: Got endpoints: latency-svc-td969 [752.709667ms]
    Jan 18 21:14:12.484: INFO: Created: latency-svc-6nskx
    Jan 18 21:14:12.523: INFO: Got endpoints: latency-svc-bzt4d [762.44767ms]
    Jan 18 21:14:12.552: INFO: Created: latency-svc-swwvk
    Jan 18 21:14:12.563: INFO: Got endpoints: latency-svc-5cc6k [752.05206ms]
    Jan 18 21:14:12.585: INFO: Created: latency-svc-dtq9d
    Jan 18 21:14:12.613: INFO: Got endpoints: latency-svc-6cf55 [751.510454ms]
    Jan 18 21:14:12.635: INFO: Created: latency-svc-mxgjp
    Jan 18 21:14:12.668: INFO: Got endpoints: latency-svc-s9hbd [756.166203ms]
    Jan 18 21:14:12.692: INFO: Created: latency-svc-m9fmc
    Jan 18 21:14:12.721: INFO: Got endpoints: latency-svc-hl5t5 [759.583739ms]
    Jan 18 21:14:12.748: INFO: Created: latency-svc-nqbp8
    Jan 18 21:14:12.763: INFO: Got endpoints: latency-svc-chk29 [752.295262ms]
    Jan 18 21:14:12.789: INFO: Created: latency-svc-28bm6
    Jan 18 21:14:12.811: INFO: Got endpoints: latency-svc-52tbc [743.502168ms]
    Jan 18 21:14:12.833: INFO: Created: latency-svc-fqpkn
    Jan 18 21:14:12.865: INFO: Got endpoints: latency-svc-45h5v [752.08296ms]
    Jan 18 21:14:12.902: INFO: Created: latency-svc-wcsm6
    Jan 18 21:14:12.922: INFO: Got endpoints: latency-svc-bgh6c [750.619944ms]
    Jan 18 21:14:12.950: INFO: Created: latency-svc-7ggts
    Jan 18 21:14:12.964: INFO: Got endpoints: latency-svc-6l2sv [753.665377ms]
    Jan 18 21:14:12.995: INFO: Created: latency-svc-8hw7v
    Jan 18 21:14:13.023: INFO: Got endpoints: latency-svc-sj25h [751.948758ms]
    Jan 18 21:14:13.048: INFO: Created: latency-svc-ld75m
    Jan 18 21:14:13.085: INFO: Got endpoints: latency-svc-hj87p [768.325134ms]
    Jan 18 21:14:13.106: INFO: Created: latency-svc-cfnzb
    Jan 18 21:14:13.113: INFO: Got endpoints: latency-svc-pmgbr [752.917469ms]
    Jan 18 21:14:13.133: INFO: Created: latency-svc-jzs99
    Jan 18 21:14:13.161: INFO: Got endpoints: latency-svc-mdj4s [742.757159ms]
    Jan 18 21:14:13.183: INFO: Created: latency-svc-7bhbp
    Jan 18 21:14:13.211: INFO: Got endpoints: latency-svc-6nskx [749.37103ms]
    Jan 18 21:14:13.232: INFO: Created: latency-svc-97sxz
    Jan 18 21:14:13.261: INFO: Got endpoints: latency-svc-swwvk [737.671505ms]
    Jan 18 21:14:13.282: INFO: Created: latency-svc-ttq6q
    Jan 18 21:14:13.311: INFO: Got endpoints: latency-svc-dtq9d [748.070417ms]
    Jan 18 21:14:13.331: INFO: Created: latency-svc-8xfk8
    Jan 18 21:14:13.378: INFO: Got endpoints: latency-svc-mxgjp [765.436103ms]
    Jan 18 21:14:13.402: INFO: Created: latency-svc-9lvjm
    Jan 18 21:14:13.416: INFO: Got endpoints: latency-svc-m9fmc [747.922215ms]
    Jan 18 21:14:13.442: INFO: Created: latency-svc-hlcvk
    Jan 18 21:14:13.463: INFO: Got endpoints: latency-svc-nqbp8 [741.743849ms]
    Jan 18 21:14:13.484: INFO: Created: latency-svc-6p7st
    Jan 18 21:14:13.512: INFO: Got endpoints: latency-svc-28bm6 [748.952726ms]
    Jan 18 21:14:13.533: INFO: Created: latency-svc-5prdn
    Jan 18 21:14:13.562: INFO: Got endpoints: latency-svc-fqpkn [750.949847ms]
    Jan 18 21:14:13.582: INFO: Created: latency-svc-f7cbc
    Jan 18 21:14:13.618: INFO: Got endpoints: latency-svc-wcsm6 [753.445774ms]
    Jan 18 21:14:13.652: INFO: Created: latency-svc-qvknc
    Jan 18 21:14:13.663: INFO: Got endpoints: latency-svc-7ggts [741.224443ms]
    Jan 18 21:14:13.689: INFO: Created: latency-svc-ps7l9
    Jan 18 21:14:13.711: INFO: Got endpoints: latency-svc-8hw7v [747.39011ms]
    Jan 18 21:14:13.736: INFO: Created: latency-svc-85f26
    Jan 18 21:14:13.769: INFO: Got endpoints: latency-svc-ld75m [745.827093ms]
    Jan 18 21:14:13.790: INFO: Created: latency-svc-nk24l
    Jan 18 21:14:13.811: INFO: Got endpoints: latency-svc-cfnzb [726.117981ms]
    Jan 18 21:14:13.835: INFO: Created: latency-svc-qv5pd
    Jan 18 21:14:13.863: INFO: Got endpoints: latency-svc-jzs99 [749.750235ms]
    Jan 18 21:14:13.888: INFO: Created: latency-svc-5b9g4
    Jan 18 21:14:13.914: INFO: Got endpoints: latency-svc-7bhbp [752.851668ms]
    Jan 18 21:14:13.937: INFO: Created: latency-svc-srpnh
    Jan 18 21:14:13.968: INFO: Got endpoints: latency-svc-97sxz [756.522407ms]
    Jan 18 21:14:13.990: INFO: Created: latency-svc-bv4x7
    Jan 18 21:14:14.017: INFO: Got endpoints: latency-svc-ttq6q [756.224904ms]
    Jan 18 21:14:14.040: INFO: Created: latency-svc-dm9fn
    Jan 18 21:14:14.086: INFO: Got endpoints: latency-svc-8xfk8 [774.243597ms]
    Jan 18 21:14:14.117: INFO: Got endpoints: latency-svc-9lvjm [739.173921ms]
    Jan 18 21:14:14.128: INFO: Created: latency-svc-p2kjc
    Jan 18 21:14:14.150: INFO: Created: latency-svc-sdzkg
    Jan 18 21:14:14.168: INFO: Got endpoints: latency-svc-hlcvk [751.384952ms]
    Jan 18 21:14:14.201: INFO: Created: latency-svc-p2hvd
    Jan 18 21:14:14.213: INFO: Got endpoints: latency-svc-6p7st [749.610633ms]
    Jan 18 21:14:14.240: INFO: Created: latency-svc-wmsgq
    Jan 18 21:14:14.279: INFO: Got endpoints: latency-svc-5prdn [766.917418ms]
    Jan 18 21:14:14.307: INFO: Created: latency-svc-l4qtt
    Jan 18 21:14:14.318: INFO: Got endpoints: latency-svc-f7cbc [756.686309ms]
    Jan 18 21:14:14.345: INFO: Created: latency-svc-mdfm9
    Jan 18 21:14:14.361: INFO: Got endpoints: latency-svc-qvknc [742.729159ms]
    Jan 18 21:14:14.411: INFO: Got endpoints: latency-svc-ps7l9 [748.36732ms]
    Jan 18 21:14:14.462: INFO: Got endpoints: latency-svc-85f26 [750.24334ms]
    Jan 18 21:14:14.519: INFO: Got endpoints: latency-svc-nk24l [749.388331ms]
    Jan 18 21:14:14.568: INFO: Got endpoints: latency-svc-qv5pd [756.967511ms]
    Jan 18 21:14:14.623: INFO: Got endpoints: latency-svc-5b9g4 [759.949844ms]
    Jan 18 21:14:14.662: INFO: Got endpoints: latency-svc-srpnh [748.514722ms]
    Jan 18 21:14:14.713: INFO: Got endpoints: latency-svc-bv4x7 [744.883182ms]
    Jan 18 21:14:14.767: INFO: Got endpoints: latency-svc-dm9fn [750.33074ms]
    Jan 18 21:14:14.812: INFO: Got endpoints: latency-svc-p2kjc [726.098981ms]
    Jan 18 21:14:14.861: INFO: Got endpoints: latency-svc-sdzkg [743.766271ms]
    Jan 18 21:14:14.913: INFO: Got endpoints: latency-svc-p2hvd [745.677291ms]
    Jan 18 21:14:14.967: INFO: Got endpoints: latency-svc-wmsgq [754.02378ms]
    Jan 18 21:14:15.013: INFO: Got endpoints: latency-svc-l4qtt [733.863964ms]
    Jan 18 21:14:15.071: INFO: Got endpoints: latency-svc-mdfm9 [752.214879ms]
    Jan 18 21:14:15.071: INFO: Latencies: [54.385909ms 73.058083ms 101.787153ms 121.605739ms 158.532084ms 176.24675ms 202.155192ms 234.504496ms 256.3763ms 283.238452ms 295.659368ms 303.420052ms 308.125602ms 314.032666ms 315.633354ms 331.360051ms 337.596861ms 371.988387ms 380.077958ms 385.871335ms 399.803643ms 406.742159ms 407.913571ms 410.756801ms 410.956647ms 412.386119ms 413.32183ms 414.246239ms 419.732098ms 419.81923ms 422.477755ms 423.157635ms 423.60174ms 434.254206ms 434.395058ms 435.648379ms 438.052694ms 442.32674ms 445.274199ms 446.411684ms 446.785988ms 448.225003ms 449.209014ms 449.944622ms 450.539329ms 451.800258ms 452.448048ms 453.230757ms 455.229815ms 457.182161ms 459.681398ms 459.761833ms 462.782259ms 464.69848ms 465.441388ms 468.758424ms 468.916544ms 470.274039ms 473.533474ms 473.565178ms 474.453584ms 474.754287ms 475.8919ms 476.075881ms 477.579718ms 479.398667ms 481.501506ms 484.26339ms 484.819796ms 485.637604ms 487.050619ms 488.05213ms 488.097031ms 488.440434ms 489.014241ms 491.951772ms 492.143174ms 496.61252ms 497.012426ms 500.350918ms 505.497617ms 505.76322ms 511.738384ms 512.354191ms 513.835607ms 532.024501ms 533.76522ms 556.11936ms 558.312483ms 568.042687ms 576.383577ms 585.223872ms 597.034898ms 597.647305ms 603.416167ms 612.17426ms 645.076813ms 645.331616ms 649.198857ms 652.327791ms 663.234308ms 663.294408ms 678.806775ms 702.67483ms 707.647684ms 720.250118ms 726.098981ms 726.117981ms 733.863964ms 735.035777ms 736.213989ms 736.859596ms 737.671505ms 737.961908ms 738.553515ms 739.173921ms 741.224443ms 741.743849ms 741.751849ms 742.017352ms 742.351155ms 742.729159ms 742.74566ms 742.757159ms 743.082063ms 743.502168ms 743.766271ms 744.223575ms 744.883182ms 745.168086ms 745.498089ms 745.677291ms 745.813092ms 745.827093ms 745.970994ms 746.923104ms 747.39011ms 747.709813ms 747.922215ms 748.070417ms 748.36732ms 748.514722ms 748.663123ms 748.952726ms 749.334531ms 749.37103ms 749.388331ms 749.428932ms 749.610633ms 749.750235ms 750.22164ms 750.24334ms 750.33074ms 750.619944ms 750.949847ms 751.384952ms 751.510454ms 751.948758ms 752.05206ms 752.08296ms 752.214879ms 752.295262ms 752.485364ms 752.709667ms 752.851668ms 752.917469ms 753.445774ms 753.665377ms 753.780378ms 754.02378ms 754.137682ms 756.166203ms 756.224904ms 756.522407ms 756.686309ms 756.967511ms 758.096124ms 759.583739ms 759.64944ms 759.949844ms 762.44767ms 765.436103ms 766.917418ms 768.325134ms 774.243597ms 839.589997ms 862.453943ms 893.466275ms 916.629623ms 955.158335ms 991.112221ms 1.029748935s 1.064483007s 1.091680099s 1.133919352s 1.157713907s 1.164682081s 1.239924788s 1.245688949s 1.251762314s]
    Jan 18 21:14:15.071: INFO: 50 %ile: 663.234308ms
    Jan 18 21:14:15.071: INFO: 90 %ile: 762.44767ms
    Jan 18 21:14:15.071: INFO: 99 %ile: 1.245688949s
    Jan 18 21:14:15.071: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:15.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-2123" for this suite. 01/18/23 21:14:15.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:15.102
Jan 18 21:14:15.102: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename controllerrevisions 01/18/23 21:14:15.104
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:15.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:15.181
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-ncdcf-daemon-set" 01/18/23 21:14:15.248
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:14:15.262
Jan 18 21:14:15.280: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
Jan 18 21:14:15.280: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:14:16.292: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
Jan 18 21:14:16.292: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:14:17.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
Jan 18 21:14:17.292: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:14:18.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
Jan 18 21:14:18.291: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:14:19.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
Jan 18 21:14:19.291: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:14:20.295: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
Jan 18 21:14:20.295: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:14:21.293: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 2
Jan 18 21:14:21.293: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ncdcf-daemon-set
STEP: Confirm DaemonSet "e2e-ncdcf-daemon-set" successfully created with "daemonset-name=e2e-ncdcf-daemon-set" label 01/18/23 21:14:21.297
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ncdcf-daemon-set" 01/18/23 21:14:21.317
Jan 18 21:14:21.323: INFO: Located ControllerRevision: "e2e-ncdcf-daemon-set-858dd5d745"
STEP: Patching ControllerRevision "e2e-ncdcf-daemon-set-858dd5d745" 01/18/23 21:14:21.326
Jan 18 21:14:21.341: INFO: e2e-ncdcf-daemon-set-858dd5d745 has been patched
STEP: Create a new ControllerRevision 01/18/23 21:14:21.341
Jan 18 21:14:21.359: INFO: Created ControllerRevision: e2e-ncdcf-daemon-set-57767c4c69
STEP: Confirm that there are two ControllerRevisions 01/18/23 21:14:21.359
Jan 18 21:14:21.359: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 21:14:21.363: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-ncdcf-daemon-set-858dd5d745" 01/18/23 21:14:21.363
STEP: Confirm that there is only one ControllerRevision 01/18/23 21:14:21.389
Jan 18 21:14:21.389: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 21:14:21.394: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-ncdcf-daemon-set-57767c4c69" 01/18/23 21:14:21.398
Jan 18 21:14:21.418: INFO: e2e-ncdcf-daemon-set-57767c4c69 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 21:14:21.418
W0118 21:14:21.446887      20 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/18/23 21:14:21.447
Jan 18 21:14:21.447: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 21:14:22.451: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 21:14:22.459: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ncdcf-daemon-set-57767c4c69=updated" 01/18/23 21:14:22.459
STEP: Confirm that there is only one ControllerRevision 01/18/23 21:14:22.502
Jan 18 21:14:22.503: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 21:14:22.506: INFO: Found 1 ControllerRevisions
Jan 18 21:14:22.510: INFO: ControllerRevision "e2e-ncdcf-daemon-set-77856b944" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-ncdcf-daemon-set" 01/18/23 21:14:22.514
STEP: deleting DaemonSet.extensions e2e-ncdcf-daemon-set in namespace controllerrevisions-8936, will wait for the garbage collector to delete the pods 01/18/23 21:14:22.514
Jan 18 21:14:22.603: INFO: Deleting DaemonSet.extensions e2e-ncdcf-daemon-set took: 32.774783ms
Jan 18 21:14:22.704: INFO: Terminating DaemonSet.extensions e2e-ncdcf-daemon-set pods took: 100.420473ms
Jan 18 21:14:26.113: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
Jan 18 21:14:26.113: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ncdcf-daemon-set
Jan 18 21:14:26.122: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8566"},"items":null}

Jan 18 21:14:26.130: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8567"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:26.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-8936" for this suite. 01/18/23 21:14:26.148
------------------------------
• [SLOW TEST] [11.069 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:15.102
    Jan 18 21:14:15.102: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename controllerrevisions 01/18/23 21:14:15.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:15.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:15.181
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-ncdcf-daemon-set" 01/18/23 21:14:15.248
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:14:15.262
    Jan 18 21:14:15.280: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
    Jan 18 21:14:15.280: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:14:16.292: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
    Jan 18 21:14:16.292: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:14:17.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
    Jan 18 21:14:17.292: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:14:18.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
    Jan 18 21:14:18.291: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:14:19.291: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
    Jan 18 21:14:19.291: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:14:20.295: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 1
    Jan 18 21:14:20.295: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:14:21.293: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 2
    Jan 18 21:14:21.293: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-ncdcf-daemon-set
    STEP: Confirm DaemonSet "e2e-ncdcf-daemon-set" successfully created with "daemonset-name=e2e-ncdcf-daemon-set" label 01/18/23 21:14:21.297
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ncdcf-daemon-set" 01/18/23 21:14:21.317
    Jan 18 21:14:21.323: INFO: Located ControllerRevision: "e2e-ncdcf-daemon-set-858dd5d745"
    STEP: Patching ControllerRevision "e2e-ncdcf-daemon-set-858dd5d745" 01/18/23 21:14:21.326
    Jan 18 21:14:21.341: INFO: e2e-ncdcf-daemon-set-858dd5d745 has been patched
    STEP: Create a new ControllerRevision 01/18/23 21:14:21.341
    Jan 18 21:14:21.359: INFO: Created ControllerRevision: e2e-ncdcf-daemon-set-57767c4c69
    STEP: Confirm that there are two ControllerRevisions 01/18/23 21:14:21.359
    Jan 18 21:14:21.359: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 21:14:21.363: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-ncdcf-daemon-set-858dd5d745" 01/18/23 21:14:21.363
    STEP: Confirm that there is only one ControllerRevision 01/18/23 21:14:21.389
    Jan 18 21:14:21.389: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 21:14:21.394: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-ncdcf-daemon-set-57767c4c69" 01/18/23 21:14:21.398
    Jan 18 21:14:21.418: INFO: e2e-ncdcf-daemon-set-57767c4c69 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 21:14:21.418
    W0118 21:14:21.446887      20 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/18/23 21:14:21.447
    Jan 18 21:14:21.447: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 21:14:22.451: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 21:14:22.459: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ncdcf-daemon-set-57767c4c69=updated" 01/18/23 21:14:22.459
    STEP: Confirm that there is only one ControllerRevision 01/18/23 21:14:22.502
    Jan 18 21:14:22.503: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 21:14:22.506: INFO: Found 1 ControllerRevisions
    Jan 18 21:14:22.510: INFO: ControllerRevision "e2e-ncdcf-daemon-set-77856b944" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-ncdcf-daemon-set" 01/18/23 21:14:22.514
    STEP: deleting DaemonSet.extensions e2e-ncdcf-daemon-set in namespace controllerrevisions-8936, will wait for the garbage collector to delete the pods 01/18/23 21:14:22.514
    Jan 18 21:14:22.603: INFO: Deleting DaemonSet.extensions e2e-ncdcf-daemon-set took: 32.774783ms
    Jan 18 21:14:22.704: INFO: Terminating DaemonSet.extensions e2e-ncdcf-daemon-set pods took: 100.420473ms
    Jan 18 21:14:26.113: INFO: Number of nodes with available pods controlled by daemonset e2e-ncdcf-daemon-set: 0
    Jan 18 21:14:26.113: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ncdcf-daemon-set
    Jan 18 21:14:26.122: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8566"},"items":null}

    Jan 18 21:14:26.130: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8567"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:26.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-8936" for this suite. 01/18/23 21:14:26.148
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:26.172
Jan 18 21:14:26.172: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:14:26.173
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:26.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:26.237
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/18/23 21:14:26.247
Jan 18 21:14:26.247: INFO: Creating simple deployment test-deployment-gdsdl
Jan 18 21:14:26.287: INFO: new replicaset for deployment "test-deployment-gdsdl" is yet to be created
Jan 18 21:14:28.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gdsdl-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/18/23 21:14:30.311
Jan 18 21:14:30.315: INFO: Deployment test-deployment-gdsdl has Conditions: [{Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/18/23 21:14:30.315
Jan 18 21:14:30.344: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gdsdl-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/18/23 21:14:30.344
Jan 18 21:14:30.346: INFO: Observed &Deployment event: ADDED
Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
Jan 18 21:14:30.346: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:14:30.346: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gdsdl-54bc444df" is progressing.}
Jan 18 21:14:30.347: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
Jan 18 21:14:30.347: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
Jan 18 21:14:30.347: INFO: Found Deployment test-deployment-gdsdl in namespace deployment-3148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:14:30.347: INFO: Deployment test-deployment-gdsdl has an updated status
STEP: patching the Statefulset Status 01/18/23 21:14:30.347
Jan 18 21:14:30.347: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 21:14:30.364: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/18/23 21:14:30.364
Jan 18 21:14:30.365: INFO: Observed &Deployment event: ADDED
Jan 18 21:14:30.365: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gdsdl-54bc444df" is progressing.}
Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
Jan 18 21:14:30.366: INFO: Found deployment test-deployment-gdsdl in namespace deployment-3148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 21:14:30.366: INFO: Deployment test-deployment-gdsdl has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:14:30.378: INFO: Deployment "test-deployment-gdsdl":
&Deployment{ObjectMeta:{test-deployment-gdsdl  deployment-3148  0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c 8939 1 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-18 21:14:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00156cae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 21:14:30.385: INFO: New ReplicaSet "test-deployment-gdsdl-54bc444df" of Deployment "test-deployment-gdsdl":
&ReplicaSet{ObjectMeta:{test-deployment-gdsdl-54bc444df  deployment-3148  6c255f65-f39b-4e42-bbab-31d5a0dec30c 8822 1 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gdsdl 0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c 0xc003d8d007 0xc003d8d008}] [] [{kubelite Update apps/v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:14:30.393: INFO: Pod "test-deployment-gdsdl-54bc444df-lw4bm" is available:
&Pod{ObjectMeta:{test-deployment-gdsdl-54bc444df-lw4bm test-deployment-gdsdl-54bc444df- deployment-3148  36752897-0f20-4f38-8527-3e377f143618 8820 0 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:ee821790ecaca62e16870eebdec5b1d932a695d6fbc6654937b9d31038d4941a cni.projectcalico.org/podIP:10.1.132.45/32 cni.projectcalico.org/podIPs:10.1.132.45/32] [{apps/v1 ReplicaSet test-deployment-gdsdl-54bc444df 6c255f65-f39b-4e42-bbab-31d5a0dec30c 0xc0055d6537 0xc0055d6538}] [] [{kubelite Update v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c255f65-f39b-4e42-bbab-31d5a0dec30c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vgvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vgvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.45,StartTime:2023-01-18 21:14:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:14:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fefd861a36e15ee3b53b621da0cc80c849f375efc491878483b7a787e95a75a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3148" for this suite. 01/18/23 21:14:30.4
------------------------------
• [4.270 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:26.172
    Jan 18 21:14:26.172: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:14:26.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:26.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:26.237
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/18/23 21:14:26.247
    Jan 18 21:14:26.247: INFO: Creating simple deployment test-deployment-gdsdl
    Jan 18 21:14:26.287: INFO: new replicaset for deployment "test-deployment-gdsdl" is yet to be created
    Jan 18 21:14:28.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gdsdl-54bc444df\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/18/23 21:14:30.311
    Jan 18 21:14:30.315: INFO: Deployment test-deployment-gdsdl has Conditions: [{Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/18/23 21:14:30.315
    Jan 18 21:14:30.344: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 14, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 14, 26, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gdsdl-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/18/23 21:14:30.344
    Jan 18 21:14:30.346: INFO: Observed &Deployment event: ADDED
    Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
    Jan 18 21:14:30.346: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
    Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:14:30.346: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:14:30.346: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gdsdl-54bc444df" is progressing.}
    Jan 18 21:14:30.347: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
    Jan 18 21:14:30.347: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:14:30.347: INFO: Observed Deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
    Jan 18 21:14:30.347: INFO: Found Deployment test-deployment-gdsdl in namespace deployment-3148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:14:30.347: INFO: Deployment test-deployment-gdsdl has an updated status
    STEP: patching the Statefulset Status 01/18/23 21:14:30.347
    Jan 18 21:14:30.347: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 21:14:30.364: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/18/23 21:14:30.364
    Jan 18 21:14:30.365: INFO: Observed &Deployment event: ADDED
    Jan 18 21:14:30.365: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
    Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gdsdl-54bc444df"}
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:26 +0000 UTC 2023-01-18 21:14:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gdsdl-54bc444df" is progressing.}
    Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
    Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 21:14:29 +0000 UTC 2023-01-18 21:14:26 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gdsdl-54bc444df" has successfully progressed.}
    Jan 18 21:14:30.366: INFO: Observed deployment test-deployment-gdsdl in namespace deployment-3148 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:14:30.366: INFO: Observed &Deployment event: MODIFIED
    Jan 18 21:14:30.366: INFO: Found deployment test-deployment-gdsdl in namespace deployment-3148 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 21:14:30.366: INFO: Deployment test-deployment-gdsdl has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:14:30.378: INFO: Deployment "test-deployment-gdsdl":
    &Deployment{ObjectMeta:{test-deployment-gdsdl  deployment-3148  0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c 8939 1 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-18 21:14:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00156cae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 21:14:30.385: INFO: New ReplicaSet "test-deployment-gdsdl-54bc444df" of Deployment "test-deployment-gdsdl":
    &ReplicaSet{ObjectMeta:{test-deployment-gdsdl-54bc444df  deployment-3148  6c255f65-f39b-4e42-bbab-31d5a0dec30c 8822 1 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gdsdl 0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c 0xc003d8d007 0xc003d8d008}] [] [{kubelite Update apps/v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a1fbcb0-3b08-4f32-bab3-2663ee3ffa6c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:14:30.393: INFO: Pod "test-deployment-gdsdl-54bc444df-lw4bm" is available:
    &Pod{ObjectMeta:{test-deployment-gdsdl-54bc444df-lw4bm test-deployment-gdsdl-54bc444df- deployment-3148  36752897-0f20-4f38-8527-3e377f143618 8820 0 2023-01-18 21:14:26 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:ee821790ecaca62e16870eebdec5b1d932a695d6fbc6654937b9d31038d4941a cni.projectcalico.org/podIP:10.1.132.45/32 cni.projectcalico.org/podIPs:10.1.132.45/32] [{apps/v1 ReplicaSet test-deployment-gdsdl-54bc444df 6c255f65-f39b-4e42-bbab-31d5a0dec30c 0xc0055d6537 0xc0055d6538}] [] [{kubelite Update v1 2023-01-18 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6c255f65-f39b-4e42-bbab-31d5a0dec30c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:14:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:14:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6vgvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6vgvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.45,StartTime:2023-01-18 21:14:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:14:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fefd861a36e15ee3b53b621da0cc80c849f375efc491878483b7a787e95a75a9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:30.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3148" for this suite. 01/18/23 21:14:30.4
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:30.442
Jan 18 21:14:30.442: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:14:30.444
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:30.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:30.499
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:14:30.508
Jan 18 21:14:30.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43" in namespace "projected-4050" to be "Succeeded or Failed"
Jan 18 21:14:30.543: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Pending", Reason="", readiness=false. Elapsed: 9.175804ms
Jan 18 21:14:32.549: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01530071s
Jan 18 21:14:34.550: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016312182s
STEP: Saw pod success 01/18/23 21:14:34.55
Jan 18 21:14:34.550: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43" satisfied condition "Succeeded or Failed"
Jan 18 21:14:34.555: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 container client-container: <nil>
STEP: delete the pod 01/18/23 21:14:34.564
Jan 18 21:14:34.590: INFO: Waiting for pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 to disappear
Jan 18 21:14:34.597: INFO: Pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4050" for this suite. 01/18/23 21:14:34.602
------------------------------
• [4.173 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:30.442
    Jan 18 21:14:30.442: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:14:30.444
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:30.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:30.499
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:14:30.508
    Jan 18 21:14:30.534: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43" in namespace "projected-4050" to be "Succeeded or Failed"
    Jan 18 21:14:30.543: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Pending", Reason="", readiness=false. Elapsed: 9.175804ms
    Jan 18 21:14:32.549: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01530071s
    Jan 18 21:14:34.550: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016312182s
    STEP: Saw pod success 01/18/23 21:14:34.55
    Jan 18 21:14:34.550: INFO: Pod "downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43" satisfied condition "Succeeded or Failed"
    Jan 18 21:14:34.555: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:14:34.564
    Jan 18 21:14:34.590: INFO: Waiting for pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 to disappear
    Jan 18 21:14:34.597: INFO: Pod downwardapi-volume-a294a81a-faf4-46d5-9ba2-21f2f5263f43 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:34.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4050" for this suite. 01/18/23 21:14:34.602
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:34.616
Jan 18 21:14:34.616: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename hostport 01/18/23 21:14:34.618
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:34.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:34.653
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 21:14:34.66
Jan 18 21:14:34.678: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2529" to be "running and ready"
Jan 18 21:14:34.683: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63187ms
Jan 18 21:14:34.683: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:14:36.692: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01343136s
Jan 18 21:14:36.692: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 21:14:36.692: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.5 on the node which pod1 resides and expect scheduled 01/18/23 21:14:36.692
Jan 18 21:14:36.708: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2529" to be "running and ready"
Jan 18 21:14:36.716: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826318ms
Jan 18 21:14:36.716: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:14:38.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013946368s
Jan 18 21:14:38.722: INFO: The phase of Pod pod2 is Running (Ready = false)
Jan 18 21:14:40.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013973019s
Jan 18 21:14:40.722: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 21:14:40.722: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.5 but use UDP protocol on the node which pod2 resides 01/18/23 21:14:40.722
Jan 18 21:14:40.738: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2529" to be "running and ready"
Jan 18 21:14:40.743: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465955ms
Jan 18 21:14:40.743: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:14:42.749: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01006208s
Jan 18 21:14:42.749: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 18 21:14:42.749: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 18 21:14:42.760: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2529" to be "running and ready"
Jan 18 21:14:42.764: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860961ms
Jan 18 21:14:42.765: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:14:44.771: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011908603s
Jan 18 21:14:44.772: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 18 21:14:44.772: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 21:14:44.776
Jan 18 21:14:44.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:14:44.776: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:14:44.777: INFO: ExecWithOptions: Clientset creation
Jan 18 21:14:44.777: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54323 01/18/23 21:14:44.867
Jan 18 21:14:44.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54323/hostname] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:14:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:14:44.868: INFO: ExecWithOptions: Clientset creation
Jan 18 21:14:44.868: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54323 UDP 01/18/23 21:14:44.957
Jan 18 21:14:44.957: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.5 54323] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:14:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:14:44.958: INFO: ExecWithOptions: Clientset creation
Jan 18 21:14:44.958: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:50.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-2529" for this suite. 01/18/23 21:14:50.064
------------------------------
• [SLOW TEST] [15.461 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:34.616
    Jan 18 21:14:34.616: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename hostport 01/18/23 21:14:34.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:34.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:34.653
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 21:14:34.66
    Jan 18 21:14:34.678: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-2529" to be "running and ready"
    Jan 18 21:14:34.683: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63187ms
    Jan 18 21:14:34.683: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:14:36.692: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01343136s
    Jan 18 21:14:36.692: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 21:14:36.692: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.0.5 on the node which pod1 resides and expect scheduled 01/18/23 21:14:36.692
    Jan 18 21:14:36.708: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-2529" to be "running and ready"
    Jan 18 21:14:36.716: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.826318ms
    Jan 18 21:14:36.716: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:14:38.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.013946368s
    Jan 18 21:14:38.722: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jan 18 21:14:40.722: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.013973019s
    Jan 18 21:14:40.722: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 21:14:40.722: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.0.5 but use UDP protocol on the node which pod2 resides 01/18/23 21:14:40.722
    Jan 18 21:14:40.738: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-2529" to be "running and ready"
    Jan 18 21:14:40.743: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.465955ms
    Jan 18 21:14:40.743: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:14:42.749: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.01006208s
    Jan 18 21:14:42.749: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 18 21:14:42.749: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 18 21:14:42.760: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-2529" to be "running and ready"
    Jan 18 21:14:42.764: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860961ms
    Jan 18 21:14:42.765: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:14:44.771: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.011908603s
    Jan 18 21:14:44.772: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 18 21:14:44.772: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 21:14:44.776
    Jan 18 21:14:44.776: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.0.5 http://127.0.0.1:54323/hostname] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:14:44.776: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:14:44.777: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:14:44.777: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.0.5+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54323 01/18/23 21:14:44.867
    Jan 18 21:14:44.867: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.0.5:54323/hostname] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:14:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:14:44.868: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:14:44.868: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.0.5%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.0.5, port: 54323 UDP 01/18/23 21:14:44.957
    Jan 18 21:14:44.957: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.0.5 54323] Namespace:hostport-2529 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:14:44.957: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:14:44.958: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:14:44.958: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/hostport-2529/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.0.5+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:50.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-2529" for this suite. 01/18/23 21:14:50.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:50.081
Jan 18 21:14:50.081: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:14:50.082
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:50.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:50.12
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:14:50.155
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:14:50.685
STEP: Deploying the webhook pod 01/18/23 21:14:50.704
STEP: Wait for the deployment to be ready 01/18/23 21:14:50.735
Jan 18 21:14:50.756: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:14:52.775
STEP: Verifying the service has paired with the endpoint 01/18/23 21:14:52.89
Jan 18 21:14:53.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 21:14:53.896
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 21:14:54.926
STEP: Creating a dummy validating-webhook-configuration object 01/18/23 21:14:55.984
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 21:14:56.192
STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 21:14:56.221
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 21:14:56.267
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:14:56.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4992" for this suite. 01/18/23 21:14:56.525
STEP: Destroying namespace "webhook-4992-markers" for this suite. 01/18/23 21:14:56.563
------------------------------
• [SLOW TEST] [6.506 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:50.081
    Jan 18 21:14:50.081: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:14:50.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:50.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:50.12
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:14:50.155
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:14:50.685
    STEP: Deploying the webhook pod 01/18/23 21:14:50.704
    STEP: Wait for the deployment to be ready 01/18/23 21:14:50.735
    Jan 18 21:14:50.756: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:14:52.775
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:14:52.89
    Jan 18 21:14:53.891: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 21:14:53.896
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 21:14:54.926
    STEP: Creating a dummy validating-webhook-configuration object 01/18/23 21:14:55.984
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 21:14:56.192
    STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 21:14:56.221
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 21:14:56.267
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:14:56.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4992" for this suite. 01/18/23 21:14:56.525
    STEP: Destroying namespace "webhook-4992-markers" for this suite. 01/18/23 21:14:56.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:14:56.588
Jan 18 21:14:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:14:56.589
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:56.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:56.719
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-ff30cdf3-80c8-46f7-9bfc-f5cbf902ab10 01/18/23 21:14:56.721
STEP: Creating a pod to test consume configMaps 01/18/23 21:14:56.742
Jan 18 21:14:56.873: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c" in namespace "projected-5014" to be "Succeeded or Failed"
Jan 18 21:14:56.881: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.514263ms
Jan 18 21:14:58.887: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013456599s
Jan 18 21:15:00.888: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014071407s
STEP: Saw pod success 01/18/23 21:15:00.888
Jan 18 21:15:00.888: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c" satisfied condition "Succeeded or Failed"
Jan 18 21:15:00.893: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:15:00.902
Jan 18 21:15:00.941: INFO: Waiting for pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c to disappear
Jan 18 21:15:00.956: INFO: Pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:15:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5014" for this suite. 01/18/23 21:15:00.963
------------------------------
• [4.397 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:14:56.588
    Jan 18 21:14:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:14:56.589
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:14:56.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:14:56.719
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-ff30cdf3-80c8-46f7-9bfc-f5cbf902ab10 01/18/23 21:14:56.721
    STEP: Creating a pod to test consume configMaps 01/18/23 21:14:56.742
    Jan 18 21:14:56.873: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c" in namespace "projected-5014" to be "Succeeded or Failed"
    Jan 18 21:14:56.881: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.514263ms
    Jan 18 21:14:58.887: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013456599s
    Jan 18 21:15:00.888: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014071407s
    STEP: Saw pod success 01/18/23 21:15:00.888
    Jan 18 21:15:00.888: INFO: Pod "pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c" satisfied condition "Succeeded or Failed"
    Jan 18 21:15:00.893: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:15:00.902
    Jan 18 21:15:00.941: INFO: Waiting for pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c to disappear
    Jan 18 21:15:00.956: INFO: Pod pod-projected-configmaps-61fe9430-6848-4c3a-b964-0cb5a537986c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:15:00.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5014" for this suite. 01/18/23 21:15:00.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:15:00.988
Jan 18 21:15:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:15:00.989
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:15:01.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:15:01.035
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-5ba6e9cb-f38f-497a-989a-6d5b0450051a 01/18/23 21:15:01.045
STEP: Creating secret with name s-test-opt-upd-63d8fe80-e756-4917-a0b6-ab4c7a1c7182 01/18/23 21:15:01.058
STEP: Creating the pod 01/18/23 21:15:01.07
Jan 18 21:15:01.098: INFO: Waiting up to 5m0s for pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4" in namespace "secrets-6964" to be "running and ready"
Jan 18 21:15:01.105: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421489ms
Jan 18 21:15:01.105: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:15:03.112: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013765065s
Jan 18 21:15:03.112: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:15:05.113: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013871736s
Jan 18 21:15:05.113: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Running (Ready = true)
Jan 18 21:15:05.113: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-5ba6e9cb-f38f-497a-989a-6d5b0450051a 01/18/23 21:15:05.143
STEP: Updating secret s-test-opt-upd-63d8fe80-e756-4917-a0b6-ab4c7a1c7182 01/18/23 21:15:05.158
STEP: Creating secret with name s-test-opt-create-34cb416c-a6da-48aa-8b48-def63c70bfd0 01/18/23 21:15:05.17
STEP: waiting to observe update in volume 01/18/23 21:15:05.186
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:16:19.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6964" for this suite. 01/18/23 21:16:19.67
------------------------------
• [SLOW TEST] [78.698 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:15:00.988
    Jan 18 21:15:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:15:00.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:15:01.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:15:01.035
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-5ba6e9cb-f38f-497a-989a-6d5b0450051a 01/18/23 21:15:01.045
    STEP: Creating secret with name s-test-opt-upd-63d8fe80-e756-4917-a0b6-ab4c7a1c7182 01/18/23 21:15:01.058
    STEP: Creating the pod 01/18/23 21:15:01.07
    Jan 18 21:15:01.098: INFO: Waiting up to 5m0s for pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4" in namespace "secrets-6964" to be "running and ready"
    Jan 18 21:15:01.105: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.421489ms
    Jan 18 21:15:01.105: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:15:03.112: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013765065s
    Jan 18 21:15:03.112: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:15:05.113: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013871736s
    Jan 18 21:15:05.113: INFO: The phase of Pod pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4 is Running (Ready = true)
    Jan 18 21:15:05.113: INFO: Pod "pod-secrets-b3376a0f-0514-4bf6-b200-7962ad6962f4" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-5ba6e9cb-f38f-497a-989a-6d5b0450051a 01/18/23 21:15:05.143
    STEP: Updating secret s-test-opt-upd-63d8fe80-e756-4917-a0b6-ab4c7a1c7182 01/18/23 21:15:05.158
    STEP: Creating secret with name s-test-opt-create-34cb416c-a6da-48aa-8b48-def63c70bfd0 01/18/23 21:15:05.17
    STEP: waiting to observe update in volume 01/18/23 21:15:05.186
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:16:19.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6964" for this suite. 01/18/23 21:16:19.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:16:19.688
Jan 18 21:16:19.688: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 21:16:19.689
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:16:19.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:16:19.725
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan 18 21:16:19.730: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:17:19.743: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan 18 21:17:19.748: INFO: Starting informer...
STEP: Starting pods... 01/18/23 21:17:19.748
Jan 18 21:17:19.985: INFO: Pod1 is running on test-vm-1. Tainting Node
Jan 18 21:17:20.406: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-353" to be "running"
Jan 18 21:17:20.410: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374742ms
Jan 18 21:17:22.415: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009611213s
Jan 18 21:17:22.415: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 18 21:17:22.415: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-353" to be "running"
Jan 18 21:17:22.420: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.14384ms
Jan 18 21:17:22.420: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 18 21:17:22.420: INFO: Pod2 is running on test-vm-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 21:17:22.42
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:17:22.437
STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 21:17:22.442
Jan 18 21:17:28.906: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 18 21:17:48.887: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:17:48.903
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:17:48.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-353" for this suite. 01/18/23 21:17:48.912
------------------------------
• [SLOW TEST] [89.241 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:16:19.688
    Jan 18 21:16:19.688: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 21:16:19.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:16:19.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:16:19.725
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan 18 21:16:19.730: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:17:19.743: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan 18 21:17:19.748: INFO: Starting informer...
    STEP: Starting pods... 01/18/23 21:17:19.748
    Jan 18 21:17:19.985: INFO: Pod1 is running on test-vm-1. Tainting Node
    Jan 18 21:17:20.406: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-353" to be "running"
    Jan 18 21:17:20.410: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374742ms
    Jan 18 21:17:22.415: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009611213s
    Jan 18 21:17:22.415: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 18 21:17:22.415: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-353" to be "running"
    Jan 18 21:17:22.420: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.14384ms
    Jan 18 21:17:22.420: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 18 21:17:22.420: INFO: Pod2 is running on test-vm-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 21:17:22.42
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:17:22.437
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 21:17:22.442
    Jan 18 21:17:28.906: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 18 21:17:48.887: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:17:48.903
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:17:48.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-353" for this suite. 01/18/23 21:17:48.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:17:48.931
Jan 18 21:17:48.931: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:17:48.934
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:48.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:48.968
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-22bd3f8f-809d-4740-814f-0f13e8caf1de 01/18/23 21:17:48.977
STEP: Creating secret with name s-test-opt-upd-93cbd06b-4f4e-4d32-98a5-c7c8fecb3eed 01/18/23 21:17:48.987
STEP: Creating the pod 01/18/23 21:17:48.997
Jan 18 21:17:49.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee" in namespace "projected-1603" to be "running and ready"
Jan 18 21:17:49.019: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535561ms
Jan 18 21:17:49.019: INFO: The phase of Pod pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:17:51.025: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.010289685s
Jan 18 21:17:51.025: INFO: The phase of Pod pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee is Running (Ready = true)
Jan 18 21:17:51.025: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-22bd3f8f-809d-4740-814f-0f13e8caf1de 01/18/23 21:17:51.135
STEP: Updating secret s-test-opt-upd-93cbd06b-4f4e-4d32-98a5-c7c8fecb3eed 01/18/23 21:17:51.149
STEP: Creating secret with name s-test-opt-create-bf840d71-8231-4c8a-9da8-ef8fa0e605e7 01/18/23 21:17:51.161
STEP: waiting to observe update in volume 01/18/23 21:17:51.171
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 21:17:53.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1603" for this suite. 01/18/23 21:17:53.211
------------------------------
• [4.294 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:17:48.931
    Jan 18 21:17:48.931: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:17:48.934
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:48.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:48.968
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-22bd3f8f-809d-4740-814f-0f13e8caf1de 01/18/23 21:17:48.977
    STEP: Creating secret with name s-test-opt-upd-93cbd06b-4f4e-4d32-98a5-c7c8fecb3eed 01/18/23 21:17:48.987
    STEP: Creating the pod 01/18/23 21:17:48.997
    Jan 18 21:17:49.015: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee" in namespace "projected-1603" to be "running and ready"
    Jan 18 21:17:49.019: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535561ms
    Jan 18 21:17:49.019: INFO: The phase of Pod pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:17:51.025: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee": Phase="Running", Reason="", readiness=true. Elapsed: 2.010289685s
    Jan 18 21:17:51.025: INFO: The phase of Pod pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee is Running (Ready = true)
    Jan 18 21:17:51.025: INFO: Pod "pod-projected-secrets-cf56c450-fe48-41b6-80b0-3f86882894ee" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-22bd3f8f-809d-4740-814f-0f13e8caf1de 01/18/23 21:17:51.135
    STEP: Updating secret s-test-opt-upd-93cbd06b-4f4e-4d32-98a5-c7c8fecb3eed 01/18/23 21:17:51.149
    STEP: Creating secret with name s-test-opt-create-bf840d71-8231-4c8a-9da8-ef8fa0e605e7 01/18/23 21:17:51.161
    STEP: waiting to observe update in volume 01/18/23 21:17:51.171
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:17:53.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1603" for this suite. 01/18/23 21:17:53.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:17:53.233
Jan 18 21:17:53.233: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 21:17:53.234
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:53.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:53.279
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/18/23 21:17:53.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:53.372
STEP: Creating a service in the namespace 01/18/23 21:17:53.375
STEP: Deleting the namespace 01/18/23 21:17:53.405
STEP: Waiting for the namespace to be removed. 01/18/23 21:17:53.435
STEP: Recreating the namespace 01/18/23 21:17:59.443
STEP: Verifying there is no service in the namespace 01/18/23 21:17:59.497
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:17:59.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4982" for this suite. 01/18/23 21:17:59.508
STEP: Destroying namespace "nsdeletetest-8032" for this suite. 01/18/23 21:17:59.524
Jan 18 21:17:59.530: INFO: Namespace nsdeletetest-8032 was already deleted
STEP: Destroying namespace "nsdeletetest-3746" for this suite. 01/18/23 21:17:59.53
------------------------------
• [SLOW TEST] [6.314 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:17:53.233
    Jan 18 21:17:53.233: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:17:53.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:53.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:53.279
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/18/23 21:17:53.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:53.372
    STEP: Creating a service in the namespace 01/18/23 21:17:53.375
    STEP: Deleting the namespace 01/18/23 21:17:53.405
    STEP: Waiting for the namespace to be removed. 01/18/23 21:17:53.435
    STEP: Recreating the namespace 01/18/23 21:17:59.443
    STEP: Verifying there is no service in the namespace 01/18/23 21:17:59.497
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:17:59.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4982" for this suite. 01/18/23 21:17:59.508
    STEP: Destroying namespace "nsdeletetest-8032" for this suite. 01/18/23 21:17:59.524
    Jan 18 21:17:59.530: INFO: Namespace nsdeletetest-8032 was already deleted
    STEP: Destroying namespace "nsdeletetest-3746" for this suite. 01/18/23 21:17:59.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:17:59.555
Jan 18 21:17:59.558: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:17:59.56
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:59.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:59.614
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/18/23 21:17:59.616
Jan 18 21:17:59.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 create -f -'
Jan 18 21:18:00.606: INFO: stderr: ""
Jan 18 21:18:00.606: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 21:18:00.606
Jan 18 21:18:00.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 21:18:00.697: INFO: stderr: ""
Jan 18 21:18:00.697: INFO: stdout: "update-demo-nautilus-d6cvz update-demo-nautilus-rv7s7 "
Jan 18 21:18:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:18:00.797: INFO: stderr: ""
Jan 18 21:18:00.797: INFO: stdout: ""
Jan 18 21:18:00.797: INFO: update-demo-nautilus-d6cvz is created but not running
Jan 18 21:18:05.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 21:18:05.884: INFO: stderr: ""
Jan 18 21:18:05.884: INFO: stdout: "update-demo-nautilus-d6cvz update-demo-nautilus-rv7s7 "
Jan 18 21:18:05.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:18:05.984: INFO: stderr: ""
Jan 18 21:18:05.984: INFO: stdout: "true"
Jan 18 21:18:05.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 21:18:06.068: INFO: stderr: ""
Jan 18 21:18:06.068: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 21:18:06.068: INFO: validating pod update-demo-nautilus-d6cvz
Jan 18 21:18:06.077: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 21:18:06.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 21:18:06.077: INFO: update-demo-nautilus-d6cvz is verified up and running
Jan 18 21:18:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-rv7s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 21:18:06.160: INFO: stderr: ""
Jan 18 21:18:06.160: INFO: stdout: "true"
Jan 18 21:18:06.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-rv7s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 21:18:06.249: INFO: stderr: ""
Jan 18 21:18:06.249: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 21:18:06.249: INFO: validating pod update-demo-nautilus-rv7s7
Jan 18 21:18:06.256: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 21:18:06.256: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 21:18:06.256: INFO: update-demo-nautilus-rv7s7 is verified up and running
STEP: using delete to clean up resources 01/18/23 21:18:06.256
Jan 18 21:18:06.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 delete --grace-period=0 --force -f -'
Jan 18 21:18:06.350: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:18:06.350: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 21:18:06.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get rc,svc -l name=update-demo --no-headers'
Jan 18 21:18:06.450: INFO: stderr: "No resources found in kubectl-1722 namespace.\n"
Jan 18 21:18:06.450: INFO: stdout: ""
Jan 18 21:18:06.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 21:18:06.547: INFO: stderr: ""
Jan 18 21:18:06.547: INFO: stdout: "update-demo-nautilus-d6cvz\nupdate-demo-nautilus-rv7s7\n"
Jan 18 21:18:07.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get rc,svc -l name=update-demo --no-headers'
Jan 18 21:18:07.150: INFO: stderr: "No resources found in kubectl-1722 namespace.\n"
Jan 18 21:18:07.150: INFO: stdout: ""
Jan 18 21:18:07.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 21:18:07.235: INFO: stderr: ""
Jan 18 21:18:07.235: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1722" for this suite. 01/18/23 21:18:07.241
------------------------------
• [SLOW TEST] [8.574 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:17:59.555
    Jan 18 21:17:59.558: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:17:59.56
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:17:59.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:17:59.614
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/18/23 21:17:59.616
    Jan 18 21:17:59.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 create -f -'
    Jan 18 21:18:00.606: INFO: stderr: ""
    Jan 18 21:18:00.606: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 21:18:00.606
    Jan 18 21:18:00.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 21:18:00.697: INFO: stderr: ""
    Jan 18 21:18:00.697: INFO: stdout: "update-demo-nautilus-d6cvz update-demo-nautilus-rv7s7 "
    Jan 18 21:18:00.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:18:00.797: INFO: stderr: ""
    Jan 18 21:18:00.797: INFO: stdout: ""
    Jan 18 21:18:00.797: INFO: update-demo-nautilus-d6cvz is created but not running
    Jan 18 21:18:05.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 21:18:05.884: INFO: stderr: ""
    Jan 18 21:18:05.884: INFO: stdout: "update-demo-nautilus-d6cvz update-demo-nautilus-rv7s7 "
    Jan 18 21:18:05.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:18:05.984: INFO: stderr: ""
    Jan 18 21:18:05.984: INFO: stdout: "true"
    Jan 18 21:18:05.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-d6cvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 21:18:06.068: INFO: stderr: ""
    Jan 18 21:18:06.068: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 21:18:06.068: INFO: validating pod update-demo-nautilus-d6cvz
    Jan 18 21:18:06.077: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 21:18:06.077: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 21:18:06.077: INFO: update-demo-nautilus-d6cvz is verified up and running
    Jan 18 21:18:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-rv7s7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 21:18:06.160: INFO: stderr: ""
    Jan 18 21:18:06.160: INFO: stdout: "true"
    Jan 18 21:18:06.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods update-demo-nautilus-rv7s7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 21:18:06.249: INFO: stderr: ""
    Jan 18 21:18:06.249: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 21:18:06.249: INFO: validating pod update-demo-nautilus-rv7s7
    Jan 18 21:18:06.256: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 21:18:06.256: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 21:18:06.256: INFO: update-demo-nautilus-rv7s7 is verified up and running
    STEP: using delete to clean up resources 01/18/23 21:18:06.256
    Jan 18 21:18:06.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 delete --grace-period=0 --force -f -'
    Jan 18 21:18:06.350: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:18:06.350: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 21:18:06.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get rc,svc -l name=update-demo --no-headers'
    Jan 18 21:18:06.450: INFO: stderr: "No resources found in kubectl-1722 namespace.\n"
    Jan 18 21:18:06.450: INFO: stdout: ""
    Jan 18 21:18:06.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 21:18:06.547: INFO: stderr: ""
    Jan 18 21:18:06.547: INFO: stdout: "update-demo-nautilus-d6cvz\nupdate-demo-nautilus-rv7s7\n"
    Jan 18 21:18:07.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get rc,svc -l name=update-demo --no-headers'
    Jan 18 21:18:07.150: INFO: stderr: "No resources found in kubectl-1722 namespace.\n"
    Jan 18 21:18:07.150: INFO: stdout: ""
    Jan 18 21:18:07.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1722 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 21:18:07.235: INFO: stderr: ""
    Jan 18 21:18:07.235: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1722" for this suite. 01/18/23 21:18:07.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:08.13
Jan 18 21:18:08.130: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:18:08.132
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:08.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:08.189
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/18/23 21:18:08.191
Jan 18 21:18:08.228: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c" in namespace "emptydir-5856" to be "running"
Jan 18 21:18:08.235: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.576786ms
Jan 18 21:18:10.242: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c": Phase="Running", Reason="", readiness=false. Elapsed: 2.013563897s
Jan 18 21:18:10.242: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/18/23 21:18:10.242
Jan 18 21:18:10.242: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5856 PodName:pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:18:10.242: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:18:10.243: INFO: ExecWithOptions: Clientset creation
Jan 18 21:18:10.243: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-5856/pods/pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 21:18:10.395: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5856" for this suite. 01/18/23 21:18:10.401
------------------------------
• [2.285 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:08.13
    Jan 18 21:18:08.130: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:18:08.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:08.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:08.189
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/18/23 21:18:08.191
    Jan 18 21:18:08.228: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c" in namespace "emptydir-5856" to be "running"
    Jan 18 21:18:08.235: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.576786ms
    Jan 18 21:18:10.242: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c": Phase="Running", Reason="", readiness=false. Elapsed: 2.013563897s
    Jan 18 21:18:10.242: INFO: Pod "pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/18/23 21:18:10.242
    Jan 18 21:18:10.242: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5856 PodName:pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:18:10.242: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:18:10.243: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:18:10.243: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/emptydir-5856/pods/pod-sharedvolume-85273f0c-8101-42eb-9310-35614cf9052c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 18 21:18:10.395: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5856" for this suite. 01/18/23 21:18:10.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:10.42
Jan 18 21:18:10.420: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:18:10.422
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:10.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:10.467
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:10.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-62" for this suite. 01/18/23 21:18:10.522
------------------------------
• [0.115 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:10.42
    Jan 18 21:18:10.420: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:18:10.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:10.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:10.467
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:10.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-62" for this suite. 01/18/23 21:18:10.522
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:10.535
Jan 18 21:18:10.535: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption 01/18/23 21:18:10.537
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:10.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:10.664
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/18/23 21:18:10.692
STEP: Updating PodDisruptionBudget status 01/18/23 21:18:12.702
STEP: Waiting for all pods to be running 01/18/23 21:18:12.721
Jan 18 21:18:12.726: INFO: running pods: 0 < 1
Jan 18 21:18:14.731: INFO: running pods: 0 < 1
STEP: locating a running pod 01/18/23 21:18:16.731
STEP: Waiting for the pdb to be processed 01/18/23 21:18:16.762
STEP: Patching PodDisruptionBudget status 01/18/23 21:18:16.772
STEP: Waiting for the pdb to be processed 01/18/23 21:18:16.813
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:16.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5981" for this suite. 01/18/23 21:18:16.823
------------------------------
• [SLOW TEST] [6.494 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:10.535
    Jan 18 21:18:10.535: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption 01/18/23 21:18:10.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:10.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:10.664
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/18/23 21:18:10.692
    STEP: Updating PodDisruptionBudget status 01/18/23 21:18:12.702
    STEP: Waiting for all pods to be running 01/18/23 21:18:12.721
    Jan 18 21:18:12.726: INFO: running pods: 0 < 1
    Jan 18 21:18:14.731: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/18/23 21:18:16.731
    STEP: Waiting for the pdb to be processed 01/18/23 21:18:16.762
    STEP: Patching PodDisruptionBudget status 01/18/23 21:18:16.772
    STEP: Waiting for the pdb to be processed 01/18/23 21:18:16.813
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:16.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5981" for this suite. 01/18/23 21:18:16.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:17.032
Jan 18 21:18:17.032: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:18:17.033
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:17.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:17.129
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/18/23 21:18:17.132
STEP: watching for the ServiceAccount to be added 01/18/23 21:18:17.157
STEP: patching the ServiceAccount 01/18/23 21:18:17.158
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 21:18:17.177
STEP: deleting the ServiceAccount 01/18/23 21:18:17.185
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:17.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9264" for this suite. 01/18/23 21:18:17.262
------------------------------
• [0.253 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:17.032
    Jan 18 21:18:17.032: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:18:17.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:17.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:17.129
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/18/23 21:18:17.132
    STEP: watching for the ServiceAccount to be added 01/18/23 21:18:17.157
    STEP: patching the ServiceAccount 01/18/23 21:18:17.158
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 21:18:17.177
    STEP: deleting the ServiceAccount 01/18/23 21:18:17.185
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:17.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9264" for this suite. 01/18/23 21:18:17.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:17.287
Jan 18 21:18:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:18:17.288
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:17.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:17.377
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-01c05e82-6ccf-4cc2-8306-1447fd05b7af 01/18/23 21:18:17.381
STEP: Creating a pod to test consume configMaps 01/18/23 21:18:17.406
Jan 18 21:18:17.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145" in namespace "projected-5786" to be "Succeeded or Failed"
Jan 18 21:18:17.452: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082005ms
Jan 18 21:18:19.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014545434s
Jan 18 21:18:21.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014156073s
STEP: Saw pod success 01/18/23 21:18:21.458
Jan 18 21:18:21.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145" satisfied condition "Succeeded or Failed"
Jan 18 21:18:21.463: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:18:21.471
Jan 18 21:18:21.500: INFO: Waiting for pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 to disappear
Jan 18 21:18:21.510: INFO: Pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5786" for this suite. 01/18/23 21:18:21.515
------------------------------
• [4.243 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:17.287
    Jan 18 21:18:17.287: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:18:17.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:17.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:17.377
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-01c05e82-6ccf-4cc2-8306-1447fd05b7af 01/18/23 21:18:17.381
    STEP: Creating a pod to test consume configMaps 01/18/23 21:18:17.406
    Jan 18 21:18:17.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145" in namespace "projected-5786" to be "Succeeded or Failed"
    Jan 18 21:18:17.452: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082005ms
    Jan 18 21:18:19.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014545434s
    Jan 18 21:18:21.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014156073s
    STEP: Saw pod success 01/18/23 21:18:21.458
    Jan 18 21:18:21.458: INFO: Pod "pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145" satisfied condition "Succeeded or Failed"
    Jan 18 21:18:21.463: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:18:21.471
    Jan 18 21:18:21.500: INFO: Waiting for pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 to disappear
    Jan 18 21:18:21.510: INFO: Pod pod-projected-configmaps-39ab4f06-d3a7-4c2c-816f-f8f38bae3145 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5786" for this suite. 01/18/23 21:18:21.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:21.532
Jan 18 21:18:21.533: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:18:21.533
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:21.568
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:18:21.572
Jan 18 21:18:21.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d" in namespace "projected-337" to be "Succeeded or Failed"
Jan 18 21:18:21.603: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699549ms
Jan 18 21:18:23.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009160099s
Jan 18 21:18:25.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Running", Reason="", readiness=false. Elapsed: 4.008862766s
Jan 18 21:18:27.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009281141s
STEP: Saw pod success 01/18/23 21:18:27.608
Jan 18 21:18:27.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d" satisfied condition "Succeeded or Failed"
Jan 18 21:18:27.612: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d container client-container: <nil>
STEP: delete the pod 01/18/23 21:18:27.621
Jan 18 21:18:27.649: INFO: Waiting for pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d to disappear
Jan 18 21:18:27.659: INFO: Pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-337" for this suite. 01/18/23 21:18:27.663
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:21.532
    Jan 18 21:18:21.533: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:18:21.533
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:21.568
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:18:21.572
    Jan 18 21:18:21.599: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d" in namespace "projected-337" to be "Succeeded or Failed"
    Jan 18 21:18:21.603: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.699549ms
    Jan 18 21:18:23.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009160099s
    Jan 18 21:18:25.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Running", Reason="", readiness=false. Elapsed: 4.008862766s
    Jan 18 21:18:27.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009281141s
    STEP: Saw pod success 01/18/23 21:18:27.608
    Jan 18 21:18:27.608: INFO: Pod "downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d" satisfied condition "Succeeded or Failed"
    Jan 18 21:18:27.612: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d container client-container: <nil>
    STEP: delete the pod 01/18/23 21:18:27.621
    Jan 18 21:18:27.649: INFO: Waiting for pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d to disappear
    Jan 18 21:18:27.659: INFO: Pod downwardapi-volume-dcd057c1-dfea-40ca-b337-077b2e4ff61d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:27.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-337" for this suite. 01/18/23 21:18:27.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:27.678
Jan 18 21:18:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename endpointslice 01/18/23 21:18:27.679
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:27.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:27.717
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/18/23 21:18:32.868
STEP: referencing matching pods with named port 01/18/23 21:18:37.878
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 21:18:42.889
STEP: recreating EndpointSlices after they've been deleted 01/18/23 21:18:47.899
Jan 18 21:18:47.936: INFO: EndpointSlice for Service endpointslice-4426/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:57.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4426" for this suite. 01/18/23 21:18:57.955
------------------------------
• [SLOW TEST] [30.291 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:27.678
    Jan 18 21:18:27.678: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename endpointslice 01/18/23 21:18:27.679
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:27.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:27.717
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/18/23 21:18:32.868
    STEP: referencing matching pods with named port 01/18/23 21:18:37.878
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 21:18:42.889
    STEP: recreating EndpointSlices after they've been deleted 01/18/23 21:18:47.899
    Jan 18 21:18:47.936: INFO: EndpointSlice for Service endpointslice-4426/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:57.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4426" for this suite. 01/18/23 21:18:57.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:57.973
Jan 18 21:18:57.973: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename ingressclass 01/18/23 21:18:57.974
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:58.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:58.018
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/18/23 21:18:58.02
STEP: getting /apis/networking.k8s.io 01/18/23 21:18:58.022
STEP: getting /apis/networking.k8s.iov1 01/18/23 21:18:58.023
STEP: creating 01/18/23 21:18:58.024
STEP: getting 01/18/23 21:18:58.055
STEP: listing 01/18/23 21:18:58.06
STEP: watching 01/18/23 21:18:58.064
Jan 18 21:18:58.064: INFO: starting watch
STEP: patching 01/18/23 21:18:58.065
STEP: updating 01/18/23 21:18:58.077
Jan 18 21:18:58.088: INFO: waiting for watch events with expected annotations
Jan 18 21:18:58.088: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 21:18:58.088
STEP: deleting a collection 01/18/23 21:18:58.11
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:18:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-428" for this suite. 01/18/23 21:18:58.16
------------------------------
• [0.202 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:57.973
    Jan 18 21:18:57.973: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename ingressclass 01/18/23 21:18:57.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:58.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:58.018
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/18/23 21:18:58.02
    STEP: getting /apis/networking.k8s.io 01/18/23 21:18:58.022
    STEP: getting /apis/networking.k8s.iov1 01/18/23 21:18:58.023
    STEP: creating 01/18/23 21:18:58.024
    STEP: getting 01/18/23 21:18:58.055
    STEP: listing 01/18/23 21:18:58.06
    STEP: watching 01/18/23 21:18:58.064
    Jan 18 21:18:58.064: INFO: starting watch
    STEP: patching 01/18/23 21:18:58.065
    STEP: updating 01/18/23 21:18:58.077
    Jan 18 21:18:58.088: INFO: waiting for watch events with expected annotations
    Jan 18 21:18:58.088: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 21:18:58.088
    STEP: deleting a collection 01/18/23 21:18:58.11
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:18:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-428" for this suite. 01/18/23 21:18:58.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:18:58.18
Jan 18 21:18:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:18:58.181
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:58.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:58.238
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 18 21:18:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8113" for this suite. 01/18/23 21:19:01.53
------------------------------
• [3.373 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:18:58.18
    Jan 18 21:18:58.180: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:18:58.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:18:58.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:18:58.238
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 18 21:18:58.240: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:01.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8113" for this suite. 01/18/23 21:19:01.53
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:01.553
Jan 18 21:19:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:19:01.555
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:01.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:01.593
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 18 21:19:01.596: INFO: Creating deployment "test-recreate-deployment"
Jan 18 21:19:01.607: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 21:19:01.618: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 18 21:19:03.631: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 21:19:03.635: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 21:19:03.656: INFO: Updating deployment test-recreate-deployment
Jan 18 21:19:03.656: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:19:04.008: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4145  0e057775-f0b2-4b6b-99b4-87f414094753 10329 2 2023-01-18 21:19:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e974d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 21:19:03 +0000 UTC,LastTransitionTime:2023-01-18 21:19:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-18 21:19:03 +0000 UTC,LastTransitionTime:2023-01-18 21:19:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 21:19:04.014: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4145  1ea13e27-28f8-4639-a223-7acc2a56bcb4 10326 1 2023-01-18 21:19:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0e057775-f0b2-4b6b-99b4-87f414094753 0xc002e97990 0xc002e97991}] [] [{kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e057775-f0b2-4b6b-99b4-87f414094753\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e97a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:19:04.014: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 21:19:04.014: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4145  733d3905-9315-49b9-ae39-2f671e4e9e60 10316 2 2023-01-18 21:19:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0e057775-f0b2-4b6b-99b4-87f414094753 0xc002e97867 0xc002e97868}] [] [{kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e057775-f0b2-4b6b-99b4-87f414094753\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e97928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:19:04.021: INFO: Pod "test-recreate-deployment-cff6dc657-qml5q" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qml5q test-recreate-deployment-cff6dc657- deployment-4145  9c22129c-95a2-45b2-b3ef-6b200b58328f 10319 0 2023-01-18 21:19:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1ea13e27-28f8-4639-a223-7acc2a56bcb4 0xc002e97eb0 0xc002e97eb1}] [] [{kubelite Update v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ea13e27-28f8-4639-a223-7acc2a56bcb4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhkq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhkq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:04.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4145" for this suite. 01/18/23 21:19:04.027
------------------------------
• [2.491 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:01.553
    Jan 18 21:19:01.553: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:19:01.555
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:01.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:01.593
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 18 21:19:01.596: INFO: Creating deployment "test-recreate-deployment"
    Jan 18 21:19:01.607: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 18 21:19:01.618: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 18 21:19:03.631: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 18 21:19:03.635: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 18 21:19:03.656: INFO: Updating deployment test-recreate-deployment
    Jan 18 21:19:03.656: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:19:04.008: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4145  0e057775-f0b2-4b6b-99b4-87f414094753 10329 2 2023-01-18 21:19:01 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e974d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 21:19:03 +0000 UTC,LastTransitionTime:2023-01-18 21:19:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-18 21:19:03 +0000 UTC,LastTransitionTime:2023-01-18 21:19:01 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 21:19:04.014: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4145  1ea13e27-28f8-4639-a223-7acc2a56bcb4 10326 1 2023-01-18 21:19:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0e057775-f0b2-4b6b-99b4-87f414094753 0xc002e97990 0xc002e97991}] [] [{kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e057775-f0b2-4b6b-99b4-87f414094753\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e97a58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:19:04.014: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 18 21:19:04.014: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4145  733d3905-9315-49b9-ae39-2f671e4e9e60 10316 2 2023-01-18 21:19:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0e057775-f0b2-4b6b-99b4-87f414094753 0xc002e97867 0xc002e97868}] [] [{kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e057775-f0b2-4b6b-99b4-87f414094753\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002e97928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:19:04.021: INFO: Pod "test-recreate-deployment-cff6dc657-qml5q" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-qml5q test-recreate-deployment-cff6dc657- deployment-4145  9c22129c-95a2-45b2-b3ef-6b200b58328f 10319 0 2023-01-18 21:19:03 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 1ea13e27-28f8-4639-a223-7acc2a56bcb4 0xc002e97eb0 0xc002e97eb1}] [] [{kubelite Update v1 2023-01-18 21:19:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1ea13e27-28f8-4639-a223-7acc2a56bcb4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mhkq2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mhkq2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:04.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4145" for this suite. 01/18/23 21:19:04.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:04.045
Jan 18 21:19:04.046: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename podtemplate 01/18/23 21:19:04.047
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:04.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:04.104
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/18/23 21:19:04.106
STEP: Replace a pod template 01/18/23 21:19:04.124
Jan 18 21:19:04.142: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2596" for this suite. 01/18/23 21:19:04.148
------------------------------
• [0.117 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:04.045
    Jan 18 21:19:04.046: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename podtemplate 01/18/23 21:19:04.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:04.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:04.104
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/18/23 21:19:04.106
    STEP: Replace a pod template 01/18/23 21:19:04.124
    Jan 18 21:19:04.142: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2596" for this suite. 01/18/23 21:19:04.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:04.165
Jan 18 21:19:04.165: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:19:04.165
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:04.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:04.21
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/18/23 21:19:04.213
Jan 18 21:19:04.213: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 21:19:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:05.585: INFO: stderr: ""
Jan 18 21:19:05.585: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 21:19:05.585: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 21:19:05.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:05.827: INFO: stderr: ""
Jan 18 21:19:05.827: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 21:19:05.827: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 21:19:05.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:06.068: INFO: stderr: ""
Jan 18 21:19:06.068: INFO: stdout: "service/frontend created\n"
Jan 18 21:19:06.068: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 21:19:06.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:06.302: INFO: stderr: ""
Jan 18 21:19:06.302: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 21:19:06.302: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 21:19:06.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:07.671: INFO: stderr: ""
Jan 18 21:19:07.671: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 21:19:07.671: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 21:19:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
Jan 18 21:19:08.035: INFO: stderr: ""
Jan 18 21:19:08.035: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/18/23 21:19:08.035
Jan 18 21:19:08.035: INFO: Waiting for all frontend pods to be Running.
Jan 18 21:19:13.086: INFO: Waiting for frontend to serve content.
Jan 18 21:19:13.104: INFO: Trying to add a new entry to the guestbook.
Jan 18 21:19:13.122: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/18/23 21:19:13.134
Jan 18 21:19:13.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.269: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 21:19:13.269
Jan 18 21:19:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.403: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 21:19:13.403
Jan 18 21:19:13.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.564: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.564: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 21:19:13.564
Jan 18 21:19:13.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.658: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.659: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 21:19:13.659
Jan 18 21:19:13.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.818: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.818: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 21:19:13.818
Jan 18 21:19:13.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
Jan 18 21:19:13.981: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:19:13.981: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8209" for this suite. 01/18/23 21:19:13.993
------------------------------
• [SLOW TEST] [9.852 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:04.165
    Jan 18 21:19:04.165: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:19:04.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:04.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:04.21
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/18/23 21:19:04.213
    Jan 18 21:19:04.213: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 18 21:19:04.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:05.585: INFO: stderr: ""
    Jan 18 21:19:05.585: INFO: stdout: "service/agnhost-replica created\n"
    Jan 18 21:19:05.585: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 18 21:19:05.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:05.827: INFO: stderr: ""
    Jan 18 21:19:05.827: INFO: stdout: "service/agnhost-primary created\n"
    Jan 18 21:19:05.827: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 18 21:19:05.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:06.068: INFO: stderr: ""
    Jan 18 21:19:06.068: INFO: stdout: "service/frontend created\n"
    Jan 18 21:19:06.068: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 18 21:19:06.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:06.302: INFO: stderr: ""
    Jan 18 21:19:06.302: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 18 21:19:06.302: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 21:19:06.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:07.671: INFO: stderr: ""
    Jan 18 21:19:07.671: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 18 21:19:07.671: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 21:19:07.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 create -f -'
    Jan 18 21:19:08.035: INFO: stderr: ""
    Jan 18 21:19:08.035: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/18/23 21:19:08.035
    Jan 18 21:19:08.035: INFO: Waiting for all frontend pods to be Running.
    Jan 18 21:19:13.086: INFO: Waiting for frontend to serve content.
    Jan 18 21:19:13.104: INFO: Trying to add a new entry to the guestbook.
    Jan 18 21:19:13.122: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/18/23 21:19:13.134
    Jan 18 21:19:13.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.269: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 21:19:13.269
    Jan 18 21:19:13.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.403: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.403: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 21:19:13.403
    Jan 18 21:19:13.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.564: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.564: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 21:19:13.564
    Jan 18 21:19:13.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.658: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.659: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 21:19:13.659
    Jan 18 21:19:13.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.818: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.818: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 21:19:13.818
    Jan 18 21:19:13.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8209 delete --grace-period=0 --force -f -'
    Jan 18 21:19:13.981: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:19:13.981: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8209" for this suite. 01/18/23 21:19:13.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:14.022
Jan 18 21:19:14.022: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 21:19:14.023
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:14.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:14.133
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/18/23 21:19:14.142
Jan 18 21:19:14.234: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2033" to be "running and ready"
Jan 18 21:19:14.240: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249065ms
Jan 18 21:19:14.240: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:19:16.245: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418482s
Jan 18 21:19:16.245: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 18 21:19:16.245: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/18/23 21:19:16.25
Jan 18 21:19:16.269: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2033" to be "container debugger running"
Jan 18 21:19:16.273: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.584448ms
Jan 18 21:19:18.283: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013702906s
Jan 18 21:19:20.280: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01157692s
Jan 18 21:19:20.280: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/18/23 21:19:20.28
Jan 18 21:19:20.281: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2033 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:19:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:19:20.281: INFO: ExecWithOptions: Clientset creation
Jan 18 21:19:20.281: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-2033/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 18 21:19:20.448: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:20.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-2033" for this suite. 01/18/23 21:19:20.464
------------------------------
• [SLOW TEST] [6.456 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:14.022
    Jan 18 21:19:14.022: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 21:19:14.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:14.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:14.133
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/18/23 21:19:14.142
    Jan 18 21:19:14.234: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2033" to be "running and ready"
    Jan 18 21:19:14.240: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.249065ms
    Jan 18 21:19:14.240: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:19:16.245: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011418482s
    Jan 18 21:19:16.245: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 18 21:19:16.245: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/18/23 21:19:16.25
    Jan 18 21:19:16.269: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-2033" to be "container debugger running"
    Jan 18 21:19:16.273: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.584448ms
    Jan 18 21:19:18.283: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013702906s
    Jan 18 21:19:20.280: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.01157692s
    Jan 18 21:19:20.280: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/18/23 21:19:20.28
    Jan 18 21:19:20.281: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-2033 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:19:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:19:20.281: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:19:20.281: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/ephemeral-containers-test-2033/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 18 21:19:20.448: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:20.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-2033" for this suite. 01/18/23 21:19:20.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:20.482
Jan 18 21:19:20.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename limitrange 01/18/23 21:19:20.483
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:20.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:20.522
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-rk82x" in namespace "limitrange-5164" 01/18/23 21:19:20.527
STEP: Creating another limitRange in another namespace 01/18/23 21:19:20.541
Jan 18 21:19:20.588: INFO: Namespace "e2e-limitrange-rk82x-5747" created
Jan 18 21:19:20.588: INFO: Creating LimitRange "e2e-limitrange-rk82x" in namespace "e2e-limitrange-rk82x-5747"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-rk82x" 01/18/23 21:19:20.6
Jan 18 21:19:20.605: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-rk82x" in "limitrange-5164" namespace 01/18/23 21:19:20.605
Jan 18 21:19:20.620: INFO: LimitRange "e2e-limitrange-rk82x" has been patched
STEP: Delete LimitRange "e2e-limitrange-rk82x" by Collection with labelSelector: "e2e-limitrange-rk82x=patched" 01/18/23 21:19:20.62
STEP: Confirm that the limitRange "e2e-limitrange-rk82x" has been deleted 01/18/23 21:19:20.636
Jan 18 21:19:20.636: INFO: Requesting list of LimitRange to confirm quantity
Jan 18 21:19:20.644: INFO: Found 0 LimitRange with label "e2e-limitrange-rk82x=patched"
Jan 18 21:19:20.644: INFO: LimitRange "e2e-limitrange-rk82x" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-rk82x" 01/18/23 21:19:20.644
Jan 18 21:19:20.648: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:20.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5164" for this suite. 01/18/23 21:19:20.653
STEP: Destroying namespace "e2e-limitrange-rk82x-5747" for this suite. 01/18/23 21:19:20.666
------------------------------
• [0.198 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:20.482
    Jan 18 21:19:20.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename limitrange 01/18/23 21:19:20.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:20.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:20.522
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-rk82x" in namespace "limitrange-5164" 01/18/23 21:19:20.527
    STEP: Creating another limitRange in another namespace 01/18/23 21:19:20.541
    Jan 18 21:19:20.588: INFO: Namespace "e2e-limitrange-rk82x-5747" created
    Jan 18 21:19:20.588: INFO: Creating LimitRange "e2e-limitrange-rk82x" in namespace "e2e-limitrange-rk82x-5747"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-rk82x" 01/18/23 21:19:20.6
    Jan 18 21:19:20.605: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-rk82x" in "limitrange-5164" namespace 01/18/23 21:19:20.605
    Jan 18 21:19:20.620: INFO: LimitRange "e2e-limitrange-rk82x" has been patched
    STEP: Delete LimitRange "e2e-limitrange-rk82x" by Collection with labelSelector: "e2e-limitrange-rk82x=patched" 01/18/23 21:19:20.62
    STEP: Confirm that the limitRange "e2e-limitrange-rk82x" has been deleted 01/18/23 21:19:20.636
    Jan 18 21:19:20.636: INFO: Requesting list of LimitRange to confirm quantity
    Jan 18 21:19:20.644: INFO: Found 0 LimitRange with label "e2e-limitrange-rk82x=patched"
    Jan 18 21:19:20.644: INFO: LimitRange "e2e-limitrange-rk82x" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-rk82x" 01/18/23 21:19:20.644
    Jan 18 21:19:20.648: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:20.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5164" for this suite. 01/18/23 21:19:20.653
    STEP: Destroying namespace "e2e-limitrange-rk82x-5747" for this suite. 01/18/23 21:19:20.666
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:20.681
Jan 18 21:19:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:19:20.682
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:20.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:20.72
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/18/23 21:19:20.729
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:20.742
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:20.743
STEP: creating a pod to probe DNS 01/18/23 21:19:20.743
STEP: submitting the pod to kubernetes 01/18/23 21:19:20.743
Jan 18 21:19:20.774: INFO: Waiting up to 15m0s for pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f" in namespace "dns-7527" to be "running"
Jan 18 21:19:20.780: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921373ms
Jan 18 21:19:22.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013633815s
Jan 18 21:19:24.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01422847s
Jan 18 21:19:26.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014219517s
Jan 18 21:19:28.789: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015337869s
Jan 18 21:19:30.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Running", Reason="", readiness=true. Elapsed: 10.013991518s
Jan 18 21:19:30.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:19:30.788
STEP: looking for the results for each expected name from probers 01/18/23 21:19:30.795
Jan 18 21:19:30.815: INFO: DNS probes using dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f succeeded

STEP: deleting the pod 01/18/23 21:19:30.815
STEP: changing the externalName to bar.example.com 01/18/23 21:19:30.845
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:30.86
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:30.86
STEP: creating a second pod to probe DNS 01/18/23 21:19:30.86
STEP: submitting the pod to kubernetes 01/18/23 21:19:30.86
Jan 18 21:19:30.875: INFO: Waiting up to 15m0s for pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717" in namespace "dns-7527" to be "running"
Jan 18 21:19:30.880: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Pending", Reason="", readiness=false. Elapsed: 4.779154ms
Jan 18 21:19:32.888: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013225013s
Jan 18 21:19:34.890: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Running", Reason="", readiness=true. Elapsed: 4.015302901s
Jan 18 21:19:34.890: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:19:34.89
STEP: looking for the results for each expected name from probers 01/18/23 21:19:34.896
Jan 18 21:19:34.908: INFO: DNS probes using dns-test-f672aa05-725f-464e-b7fa-f588d33a9717 succeeded

STEP: deleting the pod 01/18/23 21:19:34.908
STEP: changing the service to type=ClusterIP 01/18/23 21:19:34.937
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:34.963
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
 01/18/23 21:19:34.964
STEP: creating a third pod to probe DNS 01/18/23 21:19:34.964
STEP: submitting the pod to kubernetes 01/18/23 21:19:34.968
Jan 18 21:19:34.979: INFO: Waiting up to 15m0s for pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643" in namespace "dns-7527" to be "running"
Jan 18 21:19:34.983: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065645ms
Jan 18 21:19:36.991: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011374282s
Jan 18 21:19:38.989: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Running", Reason="", readiness=true. Elapsed: 4.009840538s
Jan 18 21:19:38.989: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:19:38.989
STEP: looking for the results for each expected name from probers 01/18/23 21:19:38.994
Jan 18 21:19:39.008: INFO: DNS probes using dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643 succeeded

STEP: deleting the pod 01/18/23 21:19:39.008
STEP: deleting the test externalName service 01/18/23 21:19:39.038
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7527" for this suite. 01/18/23 21:19:39.078
------------------------------
• [SLOW TEST] [18.412 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:20.681
    Jan 18 21:19:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:19:20.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:20.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:20.72
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/18/23 21:19:20.729
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:20.742
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:20.743
    STEP: creating a pod to probe DNS 01/18/23 21:19:20.743
    STEP: submitting the pod to kubernetes 01/18/23 21:19:20.743
    Jan 18 21:19:20.774: INFO: Waiting up to 15m0s for pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f" in namespace "dns-7527" to be "running"
    Jan 18 21:19:20.780: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.921373ms
    Jan 18 21:19:22.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013633815s
    Jan 18 21:19:24.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01422847s
    Jan 18 21:19:26.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014219517s
    Jan 18 21:19:28.789: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015337869s
    Jan 18 21:19:30.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f": Phase="Running", Reason="", readiness=true. Elapsed: 10.013991518s
    Jan 18 21:19:30.788: INFO: Pod "dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:19:30.788
    STEP: looking for the results for each expected name from probers 01/18/23 21:19:30.795
    Jan 18 21:19:30.815: INFO: DNS probes using dns-test-392753da-01dc-44e9-9505-0e5a3d93eb3f succeeded

    STEP: deleting the pod 01/18/23 21:19:30.815
    STEP: changing the externalName to bar.example.com 01/18/23 21:19:30.845
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:30.86
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:30.86
    STEP: creating a second pod to probe DNS 01/18/23 21:19:30.86
    STEP: submitting the pod to kubernetes 01/18/23 21:19:30.86
    Jan 18 21:19:30.875: INFO: Waiting up to 15m0s for pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717" in namespace "dns-7527" to be "running"
    Jan 18 21:19:30.880: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Pending", Reason="", readiness=false. Elapsed: 4.779154ms
    Jan 18 21:19:32.888: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013225013s
    Jan 18 21:19:34.890: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717": Phase="Running", Reason="", readiness=true. Elapsed: 4.015302901s
    Jan 18 21:19:34.890: INFO: Pod "dns-test-f672aa05-725f-464e-b7fa-f588d33a9717" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:19:34.89
    STEP: looking for the results for each expected name from probers 01/18/23 21:19:34.896
    Jan 18 21:19:34.908: INFO: DNS probes using dns-test-f672aa05-725f-464e-b7fa-f588d33a9717 succeeded

    STEP: deleting the pod 01/18/23 21:19:34.908
    STEP: changing the service to type=ClusterIP 01/18/23 21:19:34.937
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:34.963
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7527.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7527.svc.cluster.local; sleep 1; done
     01/18/23 21:19:34.964
    STEP: creating a third pod to probe DNS 01/18/23 21:19:34.964
    STEP: submitting the pod to kubernetes 01/18/23 21:19:34.968
    Jan 18 21:19:34.979: INFO: Waiting up to 15m0s for pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643" in namespace "dns-7527" to be "running"
    Jan 18 21:19:34.983: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065645ms
    Jan 18 21:19:36.991: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011374282s
    Jan 18 21:19:38.989: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643": Phase="Running", Reason="", readiness=true. Elapsed: 4.009840538s
    Jan 18 21:19:38.989: INFO: Pod "dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:19:38.989
    STEP: looking for the results for each expected name from probers 01/18/23 21:19:38.994
    Jan 18 21:19:39.008: INFO: DNS probes using dns-test-f5f83ec5-8117-466b-80c2-2ad129c5e643 succeeded

    STEP: deleting the pod 01/18/23 21:19:39.008
    STEP: deleting the test externalName service 01/18/23 21:19:39.038
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:39.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7527" for this suite. 01/18/23 21:19:39.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:39.094
Jan 18 21:19:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:19:39.096
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:39.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:39.139
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-53291590-0246-45a3-88b4-7a65d7f03f3b 01/18/23 21:19:39.142
STEP: Creating a pod to test consume configMaps 01/18/23 21:19:39.152
Jan 18 21:19:39.172: INFO: Waiting up to 5m0s for pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7" in namespace "configmap-7215" to be "Succeeded or Failed"
Jan 18 21:19:39.176: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459294ms
Jan 18 21:19:41.182: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010493407s
Jan 18 21:19:43.183: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010731898s
STEP: Saw pod success 01/18/23 21:19:43.183
Jan 18 21:19:43.183: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7" satisfied condition "Succeeded or Failed"
Jan 18 21:19:43.187: INFO: Trying to get logs from node test-vm-2 pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 container configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 21:19:43.368
Jan 18 21:19:43.395: INFO: Waiting for pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 to disappear
Jan 18 21:19:43.403: INFO: Pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:43.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7215" for this suite. 01/18/23 21:19:43.408
------------------------------
• [4.327 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:39.094
    Jan 18 21:19:39.095: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:19:39.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:39.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:39.139
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-53291590-0246-45a3-88b4-7a65d7f03f3b 01/18/23 21:19:39.142
    STEP: Creating a pod to test consume configMaps 01/18/23 21:19:39.152
    Jan 18 21:19:39.172: INFO: Waiting up to 5m0s for pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7" in namespace "configmap-7215" to be "Succeeded or Failed"
    Jan 18 21:19:39.176: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.459294ms
    Jan 18 21:19:41.182: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010493407s
    Jan 18 21:19:43.183: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010731898s
    STEP: Saw pod success 01/18/23 21:19:43.183
    Jan 18 21:19:43.183: INFO: Pod "pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7" satisfied condition "Succeeded or Failed"
    Jan 18 21:19:43.187: INFO: Trying to get logs from node test-vm-2 pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 container configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:19:43.368
    Jan 18 21:19:43.395: INFO: Waiting for pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 to disappear
    Jan 18 21:19:43.403: INFO: Pod pod-configmaps-491dfcd7-cc81-41f2-8edc-4b3ac98ab6d7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:43.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7215" for this suite. 01/18/23 21:19:43.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:43.425
Jan 18 21:19:43.426: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:19:43.427
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:43.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:43.495
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/18/23 21:19:43.502
Jan 18 21:19:43.523: INFO: Waiting up to 5m0s for pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44" in namespace "downward-api-6462" to be "Succeeded or Failed"
Jan 18 21:19:43.529: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383475ms
Jan 18 21:19:45.536: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013376771s
Jan 18 21:19:47.535: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011880667s
STEP: Saw pod success 01/18/23 21:19:47.535
Jan 18 21:19:47.535: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44" satisfied condition "Succeeded or Failed"
Jan 18 21:19:47.539: INFO: Trying to get logs from node test-vm-1 pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:19:47.548
Jan 18 21:19:47.580: INFO: Waiting for pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 to disappear
Jan 18 21:19:47.587: INFO: Pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:47.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6462" for this suite. 01/18/23 21:19:47.593
------------------------------
• [4.181 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:43.425
    Jan 18 21:19:43.426: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:19:43.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:43.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:43.495
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/18/23 21:19:43.502
    Jan 18 21:19:43.523: INFO: Waiting up to 5m0s for pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44" in namespace "downward-api-6462" to be "Succeeded or Failed"
    Jan 18 21:19:43.529: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.383475ms
    Jan 18 21:19:45.536: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013376771s
    Jan 18 21:19:47.535: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011880667s
    STEP: Saw pod success 01/18/23 21:19:47.535
    Jan 18 21:19:47.535: INFO: Pod "downward-api-57754823-427d-467e-ae40-5354ff15ff44" satisfied condition "Succeeded or Failed"
    Jan 18 21:19:47.539: INFO: Trying to get logs from node test-vm-1 pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:19:47.548
    Jan 18 21:19:47.580: INFO: Waiting for pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 to disappear
    Jan 18 21:19:47.587: INFO: Pod downward-api-57754823-427d-467e-ae40-5354ff15ff44 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:47.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6462" for this suite. 01/18/23 21:19:47.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:47.607
Jan 18 21:19:47.607: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:19:47.608
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:47.649
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:47.652
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 21:19:47.654
Jan 18 21:19:47.672: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2015  46af2335-579b-49af-abed-caf2d2f3b9ea 10937 0 2023-01-18 21:19:47 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 21:19:47 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m54kc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m54kc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:19:47.673: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2015" to be "running and ready"
Jan 18 21:19:47.677: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819757ms
Jan 18 21:19:47.677: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:19:49.683: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.01077164s
Jan 18 21:19:49.683: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 18 21:19:49.683: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 21:19:49.683
Jan 18 21:19:49.684: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2015 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:19:49.684: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:19:49.684: INFO: ExecWithOptions: Clientset creation
Jan 18 21:19:49.684: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-2015/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/18/23 21:19:49.865
Jan 18 21:19:49.865: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2015 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:19:49.865: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:19:49.865: INFO: ExecWithOptions: Clientset creation
Jan 18 21:19:49.865: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-2015/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 21:19:50.039: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:19:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2015" for this suite. 01/18/23 21:19:50.08
------------------------------
• [2.486 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:47.607
    Jan 18 21:19:47.607: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:19:47.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:47.649
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:47.652
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 21:19:47.654
    Jan 18 21:19:47.672: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2015  46af2335-579b-49af-abed-caf2d2f3b9ea 10937 0 2023-01-18 21:19:47 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 21:19:47 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m54kc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m54kc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:19:47.673: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2015" to be "running and ready"
    Jan 18 21:19:47.677: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819757ms
    Jan 18 21:19:47.677: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:19:49.683: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.01077164s
    Jan 18 21:19:49.683: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 18 21:19:49.683: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 21:19:49.683
    Jan 18 21:19:49.684: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2015 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:19:49.684: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:19:49.684: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:19:49.684: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-2015/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/18/23 21:19:49.865
    Jan 18 21:19:49.865: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2015 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:19:49.865: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:19:49.865: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:19:49.865: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/dns-2015/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 21:19:50.039: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:19:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2015" for this suite. 01/18/23 21:19:50.08
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:19:50.095
Jan 18 21:19:50.095: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:19:50.096
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:50.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:50.142
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 21:19:50.146
Jan 18 21:19:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 21:19:57.121
Jan 18 21:19:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:19:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4341" for this suite. 01/18/23 21:20:06.714
------------------------------
• [SLOW TEST] [16.638 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:19:50.095
    Jan 18 21:19:50.095: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:19:50.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:19:50.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:19:50.142
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 21:19:50.146
    Jan 18 21:19:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 21:19:57.121
    Jan 18 21:19:57.122: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:19:59.001: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:06.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4341" for this suite. 01/18/23 21:20:06.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:06.734
Jan 18 21:20:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:20:06.735
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:07.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:07.751
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-5403 01/18/23 21:20:07.753
STEP: creating service affinity-clusterip-transition in namespace services-5403 01/18/23 21:20:07.754
STEP: creating replication controller affinity-clusterip-transition in namespace services-5403 01/18/23 21:20:07.784
I0118 21:20:07.800886      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5403, replica count: 3
I0118 21:20:10.851995      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:20:13.852293      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:20:13.861: INFO: Creating new exec pod
Jan 18 21:20:13.883: INFO: Waiting up to 5m0s for pod "execpod-affinityzbqcr" in namespace "services-5403" to be "running"
Jan 18 21:20:13.888: INFO: Pod "execpod-affinityzbqcr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435382ms
Jan 18 21:20:15.894: INFO: Pod "execpod-affinityzbqcr": Phase="Running", Reason="", readiness=true. Elapsed: 2.011337039s
Jan 18 21:20:15.894: INFO: Pod "execpod-affinityzbqcr" satisfied condition "running"
Jan 18 21:20:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan 18 21:20:17.141: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 21:20:17.141: INFO: stdout: ""
Jan 18 21:20:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c nc -v -z -w 2 10.152.183.30 80'
Jan 18 21:20:17.320: INFO: stderr: "+ nc -v -z -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
Jan 18 21:20:17.320: INFO: stdout: ""
Jan 18 21:20:17.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.30:80/ ; done'
Jan 18 21:20:17.675: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n"
Jan 18 21:20:17.676: INFO: stdout: "\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt"
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
Jan 18 21:20:17.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.30:80/ ; done'
Jan 18 21:20:17.978: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n"
Jan 18 21:20:17.978: INFO: stdout: "\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg"
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
Jan 18 21:20:17.978: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5403, will wait for the garbage collector to delete the pods 01/18/23 21:20:18.03
Jan 18 21:20:18.202: INFO: Deleting ReplicationController affinity-clusterip-transition took: 115.026675ms
Jan 18 21:20:18.503: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 301.222163ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5403" for this suite. 01/18/23 21:20:21.052
------------------------------
• [SLOW TEST] [14.334 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:06.734
    Jan 18 21:20:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:20:06.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:07.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:07.751
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-5403 01/18/23 21:20:07.753
    STEP: creating service affinity-clusterip-transition in namespace services-5403 01/18/23 21:20:07.754
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5403 01/18/23 21:20:07.784
    I0118 21:20:07.800886      20 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5403, replica count: 3
    I0118 21:20:10.851995      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:20:13.852293      20 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:20:13.861: INFO: Creating new exec pod
    Jan 18 21:20:13.883: INFO: Waiting up to 5m0s for pod "execpod-affinityzbqcr" in namespace "services-5403" to be "running"
    Jan 18 21:20:13.888: INFO: Pod "execpod-affinityzbqcr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.435382ms
    Jan 18 21:20:15.894: INFO: Pod "execpod-affinityzbqcr": Phase="Running", Reason="", readiness=true. Elapsed: 2.011337039s
    Jan 18 21:20:15.894: INFO: Pod "execpod-affinityzbqcr" satisfied condition "running"
    Jan 18 21:20:16.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan 18 21:20:17.141: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 21:20:17.141: INFO: stdout: ""
    Jan 18 21:20:17.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c nc -v -z -w 2 10.152.183.30 80'
    Jan 18 21:20:17.320: INFO: stderr: "+ nc -v -z -w 2 10.152.183.30 80\nConnection to 10.152.183.30 80 port [tcp/http] succeeded!\n"
    Jan 18 21:20:17.320: INFO: stdout: ""
    Jan 18 21:20:17.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.30:80/ ; done'
    Jan 18 21:20:17.675: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n"
    Jan 18 21:20:17.676: INFO: stdout: "\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-kv8pt\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-6wzxx\naffinity-clusterip-transition-kv8pt"
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-6wzxx
    Jan 18 21:20:17.676: INFO: Received response from host: affinity-clusterip-transition-kv8pt
    Jan 18 21:20:17.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5403 exec execpod-affinityzbqcr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.30:80/ ; done'
    Jan 18 21:20:17.978: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.30:80/\n"
    Jan 18 21:20:17.978: INFO: stdout: "\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg\naffinity-clusterip-transition-c9xbg"
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Received response from host: affinity-clusterip-transition-c9xbg
    Jan 18 21:20:17.978: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5403, will wait for the garbage collector to delete the pods 01/18/23 21:20:18.03
    Jan 18 21:20:18.202: INFO: Deleting ReplicationController affinity-clusterip-transition took: 115.026675ms
    Jan 18 21:20:18.503: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 301.222163ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:21.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5403" for this suite. 01/18/23 21:20:21.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:21.069
Jan 18 21:20:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:20:21.071
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:21.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:21.519
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-c2932437-c344-40ca-9b4c-d6395c072ea7 01/18/23 21:20:21.522
STEP: Creating a pod to test consume configMaps 01/18/23 21:20:21.537
Jan 18 21:20:21.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3" in namespace "projected-4651" to be "Succeeded or Failed"
Jan 18 21:20:21.569: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315693ms
Jan 18 21:20:23.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01292637s
Jan 18 21:20:25.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013617129s
STEP: Saw pod success 01/18/23 21:20:25.575
Jan 18 21:20:25.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3" satisfied condition "Succeeded or Failed"
Jan 18 21:20:25.580: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 21:20:25.663
Jan 18 21:20:25.692: INFO: Waiting for pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 to disappear
Jan 18 21:20:25.700: INFO: Pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:25.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4651" for this suite. 01/18/23 21:20:25.705
------------------------------
• [4.650 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:21.069
    Jan 18 21:20:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:20:21.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:21.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:21.519
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-c2932437-c344-40ca-9b4c-d6395c072ea7 01/18/23 21:20:21.522
    STEP: Creating a pod to test consume configMaps 01/18/23 21:20:21.537
    Jan 18 21:20:21.562: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3" in namespace "projected-4651" to be "Succeeded or Failed"
    Jan 18 21:20:21.569: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.315693ms
    Jan 18 21:20:23.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01292637s
    Jan 18 21:20:25.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013617129s
    STEP: Saw pod success 01/18/23 21:20:25.575
    Jan 18 21:20:25.575: INFO: Pod "pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3" satisfied condition "Succeeded or Failed"
    Jan 18 21:20:25.580: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:20:25.663
    Jan 18 21:20:25.692: INFO: Waiting for pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 to disappear
    Jan 18 21:20:25.700: INFO: Pod pod-projected-configmaps-1beeb221-09fb-4e3d-a95e-a1a77117aec3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:25.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4651" for this suite. 01/18/23 21:20:25.705
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:25.72
Jan 18 21:20:25.720: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:20:25.721
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:26.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:26.344
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan 18 21:20:26.354: INFO: Got root ca configmap in namespace "svcaccounts-1986"
Jan 18 21:20:26.377: INFO: Deleted root ca configmap in namespace "svcaccounts-1986"
STEP: waiting for a new root ca configmap created 01/18/23 21:20:26.878
Jan 18 21:20:26.883: INFO: Recreated root ca configmap in namespace "svcaccounts-1986"
Jan 18 21:20:26.898: INFO: Updated root ca configmap in namespace "svcaccounts-1986"
STEP: waiting for the root ca configmap reconciled 01/18/23 21:20:27.398
Jan 18 21:20:27.405: INFO: Reconciled root ca configmap in namespace "svcaccounts-1986"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:27.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1986" for this suite. 01/18/23 21:20:27.412
------------------------------
• [1.709 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:25.72
    Jan 18 21:20:25.720: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:20:25.721
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:26.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:26.344
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan 18 21:20:26.354: INFO: Got root ca configmap in namespace "svcaccounts-1986"
    Jan 18 21:20:26.377: INFO: Deleted root ca configmap in namespace "svcaccounts-1986"
    STEP: waiting for a new root ca configmap created 01/18/23 21:20:26.878
    Jan 18 21:20:26.883: INFO: Recreated root ca configmap in namespace "svcaccounts-1986"
    Jan 18 21:20:26.898: INFO: Updated root ca configmap in namespace "svcaccounts-1986"
    STEP: waiting for the root ca configmap reconciled 01/18/23 21:20:27.398
    Jan 18 21:20:27.405: INFO: Reconciled root ca configmap in namespace "svcaccounts-1986"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:27.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1986" for this suite. 01/18/23 21:20:27.412
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:27.429
Jan 18 21:20:27.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename events 01/18/23 21:20:27.431
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:27.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:27.874
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/18/23 21:20:27.877
STEP: listing all events in all namespaces 01/18/23 21:20:27.894
STEP: patching the test event 01/18/23 21:20:27.904
STEP: fetching the test event 01/18/23 21:20:27.918
STEP: updating the test event 01/18/23 21:20:27.922
STEP: getting the test event 01/18/23 21:20:27.942
STEP: deleting the test event 01/18/23 21:20:27.946
STEP: listing all events in all namespaces 01/18/23 21:20:27.962
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:27.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2685" for this suite. 01/18/23 21:20:27.973
------------------------------
• [0.561 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:27.429
    Jan 18 21:20:27.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename events 01/18/23 21:20:27.431
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:27.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:27.874
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/18/23 21:20:27.877
    STEP: listing all events in all namespaces 01/18/23 21:20:27.894
    STEP: patching the test event 01/18/23 21:20:27.904
    STEP: fetching the test event 01/18/23 21:20:27.918
    STEP: updating the test event 01/18/23 21:20:27.922
    STEP: getting the test event 01/18/23 21:20:27.942
    STEP: deleting the test event 01/18/23 21:20:27.946
    STEP: listing all events in all namespaces 01/18/23 21:20:27.962
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:27.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2685" for this suite. 01/18/23 21:20:27.973
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:27.991
Jan 18 21:20:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption 01/18/23 21:20:27.992
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:28.447
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:28.449
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 21:20:28.452
STEP: Waiting for the pdb to be processed 01/18/23 21:20:28.469
STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 21:20:30.492
STEP: Waiting for all pods to be running 01/18/23 21:20:30.493
Jan 18 21:20:30.501: INFO: pods: 0 < 3
Jan 18 21:20:32.512: INFO: running pods: 2 < 3
STEP: locating a running pod 01/18/23 21:20:34.508
STEP: Updating the pdb to allow a pod to be evicted 01/18/23 21:20:34.522
STEP: Waiting for the pdb to be processed 01/18/23 21:20:34.537
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:20:36.548
STEP: Waiting for all pods to be running 01/18/23 21:20:36.549
STEP: Waiting for the pdb to observed all healthy pods 01/18/23 21:20:36.553
STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 21:20:36.606
STEP: Waiting for the pdb to be processed 01/18/23 21:20:36.642
STEP: Waiting for all pods to be running 01/18/23 21:20:38.654
STEP: locating a running pod 01/18/23 21:20:38.66
STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 21:20:38.674
STEP: Waiting for the pdb to be deleted 01/18/23 21:20:38.71
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:20:38.715
STEP: Waiting for all pods to be running 01/18/23 21:20:38.715
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2230" for this suite. 01/18/23 21:20:38.778
------------------------------
• [SLOW TEST] [10.815 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:27.991
    Jan 18 21:20:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption 01/18/23 21:20:27.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:28.447
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:28.449
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 21:20:28.452
    STEP: Waiting for the pdb to be processed 01/18/23 21:20:28.469
    STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 21:20:30.492
    STEP: Waiting for all pods to be running 01/18/23 21:20:30.493
    Jan 18 21:20:30.501: INFO: pods: 0 < 3
    Jan 18 21:20:32.512: INFO: running pods: 2 < 3
    STEP: locating a running pod 01/18/23 21:20:34.508
    STEP: Updating the pdb to allow a pod to be evicted 01/18/23 21:20:34.522
    STEP: Waiting for the pdb to be processed 01/18/23 21:20:34.537
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:20:36.548
    STEP: Waiting for all pods to be running 01/18/23 21:20:36.549
    STEP: Waiting for the pdb to observed all healthy pods 01/18/23 21:20:36.553
    STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 21:20:36.606
    STEP: Waiting for the pdb to be processed 01/18/23 21:20:36.642
    STEP: Waiting for all pods to be running 01/18/23 21:20:38.654
    STEP: locating a running pod 01/18/23 21:20:38.66
    STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 21:20:38.674
    STEP: Waiting for the pdb to be deleted 01/18/23 21:20:38.71
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 21:20:38.715
    STEP: Waiting for all pods to be running 01/18/23 21:20:38.715
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:38.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2230" for this suite. 01/18/23 21:20:38.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:38.81
Jan 18 21:20:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:20:38.811
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:39.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:39.212
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/18/23 21:20:39.254
Jan 18 21:20:39.254: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b" in namespace "kubelet-test-923" to be "completed"
Jan 18 21:20:39.259: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323157ms
Jan 18 21:20:41.265: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011058496s
Jan 18 21:20:43.270: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015903264s
Jan 18 21:20:43.271: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:20:43.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-923" for this suite. 01/18/23 21:20:43.29
------------------------------
• [4.498 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:38.81
    Jan 18 21:20:38.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:20:38.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:39.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:39.212
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/18/23 21:20:39.254
    Jan 18 21:20:39.254: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b" in namespace "kubelet-test-923" to be "completed"
    Jan 18 21:20:39.259: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.323157ms
    Jan 18 21:20:41.265: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011058496s
    Jan 18 21:20:43.270: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015903264s
    Jan 18 21:20:43.271: INFO: Pod "agnhost-host-aliasesd3748967-7616-430c-8020-6c8ba4dd1f6b" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:20:43.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-923" for this suite. 01/18/23 21:20:43.29
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:20:43.309
Jan 18 21:20:43.309: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:20:43.31
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:43.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:43.545
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7943 01/18/23 21:20:43.548
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/18/23 21:20:43.567
Jan 18 21:20:43.594: INFO: Found 0 stateful pods, waiting for 3
Jan 18 21:20:53.602: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:20:53.602: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:20:53.602: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 21:20:53.616
Jan 18 21:20:53.645: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 21:20:53.645
STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 21:21:03.671
STEP: Performing a canary update 01/18/23 21:21:03.672
Jan 18 21:21:03.700: INFO: Updating stateful set ss2
Jan 18 21:21:03.710: INFO: Waiting for Pod statefulset-7943/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 21:21:13.724
Jan 18 21:21:14.012: INFO: Found 1 stateful pods, waiting for 3
Jan 18 21:21:24.022: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:21:24.022: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:21:24.022: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/18/23 21:21:24.032
Jan 18 21:21:24.064: INFO: Updating stateful set ss2
Jan 18 21:21:24.073: INFO: Waiting for Pod statefulset-7943/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan 18 21:21:34.120: INFO: Updating stateful set ss2
Jan 18 21:21:34.130: INFO: Waiting for StatefulSet statefulset-7943/ss2 to complete update
Jan 18 21:21:34.130: INFO: Waiting for Pod statefulset-7943/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:21:44.144: INFO: Deleting all statefulset in ns statefulset-7943
Jan 18 21:21:44.148: INFO: Scaling statefulset ss2 to 0
Jan 18 21:21:54.179: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:21:54.183: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:21:54.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7943" for this suite. 01/18/23 21:21:54.217
------------------------------
• [SLOW TEST] [70.926 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:20:43.309
    Jan 18 21:20:43.309: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:20:43.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:20:43.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:20:43.545
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7943 01/18/23 21:20:43.548
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/18/23 21:20:43.567
    Jan 18 21:20:43.594: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 21:20:53.602: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:20:53.602: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:20:53.602: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 21:20:53.616
    Jan 18 21:20:53.645: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 21:20:53.645
    STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 21:21:03.671
    STEP: Performing a canary update 01/18/23 21:21:03.672
    Jan 18 21:21:03.700: INFO: Updating stateful set ss2
    Jan 18 21:21:03.710: INFO: Waiting for Pod statefulset-7943/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 21:21:13.724
    Jan 18 21:21:14.012: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 21:21:24.022: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:21:24.022: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:21:24.022: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/18/23 21:21:24.032
    Jan 18 21:21:24.064: INFO: Updating stateful set ss2
    Jan 18 21:21:24.073: INFO: Waiting for Pod statefulset-7943/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan 18 21:21:34.120: INFO: Updating stateful set ss2
    Jan 18 21:21:34.130: INFO: Waiting for StatefulSet statefulset-7943/ss2 to complete update
    Jan 18 21:21:34.130: INFO: Waiting for Pod statefulset-7943/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:21:44.144: INFO: Deleting all statefulset in ns statefulset-7943
    Jan 18 21:21:44.148: INFO: Scaling statefulset ss2 to 0
    Jan 18 21:21:54.179: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:21:54.183: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:21:54.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7943" for this suite. 01/18/23 21:21:54.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:21:54.238
Jan 18 21:21:54.238: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:21:54.239
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:21:54.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:21:54.875
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 21:21:54.877
Jan 18 21:21:54.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:21:57.279: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:05.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5706" for this suite. 01/18/23 21:22:05.209
------------------------------
• [SLOW TEST] [10.986 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:21:54.238
    Jan 18 21:21:54.238: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:21:54.239
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:21:54.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:21:54.875
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 21:21:54.877
    Jan 18 21:21:54.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:21:57.279: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:05.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5706" for this suite. 01/18/23 21:22:05.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:05.226
Jan 18 21:22:05.226: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:22:05.227
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:05.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:05.262
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:22:05.3
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:22:06.099
STEP: Deploying the webhook pod 01/18/23 21:22:06.118
STEP: Wait for the deployment to be ready 01/18/23 21:22:06.142
Jan 18 21:22:06.156: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:22:08.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:22:10.179
STEP: Verifying the service has paired with the endpoint 01/18/23 21:22:10.203
Jan 18 21:22:11.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan 18 21:22:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7101-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:22:11.737
STEP: Creating a custom resource while v1 is storage version 01/18/23 21:22:12.782
STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 21:22:16.041
STEP: Patching the custom resource while v2 is storage version 01/18/23 21:22:16.072
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:16.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9337" for this suite. 01/18/23 21:22:16.746
STEP: Destroying namespace "webhook-9337-markers" for this suite. 01/18/23 21:22:16.771
------------------------------
• [SLOW TEST] [11.560 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:05.226
    Jan 18 21:22:05.226: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:22:05.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:05.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:05.262
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:22:05.3
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:22:06.099
    STEP: Deploying the webhook pod 01/18/23 21:22:06.118
    STEP: Wait for the deployment to be ready 01/18/23 21:22:06.142
    Jan 18 21:22:06.156: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:22:08.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:22:10.179
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:22:10.203
    Jan 18 21:22:11.203: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan 18 21:22:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7101-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:22:11.737
    STEP: Creating a custom resource while v1 is storage version 01/18/23 21:22:12.782
    STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 21:22:16.041
    STEP: Patching the custom resource while v2 is storage version 01/18/23 21:22:16.072
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:16.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9337" for this suite. 01/18/23 21:22:16.746
    STEP: Destroying namespace "webhook-9337-markers" for this suite. 01/18/23 21:22:16.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:16.787
Jan 18 21:22:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:22:16.789
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:16.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:16.826
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/18/23 21:22:16.83
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:22:16.841
STEP: Creating a ResourceQuota with not terminating scope 01/18/23 21:22:18.847
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:22:18.858
STEP: Creating a long running pod 01/18/23 21:22:20.865
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 21:22:20.898
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 21:22:22.905
STEP: Deleting the pod 01/18/23 21:22:24.912
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:22:25.015
STEP: Creating a terminating pod 01/18/23 21:22:27.022
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 21:22:27.072
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 21:22:29.078
STEP: Deleting the pod 01/18/23 21:22:31.085
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:22:31.361
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:33.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7451" for this suite. 01/18/23 21:22:33.373
------------------------------
• [SLOW TEST] [16.623 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:16.787
    Jan 18 21:22:16.788: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:22:16.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:16.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:16.826
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/18/23 21:22:16.83
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:22:16.841
    STEP: Creating a ResourceQuota with not terminating scope 01/18/23 21:22:18.847
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:22:18.858
    STEP: Creating a long running pod 01/18/23 21:22:20.865
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 21:22:20.898
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 21:22:22.905
    STEP: Deleting the pod 01/18/23 21:22:24.912
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:22:25.015
    STEP: Creating a terminating pod 01/18/23 21:22:27.022
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 21:22:27.072
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 21:22:29.078
    STEP: Deleting the pod 01/18/23 21:22:31.085
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:22:31.361
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:33.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7451" for this suite. 01/18/23 21:22:33.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:33.412
Jan 18 21:22:33.412: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:22:33.413
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:33.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:33.552
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:22:33.555
Jan 18 21:22:33.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2" in namespace "projected-5274" to be "Succeeded or Failed"
Jan 18 21:22:33.588: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.490162ms
Jan 18 21:22:35.595: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012432206s
Jan 18 21:22:37.593: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010997456s
STEP: Saw pod success 01/18/23 21:22:37.594
Jan 18 21:22:37.594: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2" satisfied condition "Succeeded or Failed"
Jan 18 21:22:37.598: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 container client-container: <nil>
STEP: delete the pod 01/18/23 21:22:37.684
Jan 18 21:22:37.766: INFO: Waiting for pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 to disappear
Jan 18 21:22:37.775: INFO: Pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:37.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5274" for this suite. 01/18/23 21:22:37.823
------------------------------
• [4.432 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:33.412
    Jan 18 21:22:33.412: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:22:33.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:33.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:33.552
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:22:33.555
    Jan 18 21:22:33.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2" in namespace "projected-5274" to be "Succeeded or Failed"
    Jan 18 21:22:33.588: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.490162ms
    Jan 18 21:22:35.595: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012432206s
    Jan 18 21:22:37.593: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010997456s
    STEP: Saw pod success 01/18/23 21:22:37.594
    Jan 18 21:22:37.594: INFO: Pod "downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2" satisfied condition "Succeeded or Failed"
    Jan 18 21:22:37.598: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:22:37.684
    Jan 18 21:22:37.766: INFO: Waiting for pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 to disappear
    Jan 18 21:22:37.775: INFO: Pod downwardapi-volume-0a313f9e-f0ea-4ae7-9822-a1eedd8e0ec2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:37.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5274" for this suite. 01/18/23 21:22:37.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:37.845
Jan 18 21:22:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:22:37.846
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:37.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:37.902
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:41.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2673" for this suite. 01/18/23 21:22:41.94
------------------------------
• [4.135 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:37.845
    Jan 18 21:22:37.845: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:22:37.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:37.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:37.902
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:41.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2673" for this suite. 01/18/23 21:22:41.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:41.989
Jan 18 21:22:41.989: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 21:22:41.99
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:42.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:42.061
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/18/23 21:22:42.067
STEP: Ensure pods equal to parallelism count is attached to the job 01/18/23 21:22:42.097
STEP: patching /status 01/18/23 21:22:46.103
STEP: updating /status 01/18/23 21:22:46.165
STEP: get /status 01/18/23 21:22:46.176
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:46.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8840" for this suite. 01/18/23 21:22:46.186
------------------------------
• [4.210 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:41.989
    Jan 18 21:22:41.989: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 21:22:41.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:42.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:42.061
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/18/23 21:22:42.067
    STEP: Ensure pods equal to parallelism count is attached to the job 01/18/23 21:22:42.097
    STEP: patching /status 01/18/23 21:22:46.103
    STEP: updating /status 01/18/23 21:22:46.165
    STEP: get /status 01/18/23 21:22:46.176
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:46.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8840" for this suite. 01/18/23 21:22:46.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:46.202
Jan 18 21:22:46.202: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:22:46.203
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:46.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:46.263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:22:46.294
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:22:46.803
STEP: Deploying the webhook pod 01/18/23 21:22:46.853
STEP: Wait for the deployment to be ready 01/18/23 21:22:46.914
Jan 18 21:22:46.926: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:22:48.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:22:50.952
STEP: Verifying the service has paired with the endpoint 01/18/23 21:22:51.061
Jan 18 21:22:52.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan 18 21:22:52.068: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1695-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:22:52.622
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:22:53.648
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:22:57.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1220" for this suite. 01/18/23 21:22:57.717
STEP: Destroying namespace "webhook-1220-markers" for this suite. 01/18/23 21:22:57.732
------------------------------
• [SLOW TEST] [11.574 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:46.202
    Jan 18 21:22:46.202: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:22:46.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:46.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:46.263
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:22:46.294
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:22:46.803
    STEP: Deploying the webhook pod 01/18/23 21:22:46.853
    STEP: Wait for the deployment to be ready 01/18/23 21:22:46.914
    Jan 18 21:22:46.926: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:22:48.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:22:50.952
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:22:51.061
    Jan 18 21:22:52.061: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan 18 21:22:52.068: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1695-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:22:52.622
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:22:53.648
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:22:57.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1220" for this suite. 01/18/23 21:22:57.717
    STEP: Destroying namespace "webhook-1220-markers" for this suite. 01/18/23 21:22:57.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:22:57.778
Jan 18 21:22:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename aggregator 01/18/23 21:22:57.779
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:57.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:57.821
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 18 21:22:57.824: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/18/23 21:22:57.825
Jan 18 21:22:58.534: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Jan 18 21:23:00.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:02.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:04.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:06.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:08.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:10.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:12.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:14.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:16.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:18.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:20.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:23:26.524: INFO: Waited 1.353094526s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 21:23:26.576
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 21:23:26.58
STEP: List APIServices 01/18/23 21:23:26.595
Jan 18 21:23:26.603: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan 18 21:23:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-4210" for this suite. 01/18/23 21:23:27.873
------------------------------
• [SLOW TEST] [30.158 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:22:57.778
    Jan 18 21:22:57.778: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename aggregator 01/18/23 21:22:57.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:22:57.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:22:57.821
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 18 21:22:57.824: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/18/23 21:22:57.825
    Jan 18 21:22:58.534: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
    Jan 18 21:23:00.619: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:02.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:04.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:06.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:08.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:10.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:12.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:14.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:16.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:18.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:20.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 22, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:23:26.524: INFO: Waited 1.353094526s for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 21:23:26.576
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 21:23:26.58
    STEP: List APIServices 01/18/23 21:23:26.595
    Jan 18 21:23:26.603: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:23:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-4210" for this suite. 01/18/23 21:23:27.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:23:27.942
Jan 18 21:23:27.942: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:23:27.944
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:27.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:27.993
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:23:28.02
Jan 18 21:23:28.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289" in namespace "downward-api-8383" to be "Succeeded or Failed"
Jan 18 21:23:28.045: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.902966ms
Jan 18 21:23:30.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010922452s
Jan 18 21:23:32.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011324762s
Jan 18 21:23:34.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011270967s
STEP: Saw pod success 01/18/23 21:23:34.051
Jan 18 21:23:34.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289" satisfied condition "Succeeded or Failed"
Jan 18 21:23:34.055: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 container client-container: <nil>
STEP: delete the pod 01/18/23 21:23:34.063
Jan 18 21:23:34.089: INFO: Waiting for pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 to disappear
Jan 18 21:23:34.097: INFO: Pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:23:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8383" for this suite. 01/18/23 21:23:34.102
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:23:27.942
    Jan 18 21:23:27.942: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:23:27.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:27.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:27.993
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:23:28.02
    Jan 18 21:23:28.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289" in namespace "downward-api-8383" to be "Succeeded or Failed"
    Jan 18 21:23:28.045: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.902966ms
    Jan 18 21:23:30.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010922452s
    Jan 18 21:23:32.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011324762s
    Jan 18 21:23:34.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011270967s
    STEP: Saw pod success 01/18/23 21:23:34.051
    Jan 18 21:23:34.051: INFO: Pod "downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289" satisfied condition "Succeeded or Failed"
    Jan 18 21:23:34.055: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:23:34.063
    Jan 18 21:23:34.089: INFO: Waiting for pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 to disappear
    Jan 18 21:23:34.097: INFO: Pod downwardapi-volume-b37e5e2e-151f-4c90-a056-3a640a96d289 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:23:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8383" for this suite. 01/18/23 21:23:34.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:23:34.118
Jan 18 21:23:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:23:34.119
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:34.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:34.151
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-7349ada8-eaf9-43ac-b2ab-fbd67046ca29 01/18/23 21:23:34.155
STEP: Creating a pod to test consume configMaps 01/18/23 21:23:34.165
Jan 18 21:23:34.182: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167" in namespace "projected-2644" to be "Succeeded or Failed"
Jan 18 21:23:34.186: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657649ms
Jan 18 21:23:36.192: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009988556s
Jan 18 21:23:38.191: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009053937s
STEP: Saw pod success 01/18/23 21:23:38.191
Jan 18 21:23:38.191: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167" satisfied condition "Succeeded or Failed"
Jan 18 21:23:38.195: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:23:38.203
Jan 18 21:23:38.228: INFO: Waiting for pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 to disappear
Jan 18 21:23:38.235: INFO: Pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:23:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2644" for this suite. 01/18/23 21:23:38.239
------------------------------
• [4.134 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:23:34.118
    Jan 18 21:23:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:23:34.119
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:34.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:34.151
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-7349ada8-eaf9-43ac-b2ab-fbd67046ca29 01/18/23 21:23:34.155
    STEP: Creating a pod to test consume configMaps 01/18/23 21:23:34.165
    Jan 18 21:23:34.182: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167" in namespace "projected-2644" to be "Succeeded or Failed"
    Jan 18 21:23:34.186: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Pending", Reason="", readiness=false. Elapsed: 3.657649ms
    Jan 18 21:23:36.192: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009988556s
    Jan 18 21:23:38.191: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009053937s
    STEP: Saw pod success 01/18/23 21:23:38.191
    Jan 18 21:23:38.191: INFO: Pod "pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167" satisfied condition "Succeeded or Failed"
    Jan 18 21:23:38.195: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:23:38.203
    Jan 18 21:23:38.228: INFO: Waiting for pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 to disappear
    Jan 18 21:23:38.235: INFO: Pod pod-projected-configmaps-0252716e-3bd7-4df0-ab16-8c55e0dc5167 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:23:38.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2644" for this suite. 01/18/23 21:23:38.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:23:38.253
Jan 18 21:23:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename watch 01/18/23 21:23:38.254
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:38.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:38.286
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/18/23 21:23:38.289
STEP: creating a new configmap 01/18/23 21:23:38.29
STEP: modifying the configmap once 01/18/23 21:23:38.299
STEP: changing the label value of the configmap 01/18/23 21:23:38.312
STEP: Expecting to observe a delete notification for the watched object 01/18/23 21:23:38.326
Jan 18 21:23:38.326: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12634 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 21:23:38.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12635 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 21:23:38.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12636 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/18/23 21:23:38.327
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 21:23:38.34
STEP: changing the label value of the configmap back 01/18/23 21:23:48.341
STEP: modifying the configmap a third time 01/18/23 21:23:48.357
STEP: deleting the configmap 01/18/23 21:23:48.37
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 21:23:48.383
Jan 18 21:23:48.383: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12672 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 21:23:48.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12673 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 21:23:48.384: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12674 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 21:23:48.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8260" for this suite. 01/18/23 21:23:48.389
------------------------------
• [SLOW TEST] [10.151 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:23:38.253
    Jan 18 21:23:38.253: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename watch 01/18/23 21:23:38.254
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:38.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:38.286
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/18/23 21:23:38.289
    STEP: creating a new configmap 01/18/23 21:23:38.29
    STEP: modifying the configmap once 01/18/23 21:23:38.299
    STEP: changing the label value of the configmap 01/18/23 21:23:38.312
    STEP: Expecting to observe a delete notification for the watched object 01/18/23 21:23:38.326
    Jan 18 21:23:38.326: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12634 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 21:23:38.326: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12635 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 21:23:38.326: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12636 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:38 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/18/23 21:23:38.327
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 21:23:38.34
    STEP: changing the label value of the configmap back 01/18/23 21:23:48.341
    STEP: modifying the configmap a third time 01/18/23 21:23:48.357
    STEP: deleting the configmap 01/18/23 21:23:48.37
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 21:23:48.383
    Jan 18 21:23:48.383: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12672 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 21:23:48.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12673 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 21:23:48.384: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8260  e24f9867-7172-49ad-9417-9c8148af42bc 12674 0 2023-01-18 21:23:38 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 21:23:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:23:48.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8260" for this suite. 01/18/23 21:23:48.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:23:48.406
Jan 18 21:23:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename prestop 01/18/23 21:23:48.407
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:48.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:48.452
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6281 01/18/23 21:23:48.455
STEP: Waiting for pods to come up. 01/18/23 21:23:48.47
Jan 18 21:23:48.470: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6281" to be "running"
Jan 18 21:23:48.474: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447047ms
Jan 18 21:23:50.479: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008978729s
Jan 18 21:23:50.479: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6281 01/18/23 21:23:50.483
Jan 18 21:23:50.496: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6281" to be "running"
Jan 18 21:23:50.500: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231257ms
Jan 18 21:23:52.505: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009261128s
Jan 18 21:23:52.505: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/18/23 21:23:52.505
Jan 18 21:23:57.526: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/18/23 21:23:57.526
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan 18 21:23:57.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-6281" for this suite. 01/18/23 21:23:57.56
------------------------------
• [SLOW TEST] [9.169 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:23:48.406
    Jan 18 21:23:48.406: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename prestop 01/18/23 21:23:48.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:48.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:48.452
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6281 01/18/23 21:23:48.455
    STEP: Waiting for pods to come up. 01/18/23 21:23:48.47
    Jan 18 21:23:48.470: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6281" to be "running"
    Jan 18 21:23:48.474: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447047ms
    Jan 18 21:23:50.479: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008978729s
    Jan 18 21:23:50.479: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6281 01/18/23 21:23:50.483
    Jan 18 21:23:50.496: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6281" to be "running"
    Jan 18 21:23:50.500: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 4.231257ms
    Jan 18 21:23:52.505: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.009261128s
    Jan 18 21:23:52.505: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/18/23 21:23:52.505
    Jan 18 21:23:57.526: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/18/23 21:23:57.526
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:23:57.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-6281" for this suite. 01/18/23 21:23:57.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:23:57.576
Jan 18 21:23:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:23:57.578
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:57.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:57.617
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-3312 01/18/23 21:23:57.62
STEP: creating a selector 01/18/23 21:23:57.62
STEP: Creating the service pods in kubernetes 01/18/23 21:23:57.62
Jan 18 21:23:57.620: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 21:23:57.660: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3312" to be "running and ready"
Jan 18 21:23:57.665: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787955ms
Jan 18 21:23:57.665: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:23:59.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010839414s
Jan 18 21:23:59.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:01.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010779365s
Jan 18 21:24:01.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:03.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01074789s
Jan 18 21:24:03.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:05.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0095404s
Jan 18 21:24:05.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:07.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00997333s
Jan 18 21:24:07.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:09.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012487778s
Jan 18 21:24:09.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:11.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010726367s
Jan 18 21:24:11.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:13.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009753865s
Jan 18 21:24:13.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:15.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010451582s
Jan 18 21:24:15.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:17.697: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.037121718s
Jan 18 21:24:17.697: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 21:24:19.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010954843s
Jan 18 21:24:19.671: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 21:24:19.671: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 21:24:19.676: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3312" to be "running and ready"
Jan 18 21:24:19.681: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.901563ms
Jan 18 21:24:19.681: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 21:24:19.681: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 21:24:19.686
Jan 18 21:24:19.699: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3312" to be "running"
Jan 18 21:24:19.705: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081078ms
Jan 18 21:24:21.709: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010710036s
Jan 18 21:24:21.709: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 21:24:21.719: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 21:24:21.719: INFO: Breadth first check of 10.1.132.27 on host 10.0.0.5...
Jan 18 21:24:21.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.63:9080/dial?request=hostname&protocol=udp&host=10.1.132.27&port=8081&tries=1'] Namespace:pod-network-test-3312 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:24:21.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:24:21.725: INFO: ExecWithOptions: Clientset creation
Jan 18 21:24:21.725: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3312/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.63%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.132.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:24:21.886: INFO: Waiting for responses: map[]
Jan 18 21:24:21.886: INFO: reached 10.1.132.27 after 0/1 tries
Jan 18 21:24:21.886: INFO: Breadth first check of 10.1.192.45 on host 10.0.0.4...
Jan 18 21:24:21.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.63:9080/dial?request=hostname&protocol=udp&host=10.1.192.45&port=8081&tries=1'] Namespace:pod-network-test-3312 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 21:24:21.891: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 21:24:21.892: INFO: ExecWithOptions: Clientset creation
Jan 18 21:24:21.892: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3312/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.63%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.192.45%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 21:24:22.053: INFO: Waiting for responses: map[]
Jan 18 21:24:22.053: INFO: reached 10.1.192.45 after 0/1 tries
Jan 18 21:24:22.053: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:22.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3312" for this suite. 01/18/23 21:24:22.06
------------------------------
• [SLOW TEST] [24.502 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:23:57.576
    Jan 18 21:23:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 21:23:57.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:23:57.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:23:57.617
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-3312 01/18/23 21:23:57.62
    STEP: creating a selector 01/18/23 21:23:57.62
    STEP: Creating the service pods in kubernetes 01/18/23 21:23:57.62
    Jan 18 21:23:57.620: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 21:23:57.660: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3312" to be "running and ready"
    Jan 18 21:23:57.665: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.787955ms
    Jan 18 21:23:57.665: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:23:59.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010839414s
    Jan 18 21:23:59.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:01.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010779365s
    Jan 18 21:24:01.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:03.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01074789s
    Jan 18 21:24:03.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:05.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.0095404s
    Jan 18 21:24:05.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:07.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.00997333s
    Jan 18 21:24:07.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:09.673: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.012487778s
    Jan 18 21:24:09.673: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:11.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010726367s
    Jan 18 21:24:11.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:13.670: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009753865s
    Jan 18 21:24:13.670: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:15.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010451582s
    Jan 18 21:24:15.671: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:17.697: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.037121718s
    Jan 18 21:24:17.697: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 21:24:19.671: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010954843s
    Jan 18 21:24:19.671: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 21:24:19.671: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 21:24:19.676: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3312" to be "running and ready"
    Jan 18 21:24:19.681: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.901563ms
    Jan 18 21:24:19.681: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 21:24:19.681: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 21:24:19.686
    Jan 18 21:24:19.699: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3312" to be "running"
    Jan 18 21:24:19.705: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081078ms
    Jan 18 21:24:21.709: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010710036s
    Jan 18 21:24:21.709: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 21:24:21.719: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 21:24:21.719: INFO: Breadth first check of 10.1.132.27 on host 10.0.0.5...
    Jan 18 21:24:21.724: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.63:9080/dial?request=hostname&protocol=udp&host=10.1.132.27&port=8081&tries=1'] Namespace:pod-network-test-3312 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:24:21.724: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:24:21.725: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:24:21.725: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3312/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.63%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.132.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:24:21.886: INFO: Waiting for responses: map[]
    Jan 18 21:24:21.886: INFO: reached 10.1.132.27 after 0/1 tries
    Jan 18 21:24:21.886: INFO: Breadth first check of 10.1.192.45 on host 10.0.0.4...
    Jan 18 21:24:21.891: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.63:9080/dial?request=hostname&protocol=udp&host=10.1.192.45&port=8081&tries=1'] Namespace:pod-network-test-3312 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 21:24:21.891: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 21:24:21.892: INFO: ExecWithOptions: Clientset creation
    Jan 18 21:24:21.892: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-3312/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.63%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.1.192.45%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 21:24:22.053: INFO: Waiting for responses: map[]
    Jan 18 21:24:22.053: INFO: reached 10.1.192.45 after 0/1 tries
    Jan 18 21:24:22.053: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:22.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3312" for this suite. 01/18/23 21:24:22.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:22.081
Jan 18 21:24:22.081: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:24:22.082
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:22.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:22.12
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/18/23 21:24:22.122
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:24:22.136
STEP: Creating a ResourceQuota with not best effort scope 01/18/23 21:24:24.142
STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:24:24.156
STEP: Creating a best-effort pod 01/18/23 21:24:26.162
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 21:24:26.194
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 21:24:28.2
STEP: Deleting the pod 01/18/23 21:24:30.207
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:24:30.238
STEP: Creating a not best-effort pod 01/18/23 21:24:32.243
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 21:24:32.266
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 21:24:34.271
STEP: Deleting the pod 01/18/23 21:24:36.275
STEP: Ensuring resource quota status released the pod usage 01/18/23 21:24:36.301
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:38.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5413" for this suite. 01/18/23 21:24:38.312
------------------------------
• [SLOW TEST] [16.244 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:22.081
    Jan 18 21:24:22.081: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:24:22.082
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:22.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:22.12
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/18/23 21:24:22.122
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:24:22.136
    STEP: Creating a ResourceQuota with not best effort scope 01/18/23 21:24:24.142
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 21:24:24.156
    STEP: Creating a best-effort pod 01/18/23 21:24:26.162
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 21:24:26.194
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 21:24:28.2
    STEP: Deleting the pod 01/18/23 21:24:30.207
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:24:30.238
    STEP: Creating a not best-effort pod 01/18/23 21:24:32.243
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 21:24:32.266
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 21:24:34.271
    STEP: Deleting the pod 01/18/23 21:24:36.275
    STEP: Ensuring resource quota status released the pod usage 01/18/23 21:24:36.301
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:38.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5413" for this suite. 01/18/23 21:24:38.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:38.329
Jan 18 21:24:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:24:38.33
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:38.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:38.372
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 21:24:38.375
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:24:38.85
STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:24:38.868
STEP: Wait for the deployment to be ready 01/18/23 21:24:38.89
Jan 18 21:24:38.901: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:24:40.915
STEP: Verifying the service has paired with the endpoint 01/18/23 21:24:40.94
Jan 18 21:24:41.941: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 18 21:24:41.945: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Creating a v1 custom resource 01/18/23 21:24:45.584
STEP: v2 custom resource should be converted 01/18/23 21:24:45.597
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-1202" for this suite. 01/18/23 21:24:46.235
------------------------------
• [SLOW TEST] [7.949 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:38.329
    Jan 18 21:24:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 21:24:38.33
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:38.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:38.372
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 21:24:38.375
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 21:24:38.85
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 21:24:38.868
    STEP: Wait for the deployment to be ready 01/18/23 21:24:38.89
    Jan 18 21:24:38.901: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:24:40.915
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:24:40.94
    Jan 18 21:24:41.941: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 18 21:24:41.945: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Creating a v1 custom resource 01/18/23 21:24:45.584
    STEP: v2 custom resource should be converted 01/18/23 21:24:45.597
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-1202" for this suite. 01/18/23 21:24:46.235
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:46.278
Jan 18 21:24:46.278: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:24:46.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:46.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:46.328
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/18/23 21:24:46.378
Jan 18 21:24:46.401: INFO: Waiting up to 5m0s for pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1" in namespace "downward-api-3041" to be "Succeeded or Failed"
Jan 18 21:24:46.405: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555063ms
Jan 18 21:24:48.412: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010855757s
Jan 18 21:24:50.411: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010566499s
STEP: Saw pod success 01/18/23 21:24:50.411
Jan 18 21:24:50.412: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1" satisfied condition "Succeeded or Failed"
Jan 18 21:24:50.418: INFO: Trying to get logs from node test-vm-1 pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:24:50.429
Jan 18 21:24:50.455: INFO: Waiting for pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 to disappear
Jan 18 21:24:50.462: INFO: Pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:50.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3041" for this suite. 01/18/23 21:24:50.466
------------------------------
• [4.200 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:46.278
    Jan 18 21:24:46.278: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:24:46.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:46.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:46.328
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/18/23 21:24:46.378
    Jan 18 21:24:46.401: INFO: Waiting up to 5m0s for pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1" in namespace "downward-api-3041" to be "Succeeded or Failed"
    Jan 18 21:24:46.405: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.555063ms
    Jan 18 21:24:48.412: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010855757s
    Jan 18 21:24:50.411: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010566499s
    STEP: Saw pod success 01/18/23 21:24:50.411
    Jan 18 21:24:50.412: INFO: Pod "downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1" satisfied condition "Succeeded or Failed"
    Jan 18 21:24:50.418: INFO: Trying to get logs from node test-vm-1 pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:24:50.429
    Jan 18 21:24:50.455: INFO: Waiting for pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 to disappear
    Jan 18 21:24:50.462: INFO: Pod downward-api-3cf64599-99cb-4ec4-87dd-2a0ae020dcb1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:50.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3041" for this suite. 01/18/23 21:24:50.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:50.482
Jan 18 21:24:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:24:50.483
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:50.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:50.513
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6542.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/18/23 21:24:50.518
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6542.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/18/23 21:24:50.518
STEP: creating a pod to probe /etc/hosts 01/18/23 21:24:50.518
STEP: submitting the pod to kubernetes 01/18/23 21:24:50.518
Jan 18 21:24:50.535: INFO: Waiting up to 15m0s for pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5" in namespace "dns-6542" to be "running"
Jan 18 21:24:50.539: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386455ms
Jan 18 21:24:52.547: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0123516s
Jan 18 21:24:54.549: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014000166s
Jan 18 21:24:54.549: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:24:54.549
STEP: looking for the results for each expected name from probers 01/18/23 21:24:54.553
Jan 18 21:24:54.572: INFO: DNS probes using dns-6542/dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5 succeeded

STEP: deleting the pod 01/18/23 21:24:54.572
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:54.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6542" for this suite. 01/18/23 21:24:54.609
------------------------------
• [4.139 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:50.482
    Jan 18 21:24:50.482: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:24:50.483
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:50.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:50.513
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6542.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/18/23 21:24:50.518
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6542.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6542.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/18/23 21:24:50.518
    STEP: creating a pod to probe /etc/hosts 01/18/23 21:24:50.518
    STEP: submitting the pod to kubernetes 01/18/23 21:24:50.518
    Jan 18 21:24:50.535: INFO: Waiting up to 15m0s for pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5" in namespace "dns-6542" to be "running"
    Jan 18 21:24:50.539: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386455ms
    Jan 18 21:24:52.547: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0123516s
    Jan 18 21:24:54.549: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5": Phase="Running", Reason="", readiness=true. Elapsed: 4.014000166s
    Jan 18 21:24:54.549: INFO: Pod "dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:24:54.549
    STEP: looking for the results for each expected name from probers 01/18/23 21:24:54.553
    Jan 18 21:24:54.572: INFO: DNS probes using dns-6542/dns-test-c02e74cb-51ca-482f-b78f-8e2a6e5647b5 succeeded

    STEP: deleting the pod 01/18/23 21:24:54.572
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:54.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6542" for this suite. 01/18/23 21:24:54.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:54.622
Jan 18 21:24:54.622: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:24:54.623
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:54.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:54.659
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/18/23 21:24:54.67
STEP: watching for the Service to be added 01/18/23 21:24:54.692
Jan 18 21:24:54.697: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 21:24:54.697: INFO: Service test-service-vtzmk created
STEP: Getting /status 01/18/23 21:24:54.697
Jan 18 21:24:54.702: INFO: Service test-service-vtzmk has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/18/23 21:24:54.702
STEP: watching for the Service to be patched 01/18/23 21:24:54.714
Jan 18 21:24:54.716: INFO: observed Service test-service-vtzmk in namespace services-5789 with annotations: map[] & LoadBalancer: {[]}
Jan 18 21:24:54.716: INFO: Found Service test-service-vtzmk in namespace services-5789 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 21:24:54.716: INFO: Service test-service-vtzmk has service status patched
STEP: updating the ServiceStatus 01/18/23 21:24:54.716
Jan 18 21:24:54.732: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/18/23 21:24:54.732
Jan 18 21:24:54.735: INFO: Observed Service test-service-vtzmk in namespace services-5789 with annotations: map[] & Conditions: {[]}
Jan 18 21:24:54.735: INFO: Observed event: &Service{ObjectMeta:{test-service-vtzmk  services-5789  9c17da36-608a-44a5-9cb3-95038595b288 13087 0 2023-01-18 21:24:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 21:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 21:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.21,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.21],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 21:24:54.735: INFO: Found Service test-service-vtzmk in namespace services-5789 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:24:54.735: INFO: Service test-service-vtzmk has service status updated
STEP: patching the service 01/18/23 21:24:54.735
STEP: watching for the Service to be patched 01/18/23 21:24:54.754
Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
Jan 18 21:24:54.756: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service:patched test-service-static:true]
Jan 18 21:24:54.756: INFO: Service test-service-vtzmk patched
STEP: deleting the service 01/18/23 21:24:54.756
STEP: watching for the Service to be deleted 01/18/23 21:24:54.783
Jan 18 21:24:54.784: INFO: Observed event: ADDED
Jan 18 21:24:54.784: INFO: Observed event: MODIFIED
Jan 18 21:24:54.785: INFO: Observed event: MODIFIED
Jan 18 21:24:54.785: INFO: Observed event: MODIFIED
Jan 18 21:24:54.785: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 21:24:54.785: INFO: Service test-service-vtzmk deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:54.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5789" for this suite. 01/18/23 21:24:54.79
------------------------------
• [0.188 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:54.622
    Jan 18 21:24:54.622: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:24:54.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:54.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:54.659
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/18/23 21:24:54.67
    STEP: watching for the Service to be added 01/18/23 21:24:54.692
    Jan 18 21:24:54.697: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 18 21:24:54.697: INFO: Service test-service-vtzmk created
    STEP: Getting /status 01/18/23 21:24:54.697
    Jan 18 21:24:54.702: INFO: Service test-service-vtzmk has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/18/23 21:24:54.702
    STEP: watching for the Service to be patched 01/18/23 21:24:54.714
    Jan 18 21:24:54.716: INFO: observed Service test-service-vtzmk in namespace services-5789 with annotations: map[] & LoadBalancer: {[]}
    Jan 18 21:24:54.716: INFO: Found Service test-service-vtzmk in namespace services-5789 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 18 21:24:54.716: INFO: Service test-service-vtzmk has service status patched
    STEP: updating the ServiceStatus 01/18/23 21:24:54.716
    Jan 18 21:24:54.732: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/18/23 21:24:54.732
    Jan 18 21:24:54.735: INFO: Observed Service test-service-vtzmk in namespace services-5789 with annotations: map[] & Conditions: {[]}
    Jan 18 21:24:54.735: INFO: Observed event: &Service{ObjectMeta:{test-service-vtzmk  services-5789  9c17da36-608a-44a5-9cb3-95038595b288 13087 0 2023-01-18 21:24:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 21:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 21:24:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.152.183.21,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.152.183.21],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 18 21:24:54.735: INFO: Found Service test-service-vtzmk in namespace services-5789 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:24:54.735: INFO: Service test-service-vtzmk has service status updated
    STEP: patching the service 01/18/23 21:24:54.735
    STEP: watching for the Service to be patched 01/18/23 21:24:54.754
    Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
    Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
    Jan 18 21:24:54.756: INFO: observed Service test-service-vtzmk in namespace services-5789 with labels: map[test-service-static:true]
    Jan 18 21:24:54.756: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service:patched test-service-static:true]
    Jan 18 21:24:54.756: INFO: Service test-service-vtzmk patched
    STEP: deleting the service 01/18/23 21:24:54.756
    STEP: watching for the Service to be deleted 01/18/23 21:24:54.783
    Jan 18 21:24:54.784: INFO: Observed event: ADDED
    Jan 18 21:24:54.784: INFO: Observed event: MODIFIED
    Jan 18 21:24:54.785: INFO: Observed event: MODIFIED
    Jan 18 21:24:54.785: INFO: Observed event: MODIFIED
    Jan 18 21:24:54.785: INFO: Found Service test-service-vtzmk in namespace services-5789 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 18 21:24:54.785: INFO: Service test-service-vtzmk deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:54.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5789" for this suite. 01/18/23 21:24:54.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:54.81
Jan 18 21:24:54.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:24:54.812
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:54.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:54.856
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:24:54.859
Jan 18 21:24:54.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808" in namespace "projected-1782" to be "Succeeded or Failed"
Jan 18 21:24:54.892: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Pending", Reason="", readiness=false. Elapsed: 10.949937ms
Jan 18 21:24:56.900: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018300601s
Jan 18 21:24:58.899: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017548567s
STEP: Saw pod success 01/18/23 21:24:58.899
Jan 18 21:24:58.899: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808" satisfied condition "Succeeded or Failed"
Jan 18 21:24:58.904: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 container client-container: <nil>
STEP: delete the pod 01/18/23 21:24:58.912
Jan 18 21:24:58.938: INFO: Waiting for pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 to disappear
Jan 18 21:24:58.950: INFO: Pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:58.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1782" for this suite. 01/18/23 21:24:58.954
------------------------------
• [4.159 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:54.81
    Jan 18 21:24:54.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:24:54.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:54.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:54.856
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:24:54.859
    Jan 18 21:24:54.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808" in namespace "projected-1782" to be "Succeeded or Failed"
    Jan 18 21:24:54.892: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Pending", Reason="", readiness=false. Elapsed: 10.949937ms
    Jan 18 21:24:56.900: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018300601s
    Jan 18 21:24:58.899: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017548567s
    STEP: Saw pod success 01/18/23 21:24:58.899
    Jan 18 21:24:58.899: INFO: Pod "downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808" satisfied condition "Succeeded or Failed"
    Jan 18 21:24:58.904: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:24:58.912
    Jan 18 21:24:58.938: INFO: Waiting for pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 to disappear
    Jan 18 21:24:58.950: INFO: Pod downwardapi-volume-7690031e-3860-4cb1-8cf4-ea2ca0c76808 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:58.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1782" for this suite. 01/18/23 21:24:58.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:58.971
Jan 18 21:24:58.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 21:24:58.972
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:59.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:59.012
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/18/23 21:24:59.017
STEP: patching the Namespace 01/18/23 21:24:59.046
STEP: get the Namespace and ensuring it has the label 01/18/23 21:24:59.061
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:24:59.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7584" for this suite. 01/18/23 21:24:59.07
STEP: Destroying namespace "nspatchtest-c5cdef76-946e-4c26-b961-ec464f8edac0-1542" for this suite. 01/18/23 21:24:59.083
------------------------------
• [0.130 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:58.971
    Jan 18 21:24:58.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:24:58.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:59.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:59.012
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/18/23 21:24:59.017
    STEP: patching the Namespace 01/18/23 21:24:59.046
    STEP: get the Namespace and ensuring it has the label 01/18/23 21:24:59.061
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:24:59.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7584" for this suite. 01/18/23 21:24:59.07
    STEP: Destroying namespace "nspatchtest-c5cdef76-946e-4c26-b961-ec464f8edac0-1542" for this suite. 01/18/23 21:24:59.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:24:59.102
Jan 18 21:24:59.103: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:24:59.104
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:59.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:59.142
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-e1ad0aad-3f7c-4a31-b301-1b9645acbc59 01/18/23 21:24:59.144
STEP: Creating a pod to test consume configMaps 01/18/23 21:24:59.155
Jan 18 21:24:59.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad" in namespace "configmap-4451" to be "Succeeded or Failed"
Jan 18 21:24:59.177: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036957ms
Jan 18 21:25:01.182: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.008987104s
Jan 18 21:25:03.183: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Running", Reason="", readiness=false. Elapsed: 4.009787192s
Jan 18 21:25:05.184: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010320609s
STEP: Saw pod success 01/18/23 21:25:05.184
Jan 18 21:25:05.184: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad" satisfied condition "Succeeded or Failed"
Jan 18 21:25:05.188: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:25:05.197
Jan 18 21:25:05.224: INFO: Waiting for pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad to disappear
Jan 18 21:25:05.231: INFO: Pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:05.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4451" for this suite. 01/18/23 21:25:05.236
------------------------------
• [SLOW TEST] [6.147 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:24:59.102
    Jan 18 21:24:59.103: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:24:59.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:24:59.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:24:59.142
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-e1ad0aad-3f7c-4a31-b301-1b9645acbc59 01/18/23 21:24:59.144
    STEP: Creating a pod to test consume configMaps 01/18/23 21:24:59.155
    Jan 18 21:24:59.173: INFO: Waiting up to 5m0s for pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad" in namespace "configmap-4451" to be "Succeeded or Failed"
    Jan 18 21:24:59.177: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036957ms
    Jan 18 21:25:01.182: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.008987104s
    Jan 18 21:25:03.183: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Running", Reason="", readiness=false. Elapsed: 4.009787192s
    Jan 18 21:25:05.184: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010320609s
    STEP: Saw pod success 01/18/23 21:25:05.184
    Jan 18 21:25:05.184: INFO: Pod "pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad" satisfied condition "Succeeded or Failed"
    Jan 18 21:25:05.188: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:25:05.197
    Jan 18 21:25:05.224: INFO: Waiting for pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad to disappear
    Jan 18 21:25:05.231: INFO: Pod pod-configmaps-c49e1bfc-9ea7-44aa-9d02-d666762bd0ad no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:05.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4451" for this suite. 01/18/23 21:25:05.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:05.25
Jan 18 21:25:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:25:05.253
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:05.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:05.293
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/18/23 21:25:05.295
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/18/23 21:25:05.312
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/18/23 21:25:05.312
STEP: creating a pod to probe DNS 01/18/23 21:25:05.312
STEP: submitting the pod to kubernetes 01/18/23 21:25:05.312
Jan 18 21:25:05.342: INFO: Waiting up to 15m0s for pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c" in namespace "dns-5631" to be "running"
Jan 18 21:25:05.347: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.624858ms
Jan 18 21:25:07.358: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015956331s
Jan 18 21:25:09.354: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Running", Reason="", readiness=true. Elapsed: 4.011315103s
Jan 18 21:25:09.354: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:25:09.354
STEP: looking for the results for each expected name from probers 01/18/23 21:25:09.358
Jan 18 21:25:09.379: INFO: DNS probes using dns-5631/dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c succeeded

STEP: deleting the pod 01/18/23 21:25:09.379
STEP: deleting the test headless service 01/18/23 21:25:09.409
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5631" for this suite. 01/18/23 21:25:09.448
------------------------------
• [4.215 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:05.25
    Jan 18 21:25:05.251: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:25:05.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:05.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:05.293
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/18/23 21:25:05.295
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/18/23 21:25:05.312
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5631.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/18/23 21:25:05.312
    STEP: creating a pod to probe DNS 01/18/23 21:25:05.312
    STEP: submitting the pod to kubernetes 01/18/23 21:25:05.312
    Jan 18 21:25:05.342: INFO: Waiting up to 15m0s for pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c" in namespace "dns-5631" to be "running"
    Jan 18 21:25:05.347: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.624858ms
    Jan 18 21:25:07.358: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015956331s
    Jan 18 21:25:09.354: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c": Phase="Running", Reason="", readiness=true. Elapsed: 4.011315103s
    Jan 18 21:25:09.354: INFO: Pod "dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:25:09.354
    STEP: looking for the results for each expected name from probers 01/18/23 21:25:09.358
    Jan 18 21:25:09.379: INFO: DNS probes using dns-5631/dns-test-ce786f5f-38c7-48ec-9560-cb4b4bddfd0c succeeded

    STEP: deleting the pod 01/18/23 21:25:09.379
    STEP: deleting the test headless service 01/18/23 21:25:09.409
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:09.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5631" for this suite. 01/18/23 21:25:09.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:09.468
Jan 18 21:25:09.468: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename init-container 01/18/23 21:25:09.469
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:09.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:09.504
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/18/23 21:25:09.507
Jan 18 21:25:09.507: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:13.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2438" for this suite. 01/18/23 21:25:13.684
------------------------------
• [4.229 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:09.468
    Jan 18 21:25:09.468: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename init-container 01/18/23 21:25:09.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:09.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:09.504
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/18/23 21:25:09.507
    Jan 18 21:25:09.507: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:13.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2438" for this suite. 01/18/23 21:25:13.684
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:13.699
Jan 18 21:25:13.699: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 21:25:13.7
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:13.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:13.733
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan 18 21:25:13.758: INFO: Waiting up to 2m0s for pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" in namespace "var-expansion-6568" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 21:25:13.762: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802961ms
Jan 18 21:25:15.772: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014735383s
Jan 18 21:25:15.772: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 21:25:15.772: INFO: Deleting pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" in namespace "var-expansion-6568"
Jan 18 21:25:15.786: INFO: Wait up to 5m0s for pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:19.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6568" for this suite. 01/18/23 21:25:19.81
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:13.699
    Jan 18 21:25:13.699: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 21:25:13.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:13.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:13.733
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan 18 21:25:13.758: INFO: Waiting up to 2m0s for pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" in namespace "var-expansion-6568" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 21:25:13.762: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802961ms
    Jan 18 21:25:15.772: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014735383s
    Jan 18 21:25:15.772: INFO: Pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 21:25:15.772: INFO: Deleting pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" in namespace "var-expansion-6568"
    Jan 18 21:25:15.786: INFO: Wait up to 5m0s for pod "var-expansion-70444a09-84ce-46b3-94b7-3768a15a646b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:19.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6568" for this suite. 01/18/23 21:25:19.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:19.836
Jan 18 21:25:19.836: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:25:19.837
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:19.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:19.934
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan 18 21:25:19.979: INFO: Waiting up to 5m0s for pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff" in namespace "pods-5709" to be "running and ready"
Jan 18 21:25:19.985: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854574ms
Jan 18 21:25:19.985: INFO: The phase of Pod server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:25:21.991: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff": Phase="Running", Reason="", readiness=true. Elapsed: 2.011600836s
Jan 18 21:25:21.991: INFO: The phase of Pod server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff is Running (Ready = true)
Jan 18 21:25:21.991: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff" satisfied condition "running and ready"
Jan 18 21:25:22.039: INFO: Waiting up to 5m0s for pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247" in namespace "pods-5709" to be "Succeeded or Failed"
Jan 18 21:25:22.044: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240065ms
Jan 18 21:25:24.051: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01194704s
Jan 18 21:25:26.050: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125921s
STEP: Saw pod success 01/18/23 21:25:26.05
Jan 18 21:25:26.051: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247" satisfied condition "Succeeded or Failed"
Jan 18 21:25:26.057: INFO: Trying to get logs from node test-vm-1 pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 container env3cont: <nil>
STEP: delete the pod 01/18/23 21:25:26.066
Jan 18 21:25:26.097: INFO: Waiting for pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 to disappear
Jan 18 21:25:26.102: INFO: Pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:26.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5709" for this suite. 01/18/23 21:25:26.106
------------------------------
• [SLOW TEST] [6.283 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:19.836
    Jan 18 21:25:19.836: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:25:19.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:19.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:19.934
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan 18 21:25:19.979: INFO: Waiting up to 5m0s for pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff" in namespace "pods-5709" to be "running and ready"
    Jan 18 21:25:19.985: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854574ms
    Jan 18 21:25:19.985: INFO: The phase of Pod server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:25:21.991: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff": Phase="Running", Reason="", readiness=true. Elapsed: 2.011600836s
    Jan 18 21:25:21.991: INFO: The phase of Pod server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff is Running (Ready = true)
    Jan 18 21:25:21.991: INFO: Pod "server-envvars-79382f40-5c18-4dfd-a57e-371dfbfe6dff" satisfied condition "running and ready"
    Jan 18 21:25:22.039: INFO: Waiting up to 5m0s for pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247" in namespace "pods-5709" to be "Succeeded or Failed"
    Jan 18 21:25:22.044: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Pending", Reason="", readiness=false. Elapsed: 5.240065ms
    Jan 18 21:25:24.051: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01194704s
    Jan 18 21:25:26.050: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011125921s
    STEP: Saw pod success 01/18/23 21:25:26.05
    Jan 18 21:25:26.051: INFO: Pod "client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247" satisfied condition "Succeeded or Failed"
    Jan 18 21:25:26.057: INFO: Trying to get logs from node test-vm-1 pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 container env3cont: <nil>
    STEP: delete the pod 01/18/23 21:25:26.066
    Jan 18 21:25:26.097: INFO: Waiting for pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 to disappear
    Jan 18 21:25:26.102: INFO: Pod client-envvars-c328f4de-cefe-4008-bd83-fce65b93a247 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:26.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5709" for this suite. 01/18/23 21:25:26.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:26.121
Jan 18 21:25:26.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:25:26.122
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:26.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:26.153
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 21:25:26.157
Jan 18 21:25:26.177: INFO: Waiting up to 5m0s for pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2" in namespace "emptydir-2970" to be "Succeeded or Failed"
Jan 18 21:25:26.181: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953049ms
Jan 18 21:25:28.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010227847s
Jan 18 21:25:30.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010111992s
STEP: Saw pod success 01/18/23 21:25:30.187
Jan 18 21:25:30.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2" satisfied condition "Succeeded or Failed"
Jan 18 21:25:30.191: INFO: Trying to get logs from node test-vm-1 pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 container test-container: <nil>
STEP: delete the pod 01/18/23 21:25:30.199
Jan 18 21:25:30.228: INFO: Waiting for pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 to disappear
Jan 18 21:25:30.235: INFO: Pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2970" for this suite. 01/18/23 21:25:30.24
------------------------------
• [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:26.121
    Jan 18 21:25:26.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:25:26.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:26.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:26.153
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 21:25:26.157
    Jan 18 21:25:26.177: INFO: Waiting up to 5m0s for pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2" in namespace "emptydir-2970" to be "Succeeded or Failed"
    Jan 18 21:25:26.181: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.953049ms
    Jan 18 21:25:28.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010227847s
    Jan 18 21:25:30.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010111992s
    STEP: Saw pod success 01/18/23 21:25:30.187
    Jan 18 21:25:30.187: INFO: Pod "pod-f9c809d2-13a2-4afd-b45d-49412232e0e2" satisfied condition "Succeeded or Failed"
    Jan 18 21:25:30.191: INFO: Trying to get logs from node test-vm-1 pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:25:30.199
    Jan 18 21:25:30.228: INFO: Waiting for pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 to disappear
    Jan 18 21:25:30.235: INFO: Pod pod-f9c809d2-13a2-4afd-b45d-49412232e0e2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:30.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2970" for this suite. 01/18/23 21:25:30.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:30.255
Jan 18 21:25:30.255: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename init-container 01/18/23 21:25:30.256
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:30.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:30.313
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/18/23 21:25:30.315
Jan 18 21:25:30.315: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5867" for this suite. 01/18/23 21:25:35.608
------------------------------
• [SLOW TEST] [5.368 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:30.255
    Jan 18 21:25:30.255: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename init-container 01/18/23 21:25:30.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:30.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:30.313
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/18/23 21:25:30.315
    Jan 18 21:25:30.315: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5867" for this suite. 01/18/23 21:25:35.608
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:35.623
Jan 18 21:25:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:25:35.625
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:35.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:35.665
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan 18 21:25:35.737: INFO: created pod pod-service-account-defaultsa
Jan 18 21:25:35.737: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 21:25:35.752: INFO: created pod pod-service-account-mountsa
Jan 18 21:25:35.752: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 21:25:35.765: INFO: created pod pod-service-account-nomountsa
Jan 18 21:25:35.765: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 21:25:35.777: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 21:25:35.777: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 21:25:35.797: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 21:25:35.797: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 21:25:35.824: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 21:25:35.824: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 21:25:35.840: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 21:25:35.840: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 21:25:35.868: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 21:25:35.868: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 21:25:35.917: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 21:25:35.917: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:35.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9283" for this suite. 01/18/23 21:25:35.929
------------------------------
• [0.339 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:35.623
    Jan 18 21:25:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 21:25:35.625
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:35.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:35.665
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan 18 21:25:35.737: INFO: created pod pod-service-account-defaultsa
    Jan 18 21:25:35.737: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 18 21:25:35.752: INFO: created pod pod-service-account-mountsa
    Jan 18 21:25:35.752: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 18 21:25:35.765: INFO: created pod pod-service-account-nomountsa
    Jan 18 21:25:35.765: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 18 21:25:35.777: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 18 21:25:35.777: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 18 21:25:35.797: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 18 21:25:35.797: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 18 21:25:35.824: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 18 21:25:35.824: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 18 21:25:35.840: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 18 21:25:35.840: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 18 21:25:35.868: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 18 21:25:35.868: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 18 21:25:35.917: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 18 21:25:35.917: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:35.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9283" for this suite. 01/18/23 21:25:35.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:35.963
Jan 18 21:25:35.963: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 21:25:35.964
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:36.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:36.139
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/18/23 21:25:36.163
STEP: Patching the Job 01/18/23 21:25:36.207
STEP: Watching for Job to be patched 01/18/23 21:25:36.321
Jan 18 21:25:36.323: INFO: Event ADDED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 21:25:36.323: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 21:25:36.331: INFO: Event MODIFIED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/18/23 21:25:36.331
STEP: Watching for Job to be updated 01/18/23 21:25:36.386
Jan 18 21:25:36.391: INFO: Event MODIFIED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:36.391: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/18/23 21:25:36.391
Jan 18 21:25:36.402: INFO: Job: e2e-t5kbf as labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched]
STEP: Waiting for job to complete 01/18/23 21:25:36.402
STEP: Delete a job collection with a labelselector 01/18/23 21:25:48.407
STEP: Watching for Job to be deleted 01/18/23 21:25:48.424
Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 21:25:48.426: INFO: Event DELETED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/18/23 21:25:48.426
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:48.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9686" for this suite. 01/18/23 21:25:48.456
------------------------------
• [SLOW TEST] [12.525 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:35.963
    Jan 18 21:25:35.963: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 21:25:35.964
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:36.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:36.139
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/18/23 21:25:36.163
    STEP: Patching the Job 01/18/23 21:25:36.207
    STEP: Watching for Job to be patched 01/18/23 21:25:36.321
    Jan 18 21:25:36.323: INFO: Event ADDED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 21:25:36.323: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 21:25:36.331: INFO: Event MODIFIED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/18/23 21:25:36.331
    STEP: Watching for Job to be updated 01/18/23 21:25:36.386
    Jan 18 21:25:36.391: INFO: Event MODIFIED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:36.391: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/18/23 21:25:36.391
    Jan 18 21:25:36.402: INFO: Job: e2e-t5kbf as labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched]
    STEP: Waiting for job to complete 01/18/23 21:25:36.402
    STEP: Delete a job collection with a labelselector 01/18/23 21:25:48.407
    STEP: Watching for Job to be deleted 01/18/23 21:25:48.424
    Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.425: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.426: INFO: Event MODIFIED observed for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 21:25:48.426: INFO: Event DELETED found for Job e2e-t5kbf in namespace job-9686 with labels: map[e2e-job-label:e2e-t5kbf e2e-t5kbf:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/18/23 21:25:48.426
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:48.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9686" for this suite. 01/18/23 21:25:48.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:48.489
Jan 18 21:25:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-pred 01/18/23 21:25:48.49
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:48.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:48.549
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 21:25:48.551: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 21:25:48.560: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 21:25:48.564: INFO: 
Logging pods the apiserver thinks is on node test-vm-2 before test
Jan 18 21:25:48.572: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container coredns ready: true, restart count 0
Jan 18 21:25:48.572: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:25:48.572: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container e2e ready: true, restart count 0
Jan 18 21:25:48.572: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:25:48.572: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:25:48.572: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:25:48.572: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 21:25:48.572: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:36 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.572: INFO: 	Container token-test ready: false, restart count 0
Jan 18 21:25:48.572: INFO: 
Logging pods the apiserver thinks is on node test-vm-1 before test
Jan 18 21:25:48.579: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:25:48.579: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:25:48.579: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:25:48.579: INFO: pod-service-account-defaultsa from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
Jan 18 21:25:48.579: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
Jan 18 21:25:48.579: INFO: pod-service-account-mountsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
Jan 18 21:25:48.579: INFO: pod-service-account-mountsa from svcaccounts-9283 started at 2023-01-18 21:25:36 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
Jan 18 21:25:48.579: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
Jan 18 21:25:48.579: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 21:25:48.579
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b844c1954267d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/18/23 21:25:49.676
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:50.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-885" for this suite. 01/18/23 21:25:50.636
------------------------------
• [2.162 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:48.489
    Jan 18 21:25:48.489: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-pred 01/18/23 21:25:48.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:48.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:48.549
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 21:25:48.551: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 21:25:48.560: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 21:25:48.564: INFO: 
    Logging pods the apiserver thinks is on node test-vm-2 before test
    Jan 18 21:25:48.572: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 21:25:48.572: INFO: pod-service-account-defaultsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:36 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.572: INFO: 	Container token-test ready: false, restart count 0
    Jan 18 21:25:48.572: INFO: 
    Logging pods the apiserver thinks is on node test-vm-1 before test
    Jan 18 21:25:48.579: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:25:48.579: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:25:48.579: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:25:48.579: INFO: pod-service-account-defaultsa from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
    Jan 18 21:25:48.579: INFO: pod-service-account-nomountsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
    Jan 18 21:25:48.579: INFO: pod-service-account-mountsa-mountspec from svcaccounts-9283 started at 2023-01-18 21:25:35 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
    Jan 18 21:25:48.579: INFO: pod-service-account-mountsa from svcaccounts-9283 started at 2023-01-18 21:25:36 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container token-test ready: false, restart count 0
    Jan 18 21:25:48.579: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
    Jan 18 21:25:48.579: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 21:25:48.579
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b844c1954267d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 01/18/23 21:25:49.676
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:50.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-885" for this suite. 01/18/23 21:25:50.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:50.657
Jan 18 21:25:50.657: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:25:50.658
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:50.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:50.703
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-02018bf6-dd68-4bb4-84ea-92af288ced03 01/18/23 21:25:50.706
STEP: Creating a pod to test consume configMaps 01/18/23 21:25:50.719
Jan 18 21:25:50.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4" in namespace "configmap-3731" to be "Succeeded or Failed"
Jan 18 21:25:50.747: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146063ms
Jan 18 21:25:52.752: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011014157s
Jan 18 21:25:54.754: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012758806s
STEP: Saw pod success 01/18/23 21:25:54.754
Jan 18 21:25:54.754: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4" satisfied condition "Succeeded or Failed"
Jan 18 21:25:54.758: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:25:54.769
Jan 18 21:25:54.814: INFO: Waiting for pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 to disappear
Jan 18 21:25:54.823: INFO: Pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:25:54.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3731" for this suite. 01/18/23 21:25:54.828
------------------------------
• [4.187 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:50.657
    Jan 18 21:25:50.657: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:25:50.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:50.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:50.703
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-02018bf6-dd68-4bb4-84ea-92af288ced03 01/18/23 21:25:50.706
    STEP: Creating a pod to test consume configMaps 01/18/23 21:25:50.719
    Jan 18 21:25:50.741: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4" in namespace "configmap-3731" to be "Succeeded or Failed"
    Jan 18 21:25:50.747: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.146063ms
    Jan 18 21:25:52.752: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011014157s
    Jan 18 21:25:54.754: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012758806s
    STEP: Saw pod success 01/18/23 21:25:54.754
    Jan 18 21:25:54.754: INFO: Pod "pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4" satisfied condition "Succeeded or Failed"
    Jan 18 21:25:54.758: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:25:54.769
    Jan 18 21:25:54.814: INFO: Waiting for pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 to disappear
    Jan 18 21:25:54.823: INFO: Pod pod-configmaps-e9b16d67-d8ec-401b-8d84-4c855dede9c4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:25:54.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3731" for this suite. 01/18/23 21:25:54.828
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:25:54.845
Jan 18 21:25:54.845: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:25:54.846
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:54.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:54.907
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 18 21:25:54.910: INFO: Creating deployment "webserver-deployment"
Jan 18 21:25:54.923: INFO: Waiting for observed generation 1
Jan 18 21:25:56.955: INFO: Waiting for all required pods to come up
Jan 18 21:25:56.970: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/18/23 21:25:56.97
Jan 18 21:25:56.970: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xpqhk" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44jkw" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vrjjs" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bp6fh" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9wt24" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ll92f" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vkvw7" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9wwlc" in namespace "deployment-1429" to be "running"
Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4s595" in namespace "deployment-1429" to be "running"
Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw": Phase="Pending", Reason="", readiness=false. Elapsed: 44.083723ms
Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 44.050223ms
Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk": Phase="Pending", Reason="", readiness=false. Elapsed: 44.431127ms
Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs": Phase="Pending", Reason="", readiness=false. Elapsed: 44.300326ms
Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.115023ms
Jan 18 21:25:57.026: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc": Phase="Pending", Reason="", readiness=false. Elapsed: 54.743649ms
Jan 18 21:25:57.026: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24": Phase="Pending", Reason="", readiness=false. Elapsed: 55.63956ms
Jan 18 21:25:57.027: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7": Phase="Pending", Reason="", readiness=false. Elapsed: 55.208755ms
Jan 18 21:25:57.029: INFO: Pod "webserver-deployment-7f5969cbc7-4s595": Phase="Pending", Reason="", readiness=false. Elapsed: 57.585784ms
Jan 18 21:25:59.021: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.050061421s
Jan 18 21:25:59.021: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw" satisfied condition "running"
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs": Phase="Running", Reason="", readiness=true. Elapsed: 2.06089475s
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs" satisfied condition "running"
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk": Phase="Running", Reason="", readiness=true. Elapsed: 2.061445757s
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.061232154s
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh" satisfied condition "running"
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk" satisfied condition "running"
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f": Phase="Running", Reason="", readiness=true. Elapsed: 2.061065552s
Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f" satisfied condition "running"
Jan 18 21:25:59.034: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7": Phase="Running", Reason="", readiness=true. Elapsed: 2.062711171s
Jan 18 21:25:59.034: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7" satisfied condition "running"
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc": Phase="Running", Reason="", readiness=true. Elapsed: 2.072817591s
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc" satisfied condition "running"
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-4s595": Phase="Running", Reason="", readiness=true. Elapsed: 2.072582389s
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-4s595" satisfied condition "running"
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24": Phase="Running", Reason="", readiness=true. Elapsed: 2.0735723s
Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24" satisfied condition "running"
Jan 18 21:25:59.044: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 21:25:59.053: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 21:25:59.076: INFO: Updating deployment webserver-deployment
Jan 18 21:25:59.076: INFO: Waiting for observed generation 2
Jan 18 21:26:01.091: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 21:26:01.107: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 21:26:01.123: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 21:26:01.144: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 21:26:01.144: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 21:26:01.149: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 21:26:01.161: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 21:26:01.161: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 21:26:01.234: INFO: Updating deployment webserver-deployment
Jan 18 21:26:01.234: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 21:26:01.248: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 21:26:03.270: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:26:03.314: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1429  5a6322f5-7b7c-47f9-adbf-93ce4e527317 14198 3 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044963a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 21:26:01 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-18 21:26:02 +0000 UTC,LastTransitionTime:2023-01-18 21:25:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 21:26:03.336: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1429  180e762e-eb87-4a3d-b5db-3284d943a093 14182 3 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5a6322f5-7b7c-47f9-adbf-93ce4e527317 0xc00489ca07 0xc00489ca08}] [] [{kubelite Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a6322f5-7b7c-47f9-adbf-93ce4e527317\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:26:03.336: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 21:26:03.336: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1429  953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 14181 3 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5a6322f5-7b7c-47f9-adbf-93ce4e527317 0xc00489c827 0xc00489c828}] [] [{kubelite Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a6322f5-7b7c-47f9-adbf-93ce4e527317\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489c948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:26:03.355: INFO: Pod "webserver-deployment-7f5969cbc7-xxsmk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xxsmk webserver-deployment-7f5969cbc7- deployment-1429  47ec996a-54cc-4fb2-a7f0-5148cbe92cb3 13919 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a46f908aaa722433e4c1362460b0208fa67afb91d12c42e0dc8b2d8f158158ff cni.projectcalico.org/podIP:10.1.132.37/32 cni.projectcalico.org/podIPs:10.1.132.37/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044967e7 0xc0044967e8}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzdx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzdx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.37,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://93edab0a4d06bb8857d0403b9784dbdee9e53b1e1b90591715c37ad0d3700e81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.356: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9wwlc webserver-deployment-7f5969cbc7- deployment-1429  e7a2a90c-9a39-4033-90f4-a1ff3cc1419f 13950 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:61371414a142de012afe1c47552f88ed7aaa525dfd2f96c2c2a9175c86779ebf cni.projectcalico.org/podIP:10.1.192.17/32 cni.projectcalico.org/podIPs:10.1.192.17/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496a20 0xc004496a21}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bfxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bfxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.17,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e02a085b16c7ecf4ed34fec56c2e2baf884bcd1ec1ce2ccd06594a5634ee5d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.356: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-44jkw webserver-deployment-7f5969cbc7- deployment-1429  8c553849-319c-4d9a-80f6-5850af05dd9a 13954 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:077f94e58304a567289c2fbff7de3d1d0b678da5d4dfc6871ada9dc72eb90834 cni.projectcalico.org/podIP:10.1.192.24/32 cni.projectcalico.org/podIPs:10.1.192.24/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496c50 0xc004496c51}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hd2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hd2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.24,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6f61b4f5fecba2d416ee17fa574980bf120b8aab0e73098943e961f9909e1e8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.357: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xpqhk webserver-deployment-7f5969cbc7- deployment-1429  fd5ab786-110c-483f-8423-d225067a17ad 13958 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db57131942404fe6d11f8db996dde31d6f0fb3fb0855a0c389e693f60390741a cni.projectcalico.org/podIP:10.1.132.60/32 cni.projectcalico.org/podIPs:10.1.132.60/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496e80 0xc004496e81}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.60,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://109d527e94c77a427206f9a3df95d0b235e0299e2e40a4cf9577632f944d0e96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.357: INFO: Pod "webserver-deployment-7f5969cbc7-4s595" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4s595 webserver-deployment-7f5969cbc7- deployment-1429  d28af176-be8b-4412-a6e7-95f570b54019 13963 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b30940355c9f59b29138490de2f42e28f55c9735bfb1dbb40d48daf8b67254df cni.projectcalico.org/podIP:10.1.132.38/32 cni.projectcalico.org/podIPs:10.1.132.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044970c0 0xc0044970c1}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckrg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckrg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.38,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8f63cde9866bd26a5c531bf1665dd37925dced619e294510d4f0dfbe9a3a09e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.358: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ll92f webserver-deployment-7f5969cbc7- deployment-1429  a0f612d4-47c1-4d1f-accf-9afb1fc1a3d0 13969 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7f796b88b32500711e50392fa4aa4c2c0d1af9a1a2bb339f567ae7be5cf90898 cni.projectcalico.org/podIP:10.1.132.56/32 cni.projectcalico.org/podIPs:10.1.132.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004497390 0xc004497391}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zft79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zft79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.56,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d03ea4f870932caa1a00c904491ef1a8b7a1c1cff8cf730de62757127f03f00f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.358: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bp6fh webserver-deployment-7f5969cbc7- deployment-1429  f6350dc5-55f1-4c83-b9cf-402fa1c2b66d 13976 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e00afdcd3adfeae1bed96d7c5ae5bb02205d5bb69cfb22fa2848fa470261d120 cni.projectcalico.org/podIP:10.1.132.14/32 cni.projectcalico.org/podIPs:10.1.132.14/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044975d0 0xc0044975d1}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkn6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkn6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.14,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://76cac37d40bc81849f74cda706710d84ee96c0adfc2a87d1aa0554327329ceb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.359: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9wt24 webserver-deployment-7f5969cbc7- deployment-1429  feff04a0-163b-4931-899e-ef66133b9e02 13979 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0b85e176b18e3fbcb450ec0a78b876953e91c5c4439151acd25e8642b946a7e1 cni.projectcalico.org/podIP:10.1.192.5/32 cni.projectcalico.org/podIPs:10.1.192.5/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004497800 0xc004497801}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45wng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45wng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.5,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32fb1a3a8a1df1bfcc4f97777305df55ba35b13528e62391748e0787b5fdefc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.359: INFO: Pod "webserver-deployment-d9f79cb5-f7ptf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f7ptf webserver-deployment-d9f79cb5- deployment-1429  7e3e445f-7dc5-4421-a7df-063290f6bf81 14050 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3fa0db5b4c81279b6726de4b47e94e9bdfa134c4dddd21846ca6784959d056d6 cni.projectcalico.org/podIP:10.1.192.16/32 cni.projectcalico.org/podIPs:10.1.192.16/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc0044979ff 0xc004497a30}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4kqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4kqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.370: INFO: Pod "webserver-deployment-d9f79cb5-nldkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nldkv webserver-deployment-d9f79cb5- deployment-1429  7c68ebf6-af70-4f07-bf81-11653e2b0299 14059 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fb438b2101ba52ed6aededa0d383c98899239a1a7cc9bcda4036c07da55bdf90 cni.projectcalico.org/podIP:10.1.192.39/32 cni.projectcalico.org/podIPs:10.1.192.39/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc004497c2f 0xc004497c60}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j29lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j29lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.372: INFO: Pod "webserver-deployment-d9f79cb5-mg7wk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mg7wk webserver-deployment-d9f79cb5- deployment-1429  d3de3924-103a-4d47-8931-4b71e3e2b4b2 14065 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9743ceeb6ba0d88afb5b8118faeda1ee4e12383ee41e4085fb789275c8debb2f cni.projectcalico.org/podIP:10.1.192.41/32 cni.projectcalico.org/podIPs:10.1.192.41/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc004497e5f 0xc004497e90}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnxvt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnxvt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.372: INFO: Pod "webserver-deployment-7f5969cbc7-7zfm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zfm4 webserver-deployment-7f5969cbc7- deployment-1429  3c8b9149-f3f9-4cf8-9472-3e20cc3cd2c6 14090 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c0a0 0xc003d8c0a1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqzmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqzmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-mlrm7" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mlrm7 webserver-deployment-7f5969cbc7- deployment-1429  2976e0ad-7e54-4d0a-8bd5-a3d58d7a08c5 14101 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c270 0xc003d8c271}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfb24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfb24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-d9f79cb5-rhqrb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhqrb webserver-deployment-d9f79cb5- deployment-1429  59123ab1-6cea-45ca-a874-1501e52bb4c8 14103 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8c42f 0xc003d8c440}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9jnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9jnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-qrsmb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qrsmb webserver-deployment-7f5969cbc7- deployment-1429  90282329-0661-4e12-ba25-53a30d3e5774 14111 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c5c0 0xc003d8c5c1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzhc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzhc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-p8v4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p8v4f webserver-deployment-7f5969cbc7- deployment-1429  93c832b2-2ac5-4987-922c-c54585231090 14119 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c730 0xc003d8c731}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq495,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq495,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-7f5969cbc7-l686r" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l686r webserver-deployment-7f5969cbc7- deployment-1429  8f2bc70a-a261-42a7-9367-05781364e2cb 14126 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c900 0xc003d8c901}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bx6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bx6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-7f5969cbc7-4cnbm" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4cnbm webserver-deployment-7f5969cbc7- deployment-1429  0daf40d8-9638-4cae-9c48-cb82788c5dc2 14127 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8ca70 0xc003d8ca71}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpjtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpjtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-d9f79cb5-dcrgm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dcrgm webserver-deployment-d9f79cb5- deployment-1429  58a183cd-2c3a-4c19-8d80-81fea637a4f4 14132 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cbcf 0xc003d8cbe0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtl2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtl2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-cw9ff" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cw9ff webserver-deployment-d9f79cb5- deployment-1429  0d839955-bb0e-4805-8008-261ce20ab6c8 14133 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cd4f 0xc003d8cd60}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smths,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smths,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-6b4lx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6b4lx webserver-deployment-d9f79cb5- deployment-1429  e3b2ffc3-030d-4436-b3bc-f8ecca8a7185 14134 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cecf 0xc003d8cee0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x67t6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x67t6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-wc7wc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wc7wc webserver-deployment-d9f79cb5- deployment-1429  75626521-c62c-42e6-8523-3ddad194cb3b 14135 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8d05f 0xc003d8d070}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtq2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtq2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-7f5969cbc7-xcjw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xcjw5 webserver-deployment-7f5969cbc7- deployment-1429  c4538280-98ee-486f-9aae-6aed09b6527e 14148 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d1f0 0xc003d8d1f1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84c7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84c7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-7f5969cbc7-dgc7s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dgc7s webserver-deployment-7f5969cbc7- deployment-1429  de80ceb2-48e7-4006-a51a-9718fce37c7f 14151 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d360 0xc003d8d361}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drrx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drrx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-9nzxh" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9nzxh webserver-deployment-7f5969cbc7- deployment-1429  44df45de-706f-42aa-a193-474d487ea59d 14156 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d940 0xc003d8d941}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqxsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqxsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-whxxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whxxd webserver-deployment-7f5969cbc7- deployment-1429  70a6f418-f0c9-499e-a867-c4fe43fd5182 14162 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8dab0 0xc003d8dab1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwlts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwlts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-jxlt8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxlt8 webserver-deployment-7f5969cbc7- deployment-1429  5e51feae-9686-4887-8782-c873f0041e84 14163 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8dc40 0xc003d8dc41}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hdvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hdvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-d9f79cb5-r7nt7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r7nt7 webserver-deployment-d9f79cb5- deployment-1429  8b90d644-48c5-42cd-89b9-10fa2c5f1a52 14166 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8dd9f 0xc003d8ddb0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8xs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8xs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.377: INFO: Pod "webserver-deployment-d9f79cb5-dkrz8" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dkrz8 webserver-deployment-d9f79cb5- deployment-1429  5c62907c-4039-4fcc-993b-ca43445e142e 14175 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:55d882936e236ffadd42bf116e40d746ae23ff1003ab81277bed267114fb32fa cni.projectcalico.org/podIP:10.1.132.40/32 cni.projectcalico.org/podIPs:10.1.132.40/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8df1f 0xc003d8df50}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c27bj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c27bj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.40,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:87042157b1795866314b82314bead74002b6ec66aaea1e65ca05da588531be27: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.377: INFO: Pod "webserver-deployment-d9f79cb5-bxslw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bxslw webserver-deployment-d9f79cb5- deployment-1429  f9fe040a-7ced-45df-93ad-8f418d8cb714 14176 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3358c40ac0aa14fbc7e7c932ee22558136481ee5b9482064f49f1bdcae61b6f5 cni.projectcalico.org/podIP:10.1.132.34/32 cni.projectcalico.org/podIPs:10.1.132.34/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c17f 0xc00490c1b0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbtv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbtv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-d9f79cb5-lvtvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lvtvs webserver-deployment-d9f79cb5- deployment-1429  6ae8df7a-6e91-44d1-b366-d76aba16fdeb 14194 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:64bbaad14c1d0920d2e2a6879e8190b6a2ac55d7b983d94db1dca53d67693d1a cni.projectcalico.org/podIP:10.1.132.29/32 cni.projectcalico.org/podIPs:10.1.132.29/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c3af 0xc00490c3e0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fd84f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fd84f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-d9f79cb5-r5zlr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r5zlr webserver-deployment-d9f79cb5- deployment-1429  0b8eda01-7251-4789-bb96-994e46043289 14196 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:135af403bcc5f47d4d71e1740148d524ea3157855a4351b0bb6bff2a087aa7ba cni.projectcalico.org/podIP:10.1.132.61/32 cni.projectcalico.org/podIPs:10.1.132.61/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c5df 0xc00490c610}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csfbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csfbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.61,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:87042157b1795866314b82314bead74002b6ec66aaea1e65ca05da588531be27: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-7f5969cbc7-kw75n" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kw75n webserver-deployment-7f5969cbc7- deployment-1429  3b6f41e3-6584-4dc6-86ea-d5e81c569ebf 14201 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d438481e4fe196e4abb0f045ebc26cab1182142464fc4edb6225308aaf779f59 cni.projectcalico.org/podIP:10.1.132.6/32 cni.projectcalico.org/podIPs:10.1.132.6/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc00490c870 0xc00490c871}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnlzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnlzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:03.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1429" for this suite. 01/18/23 21:26:03.397
------------------------------
• [SLOW TEST] [8.616 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:25:54.845
    Jan 18 21:25:54.845: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:25:54.846
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:25:54.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:25:54.907
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 18 21:25:54.910: INFO: Creating deployment "webserver-deployment"
    Jan 18 21:25:54.923: INFO: Waiting for observed generation 1
    Jan 18 21:25:56.955: INFO: Waiting for all required pods to come up
    Jan 18 21:25:56.970: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/18/23 21:25:56.97
    Jan 18 21:25:56.970: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-xpqhk" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-44jkw" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vrjjs" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bp6fh" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9wt24" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-ll92f" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-vkvw7" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-9wwlc" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:56.971: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4s595" in namespace "deployment-1429" to be "running"
    Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw": Phase="Pending", Reason="", readiness=false. Elapsed: 44.083723ms
    Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh": Phase="Pending", Reason="", readiness=false. Elapsed: 44.050223ms
    Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk": Phase="Pending", Reason="", readiness=false. Elapsed: 44.431127ms
    Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs": Phase="Pending", Reason="", readiness=false. Elapsed: 44.300326ms
    Jan 18 21:25:57.015: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f": Phase="Pending", Reason="", readiness=false. Elapsed: 44.115023ms
    Jan 18 21:25:57.026: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc": Phase="Pending", Reason="", readiness=false. Elapsed: 54.743649ms
    Jan 18 21:25:57.026: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24": Phase="Pending", Reason="", readiness=false. Elapsed: 55.63956ms
    Jan 18 21:25:57.027: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7": Phase="Pending", Reason="", readiness=false. Elapsed: 55.208755ms
    Jan 18 21:25:57.029: INFO: Pod "webserver-deployment-7f5969cbc7-4s595": Phase="Pending", Reason="", readiness=false. Elapsed: 57.585784ms
    Jan 18 21:25:59.021: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw": Phase="Running", Reason="", readiness=true. Elapsed: 2.050061421s
    Jan 18 21:25:59.021: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw" satisfied condition "running"
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs": Phase="Running", Reason="", readiness=true. Elapsed: 2.06089475s
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-vrjjs" satisfied condition "running"
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk": Phase="Running", Reason="", readiness=true. Elapsed: 2.061445757s
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.061232154s
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh" satisfied condition "running"
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk" satisfied condition "running"
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f": Phase="Running", Reason="", readiness=true. Elapsed: 2.061065552s
    Jan 18 21:25:59.032: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f" satisfied condition "running"
    Jan 18 21:25:59.034: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7": Phase="Running", Reason="", readiness=true. Elapsed: 2.062711171s
    Jan 18 21:25:59.034: INFO: Pod "webserver-deployment-7f5969cbc7-vkvw7" satisfied condition "running"
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc": Phase="Running", Reason="", readiness=true. Elapsed: 2.072817591s
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc" satisfied condition "running"
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-4s595": Phase="Running", Reason="", readiness=true. Elapsed: 2.072582389s
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-4s595" satisfied condition "running"
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24": Phase="Running", Reason="", readiness=true. Elapsed: 2.0735723s
    Jan 18 21:25:59.044: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24" satisfied condition "running"
    Jan 18 21:25:59.044: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 18 21:25:59.053: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 18 21:25:59.076: INFO: Updating deployment webserver-deployment
    Jan 18 21:25:59.076: INFO: Waiting for observed generation 2
    Jan 18 21:26:01.091: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 18 21:26:01.107: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 18 21:26:01.123: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 21:26:01.144: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 18 21:26:01.144: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 18 21:26:01.149: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 21:26:01.161: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 18 21:26:01.161: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 18 21:26:01.234: INFO: Updating deployment webserver-deployment
    Jan 18 21:26:01.234: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 21:26:01.248: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 18 21:26:03.270: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:26:03.314: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1429  5a6322f5-7b7c-47f9-adbf-93ce4e527317 14198 3 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044963a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 21:26:01 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-18 21:26:02 +0000 UTC,LastTransitionTime:2023-01-18 21:25:54 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 18 21:26:03.336: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-1429  180e762e-eb87-4a3d-b5db-3284d943a093 14182 3 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5a6322f5-7b7c-47f9-adbf-93ce4e527317 0xc00489ca07 0xc00489ca08}] [] [{kubelite Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a6322f5-7b7c-47f9-adbf-93ce4e527317\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489cbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:26:03.336: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 18 21:26:03.336: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-1429  953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 14181 3 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5a6322f5-7b7c-47f9-adbf-93ce4e527317 0xc00489c827 0xc00489c828}] [] [{kubelite Update apps/v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a6322f5-7b7c-47f9-adbf-93ce4e527317\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00489c948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:26:03.355: INFO: Pod "webserver-deployment-7f5969cbc7-xxsmk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xxsmk webserver-deployment-7f5969cbc7- deployment-1429  47ec996a-54cc-4fb2-a7f0-5148cbe92cb3 13919 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:a46f908aaa722433e4c1362460b0208fa67afb91d12c42e0dc8b2d8f158158ff cni.projectcalico.org/podIP:10.1.132.37/32 cni.projectcalico.org/podIPs:10.1.132.37/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044967e7 0xc0044967e8}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzdx9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzdx9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.37,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://93edab0a4d06bb8857d0403b9784dbdee9e53b1e1b90591715c37ad0d3700e81,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.356: INFO: Pod "webserver-deployment-7f5969cbc7-9wwlc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9wwlc webserver-deployment-7f5969cbc7- deployment-1429  e7a2a90c-9a39-4033-90f4-a1ff3cc1419f 13950 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:61371414a142de012afe1c47552f88ed7aaa525dfd2f96c2c2a9175c86779ebf cni.projectcalico.org/podIP:10.1.192.17/32 cni.projectcalico.org/podIPs:10.1.192.17/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496a20 0xc004496a21}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bfxv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bfxv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.17,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e02a085b16c7ecf4ed34fec56c2e2baf884bcd1ec1ce2ccd06594a5634ee5d0c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.356: INFO: Pod "webserver-deployment-7f5969cbc7-44jkw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-44jkw webserver-deployment-7f5969cbc7- deployment-1429  8c553849-319c-4d9a-80f6-5850af05dd9a 13954 0 2023-01-18 21:25:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:077f94e58304a567289c2fbff7de3d1d0b678da5d4dfc6871ada9dc72eb90834 cni.projectcalico.org/podIP:10.1.192.24/32 cni.projectcalico.org/podIPs:10.1.192.24/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496c50 0xc004496c51}] [] [{kubelite Update v1 2023-01-18 21:25:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9hd2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9hd2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.24,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6f61b4f5fecba2d416ee17fa574980bf120b8aab0e73098943e961f9909e1e8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.24,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.357: INFO: Pod "webserver-deployment-7f5969cbc7-xpqhk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xpqhk webserver-deployment-7f5969cbc7- deployment-1429  fd5ab786-110c-483f-8423-d225067a17ad 13958 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db57131942404fe6d11f8db996dde31d6f0fb3fb0855a0c389e693f60390741a cni.projectcalico.org/podIP:10.1.132.60/32 cni.projectcalico.org/podIPs:10.1.132.60/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004496e80 0xc004496e81}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pf8kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pf8kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.60,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://109d527e94c77a427206f9a3df95d0b235e0299e2e40a4cf9577632f944d0e96,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.357: INFO: Pod "webserver-deployment-7f5969cbc7-4s595" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4s595 webserver-deployment-7f5969cbc7- deployment-1429  d28af176-be8b-4412-a6e7-95f570b54019 13963 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b30940355c9f59b29138490de2f42e28f55c9735bfb1dbb40d48daf8b67254df cni.projectcalico.org/podIP:10.1.132.38/32 cni.projectcalico.org/podIPs:10.1.132.38/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044970c0 0xc0044970c1}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ckrg7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ckrg7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.38,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8f63cde9866bd26a5c531bf1665dd37925dced619e294510d4f0dfbe9a3a09e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.38,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.358: INFO: Pod "webserver-deployment-7f5969cbc7-ll92f" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ll92f webserver-deployment-7f5969cbc7- deployment-1429  a0f612d4-47c1-4d1f-accf-9afb1fc1a3d0 13969 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7f796b88b32500711e50392fa4aa4c2c0d1af9a1a2bb339f567ae7be5cf90898 cni.projectcalico.org/podIP:10.1.132.56/32 cni.projectcalico.org/podIPs:10.1.132.56/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004497390 0xc004497391}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zft79,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zft79,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.56,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d03ea4f870932caa1a00c904491ef1a8b7a1c1cff8cf730de62757127f03f00f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.56,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.358: INFO: Pod "webserver-deployment-7f5969cbc7-bp6fh" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bp6fh webserver-deployment-7f5969cbc7- deployment-1429  f6350dc5-55f1-4c83-b9cf-402fa1c2b66d 13976 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e00afdcd3adfeae1bed96d7c5ae5bb02205d5bb69cfb22fa2848fa470261d120 cni.projectcalico.org/podIP:10.1.132.14/32 cni.projectcalico.org/podIPs:10.1.132.14/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc0044975d0 0xc0044975d1}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tkn6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tkn6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.14,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://76cac37d40bc81849f74cda706710d84ee96c0adfc2a87d1aa0554327329ceb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.359: INFO: Pod "webserver-deployment-7f5969cbc7-9wt24" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9wt24 webserver-deployment-7f5969cbc7- deployment-1429  feff04a0-163b-4931-899e-ef66133b9e02 13979 0 2023-01-18 21:25:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:0b85e176b18e3fbcb450ec0a78b876953e91c5c4439151acd25e8642b946a7e1 cni.projectcalico.org/podIP:10.1.192.5/32 cni.projectcalico.org/podIPs:10.1.192.5/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc004497800 0xc004497801}] [] [{kubelite Update v1 2023-01-18 21:25:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:25:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:25:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45wng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45wng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.5,StartTime:2023-01-18 21:25:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:25:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://32fb1a3a8a1df1bfcc4f97777305df55ba35b13528e62391748e0787b5fdefc3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.359: INFO: Pod "webserver-deployment-d9f79cb5-f7ptf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f7ptf webserver-deployment-d9f79cb5- deployment-1429  7e3e445f-7dc5-4421-a7df-063290f6bf81 14050 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3fa0db5b4c81279b6726de4b47e94e9bdfa134c4dddd21846ca6784959d056d6 cni.projectcalico.org/podIP:10.1.192.16/32 cni.projectcalico.org/podIPs:10.1.192.16/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc0044979ff 0xc004497a30}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4kqb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4kqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.370: INFO: Pod "webserver-deployment-d9f79cb5-nldkv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nldkv webserver-deployment-d9f79cb5- deployment-1429  7c68ebf6-af70-4f07-bf81-11653e2b0299 14059 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fb438b2101ba52ed6aededa0d383c98899239a1a7cc9bcda4036c07da55bdf90 cni.projectcalico.org/podIP:10.1.192.39/32 cni.projectcalico.org/podIPs:10.1.192.39/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc004497c2f 0xc004497c60}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j29lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j29lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.372: INFO: Pod "webserver-deployment-d9f79cb5-mg7wk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mg7wk webserver-deployment-d9f79cb5- deployment-1429  d3de3924-103a-4d47-8931-4b71e3e2b4b2 14065 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9743ceeb6ba0d88afb5b8118faeda1ee4e12383ee41e4085fb789275c8debb2f cni.projectcalico.org/podIP:10.1.192.41/32 cni.projectcalico.org/podIPs:10.1.192.41/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc004497e5f 0xc004497e90}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rnxvt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rnxvt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.372: INFO: Pod "webserver-deployment-7f5969cbc7-7zfm4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7zfm4 webserver-deployment-7f5969cbc7- deployment-1429  3c8b9149-f3f9-4cf8-9472-3e20cc3cd2c6 14090 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c0a0 0xc003d8c0a1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqzmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqzmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-mlrm7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mlrm7 webserver-deployment-7f5969cbc7- deployment-1429  2976e0ad-7e54-4d0a-8bd5-a3d58d7a08c5 14101 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c270 0xc003d8c271}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lfb24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lfb24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-d9f79cb5-rhqrb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-rhqrb webserver-deployment-d9f79cb5- deployment-1429  59123ab1-6cea-45ca-a874-1501e52bb4c8 14103 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8c42f 0xc003d8c440}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9jnn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9jnn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-qrsmb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qrsmb webserver-deployment-7f5969cbc7- deployment-1429  90282329-0661-4e12-ba25-53a30d3e5774 14111 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c5c0 0xc003d8c5c1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nzhc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nzhc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.373: INFO: Pod "webserver-deployment-7f5969cbc7-p8v4f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p8v4f webserver-deployment-7f5969cbc7- deployment-1429  93c832b2-2ac5-4987-922c-c54585231090 14119 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c730 0xc003d8c731}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq495,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq495,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-7f5969cbc7-l686r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-l686r webserver-deployment-7f5969cbc7- deployment-1429  8f2bc70a-a261-42a7-9367-05781364e2cb 14126 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8c900 0xc003d8c901}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4bx6d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4bx6d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-7f5969cbc7-4cnbm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-4cnbm webserver-deployment-7f5969cbc7- deployment-1429  0daf40d8-9638-4cae-9c48-cb82788c5dc2 14127 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8ca70 0xc003d8ca71}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hpjtl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hpjtl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.374: INFO: Pod "webserver-deployment-d9f79cb5-dcrgm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dcrgm webserver-deployment-d9f79cb5- deployment-1429  58a183cd-2c3a-4c19-8d80-81fea637a4f4 14132 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cbcf 0xc003d8cbe0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtl2f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtl2f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-cw9ff" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cw9ff webserver-deployment-d9f79cb5- deployment-1429  0d839955-bb0e-4805-8008-261ce20ab6c8 14133 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cd4f 0xc003d8cd60}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-smths,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-smths,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-6b4lx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6b4lx webserver-deployment-d9f79cb5- deployment-1429  e3b2ffc3-030d-4436-b3bc-f8ecca8a7185 14134 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8cecf 0xc003d8cee0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x67t6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x67t6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-d9f79cb5-wc7wc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-wc7wc webserver-deployment-d9f79cb5- deployment-1429  75626521-c62c-42e6-8523-3ddad194cb3b 14135 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8d05f 0xc003d8d070}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xtq2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xtq2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-7f5969cbc7-xcjw5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xcjw5 webserver-deployment-7f5969cbc7- deployment-1429  c4538280-98ee-486f-9aae-6aed09b6527e 14148 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d1f0 0xc003d8d1f1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84c7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84c7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.375: INFO: Pod "webserver-deployment-7f5969cbc7-dgc7s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dgc7s webserver-deployment-7f5969cbc7- deployment-1429  de80ceb2-48e7-4006-a51a-9718fce37c7f 14151 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d360 0xc003d8d361}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drrx5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drrx5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-9nzxh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9nzxh webserver-deployment-7f5969cbc7- deployment-1429  44df45de-706f-42aa-a193-474d487ea59d 14156 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8d940 0xc003d8d941}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cqxsd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cqxsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-whxxd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whxxd webserver-deployment-7f5969cbc7- deployment-1429  70a6f418-f0c9-499e-a867-c4fe43fd5182 14162 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8dab0 0xc003d8dab1}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwlts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwlts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-7f5969cbc7-jxlt8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxlt8 webserver-deployment-7f5969cbc7- deployment-1429  5e51feae-9686-4887-8782-c873f0041e84 14163 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc003d8dc40 0xc003d8dc41}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8hdvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8hdvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.376: INFO: Pod "webserver-deployment-d9f79cb5-r7nt7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r7nt7 webserver-deployment-d9f79cb5- deployment-1429  8b90d644-48c5-42cd-89b9-10fa2c5f1a52 14166 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8dd9f 0xc003d8ddb0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r8xs6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r8xs6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.377: INFO: Pod "webserver-deployment-d9f79cb5-dkrz8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dkrz8 webserver-deployment-d9f79cb5- deployment-1429  5c62907c-4039-4fcc-993b-ca43445e142e 14175 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:55d882936e236ffadd42bf116e40d746ae23ff1003ab81277bed267114fb32fa cni.projectcalico.org/podIP:10.1.132.40/32 cni.projectcalico.org/podIPs:10.1.132.40/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc003d8df1f 0xc003d8df50}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c27bj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c27bj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.40,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:87042157b1795866314b82314bead74002b6ec66aaea1e65ca05da588531be27: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.377: INFO: Pod "webserver-deployment-d9f79cb5-bxslw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bxslw webserver-deployment-d9f79cb5- deployment-1429  f9fe040a-7ced-45df-93ad-8f418d8cb714 14176 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:3358c40ac0aa14fbc7e7c932ee22558136481ee5b9482064f49f1bdcae61b6f5 cni.projectcalico.org/podIP:10.1.132.34/32 cni.projectcalico.org/podIPs:10.1.132.34/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c17f 0xc00490c1b0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbtv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbtv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-d9f79cb5-lvtvs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lvtvs webserver-deployment-d9f79cb5- deployment-1429  6ae8df7a-6e91-44d1-b366-d76aba16fdeb 14194 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:64bbaad14c1d0920d2e2a6879e8190b6a2ac55d7b983d94db1dca53d67693d1a cni.projectcalico.org/podIP:10.1.132.29/32 cni.projectcalico.org/podIPs:10.1.132.29/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c3af 0xc00490c3e0}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fd84f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fd84f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:,StartTime:2023-01-18 21:26:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-d9f79cb5-r5zlr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-r5zlr webserver-deployment-d9f79cb5- deployment-1429  0b8eda01-7251-4789-bb96-994e46043289 14196 0 2023-01-18 21:25:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:135af403bcc5f47d4d71e1740148d524ea3157855a4351b0bb6bff2a087aa7ba cni.projectcalico.org/podIP:10.1.132.61/32 cni.projectcalico.org/podIPs:10.1.132.61/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 180e762e-eb87-4a3d-b5db-3284d943a093 0xc00490c5df 0xc00490c610}] [] [{kubelite Update v1 2023-01-18 21:25:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"180e762e-eb87-4a3d-b5db-3284d943a093\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-csfbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-csfbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:25:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.61,StartTime:2023-01-18 21:25:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = NotFound desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to unpack image on snapshotter overlayfs: unexpected media type text/html for sha256:87042157b1795866314b82314bead74002b6ec66aaea1e65ca05da588531be27: not found,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 21:26:03.378: INFO: Pod "webserver-deployment-7f5969cbc7-kw75n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kw75n webserver-deployment-7f5969cbc7- deployment-1429  3b6f41e3-6584-4dc6-86ea-d5e81c569ebf 14201 0 2023-01-18 21:26:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:d438481e4fe196e4abb0f045ebc26cab1182142464fc4edb6225308aaf779f59 cni.projectcalico.org/podIP:10.1.132.6/32 cni.projectcalico.org/podIPs:10.1.132.6/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb 0xc00490c870 0xc00490c871}] [] [{kubelite Update v1 2023-01-18 21:26:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"953e8b6d-8b6a-4bae-ae38-33cb9d07f5eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:26:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hnlzh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hnlzh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:26:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:03.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1429" for this suite. 01/18/23 21:26:03.397
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:03.476
Jan 18 21:26:03.477: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:26:03.479
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:03.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:03.83
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan 18 21:26:03.833: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:26:06.853
Jan 18 21:26:06.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 create -f -'
Jan 18 21:26:07.716: INFO: stderr: ""
Jan 18 21:26:07.716: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 21:26:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 delete e2e-test-crd-publish-openapi-5398-crds test-cr'
Jan 18 21:26:07.837: INFO: stderr: ""
Jan 18 21:26:07.837: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 21:26:07.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 apply -f -'
Jan 18 21:26:08.078: INFO: stderr: ""
Jan 18 21:26:08.079: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 21:26:08.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 delete e2e-test-crd-publish-openapi-5398-crds test-cr'
Jan 18 21:26:08.177: INFO: stderr: ""
Jan 18 21:26:08.177: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/18/23 21:26:08.177
Jan 18 21:26:08.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 explain e2e-test-crd-publish-openapi-5398-crds'
Jan 18 21:26:08.402: INFO: stderr: ""
Jan 18 21:26:08.402: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5398-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:11.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3243" for this suite. 01/18/23 21:26:11.563
------------------------------
• [SLOW TEST] [8.267 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:03.476
    Jan 18 21:26:03.477: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:26:03.479
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:03.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:03.83
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan 18 21:26:03.833: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:26:06.853
    Jan 18 21:26:06.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 create -f -'
    Jan 18 21:26:07.716: INFO: stderr: ""
    Jan 18 21:26:07.716: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 21:26:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 delete e2e-test-crd-publish-openapi-5398-crds test-cr'
    Jan 18 21:26:07.837: INFO: stderr: ""
    Jan 18 21:26:07.837: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 18 21:26:07.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 apply -f -'
    Jan 18 21:26:08.078: INFO: stderr: ""
    Jan 18 21:26:08.079: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 21:26:08.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 --namespace=crd-publish-openapi-3243 delete e2e-test-crd-publish-openapi-5398-crds test-cr'
    Jan 18 21:26:08.177: INFO: stderr: ""
    Jan 18 21:26:08.177: INFO: stdout: "e2e-test-crd-publish-openapi-5398-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/18/23 21:26:08.177
    Jan 18 21:26:08.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-3243 explain e2e-test-crd-publish-openapi-5398-crds'
    Jan 18 21:26:08.402: INFO: stderr: ""
    Jan 18 21:26:08.402: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5398-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:11.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3243" for this suite. 01/18/23 21:26:11.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:11.746
Jan 18 21:26:11.746: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:26:11.747
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:11.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:11.981
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:26:12.023
Jan 18 21:26:12.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7" in namespace "downward-api-6640" to be "Succeeded or Failed"
Jan 18 21:26:12.206: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.411261ms
Jan 18 21:26:14.217: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024495551s
Jan 18 21:26:16.212: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01950265s
Jan 18 21:26:18.213: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020263331s
STEP: Saw pod success 01/18/23 21:26:18.213
Jan 18 21:26:18.213: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7" satisfied condition "Succeeded or Failed"
Jan 18 21:26:18.218: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 container client-container: <nil>
STEP: delete the pod 01/18/23 21:26:18.233
Jan 18 21:26:18.283: INFO: Waiting for pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 to disappear
Jan 18 21:26:18.299: INFO: Pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:18.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6640" for this suite. 01/18/23 21:26:18.312
------------------------------
• [SLOW TEST] [6.593 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:11.746
    Jan 18 21:26:11.746: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:26:11.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:11.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:11.981
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:26:12.023
    Jan 18 21:26:12.193: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7" in namespace "downward-api-6640" to be "Succeeded or Failed"
    Jan 18 21:26:12.206: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.411261ms
    Jan 18 21:26:14.217: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024495551s
    Jan 18 21:26:16.212: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01950265s
    Jan 18 21:26:18.213: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020263331s
    STEP: Saw pod success 01/18/23 21:26:18.213
    Jan 18 21:26:18.213: INFO: Pod "downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7" satisfied condition "Succeeded or Failed"
    Jan 18 21:26:18.218: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:26:18.233
    Jan 18 21:26:18.283: INFO: Waiting for pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 to disappear
    Jan 18 21:26:18.299: INFO: Pod downwardapi-volume-f3f1cba0-276f-4f2b-b3f7-62822d38cac7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:18.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6640" for this suite. 01/18/23 21:26:18.312
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:18.339
Jan 18 21:26:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:26:18.34
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:18.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:18.395
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/18/23 21:26:18.403
STEP: getting /apis/node.k8s.io 01/18/23 21:26:18.405
STEP: getting /apis/node.k8s.io/v1 01/18/23 21:26:18.405
STEP: creating 01/18/23 21:26:18.407
STEP: watching 01/18/23 21:26:18.453
Jan 18 21:26:18.454: INFO: starting watch
STEP: getting 01/18/23 21:26:18.47
STEP: listing 01/18/23 21:26:18.476
STEP: patching 01/18/23 21:26:18.481
STEP: updating 01/18/23 21:26:18.495
Jan 18 21:26:18.513: INFO: waiting for watch events with expected annotations
STEP: deleting 01/18/23 21:26:18.513
STEP: deleting a collection 01/18/23 21:26:18.57
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7054" for this suite. 01/18/23 21:26:18.631
------------------------------
• [0.309 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:18.339
    Jan 18 21:26:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:26:18.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:18.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:18.395
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/18/23 21:26:18.403
    STEP: getting /apis/node.k8s.io 01/18/23 21:26:18.405
    STEP: getting /apis/node.k8s.io/v1 01/18/23 21:26:18.405
    STEP: creating 01/18/23 21:26:18.407
    STEP: watching 01/18/23 21:26:18.453
    Jan 18 21:26:18.454: INFO: starting watch
    STEP: getting 01/18/23 21:26:18.47
    STEP: listing 01/18/23 21:26:18.476
    STEP: patching 01/18/23 21:26:18.481
    STEP: updating 01/18/23 21:26:18.495
    Jan 18 21:26:18.513: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/18/23 21:26:18.513
    STEP: deleting a collection 01/18/23 21:26:18.57
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:18.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7054" for this suite. 01/18/23 21:26:18.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:18.649
Jan 18 21:26:18.649: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:26:18.65
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:18.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:18.694
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan 18 21:26:18.700: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: creating the pod 01/18/23 21:26:18.701
STEP: submitting the pod to kubernetes 01/18/23 21:26:18.701
Jan 18 21:26:18.724: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59" in namespace "pods-4858" to be "running and ready"
Jan 18 21:26:18.729: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244463ms
Jan 18 21:26:18.729: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:26:20.734: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0107281s
Jan 18 21:26:20.734: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:26:22.735: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Running", Reason="", readiness=true. Elapsed: 4.011422881s
Jan 18 21:26:22.735: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Running (Ready = true)
Jan 18 21:26:22.735: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:22.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4858" for this suite. 01/18/23 21:26:22.841
------------------------------
• [4.211 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:18.649
    Jan 18 21:26:18.649: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:26:18.65
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:18.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:18.694
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan 18 21:26:18.700: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: creating the pod 01/18/23 21:26:18.701
    STEP: submitting the pod to kubernetes 01/18/23 21:26:18.701
    Jan 18 21:26:18.724: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59" in namespace "pods-4858" to be "running and ready"
    Jan 18 21:26:18.729: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Pending", Reason="", readiness=false. Elapsed: 5.244463ms
    Jan 18 21:26:18.729: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:26:20.734: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0107281s
    Jan 18 21:26:20.734: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:26:22.735: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59": Phase="Running", Reason="", readiness=true. Elapsed: 4.011422881s
    Jan 18 21:26:22.735: INFO: The phase of Pod pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59 is Running (Ready = true)
    Jan 18 21:26:22.735: INFO: Pod "pod-logs-websocket-bc146e1d-6bf2-440a-9b3d-037716a95b59" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:22.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4858" for this suite. 01/18/23 21:26:22.841
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:22.86
Jan 18 21:26:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:26:22.863
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:22.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:22.91
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-bb57ae02-5ab5-4c16-82f2-6e9c84b24e80 01/18/23 21:26:22.914
STEP: Creating a pod to test consume secrets 01/18/23 21:26:22.925
Jan 18 21:26:22.946: INFO: Waiting up to 5m0s for pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1" in namespace "secrets-2501" to be "Succeeded or Failed"
Jan 18 21:26:22.951: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690756ms
Jan 18 21:26:24.958: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011216277s
Jan 18 21:26:26.959: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.0127852s
Jan 18 21:26:28.958: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011989895s
STEP: Saw pod success 01/18/23 21:26:28.959
Jan 18 21:26:28.959: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1" satisfied condition "Succeeded or Failed"
Jan 18 21:26:28.963: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:26:28.972
Jan 18 21:26:29.001: INFO: Waiting for pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 to disappear
Jan 18 21:26:29.009: INFO: Pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:29.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2501" for this suite. 01/18/23 21:26:29.014
------------------------------
• [SLOW TEST] [6.168 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:22.86
    Jan 18 21:26:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:26:22.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:22.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:22.91
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-bb57ae02-5ab5-4c16-82f2-6e9c84b24e80 01/18/23 21:26:22.914
    STEP: Creating a pod to test consume secrets 01/18/23 21:26:22.925
    Jan 18 21:26:22.946: INFO: Waiting up to 5m0s for pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1" in namespace "secrets-2501" to be "Succeeded or Failed"
    Jan 18 21:26:22.951: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.690756ms
    Jan 18 21:26:24.958: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.011216277s
    Jan 18 21:26:26.959: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.0127852s
    Jan 18 21:26:28.958: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011989895s
    STEP: Saw pod success 01/18/23 21:26:28.959
    Jan 18 21:26:28.959: INFO: Pod "pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1" satisfied condition "Succeeded or Failed"
    Jan 18 21:26:28.963: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:26:28.972
    Jan 18 21:26:29.001: INFO: Waiting for pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 to disappear
    Jan 18 21:26:29.009: INFO: Pod pod-secrets-7d24e034-416a-413d-b4ab-a35b1baea3c1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:29.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2501" for this suite. 01/18/23 21:26:29.014
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:29.029
Jan 18 21:26:29.029: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:26:29.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:29.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:29.078
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 18 21:26:29.100: INFO: Waiting up to 5m0s for pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278" in namespace "kubelet-test-850" to be "running and ready"
Jan 18 21:26:29.105: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860458ms
Jan 18 21:26:29.105: INFO: The phase of Pod busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:26:31.110: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278": Phase="Running", Reason="", readiness=true. Elapsed: 2.010234727s
Jan 18 21:26:31.110: INFO: The phase of Pod busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278 is Running (Ready = true)
Jan 18 21:26:31.110: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-850" for this suite. 01/18/23 21:26:31.131
------------------------------
• [2.122 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:29.029
    Jan 18 21:26:29.029: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 21:26:29.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:29.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:29.078
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 18 21:26:29.100: INFO: Waiting up to 5m0s for pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278" in namespace "kubelet-test-850" to be "running and ready"
    Jan 18 21:26:29.105: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278": Phase="Pending", Reason="", readiness=false. Elapsed: 4.860458ms
    Jan 18 21:26:29.105: INFO: The phase of Pod busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:26:31.110: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278": Phase="Running", Reason="", readiness=true. Elapsed: 2.010234727s
    Jan 18 21:26:31.110: INFO: The phase of Pod busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278 is Running (Ready = true)
    Jan 18 21:26:31.110: INFO: Pod "busybox-scheduling-56e43d4e-c0b6-4913-8dba-78d1d60a1278" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:31.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-850" for this suite. 01/18/23 21:26:31.131
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:31.151
Jan 18 21:26:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:26:31.152
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:31.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:31.213
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/18/23 21:26:31.216
STEP: Creating a ResourceQuota 01/18/23 21:26:36.231
STEP: Ensuring resource quota status is calculated 01/18/23 21:26:36.249
STEP: Creating a ReplicationController 01/18/23 21:26:38.256
STEP: Ensuring resource quota status captures replication controller creation 01/18/23 21:26:38.286
STEP: Deleting a ReplicationController 01/18/23 21:26:40.293
STEP: Ensuring resource quota status released usage 01/18/23 21:26:40.307
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:42.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1417" for this suite. 01/18/23 21:26:42.322
------------------------------
• [SLOW TEST] [11.196 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:31.151
    Jan 18 21:26:31.151: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:26:31.152
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:31.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:31.213
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/18/23 21:26:31.216
    STEP: Creating a ResourceQuota 01/18/23 21:26:36.231
    STEP: Ensuring resource quota status is calculated 01/18/23 21:26:36.249
    STEP: Creating a ReplicationController 01/18/23 21:26:38.256
    STEP: Ensuring resource quota status captures replication controller creation 01/18/23 21:26:38.286
    STEP: Deleting a ReplicationController 01/18/23 21:26:40.293
    STEP: Ensuring resource quota status released usage 01/18/23 21:26:40.307
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:42.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1417" for this suite. 01/18/23 21:26:42.322
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:42.35
Jan 18 21:26:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:26:42.352
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:42.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:42.5
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:26:42.553
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:26:43.041
STEP: Deploying the webhook pod 01/18/23 21:26:43.066
STEP: Wait for the deployment to be ready 01/18/23 21:26:43.091
Jan 18 21:26:43.105: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:26:45.123
STEP: Verifying the service has paired with the endpoint 01/18/23 21:26:45.161
Jan 18 21:26:46.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan 18 21:26:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7070-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:26:46.684
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:26:47.761
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:51.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3693" for this suite. 01/18/23 21:26:51.665
STEP: Destroying namespace "webhook-3693-markers" for this suite. 01/18/23 21:26:51.689
------------------------------
• [SLOW TEST] [9.354 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:42.35
    Jan 18 21:26:42.350: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:26:42.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:42.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:42.5
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:26:42.553
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:26:43.041
    STEP: Deploying the webhook pod 01/18/23 21:26:43.066
    STEP: Wait for the deployment to be ready 01/18/23 21:26:43.091
    Jan 18 21:26:43.105: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:26:45.123
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:26:45.161
    Jan 18 21:26:46.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan 18 21:26:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7070-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 21:26:46.684
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 21:26:47.761
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:51.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3693" for this suite. 01/18/23 21:26:51.665
    STEP: Destroying namespace "webhook-3693-markers" for this suite. 01/18/23 21:26:51.689
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:51.707
Jan 18 21:26:51.707: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:26:51.708
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:51.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:51.752
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:26:51.755
Jan 18 21:26:51.775: INFO: Waiting up to 5m0s for pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b" in namespace "emptydir-2241" to be "Succeeded or Failed"
Jan 18 21:26:51.779: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148747ms
Jan 18 21:26:53.785: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010527057s
Jan 18 21:26:55.785: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010511096s
Jan 18 21:26:57.791: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016070595s
STEP: Saw pod success 01/18/23 21:26:57.791
Jan 18 21:26:57.791: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b" satisfied condition "Succeeded or Failed"
Jan 18 21:26:57.796: INFO: Trying to get logs from node test-vm-1 pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b container test-container: <nil>
STEP: delete the pod 01/18/23 21:26:57.805
Jan 18 21:26:57.833: INFO: Waiting for pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b to disappear
Jan 18 21:26:57.842: INFO: Pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:26:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2241" for this suite. 01/18/23 21:26:57.848
------------------------------
• [SLOW TEST] [6.155 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:51.707
    Jan 18 21:26:51.707: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:26:51.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:51.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:51.752
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:26:51.755
    Jan 18 21:26:51.775: INFO: Waiting up to 5m0s for pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b" in namespace "emptydir-2241" to be "Succeeded or Failed"
    Jan 18 21:26:51.779: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.148747ms
    Jan 18 21:26:53.785: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010527057s
    Jan 18 21:26:55.785: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010511096s
    Jan 18 21:26:57.791: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016070595s
    STEP: Saw pod success 01/18/23 21:26:57.791
    Jan 18 21:26:57.791: INFO: Pod "pod-7839ad27-eb48-413d-8996-4062ebaeb08b" satisfied condition "Succeeded or Failed"
    Jan 18 21:26:57.796: INFO: Trying to get logs from node test-vm-1 pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b container test-container: <nil>
    STEP: delete the pod 01/18/23 21:26:57.805
    Jan 18 21:26:57.833: INFO: Waiting for pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b to disappear
    Jan 18 21:26:57.842: INFO: Pod pod-7839ad27-eb48-413d-8996-4062ebaeb08b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:26:57.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2241" for this suite. 01/18/23 21:26:57.848
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:26:57.862
Jan 18 21:26:57.862: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 21:26:57.864
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:57.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:57.909
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 21:26:57.911
Jan 18 21:26:57.933: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 21:27:02.945: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:27:02.945
STEP: getting scale subresource 01/18/23 21:27:02.946
STEP: updating a scale subresource 01/18/23 21:27:02.959
STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 21:27:02.987
STEP: Patch a scale subresource 01/18/23 21:27:02.992
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:03.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5167" for this suite. 01/18/23 21:27:03.048
------------------------------
• [SLOW TEST] [5.251 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:26:57.862
    Jan 18 21:26:57.862: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 21:26:57.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:26:57.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:26:57.909
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 21:26:57.911
    Jan 18 21:26:57.933: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 21:27:02.945: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:27:02.945
    STEP: getting scale subresource 01/18/23 21:27:02.946
    STEP: updating a scale subresource 01/18/23 21:27:02.959
    STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 21:27:02.987
    STEP: Patch a scale subresource 01/18/23 21:27:02.992
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:03.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5167" for this suite. 01/18/23 21:27:03.048
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:03.115
Jan 18 21:27:03.115: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename lease-test 01/18/23 21:27:03.116
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.291
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:03.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-53" for this suite. 01/18/23 21:27:03.547
------------------------------
• [0.469 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:03.115
    Jan 18 21:27:03.115: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename lease-test 01/18/23 21:27:03.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.291
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:03.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-53" for this suite. 01/18/23 21:27:03.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:03.584
Jan 18 21:27:03.585: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 21:27:03.586
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.656
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/18/23 21:27:03.666
Jan 18 21:27:03.674: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/18/23 21:27:03.674
Jan 18 21:27:03.703: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/18/23 21:27:03.703
Jan 18 21:27:03.743: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:03.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4402" for this suite. 01/18/23 21:27:03.756
------------------------------
• [0.213 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:03.584
    Jan 18 21:27:03.585: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:27:03.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.656
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/18/23 21:27:03.666
    Jan 18 21:27:03.674: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/18/23 21:27:03.674
    Jan 18 21:27:03.703: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/18/23 21:27:03.703
    Jan 18 21:27:03.743: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:03.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4402" for this suite. 01/18/23 21:27:03.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:03.799
Jan 18 21:27:03.799: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 21:27:03.8
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.907
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/18/23 21:27:03.919
STEP: Ensuring job reaches completions 01/18/23 21:27:03.946
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:15.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6236" for this suite. 01/18/23 21:27:15.973
------------------------------
• [SLOW TEST] [12.237 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:03.799
    Jan 18 21:27:03.799: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 21:27:03.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:03.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:03.907
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/18/23 21:27:03.919
    STEP: Ensuring job reaches completions 01/18/23 21:27:03.946
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:15.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6236" for this suite. 01/18/23 21:27:15.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:16.038
Jan 18 21:27:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:27:16.039
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:16.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:16.141
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 18 21:27:16.195: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4961 to be scheduled
Jan 18 21:27:16.204: INFO: 1 pods are not scheduled: [runtimeclass-4961/test-runtimeclass-runtimeclass-4961-preconfigured-handler-b6vsc(1e6b0bb0-7c42-401c-8a02-0c82c045c1d3)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:18.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4961" for this suite. 01/18/23 21:27:18.235
------------------------------
• [2.212 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:16.038
    Jan 18 21:27:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 21:27:16.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:16.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:16.141
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 18 21:27:16.195: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4961 to be scheduled
    Jan 18 21:27:16.204: INFO: 1 pods are not scheduled: [runtimeclass-4961/test-runtimeclass-runtimeclass-4961-preconfigured-handler-b6vsc(1e6b0bb0-7c42-401c-8a02-0c82c045c1d3)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:18.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4961" for this suite. 01/18/23 21:27:18.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:18.251
Jan 18 21:27:18.251: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:27:18.253
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:18.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:18.291
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/18/23 21:27:18.293
STEP: wait for the container to reach Failed 01/18/23 21:27:18.314
STEP: get the container status 01/18/23 21:27:23.358
STEP: the container should be terminated 01/18/23 21:27:23.363
STEP: the termination message should be set 01/18/23 21:27:23.363
Jan 18 21:27:23.363: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 21:27:23.364
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:23.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2793" for this suite. 01/18/23 21:27:23.405
------------------------------
• [SLOW TEST] [5.176 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:18.251
    Jan 18 21:27:18.251: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:27:18.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:18.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:18.291
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/18/23 21:27:18.293
    STEP: wait for the container to reach Failed 01/18/23 21:27:18.314
    STEP: get the container status 01/18/23 21:27:23.358
    STEP: the container should be terminated 01/18/23 21:27:23.363
    STEP: the termination message should be set 01/18/23 21:27:23.363
    Jan 18 21:27:23.363: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 21:27:23.364
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:23.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2793" for this suite. 01/18/23 21:27:23.405
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:23.43
Jan 18 21:27:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 21:27:23.432
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:23.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:23.489
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/18/23 21:27:23.492
Jan 18 21:27:23.519: INFO: Waiting up to 5m0s for pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74" in namespace "var-expansion-4256" to be "Succeeded or Failed"
Jan 18 21:27:23.524: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265852ms
Jan 18 21:27:25.531: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011799341s
Jan 18 21:27:27.530: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011395463s
STEP: Saw pod success 01/18/23 21:27:27.53
Jan 18 21:27:27.531: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74" satisfied condition "Succeeded or Failed"
Jan 18 21:27:27.536: INFO: Trying to get logs from node test-vm-2 pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:27:27.559
Jan 18 21:27:27.664: INFO: Waiting for pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 to disappear
Jan 18 21:27:27.674: INFO: Pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:27.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4256" for this suite. 01/18/23 21:27:27.679
------------------------------
• [4.319 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:23.43
    Jan 18 21:27:23.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 21:27:23.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:23.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:23.489
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/18/23 21:27:23.492
    Jan 18 21:27:23.519: INFO: Waiting up to 5m0s for pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74" in namespace "var-expansion-4256" to be "Succeeded or Failed"
    Jan 18 21:27:23.524: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Pending", Reason="", readiness=false. Elapsed: 5.265852ms
    Jan 18 21:27:25.531: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011799341s
    Jan 18 21:27:27.530: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011395463s
    STEP: Saw pod success 01/18/23 21:27:27.53
    Jan 18 21:27:27.531: INFO: Pod "var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74" satisfied condition "Succeeded or Failed"
    Jan 18 21:27:27.536: INFO: Trying to get logs from node test-vm-2 pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:27:27.559
    Jan 18 21:27:27.664: INFO: Waiting for pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 to disappear
    Jan 18 21:27:27.674: INFO: Pod var-expansion-90ef1b5d-7e37-41a8-b0e2-92f44bf6fa74 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:27.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4256" for this suite. 01/18/23 21:27:27.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:27.751
Jan 18 21:27:27.751: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:27:27.752
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:27.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:27.801
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:27:27.811
Jan 18 21:27:27.845: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6911" to be "running and ready"
Jan 18 21:27:27.850: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021558ms
Jan 18 21:27:27.851: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:29.857: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011613695s
Jan 18 21:27:29.857: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:31.857: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011485811s
Jan 18 21:27:31.857: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 21:27:31.857: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/18/23 21:27:31.862
Jan 18 21:27:31.876: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6911" to be "running and ready"
Jan 18 21:27:31.883: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.684568ms
Jan 18 21:27:31.883: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:33.889: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012825546s
Jan 18 21:27:33.889: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:35.896: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.019494338s
Jan 18 21:27:35.896: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 18 21:27:35.896: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 21:27:35.902
STEP: delete the pod with lifecycle hook 01/18/23 21:27:35.912
Jan 18 21:27:35.927: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 21:27:35.932: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 21:27:37.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 21:27:37.939: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:37.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6911" for this suite. 01/18/23 21:27:37.945
------------------------------
• [SLOW TEST] [10.227 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:27.751
    Jan 18 21:27:27.751: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:27:27.752
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:27.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:27.801
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:27:27.811
    Jan 18 21:27:27.845: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6911" to be "running and ready"
    Jan 18 21:27:27.850: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.021558ms
    Jan 18 21:27:27.851: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:29.857: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011613695s
    Jan 18 21:27:29.857: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:31.857: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011485811s
    Jan 18 21:27:31.857: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 21:27:31.857: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/18/23 21:27:31.862
    Jan 18 21:27:31.876: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6911" to be "running and ready"
    Jan 18 21:27:31.883: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.684568ms
    Jan 18 21:27:31.883: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:33.889: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012825546s
    Jan 18 21:27:33.889: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:35.896: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.019494338s
    Jan 18 21:27:35.896: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 18 21:27:35.896: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 21:27:35.902
    STEP: delete the pod with lifecycle hook 01/18/23 21:27:35.912
    Jan 18 21:27:35.927: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 21:27:35.932: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 21:27:37.933: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 21:27:37.939: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:37.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6911" for this suite. 01/18/23 21:27:37.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:37.98
Jan 18 21:27:37.980: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 21:27:37.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:38.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:38.019
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 18 21:27:38.095: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 21:27:43.104: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:27:43.104
STEP: Scaling up "test-rs" replicaset  01/18/23 21:27:43.104
Jan 18 21:27:43.128: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/18/23 21:27:43.128
W0118 21:27:43.166694      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 21:27:43.168: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 21:27:43.209: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 21:27:43.312: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 21:27:43.411: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 21:27:45.363: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 21:27:45.569: INFO: observed Replicaset test-rs in namespace replicaset-2940 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:45.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2940" for this suite. 01/18/23 21:27:45.575
------------------------------
• [SLOW TEST] [7.628 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:37.98
    Jan 18 21:27:37.980: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 21:27:37.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:38.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:38.019
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 18 21:27:38.095: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 21:27:43.104: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:27:43.104
    STEP: Scaling up "test-rs" replicaset  01/18/23 21:27:43.104
    Jan 18 21:27:43.128: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/18/23 21:27:43.128
    W0118 21:27:43.166694      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 21:27:43.168: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 21:27:43.209: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 21:27:43.312: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 21:27:43.411: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 21:27:45.363: INFO: observed ReplicaSet test-rs in namespace replicaset-2940 with ReadyReplicas 2, AvailableReplicas 2
    Jan 18 21:27:45.569: INFO: observed Replicaset test-rs in namespace replicaset-2940 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:45.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2940" for this suite. 01/18/23 21:27:45.575
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:45.608
Jan 18 21:27:45.609: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:27:45.61
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:45.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:45.66
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:27:45.678
Jan 18 21:27:45.705: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4812" to be "running and ready"
Jan 18 21:27:45.712: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357979ms
Jan 18 21:27:45.712: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:47.720: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015169274s
Jan 18 21:27:47.720: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 21:27:47.720: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/18/23 21:27:47.725
Jan 18 21:27:47.740: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4812" to be "running and ready"
Jan 18 21:27:47.744: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.974743ms
Jan 18 21:27:47.744: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:27:49.752: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011913839s
Jan 18 21:27:49.752: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 18 21:27:49.752: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 21:27:49.76
Jan 18 21:27:49.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 21:27:49.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 21:27:51.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 21:27:51.863: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 21:27:53.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 21:27:53.856: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/18/23 21:27:53.857
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4812" for this suite. 01/18/23 21:27:53.873
------------------------------
• [SLOW TEST] [8.287 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:45.608
    Jan 18 21:27:45.609: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:27:45.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:45.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:45.66
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:27:45.678
    Jan 18 21:27:45.705: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4812" to be "running and ready"
    Jan 18 21:27:45.712: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.357979ms
    Jan 18 21:27:45.712: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:47.720: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015169274s
    Jan 18 21:27:47.720: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 21:27:47.720: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/18/23 21:27:47.725
    Jan 18 21:27:47.740: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4812" to be "running and ready"
    Jan 18 21:27:47.744: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.974743ms
    Jan 18 21:27:47.744: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:27:49.752: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.011913839s
    Jan 18 21:27:49.752: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 18 21:27:49.752: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 21:27:49.76
    Jan 18 21:27:49.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 21:27:49.850: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 21:27:51.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 21:27:51.863: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 21:27:53.851: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 21:27:53.856: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/18/23 21:27:53.857
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:53.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4812" for this suite. 01/18/23 21:27:53.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:53.897
Jan 18 21:27:53.897: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:27:53.899
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:53.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:53.935
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/18/23 21:27:53.94
STEP: Getting a ResourceQuota 01/18/23 21:27:53.953
STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 21:27:53.958
STEP: Patching the ResourceQuota 01/18/23 21:27:53.965
STEP: Deleting a Collection of ResourceQuotas 01/18/23 21:27:53.983
STEP: Verifying the deleted ResourceQuota 01/18/23 21:27:54.019
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4597" for this suite. 01/18/23 21:27:54.029
------------------------------
• [0.154 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:53.897
    Jan 18 21:27:53.897: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:27:53.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:53.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:53.935
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/18/23 21:27:53.94
    STEP: Getting a ResourceQuota 01/18/23 21:27:53.953
    STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 21:27:53.958
    STEP: Patching the ResourceQuota 01/18/23 21:27:53.965
    STEP: Deleting a Collection of ResourceQuotas 01/18/23 21:27:53.983
    STEP: Verifying the deleted ResourceQuota 01/18/23 21:27:54.019
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:54.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4597" for this suite. 01/18/23 21:27:54.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:54.054
Jan 18 21:27:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename ingress 01/18/23 21:27:54.056
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:54.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:54.11
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/18/23 21:27:54.115
STEP: getting /apis/networking.k8s.io 01/18/23 21:27:54.117
STEP: getting /apis/networking.k8s.iov1 01/18/23 21:27:54.117
STEP: creating 01/18/23 21:27:54.118
STEP: getting 01/18/23 21:27:54.153
STEP: listing 01/18/23 21:27:54.16
STEP: watching 01/18/23 21:27:54.168
Jan 18 21:27:54.168: INFO: starting watch
STEP: cluster-wide listing 01/18/23 21:27:54.169
STEP: cluster-wide watching 01/18/23 21:27:54.175
Jan 18 21:27:54.175: INFO: starting watch
STEP: patching 01/18/23 21:27:54.176
STEP: updating 01/18/23 21:27:54.192
Jan 18 21:27:54.214: INFO: waiting for watch events with expected annotations
Jan 18 21:27:54.214: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 21:27:54.214
STEP: updating /status 01/18/23 21:27:54.227
STEP: get /status 01/18/23 21:27:54.244
STEP: deleting 01/18/23 21:27:54.248
STEP: deleting a collection 01/18/23 21:27:54.289
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:27:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-5882" for this suite. 01/18/23 21:27:54.341
------------------------------
• [0.300 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:54.054
    Jan 18 21:27:54.054: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename ingress 01/18/23 21:27:54.056
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:54.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:54.11
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/18/23 21:27:54.115
    STEP: getting /apis/networking.k8s.io 01/18/23 21:27:54.117
    STEP: getting /apis/networking.k8s.iov1 01/18/23 21:27:54.117
    STEP: creating 01/18/23 21:27:54.118
    STEP: getting 01/18/23 21:27:54.153
    STEP: listing 01/18/23 21:27:54.16
    STEP: watching 01/18/23 21:27:54.168
    Jan 18 21:27:54.168: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 21:27:54.169
    STEP: cluster-wide watching 01/18/23 21:27:54.175
    Jan 18 21:27:54.175: INFO: starting watch
    STEP: patching 01/18/23 21:27:54.176
    STEP: updating 01/18/23 21:27:54.192
    Jan 18 21:27:54.214: INFO: waiting for watch events with expected annotations
    Jan 18 21:27:54.214: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 21:27:54.214
    STEP: updating /status 01/18/23 21:27:54.227
    STEP: get /status 01/18/23 21:27:54.244
    STEP: deleting 01/18/23 21:27:54.248
    STEP: deleting a collection 01/18/23 21:27:54.289
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:27:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-5882" for this suite. 01/18/23 21:27:54.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:27:54.357
Jan 18 21:27:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:27:54.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:54.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:54.396
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:27:54.448
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:27:54.835
STEP: Deploying the webhook pod 01/18/23 21:27:54.854
STEP: Wait for the deployment to be ready 01/18/23 21:27:54.88
Jan 18 21:27:54.896: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:27:56.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:27:58.918
STEP: Verifying the service has paired with the endpoint 01/18/23 21:27:58.978
Jan 18 21:27:59.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 21:27:59.985
STEP: create a namespace for the webhook 01/18/23 21:28:01.071
STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 21:28:01.095
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:02.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2182" for this suite. 01/18/23 21:28:02.294
STEP: Destroying namespace "webhook-2182-markers" for this suite. 01/18/23 21:28:02.313
------------------------------
• [SLOW TEST] [7.978 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:27:54.357
    Jan 18 21:27:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:27:54.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:27:54.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:27:54.396
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:27:54.448
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:27:54.835
    STEP: Deploying the webhook pod 01/18/23 21:27:54.854
    STEP: Wait for the deployment to be ready 01/18/23 21:27:54.88
    Jan 18 21:27:54.896: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:27:56.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 27, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:27:58.918
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:27:58.978
    Jan 18 21:27:59.978: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 21:27:59.985
    STEP: create a namespace for the webhook 01/18/23 21:28:01.071
    STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 21:28:01.095
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:02.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2182" for this suite. 01/18/23 21:28:02.294
    STEP: Destroying namespace "webhook-2182-markers" for this suite. 01/18/23 21:28:02.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:02.336
Jan 18 21:28:02.337: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:28:02.338
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:02.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:02.41
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:28:02.47
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:28:02.488
Jan 18 21:28:02.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:28:02.520: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:03.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:28:03.541: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:04.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:28:04.532: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:28:05.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:28:05.531: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 21:28:05.537
Jan 18 21:28:05.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:28:05.574: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:06.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:28:06.590: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:07.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:28:07.590: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:08.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:28:08.586: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:28:09.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:28:09.587: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:28:09.591
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8186, will wait for the garbage collector to delete the pods 01/18/23 21:28:09.591
Jan 18 21:28:09.660: INFO: Deleting DaemonSet.extensions daemon-set took: 13.859058ms
Jan 18 21:28:09.761: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.95435ms
Jan 18 21:28:12.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:28:12.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:28:12.776: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15876"},"items":null}

Jan 18 21:28:12.783: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15876"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8186" for this suite. 01/18/23 21:28:12.801
------------------------------
• [SLOW TEST] [10.479 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:02.336
    Jan 18 21:28:02.337: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:28:02.338
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:02.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:02.41
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:28:02.47
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:28:02.488
    Jan 18 21:28:02.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:28:02.520: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:03.541: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:28:03.541: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:04.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:28:04.532: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:28:05.531: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:28:05.531: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 21:28:05.537
    Jan 18 21:28:05.574: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:28:05.574: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:06.590: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:28:06.590: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:07.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:28:07.590: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:08.586: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:28:08.586: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:28:09.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:28:09.587: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:28:09.591
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8186, will wait for the garbage collector to delete the pods 01/18/23 21:28:09.591
    Jan 18 21:28:09.660: INFO: Deleting DaemonSet.extensions daemon-set took: 13.859058ms
    Jan 18 21:28:09.761: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.95435ms
    Jan 18 21:28:12.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:28:12.769: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:28:12.776: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15876"},"items":null}

    Jan 18 21:28:12.783: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15876"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8186" for this suite. 01/18/23 21:28:12.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:12.817
Jan 18 21:28:12.818: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename limitrange 01/18/23 21:28:12.819
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:12.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:12.854
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/18/23 21:28:12.86
STEP: Setting up watch 01/18/23 21:28:12.86
STEP: Submitting a LimitRange 01/18/23 21:28:12.972
STEP: Verifying LimitRange creation was observed 01/18/23 21:28:12.987
STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 21:28:12.987
Jan 18 21:28:12.993: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 21:28:12.994: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/18/23 21:28:12.994
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 21:28:13.017
Jan 18 21:28:13.025: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 21:28:13.025: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/18/23 21:28:13.025
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 21:28:13.073
Jan 18 21:28:13.077: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 21:28:13.077: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/18/23 21:28:13.077
STEP: Failing to create a Pod with more than max resources 01/18/23 21:28:13.08
STEP: Updating a LimitRange 01/18/23 21:28:13.081
STEP: Verifying LimitRange updating is effective 01/18/23 21:28:13.092
STEP: Creating a Pod with less than former min resources 01/18/23 21:28:15.099
STEP: Failing to create a Pod with more than max resources 01/18/23 21:28:15.113
STEP: Deleting a LimitRange 01/18/23 21:28:15.116
STEP: Verifying the LimitRange was deleted 01/18/23 21:28:15.131
Jan 18 21:28:20.142: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/18/23 21:28:20.142
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:20.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2303" for this suite. 01/18/23 21:28:20.165
------------------------------
• [SLOW TEST] [7.360 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:12.817
    Jan 18 21:28:12.818: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename limitrange 01/18/23 21:28:12.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:12.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:12.854
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/18/23 21:28:12.86
    STEP: Setting up watch 01/18/23 21:28:12.86
    STEP: Submitting a LimitRange 01/18/23 21:28:12.972
    STEP: Verifying LimitRange creation was observed 01/18/23 21:28:12.987
    STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 21:28:12.987
    Jan 18 21:28:12.993: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 21:28:12.994: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/18/23 21:28:12.994
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 21:28:13.017
    Jan 18 21:28:13.025: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 21:28:13.025: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/18/23 21:28:13.025
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 21:28:13.073
    Jan 18 21:28:13.077: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 18 21:28:13.077: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/18/23 21:28:13.077
    STEP: Failing to create a Pod with more than max resources 01/18/23 21:28:13.08
    STEP: Updating a LimitRange 01/18/23 21:28:13.081
    STEP: Verifying LimitRange updating is effective 01/18/23 21:28:13.092
    STEP: Creating a Pod with less than former min resources 01/18/23 21:28:15.099
    STEP: Failing to create a Pod with more than max resources 01/18/23 21:28:15.113
    STEP: Deleting a LimitRange 01/18/23 21:28:15.116
    STEP: Verifying the LimitRange was deleted 01/18/23 21:28:15.131
    Jan 18 21:28:20.142: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/18/23 21:28:20.142
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:20.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2303" for this suite. 01/18/23 21:28:20.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:20.179
Jan 18 21:28:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:28:20.18
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:20.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:20.217
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:28:20.247
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:28:20.989
STEP: Deploying the webhook pod 01/18/23 21:28:21
STEP: Wait for the deployment to be ready 01/18/23 21:28:21.025
Jan 18 21:28:21.037: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:28:23.054
STEP: Verifying the service has paired with the endpoint 01/18/23 21:28:23.08
Jan 18 21:28:24.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 21:28:24.087
STEP: create a pod that should be updated by the webhook 01/18/23 21:28:25.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9932" for this suite. 01/18/23 21:28:26.296
STEP: Destroying namespace "webhook-9932-markers" for this suite. 01/18/23 21:28:26.311
------------------------------
• [SLOW TEST] [6.147 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:20.179
    Jan 18 21:28:20.179: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:28:20.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:20.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:20.217
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:28:20.247
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:28:20.989
    STEP: Deploying the webhook pod 01/18/23 21:28:21
    STEP: Wait for the deployment to be ready 01/18/23 21:28:21.025
    Jan 18 21:28:21.037: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:28:23.054
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:28:23.08
    Jan 18 21:28:24.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 21:28:24.087
    STEP: create a pod that should be updated by the webhook 01/18/23 21:28:25.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9932" for this suite. 01/18/23 21:28:26.296
    STEP: Destroying namespace "webhook-9932-markers" for this suite. 01/18/23 21:28:26.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:26.328
Jan 18 21:28:26.328: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:28:26.329
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:26.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:26.409
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 18 21:28:26.542: INFO: Waiting up to 5m0s for pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea" in namespace "emptydir-wrapper-3164" to be "running and ready"
Jan 18 21:28:26.548: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471173ms
Jan 18 21:28:26.548: INFO: The phase of Pod pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:28:28.555: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea": Phase="Running", Reason="", readiness=true. Elapsed: 2.013319571s
Jan 18 21:28:28.555: INFO: The phase of Pod pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea is Running (Ready = true)
Jan 18 21:28:28.555: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/18/23 21:28:28.559
STEP: Cleaning up the configmap 01/18/23 21:28:28.571
STEP: Cleaning up the pod 01/18/23 21:28:28.588
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-3164" for this suite. 01/18/23 21:28:28.621
------------------------------
• [2.308 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:26.328
    Jan 18 21:28:26.328: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 21:28:26.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:26.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:26.409
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 18 21:28:26.542: INFO: Waiting up to 5m0s for pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea" in namespace "emptydir-wrapper-3164" to be "running and ready"
    Jan 18 21:28:26.548: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471173ms
    Jan 18 21:28:26.548: INFO: The phase of Pod pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:28:28.555: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea": Phase="Running", Reason="", readiness=true. Elapsed: 2.013319571s
    Jan 18 21:28:28.555: INFO: The phase of Pod pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea is Running (Ready = true)
    Jan 18 21:28:28.555: INFO: Pod "pod-secrets-de173432-2390-44ec-b6bd-369d87ccbbea" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/18/23 21:28:28.559
    STEP: Cleaning up the configmap 01/18/23 21:28:28.571
    STEP: Cleaning up the pod 01/18/23 21:28:28.588
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-3164" for this suite. 01/18/23 21:28:28.621
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:28.636
Jan 18 21:28:28.636: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:28:28.637
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:28.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:28.672
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-42178da2-9733-4667-9eec-2e805a63161a 01/18/23 21:28:28.675
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:28.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3669" for this suite. 01/18/23 21:28:28.682
------------------------------
• [0.059 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:28.636
    Jan 18 21:28:28.636: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:28:28.637
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:28.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:28.672
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-42178da2-9733-4667-9eec-2e805a63161a 01/18/23 21:28:28.675
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:28.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3669" for this suite. 01/18/23 21:28:28.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:28.697
Jan 18 21:28:28.697: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:28:28.698
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:28.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:28.737
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-82d81096-8be4-4fda-8fef-edbe9e5739ac 01/18/23 21:28:28.741
STEP: Creating a pod to test consume secrets 01/18/23 21:28:28.752
Jan 18 21:28:28.772: INFO: Waiting up to 5m0s for pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320" in namespace "secrets-6413" to be "Succeeded or Failed"
Jan 18 21:28:28.777: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Pending", Reason="", readiness=false. Elapsed: 5.25886ms
Jan 18 21:28:30.784: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012077858s
Jan 18 21:28:32.783: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011269781s
STEP: Saw pod success 01/18/23 21:28:32.783
Jan 18 21:28:32.783: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320" satisfied condition "Succeeded or Failed"
Jan 18 21:28:32.787: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:28:32.797
Jan 18 21:28:32.822: INFO: Waiting for pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 to disappear
Jan 18 21:28:32.828: INFO: Pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:32.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6413" for this suite. 01/18/23 21:28:32.833
------------------------------
• [4.150 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:28.697
    Jan 18 21:28:28.697: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:28:28.698
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:28.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:28.737
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-82d81096-8be4-4fda-8fef-edbe9e5739ac 01/18/23 21:28:28.741
    STEP: Creating a pod to test consume secrets 01/18/23 21:28:28.752
    Jan 18 21:28:28.772: INFO: Waiting up to 5m0s for pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320" in namespace "secrets-6413" to be "Succeeded or Failed"
    Jan 18 21:28:28.777: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Pending", Reason="", readiness=false. Elapsed: 5.25886ms
    Jan 18 21:28:30.784: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012077858s
    Jan 18 21:28:32.783: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011269781s
    STEP: Saw pod success 01/18/23 21:28:32.783
    Jan 18 21:28:32.783: INFO: Pod "pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320" satisfied condition "Succeeded or Failed"
    Jan 18 21:28:32.787: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:28:32.797
    Jan 18 21:28:32.822: INFO: Waiting for pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 to disappear
    Jan 18 21:28:32.828: INFO: Pod pod-secrets-4053e8fc-26ef-495c-b1fc-ab4a2a9f2320 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:32.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6413" for this suite. 01/18/23 21:28:32.833
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:32.848
Jan 18 21:28:32.848: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename init-container 01/18/23 21:28:32.849
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:32.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:32.888
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/18/23 21:28:32.893
Jan 18 21:28:32.893: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:38.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4954" for this suite. 01/18/23 21:28:38.441
------------------------------
• [SLOW TEST] [5.608 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:32.848
    Jan 18 21:28:32.848: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename init-container 01/18/23 21:28:32.849
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:32.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:32.888
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/18/23 21:28:32.893
    Jan 18 21:28:32.893: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:38.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4954" for this suite. 01/18/23 21:28:38.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:38.459
Jan 18 21:28:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:28:38.46
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:38.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:38.496
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 21:28:38.498
Jan 18 21:28:38.515: INFO: Waiting up to 5m0s for pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc" in namespace "emptydir-1731" to be "Succeeded or Failed"
Jan 18 21:28:38.520: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359648ms
Jan 18 21:28:40.528: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012079271s
Jan 18 21:28:42.527: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992381s
STEP: Saw pod success 01/18/23 21:28:42.527
Jan 18 21:28:42.528: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc" satisfied condition "Succeeded or Failed"
Jan 18 21:28:42.532: INFO: Trying to get logs from node test-vm-1 pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc container test-container: <nil>
STEP: delete the pod 01/18/23 21:28:42.542
Jan 18 21:28:42.573: INFO: Waiting for pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc to disappear
Jan 18 21:28:42.583: INFO: Pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:42.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1731" for this suite. 01/18/23 21:28:42.591
------------------------------
• [4.151 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:38.459
    Jan 18 21:28:38.459: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:28:38.46
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:38.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:38.496
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 21:28:38.498
    Jan 18 21:28:38.515: INFO: Waiting up to 5m0s for pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc" in namespace "emptydir-1731" to be "Succeeded or Failed"
    Jan 18 21:28:38.520: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359648ms
    Jan 18 21:28:40.528: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012079271s
    Jan 18 21:28:42.527: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011992381s
    STEP: Saw pod success 01/18/23 21:28:42.527
    Jan 18 21:28:42.528: INFO: Pod "pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc" satisfied condition "Succeeded or Failed"
    Jan 18 21:28:42.532: INFO: Trying to get logs from node test-vm-1 pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc container test-container: <nil>
    STEP: delete the pod 01/18/23 21:28:42.542
    Jan 18 21:28:42.573: INFO: Waiting for pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc to disappear
    Jan 18 21:28:42.583: INFO: Pod pod-2e96070f-62e1-4af8-8d1a-ff9c26ab92dc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:42.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1731" for this suite. 01/18/23 21:28:42.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:42.612
Jan 18 21:28:42.612: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:28:42.613
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:42.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:42.662
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:28:42.666
Jan 18 21:28:42.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e" in namespace "downward-api-5634" to be "Succeeded or Failed"
Jan 18 21:28:42.696: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907955ms
Jan 18 21:28:44.703: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012453749s
Jan 18 21:28:46.702: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010631738s
STEP: Saw pod success 01/18/23 21:28:46.702
Jan 18 21:28:46.702: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e" satisfied condition "Succeeded or Failed"
Jan 18 21:28:46.706: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e container client-container: <nil>
STEP: delete the pod 01/18/23 21:28:46.72
Jan 18 21:28:46.756: INFO: Waiting for pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e to disappear
Jan 18 21:28:46.766: INFO: Pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:46.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5634" for this suite. 01/18/23 21:28:46.772
------------------------------
• [4.178 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:42.612
    Jan 18 21:28:42.612: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:28:42.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:42.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:42.662
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:28:42.666
    Jan 18 21:28:42.691: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e" in namespace "downward-api-5634" to be "Succeeded or Failed"
    Jan 18 21:28:42.696: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.907955ms
    Jan 18 21:28:44.703: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012453749s
    Jan 18 21:28:46.702: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010631738s
    STEP: Saw pod success 01/18/23 21:28:46.702
    Jan 18 21:28:46.702: INFO: Pod "downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e" satisfied condition "Succeeded or Failed"
    Jan 18 21:28:46.706: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e container client-container: <nil>
    STEP: delete the pod 01/18/23 21:28:46.72
    Jan 18 21:28:46.756: INFO: Waiting for pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e to disappear
    Jan 18 21:28:46.766: INFO: Pod downwardapi-volume-a2856f8f-ecf8-4e2f-b451-5945707e914e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:46.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5634" for this suite. 01/18/23 21:28:46.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:46.792
Jan 18 21:28:46.792: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context-test 01/18/23 21:28:46.793
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:46.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:46.849
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan 18 21:28:46.874: INFO: Waiting up to 5m0s for pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85" in namespace "security-context-test-6013" to be "Succeeded or Failed"
Jan 18 21:28:46.879: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.335148ms
Jan 18 21:28:48.885: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534327s
Jan 18 21:28:50.885: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010265504s
Jan 18 21:28:52.887: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012561708s
Jan 18 21:28:52.887: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:52.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6013" for this suite. 01/18/23 21:28:52.892
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:46.792
    Jan 18 21:28:46.792: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context-test 01/18/23 21:28:46.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:46.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:46.849
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan 18 21:28:46.874: INFO: Waiting up to 5m0s for pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85" in namespace "security-context-test-6013" to be "Succeeded or Failed"
    Jan 18 21:28:46.879: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.335148ms
    Jan 18 21:28:48.885: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534327s
    Jan 18 21:28:50.885: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010265504s
    Jan 18 21:28:52.887: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012561708s
    Jan 18 21:28:52.887: INFO: Pod "busybox-user-65534-07049fe5-5b4a-419e-8596-da89b02a3a85" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:52.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6013" for this suite. 01/18/23 21:28:52.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:52.912
Jan 18 21:28:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 21:28:52.914
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:53.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:53.013
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/18/23 21:28:53.027
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 21:28:53.045
STEP: delete the deployment 01/18/23 21:28:53.588
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 21:28:53.614
STEP: Gathering metrics 01/18/23 21:28:54.17
W0118 21:28:54.182500      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 21:28:54.182: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 21:28:54.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5067" for this suite. 01/18/23 21:28:54.188
------------------------------
• [1.296 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:52.912
    Jan 18 21:28:52.913: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 21:28:52.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:53.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:53.013
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/18/23 21:28:53.027
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 21:28:53.045
    STEP: delete the deployment 01/18/23 21:28:53.588
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 21:28:53.614
    STEP: Gathering metrics 01/18/23 21:28:54.17
    W0118 21:28:54.182500      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 21:28:54.182: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:28:54.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5067" for this suite. 01/18/23 21:28:54.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:28:54.224
Jan 18 21:28:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:28:54.225
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:54.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:54.271
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:28:54.277
Jan 18 21:28:54.303: INFO: Waiting up to 5m0s for pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f" in namespace "emptydir-6119" to be "Succeeded or Failed"
Jan 18 21:28:54.307: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.54895ms
Jan 18 21:28:56.312: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552095s
Jan 18 21:28:58.315: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012459282s
Jan 18 21:29:00.314: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011472024s
STEP: Saw pod success 01/18/23 21:29:00.314
Jan 18 21:29:00.314: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f" satisfied condition "Succeeded or Failed"
Jan 18 21:29:00.319: INFO: Trying to get logs from node test-vm-1 pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f container test-container: <nil>
STEP: delete the pod 01/18/23 21:29:00.328
Jan 18 21:29:00.356: INFO: Waiting for pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f to disappear
Jan 18 21:29:00.363: INFO: Pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:29:00.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6119" for this suite. 01/18/23 21:29:00.368
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:28:54.224
    Jan 18 21:28:54.224: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:28:54.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:28:54.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:28:54.271
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 21:28:54.277
    Jan 18 21:28:54.303: INFO: Waiting up to 5m0s for pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f" in namespace "emptydir-6119" to be "Succeeded or Failed"
    Jan 18 21:28:54.307: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.54895ms
    Jan 18 21:28:56.312: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552095s
    Jan 18 21:28:58.315: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012459282s
    Jan 18 21:29:00.314: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011472024s
    STEP: Saw pod success 01/18/23 21:29:00.314
    Jan 18 21:29:00.314: INFO: Pod "pod-0235bc5b-6004-4aba-9162-f0a663a6575f" satisfied condition "Succeeded or Failed"
    Jan 18 21:29:00.319: INFO: Trying to get logs from node test-vm-1 pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f container test-container: <nil>
    STEP: delete the pod 01/18/23 21:29:00.328
    Jan 18 21:29:00.356: INFO: Waiting for pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f to disappear
    Jan 18 21:29:00.363: INFO: Pod pod-0235bc5b-6004-4aba-9162-f0a663a6575f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:29:00.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6119" for this suite. 01/18/23 21:29:00.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:29:00.383
Jan 18 21:29:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename cronjob 01/18/23 21:29:00.384
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:00.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:00.422
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/18/23 21:29:00.425
STEP: Ensuring a job is scheduled 01/18/23 21:29:00.436
STEP: Ensuring exactly one is scheduled 01/18/23 21:30:00.442
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 21:30:00.448
STEP: Ensuring the job is replaced with a new one 01/18/23 21:30:00.452
STEP: Removing cronjob 01/18/23 21:31:00.458
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 21:31:00.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-6558" for this suite. 01/18/23 21:31:00.484
------------------------------
• [SLOW TEST] [120.129 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:29:00.383
    Jan 18 21:29:00.383: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:29:00.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:29:00.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:29:00.422
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/18/23 21:29:00.425
    STEP: Ensuring a job is scheduled 01/18/23 21:29:00.436
    STEP: Ensuring exactly one is scheduled 01/18/23 21:30:00.442
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 21:30:00.448
    STEP: Ensuring the job is replaced with a new one 01/18/23 21:30:00.452
    STEP: Removing cronjob 01/18/23 21:31:00.458
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:31:00.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-6558" for this suite. 01/18/23 21:31:00.484
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:31:00.514
Jan 18 21:31:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:31:00.516
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:00.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:00.568
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4645 01/18/23 21:31:00.573
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/18/23 21:31:00.583
Jan 18 21:31:00.610: INFO: Found 0 stateful pods, waiting for 3
Jan 18 21:31:10.616: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:31:10.616: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:31:10.616: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:31:10.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:31:10.828: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:31:10.828: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:31:10.828: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 21:31:20.853
Jan 18 21:31:20.881: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 21:31:20.881
STEP: Updating Pods in reverse ordinal order 01/18/23 21:31:30.903
Jan 18 21:31:30.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:31:31.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:31:31.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:31:31.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/18/23 21:31:41.171
Jan 18 21:31:41.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:31:41.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:31:41.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:31:41.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 21:31:51.415: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/18/23 21:32:01.45
Jan 18 21:32:01.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:32:01.620: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:32:01.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:32:01.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:32:11.652: INFO: Deleting all statefulset in ns statefulset-4645
Jan 18 21:32:11.656: INFO: Scaling statefulset ss2 to 0
Jan 18 21:32:21.692: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:32:21.697: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:21.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4645" for this suite. 01/18/23 21:32:21.747
------------------------------
• [SLOW TEST] [81.253 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:31:00.514
    Jan 18 21:31:00.514: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:31:00.516
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:31:00.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:31:00.568
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4645 01/18/23 21:31:00.573
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/18/23 21:31:00.583
    Jan 18 21:31:00.610: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 21:31:10.616: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:31:10.616: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:31:10.616: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:31:10.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:31:10.828: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:31:10.828: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:31:10.828: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/18/23 21:31:20.853
    Jan 18 21:31:20.881: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 21:31:20.881
    STEP: Updating Pods in reverse ordinal order 01/18/23 21:31:30.903
    Jan 18 21:31:30.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:31:31.133: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:31:31.133: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:31:31.133: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/18/23 21:31:41.171
    Jan 18 21:31:41.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:31:41.358: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:31:41.358: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:31:41.358: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 21:31:51.415: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/18/23 21:32:01.45
    Jan 18 21:32:01.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4645 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:32:01.620: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:32:01.621: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:32:01.621: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:32:11.652: INFO: Deleting all statefulset in ns statefulset-4645
    Jan 18 21:32:11.656: INFO: Scaling statefulset ss2 to 0
    Jan 18 21:32:21.692: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:32:21.697: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:21.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4645" for this suite. 01/18/23 21:32:21.747
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:21.767
Jan 18 21:32:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:32:21.768
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:21.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:21.81
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:32:21.817
Jan 18 21:32:21.839: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1198" to be "running and ready"
Jan 18 21:32:21.843: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773467ms
Jan 18 21:32:21.843: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:32:23.849: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010387858s
Jan 18 21:32:23.849: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:32:25.849: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.01064603s
Jan 18 21:32:25.849: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 21:32:25.849: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/18/23 21:32:25.854
Jan 18 21:32:25.869: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1198" to be "running and ready"
Jan 18 21:32:25.874: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.862986ms
Jan 18 21:32:25.875: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:32:27.881: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012249819s
Jan 18 21:32:27.881: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 18 21:32:27.881: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 21:32:27.885
Jan 18 21:32:28.022: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 21:32:28.028: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 21:32:30.029: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 21:32:30.035: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 21:32:32.028: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 21:32:32.035: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/18/23 21:32:32.035
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1198" for this suite. 01/18/23 21:32:32.068
------------------------------
• [SLOW TEST] [10.323 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:21.767
    Jan 18 21:32:21.767: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:32:21.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:21.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:21.81
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:32:21.817
    Jan 18 21:32:21.839: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1198" to be "running and ready"
    Jan 18 21:32:21.843: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.773467ms
    Jan 18 21:32:21.843: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:32:23.849: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010387858s
    Jan 18 21:32:23.849: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:32:25.849: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.01064603s
    Jan 18 21:32:25.849: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 21:32:25.849: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/18/23 21:32:25.854
    Jan 18 21:32:25.869: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-1198" to be "running and ready"
    Jan 18 21:32:25.874: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.862986ms
    Jan 18 21:32:25.875: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:32:27.881: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012249819s
    Jan 18 21:32:27.881: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 18 21:32:27.881: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 21:32:27.885
    Jan 18 21:32:28.022: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 21:32:28.028: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 21:32:30.029: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 21:32:30.035: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 21:32:32.028: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 21:32:32.035: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/18/23 21:32:32.035
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:32.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1198" for this suite. 01/18/23 21:32:32.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:32.094
Jan 18 21:32:32.094: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 21:32:32.096
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:32.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:32.178
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/18/23 21:32:32.195
STEP: waiting for RC to be added 01/18/23 21:32:32.227
STEP: waiting for available Replicas 01/18/23 21:32:32.228
STEP: patching ReplicationController 01/18/23 21:32:34.449
STEP: waiting for RC to be modified 01/18/23 21:32:34.467
STEP: patching ReplicationController status 01/18/23 21:32:34.469
STEP: waiting for RC to be modified 01/18/23 21:32:34.498
STEP: waiting for available Replicas 01/18/23 21:32:34.499
STEP: fetching ReplicationController status 01/18/23 21:32:34.527
STEP: patching ReplicationController scale 01/18/23 21:32:34.535
STEP: waiting for RC to be modified 01/18/23 21:32:34.549
STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 21:32:34.551
STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 21:32:37.287
STEP: updating ReplicationController status 01/18/23 21:32:37.291
STEP: waiting for RC to be modified 01/18/23 21:32:37.305
STEP: listing all ReplicationControllers 01/18/23 21:32:37.307
STEP: checking that ReplicationController has expected values 01/18/23 21:32:37.312
STEP: deleting ReplicationControllers by collection 01/18/23 21:32:37.312
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 21:32:37.402
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5408" for this suite. 01/18/23 21:32:37.665
------------------------------
• [SLOW TEST] [5.593 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:32.094
    Jan 18 21:32:32.094: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 21:32:32.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:32.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:32.178
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/18/23 21:32:32.195
    STEP: waiting for RC to be added 01/18/23 21:32:32.227
    STEP: waiting for available Replicas 01/18/23 21:32:32.228
    STEP: patching ReplicationController 01/18/23 21:32:34.449
    STEP: waiting for RC to be modified 01/18/23 21:32:34.467
    STEP: patching ReplicationController status 01/18/23 21:32:34.469
    STEP: waiting for RC to be modified 01/18/23 21:32:34.498
    STEP: waiting for available Replicas 01/18/23 21:32:34.499
    STEP: fetching ReplicationController status 01/18/23 21:32:34.527
    STEP: patching ReplicationController scale 01/18/23 21:32:34.535
    STEP: waiting for RC to be modified 01/18/23 21:32:34.549
    STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 21:32:34.551
    STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 21:32:37.287
    STEP: updating ReplicationController status 01/18/23 21:32:37.291
    STEP: waiting for RC to be modified 01/18/23 21:32:37.305
    STEP: listing all ReplicationControllers 01/18/23 21:32:37.307
    STEP: checking that ReplicationController has expected values 01/18/23 21:32:37.312
    STEP: deleting ReplicationControllers by collection 01/18/23 21:32:37.312
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 21:32:37.402
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5408" for this suite. 01/18/23 21:32:37.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:37.689
Jan 18 21:32:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:32:37.69
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:37.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:37.835
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 18 21:32:37.841: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 18 21:32:37.899: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 21:32:42.908: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:32:42.908
Jan 18 21:32:42.908: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 21:32:42.932: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 21:32:42.950: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 21:32:44.961: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 21:32:44.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 21:32:46.972: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:32:46.986: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4600  28b3f426-1cf2-451d-a777-f0b27b7d6c6d 17455 1 2023-01-18 21:32:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 21:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:32:43 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-18 21:32:45 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 21:32:46.990: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4600  56e9c872-949a-44ac-aba1-8627d498b807 17444 1 2023-01-18 21:32:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 28b3f426-1cf2-451d-a777-f0b27b7d6c6d 0xc003901c37 0xc003901c38}] [] [{kubelite Update apps/v1 2023-01-18 21:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b3f426-1cf2-451d-a777-f0b27b7d6c6d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003901dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:32:46.990: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 21:32:46.990: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4600  03d0ef10-fc04-4747-8ae4-b07dca51259c 17454 2 2023-01-18 21:32:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 28b3f426-1cf2-451d-a777-f0b27b7d6c6d 0xc003901e27 0xc003901e28}] [] [{e2e.test Update apps/v1 2023-01-18 21:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b3f426-1cf2-451d-a777-f0b27b7d6c6d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003901ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 21:32:46.995: INFO: Pod "test-rolling-update-deployment-7549d9f46d-pml68" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-pml68 test-rolling-update-deployment-7549d9f46d- deployment-4600  3572928e-913c-46ca-b15e-5fc997d246e3 17442 0 2023-01-18 21:32:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2fdf38343fe3ce25c5c5124168292af513a369a46afd8778c027e25f9ac16d5d cni.projectcalico.org/podIP:10.1.192.46/32 cni.projectcalico.org/podIPs:10.1.192.46/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 56e9c872-949a-44ac-aba1-8627d498b807 0xc002b241f7 0xc002b241f8}] [] [{kubelite Update v1 2023-01-18 21:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56e9c872-949a-44ac-aba1-8627d498b807\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.46,StartTime:2023-01-18 21:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:32:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://971af7657f85e5544d16c1b209c9ded4c419027ce59fa485f3a187b4212ca825,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:46.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4600" for this suite. 01/18/23 21:32:47.003
------------------------------
• [SLOW TEST] [9.327 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:37.689
    Jan 18 21:32:37.689: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:32:37.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:37.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:37.835
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 18 21:32:37.841: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 18 21:32:37.899: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 21:32:42.908: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:32:42.908
    Jan 18 21:32:42.908: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 18 21:32:42.932: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 18 21:32:42.950: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 18 21:32:44.961: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 18 21:32:44.965: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 32, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 21:32:46.972: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:32:46.986: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4600  28b3f426-1cf2-451d-a777-f0b27b7d6c6d 17455 1 2023-01-18 21:32:42 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 21:32:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003b4fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 21:32:43 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-18 21:32:45 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 21:32:46.990: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4600  56e9c872-949a-44ac-aba1-8627d498b807 17444 1 2023-01-18 21:32:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 28b3f426-1cf2-451d-a777-f0b27b7d6c6d 0xc003901c37 0xc003901c38}] [] [{kubelite Update apps/v1 2023-01-18 21:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b3f426-1cf2-451d-a777-f0b27b7d6c6d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003901dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:32:46.990: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 18 21:32:46.990: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4600  03d0ef10-fc04-4747-8ae4-b07dca51259c 17454 2 2023-01-18 21:32:37 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 28b3f426-1cf2-451d-a777-f0b27b7d6c6d 0xc003901e27 0xc003901e28}] [] [{e2e.test Update apps/v1 2023-01-18 21:32:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"28b3f426-1cf2-451d-a777-f0b27b7d6c6d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kubelite Update apps/v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003901ef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 21:32:46.995: INFO: Pod "test-rolling-update-deployment-7549d9f46d-pml68" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-pml68 test-rolling-update-deployment-7549d9f46d- deployment-4600  3572928e-913c-46ca-b15e-5fc997d246e3 17442 0 2023-01-18 21:32:43 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2fdf38343fe3ce25c5c5124168292af513a369a46afd8778c027e25f9ac16d5d cni.projectcalico.org/podIP:10.1.192.46/32 cni.projectcalico.org/podIPs:10.1.192.46/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 56e9c872-949a-44ac-aba1-8627d498b807 0xc002b241f7 0xc002b241f8}] [] [{kubelite Update v1 2023-01-18 21:32:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56e9c872-949a-44ac-aba1-8627d498b807\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:32:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:32:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk9m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk9m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:32:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.46,StartTime:2023-01-18 21:32:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:32:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://971af7657f85e5544d16c1b209c9ded4c419027ce59fa485f3a187b4212ca825,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:46.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4600" for this suite. 01/18/23 21:32:47.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:47.018
Jan 18 21:32:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:32:47.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:47.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:47.073
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:32:47.075
Jan 18 21:32:47.101: INFO: Waiting up to 5m0s for pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9" in namespace "emptydir-2372" to be "Succeeded or Failed"
Jan 18 21:32:47.105: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902255ms
Jan 18 21:32:49.111: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00960607s
Jan 18 21:32:51.112: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010416045s
STEP: Saw pod success 01/18/23 21:32:51.112
Jan 18 21:32:51.112: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9" satisfied condition "Succeeded or Failed"
Jan 18 21:32:51.116: INFO: Trying to get logs from node test-vm-1 pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 container test-container: <nil>
STEP: delete the pod 01/18/23 21:32:51.142
Jan 18 21:32:51.194: INFO: Waiting for pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 to disappear
Jan 18 21:32:51.201: INFO: Pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:51.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2372" for this suite. 01/18/23 21:32:51.206
------------------------------
• [4.248 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:47.018
    Jan 18 21:32:47.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:32:47.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:47.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:47.073
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 21:32:47.075
    Jan 18 21:32:47.101: INFO: Waiting up to 5m0s for pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9" in namespace "emptydir-2372" to be "Succeeded or Failed"
    Jan 18 21:32:47.105: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.902255ms
    Jan 18 21:32:49.111: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00960607s
    Jan 18 21:32:51.112: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010416045s
    STEP: Saw pod success 01/18/23 21:32:51.112
    Jan 18 21:32:51.112: INFO: Pod "pod-8db1f956-5961-4801-97c3-e773fdec67f9" satisfied condition "Succeeded or Failed"
    Jan 18 21:32:51.116: INFO: Trying to get logs from node test-vm-1 pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:32:51.142
    Jan 18 21:32:51.194: INFO: Waiting for pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 to disappear
    Jan 18 21:32:51.201: INFO: Pod pod-8db1f956-5961-4801-97c3-e773fdec67f9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:51.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2372" for this suite. 01/18/23 21:32:51.206
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:51.271
Jan 18 21:32:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:32:51.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:51.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:51.419
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/18/23 21:32:51.444
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 21:32:51.445
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 21:32:51.445
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 21:32:51.445
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 21:32:51.446
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:32:51.446
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:32:51.447
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:51.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5802" for this suite. 01/18/23 21:32:51.456
------------------------------
• [0.224 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:51.271
    Jan 18 21:32:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:32:51.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:51.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:51.419
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/18/23 21:32:51.444
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 21:32:51.445
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 21:32:51.445
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 21:32:51.445
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 21:32:51.446
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:32:51.446
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 21:32:51.447
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:51.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5802" for this suite. 01/18/23 21:32:51.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:51.498
Jan 18 21:32:51.498: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:32:51.499
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:51.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:51.558
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-1259f5e3-68bd-4455-8ef0-5eb06a067a97 01/18/23 21:32:51.577
STEP: Creating a pod to test consume secrets 01/18/23 21:32:51.611
Jan 18 21:32:51.676: INFO: Waiting up to 5m0s for pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5" in namespace "secrets-407" to be "Succeeded or Failed"
Jan 18 21:32:51.708: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Pending", Reason="", readiness=false. Elapsed: 31.906448ms
Jan 18 21:32:53.716: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.040203528s
Jan 18 21:32:55.714: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.038274665s
Jan 18 21:32:57.715: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038664682s
STEP: Saw pod success 01/18/23 21:32:57.715
Jan 18 21:32:57.715: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5" satisfied condition "Succeeded or Failed"
Jan 18 21:32:57.719: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:32:57.728
Jan 18 21:32:57.797: INFO: Waiting for pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 to disappear
Jan 18 21:32:57.807: INFO: Pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:32:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-407" for this suite. 01/18/23 21:32:57.812
------------------------------
• [SLOW TEST] [6.367 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:51.498
    Jan 18 21:32:51.498: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:32:51.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:51.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:51.558
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-1259f5e3-68bd-4455-8ef0-5eb06a067a97 01/18/23 21:32:51.577
    STEP: Creating a pod to test consume secrets 01/18/23 21:32:51.611
    Jan 18 21:32:51.676: INFO: Waiting up to 5m0s for pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5" in namespace "secrets-407" to be "Succeeded or Failed"
    Jan 18 21:32:51.708: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Pending", Reason="", readiness=false. Elapsed: 31.906448ms
    Jan 18 21:32:53.716: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.040203528s
    Jan 18 21:32:55.714: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Running", Reason="", readiness=false. Elapsed: 4.038274665s
    Jan 18 21:32:57.715: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038664682s
    STEP: Saw pod success 01/18/23 21:32:57.715
    Jan 18 21:32:57.715: INFO: Pod "pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5" satisfied condition "Succeeded or Failed"
    Jan 18 21:32:57.719: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:32:57.728
    Jan 18 21:32:57.797: INFO: Waiting for pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 to disappear
    Jan 18 21:32:57.807: INFO: Pod pod-secrets-58f4c08b-443c-45eb-a3a3-153f79b810c5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:32:57.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-407" for this suite. 01/18/23 21:32:57.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:32:57.87
Jan 18 21:32:57.870: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:32:57.871
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:57.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:57.929
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:32:57.947
Jan 18 21:32:57.973: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3880" to be "running and ready"
Jan 18 21:32:57.977: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609965ms
Jan 18 21:32:57.977: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:32:59.984: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01133841s
Jan 18 21:32:59.984: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:33:01.984: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011426962s
Jan 18 21:33:01.984: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 21:33:01.984: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/18/23 21:33:01.988
Jan 18 21:33:02.007: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3880" to be "running and ready"
Jan 18 21:33:02.013: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714095ms
Jan 18 21:33:02.013: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:33:04.019: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012612829s
Jan 18 21:33:04.019: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 18 21:33:04.019: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 21:33:04.023
STEP: delete the pod with lifecycle hook 01/18/23 21:33:04.032
Jan 18 21:33:04.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 21:33:04.051: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 21:33:06.051: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 21:33:06.056: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 21:33:08.051: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 21:33:08.057: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan 18 21:33:08.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3880" for this suite. 01/18/23 21:33:08.063
------------------------------
• [SLOW TEST] [10.205 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:32:57.87
    Jan 18 21:32:57.870: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 21:32:57.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:32:57.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:32:57.929
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 21:32:57.947
    Jan 18 21:32:57.973: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3880" to be "running and ready"
    Jan 18 21:32:57.977: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609965ms
    Jan 18 21:32:57.977: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:32:59.984: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01133841s
    Jan 18 21:32:59.984: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:33:01.984: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011426962s
    Jan 18 21:33:01.984: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 21:33:01.984: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/18/23 21:33:01.988
    Jan 18 21:33:02.007: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-3880" to be "running and ready"
    Jan 18 21:33:02.013: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.714095ms
    Jan 18 21:33:02.013: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:33:04.019: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012612829s
    Jan 18 21:33:04.019: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 18 21:33:04.019: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 21:33:04.023
    STEP: delete the pod with lifecycle hook 01/18/23 21:33:04.032
    Jan 18 21:33:04.046: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 21:33:04.051: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 21:33:06.051: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 21:33:06.056: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 21:33:08.051: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 21:33:08.057: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:33:08.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3880" for this suite. 01/18/23 21:33:08.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:33:08.08
Jan 18 21:33:08.080: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:33:08.081
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:08.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:08.119
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/18/23 21:33:08.122
STEP: submitting the pod to kubernetes 01/18/23 21:33:08.122
Jan 18 21:33:08.138: INFO: Waiting up to 5m0s for pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" in namespace "pods-1116" to be "running and ready"
Jan 18 21:33:08.142: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756353ms
Jan 18 21:33:08.142: INFO: The phase of Pod pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:33:10.149: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010784621s
Jan 18 21:33:10.149: INFO: The phase of Pod pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e is Running (Ready = true)
Jan 18 21:33:10.149: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 21:33:10.154
STEP: updating the pod 01/18/23 21:33:10.159
Jan 18 21:33:10.683: INFO: Successfully updated pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e"
Jan 18 21:33:10.683: INFO: Waiting up to 5m0s for pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" in namespace "pods-1116" to be "running"
Jan 18 21:33:10.688: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.548464ms
Jan 18 21:33:10.688: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/18/23 21:33:10.688
Jan 18 21:33:10.691: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:33:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1116" for this suite. 01/18/23 21:33:10.696
------------------------------
• [2.630 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:33:08.08
    Jan 18 21:33:08.080: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:33:08.081
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:08.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:08.119
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/18/23 21:33:08.122
    STEP: submitting the pod to kubernetes 01/18/23 21:33:08.122
    Jan 18 21:33:08.138: INFO: Waiting up to 5m0s for pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" in namespace "pods-1116" to be "running and ready"
    Jan 18 21:33:08.142: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756353ms
    Jan 18 21:33:08.142: INFO: The phase of Pod pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:33:10.149: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010784621s
    Jan 18 21:33:10.149: INFO: The phase of Pod pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e is Running (Ready = true)
    Jan 18 21:33:10.149: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 21:33:10.154
    STEP: updating the pod 01/18/23 21:33:10.159
    Jan 18 21:33:10.683: INFO: Successfully updated pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e"
    Jan 18 21:33:10.683: INFO: Waiting up to 5m0s for pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" in namespace "pods-1116" to be "running"
    Jan 18 21:33:10.688: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e": Phase="Running", Reason="", readiness=true. Elapsed: 4.548464ms
    Jan 18 21:33:10.688: INFO: Pod "pod-update-ec7fae49-0a22-496b-9a9f-2f8825293e7e" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/18/23 21:33:10.688
    Jan 18 21:33:10.691: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:33:10.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1116" for this suite. 01/18/23 21:33:10.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:33:10.713
Jan 18 21:33:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename containers 01/18/23 21:33:10.714
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:10.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:10.754
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan 18 21:33:10.777: INFO: Waiting up to 5m0s for pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb" in namespace "containers-5720" to be "running"
Jan 18 21:33:10.782: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31426ms
Jan 18 21:33:12.788: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010390091s
Jan 18 21:33:12.788: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 21:33:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5720" for this suite. 01/18/23 21:33:12.8
------------------------------
• [2.104 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:33:10.713
    Jan 18 21:33:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename containers 01/18/23 21:33:10.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:10.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:10.754
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan 18 21:33:10.777: INFO: Waiting up to 5m0s for pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb" in namespace "containers-5720" to be "running"
    Jan 18 21:33:10.782: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31426ms
    Jan 18 21:33:12.788: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010390091s
    Jan 18 21:33:12.788: INFO: Pod "client-containers-33e3bf06-ab8d-467d-8f56-c482bbe907fb" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:33:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5720" for this suite. 01/18/23 21:33:12.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:33:12.82
Jan 18 21:33:12.820: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:33:12.821
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:12.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:12.858
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 21:33:12.902: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:34:12.927: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:34:12.933
Jan 18 21:34:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 21:34:12.935
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:12.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:12.98
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan 18 21:34:13.020: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 18 21:34:13.026: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan 18 21:34:13.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:34:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-9166" for this suite. 01/18/23 21:34:13.164
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8952" for this suite. 01/18/23 21:34:13.178
------------------------------
• [SLOW TEST] [60.373 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:33:12.82
    Jan 18 21:33:12.820: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:33:12.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:33:12.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:33:12.858
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 21:33:12.902: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:34:12.927: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:34:12.933
    Jan 18 21:34:12.933: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 21:34:12.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:12.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:12.98
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan 18 21:34:13.020: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 18 21:34:13.026: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:34:13.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:34:13.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-9166" for this suite. 01/18/23 21:34:13.164
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8952" for this suite. 01/18/23 21:34:13.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:34:13.194
Jan 18 21:34:13.194: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:34:13.195
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:13.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:13.248
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 21:34:13.25
Jan 18 21:34:13.272: INFO: Waiting up to 5m0s for pod "pod-89820291-0d74-46bd-bb58-316202d31a5c" in namespace "emptydir-6666" to be "Succeeded or Failed"
Jan 18 21:34:13.277: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080056ms
Jan 18 21:34:15.283: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010730503s
Jan 18 21:34:17.283: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.010146634s
Jan 18 21:34:19.285: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012005138s
STEP: Saw pod success 01/18/23 21:34:19.285
Jan 18 21:34:19.285: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c" satisfied condition "Succeeded or Failed"
Jan 18 21:34:19.288: INFO: Trying to get logs from node test-vm-1 pod pod-89820291-0d74-46bd-bb58-316202d31a5c container test-container: <nil>
STEP: delete the pod 01/18/23 21:34:19.296
Jan 18 21:34:19.320: INFO: Waiting for pod pod-89820291-0d74-46bd-bb58-316202d31a5c to disappear
Jan 18 21:34:19.326: INFO: Pod pod-89820291-0d74-46bd-bb58-316202d31a5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:34:19.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6666" for this suite. 01/18/23 21:34:19.331
------------------------------
• [SLOW TEST] [6.149 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:34:13.194
    Jan 18 21:34:13.194: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:34:13.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:13.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:13.248
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 21:34:13.25
    Jan 18 21:34:13.272: INFO: Waiting up to 5m0s for pod "pod-89820291-0d74-46bd-bb58-316202d31a5c" in namespace "emptydir-6666" to be "Succeeded or Failed"
    Jan 18 21:34:13.277: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080056ms
    Jan 18 21:34:15.283: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010730503s
    Jan 18 21:34:17.283: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Running", Reason="", readiness=false. Elapsed: 4.010146634s
    Jan 18 21:34:19.285: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012005138s
    STEP: Saw pod success 01/18/23 21:34:19.285
    Jan 18 21:34:19.285: INFO: Pod "pod-89820291-0d74-46bd-bb58-316202d31a5c" satisfied condition "Succeeded or Failed"
    Jan 18 21:34:19.288: INFO: Trying to get logs from node test-vm-1 pod pod-89820291-0d74-46bd-bb58-316202d31a5c container test-container: <nil>
    STEP: delete the pod 01/18/23 21:34:19.296
    Jan 18 21:34:19.320: INFO: Waiting for pod pod-89820291-0d74-46bd-bb58-316202d31a5c to disappear
    Jan 18 21:34:19.326: INFO: Pod pod-89820291-0d74-46bd-bb58-316202d31a5c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:34:19.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6666" for this suite. 01/18/23 21:34:19.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:34:19.344
Jan 18 21:34:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:34:19.345
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:19.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:19.387
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-861e0306-a225-4f15-b8d2-864befb6baf3 01/18/23 21:34:19.389
STEP: Creating a pod to test consume configMaps 01/18/23 21:34:19.399
Jan 18 21:34:19.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c" in namespace "configmap-3464" to be "Succeeded or Failed"
Jan 18 21:34:19.420: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818553ms
Jan 18 21:34:21.425: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00931332s
Jan 18 21:34:23.426: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00997292s
STEP: Saw pod success 01/18/23 21:34:23.426
Jan 18 21:34:23.426: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c" satisfied condition "Succeeded or Failed"
Jan 18 21:34:23.430: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:34:23.438
Jan 18 21:34:23.463: INFO: Waiting for pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c to disappear
Jan 18 21:34:23.469: INFO: Pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:34:23.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3464" for this suite. 01/18/23 21:34:23.473
------------------------------
• [4.143 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:34:19.344
    Jan 18 21:34:19.344: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:34:19.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:19.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:19.387
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-861e0306-a225-4f15-b8d2-864befb6baf3 01/18/23 21:34:19.389
    STEP: Creating a pod to test consume configMaps 01/18/23 21:34:19.399
    Jan 18 21:34:19.416: INFO: Waiting up to 5m0s for pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c" in namespace "configmap-3464" to be "Succeeded or Failed"
    Jan 18 21:34:19.420: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818553ms
    Jan 18 21:34:21.425: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00931332s
    Jan 18 21:34:23.426: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00997292s
    STEP: Saw pod success 01/18/23 21:34:23.426
    Jan 18 21:34:23.426: INFO: Pod "pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c" satisfied condition "Succeeded or Failed"
    Jan 18 21:34:23.430: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:34:23.438
    Jan 18 21:34:23.463: INFO: Waiting for pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c to disappear
    Jan 18 21:34:23.469: INFO: Pod pod-configmaps-9500436b-ca9c-4871-96b3-468eb024724c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:34:23.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3464" for this suite. 01/18/23 21:34:23.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:34:23.488
Jan 18 21:34:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:34:23.489
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:23.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:23.533
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-5564 01/18/23 21:34:23.535
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[] 01/18/23 21:34:23.556
Jan 18 21:34:23.565: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 21:34:24.575: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5564 01/18/23 21:34:24.575
Jan 18 21:34:24.601: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5564" to be "running and ready"
Jan 18 21:34:24.606: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122657ms
Jan 18 21:34:24.606: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:34:26.611: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009968382s
Jan 18 21:34:26.612: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 21:34:26.612: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod1:[100]] 01/18/23 21:34:26.615
Jan 18 21:34:27.633: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5564 01/18/23 21:34:27.633
Jan 18 21:34:27.644: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5564" to be "running and ready"
Jan 18 21:34:27.648: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402547ms
Jan 18 21:34:27.648: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:34:29.653: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008837143s
Jan 18 21:34:29.653: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 21:34:29.653: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 21:34:29.658
Jan 18 21:34:29.676: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/18/23 21:34:29.676
Jan 18 21:34:29.676: INFO: Creating new exec pod
Jan 18 21:34:29.687: INFO: Waiting up to 5m0s for pod "execpodldzjx" in namespace "services-5564" to be "running"
Jan 18 21:34:29.691: INFO: Pod "execpodldzjx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583648ms
Jan 18 21:34:31.697: INFO: Pod "execpodldzjx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009721254s
Jan 18 21:34:31.697: INFO: Pod "execpodldzjx" satisfied condition "running"
Jan 18 21:34:32.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan 18 21:34:32.866: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 21:34:32.866: INFO: stdout: ""
Jan 18 21:34:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 10.152.183.165 80'
Jan 18 21:34:33.038: INFO: stderr: "+ nc -v -z -w 2 10.152.183.165 80\nConnection to 10.152.183.165 80 port [tcp/http] succeeded!\n"
Jan 18 21:34:33.038: INFO: stdout: ""
Jan 18 21:34:33.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan 18 21:34:33.203: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 21:34:33.203: INFO: stdout: ""
Jan 18 21:34:33.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 10.152.183.165 81'
Jan 18 21:34:33.427: INFO: stderr: "+ nc -v -z -w 2 10.152.183.165 81\nConnection to 10.152.183.165 81 port [tcp/*] succeeded!\n"
Jan 18 21:34:33.427: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-5564 01/18/23 21:34:33.427
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod2:[101]] 01/18/23 21:34:33.464
Jan 18 21:34:35.487: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5564 01/18/23 21:34:35.487
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[] 01/18/23 21:34:35.579
Jan 18 21:34:36.603: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:34:36.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5564" for this suite. 01/18/23 21:34:36.649
------------------------------
• [SLOW TEST] [13.174 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:34:23.488
    Jan 18 21:34:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:34:23.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:23.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:23.533
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-5564 01/18/23 21:34:23.535
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[] 01/18/23 21:34:23.556
    Jan 18 21:34:23.565: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 21:34:24.575: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5564 01/18/23 21:34:24.575
    Jan 18 21:34:24.601: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5564" to be "running and ready"
    Jan 18 21:34:24.606: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122657ms
    Jan 18 21:34:24.606: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:34:26.611: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.009968382s
    Jan 18 21:34:26.612: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 21:34:26.612: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod1:[100]] 01/18/23 21:34:26.615
    Jan 18 21:34:27.633: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5564 01/18/23 21:34:27.633
    Jan 18 21:34:27.644: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5564" to be "running and ready"
    Jan 18 21:34:27.648: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402547ms
    Jan 18 21:34:27.648: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:34:29.653: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008837143s
    Jan 18 21:34:29.653: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 21:34:29.653: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 21:34:29.658
    Jan 18 21:34:29.676: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/18/23 21:34:29.676
    Jan 18 21:34:29.676: INFO: Creating new exec pod
    Jan 18 21:34:29.687: INFO: Waiting up to 5m0s for pod "execpodldzjx" in namespace "services-5564" to be "running"
    Jan 18 21:34:29.691: INFO: Pod "execpodldzjx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583648ms
    Jan 18 21:34:31.697: INFO: Pod "execpodldzjx": Phase="Running", Reason="", readiness=true. Elapsed: 2.009721254s
    Jan 18 21:34:31.697: INFO: Pod "execpodldzjx" satisfied condition "running"
    Jan 18 21:34:32.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan 18 21:34:32.866: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 18 21:34:32.866: INFO: stdout: ""
    Jan 18 21:34:32.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 10.152.183.165 80'
    Jan 18 21:34:33.038: INFO: stderr: "+ nc -v -z -w 2 10.152.183.165 80\nConnection to 10.152.183.165 80 port [tcp/http] succeeded!\n"
    Jan 18 21:34:33.038: INFO: stdout: ""
    Jan 18 21:34:33.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan 18 21:34:33.203: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 18 21:34:33.203: INFO: stdout: ""
    Jan 18 21:34:33.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-5564 exec execpodldzjx -- /bin/sh -x -c nc -v -z -w 2 10.152.183.165 81'
    Jan 18 21:34:33.427: INFO: stderr: "+ nc -v -z -w 2 10.152.183.165 81\nConnection to 10.152.183.165 81 port [tcp/*] succeeded!\n"
    Jan 18 21:34:33.427: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-5564 01/18/23 21:34:33.427
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[pod2:[101]] 01/18/23 21:34:33.464
    Jan 18 21:34:35.487: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5564 01/18/23 21:34:35.487
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5564 to expose endpoints map[] 01/18/23 21:34:35.579
    Jan 18 21:34:36.603: INFO: successfully validated that service multi-endpoint-test in namespace services-5564 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:34:36.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5564" for this suite. 01/18/23 21:34:36.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:34:36.662
Jan 18 21:34:36.662: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-pred 01/18/23 21:34:36.664
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:36.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:36.698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 21:34:36.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 21:34:36.709: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 21:34:36.712: INFO: 
Logging pods the apiserver thinks is on node test-vm-2 before test
Jan 18 21:34:36.720: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.720: INFO: 	Container coredns ready: true, restart count 0
Jan 18 21:34:36.720: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.720: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:34:36.720: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:34:36.720: INFO: 	Container e2e ready: true, restart count 0
Jan 18 21:34:36.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:34:36.720: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:34:36.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:34:36.720: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:34:36.720: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.720: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 21:34:36.720: INFO: 
Logging pods the apiserver thinks is on node test-vm-1 before test
Jan 18 21:34:36.727: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.727: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:34:36.727: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:34:36.727: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:34:36.727: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:34:36.727: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.727: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 21:34:36.727: INFO: execpodldzjx from services-5564 started at 2023-01-18 21:34:29 +0000 UTC (1 container statuses recorded)
Jan 18 21:34:36.727: INFO: 	Container agnhost-container ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 21:34:36.727
Jan 18 21:34:36.744: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1657" to be "running"
Jan 18 21:34:36.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184851ms
Jan 18 21:34:38.753: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009464175s
Jan 18 21:34:38.754: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 21:34:38.757
STEP: Trying to apply a random label on the found node. 01/18/23 21:34:38.786
STEP: verifying the node has the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 95 01/18/23 21:34:38.799
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 21:34:38.803
Jan 18 21:34:38.813: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1657" to be "not pending"
Jan 18 21:34:38.817: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.739046ms
Jan 18 21:34:40.823: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010237697s
Jan 18 21:34:40.823: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.4 on the node which pod4 resides and expect not scheduled 01/18/23 21:34:40.823
Jan 18 21:34:40.836: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1657" to be "not pending"
Jan 18 21:34:40.839: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449843ms
Jan 18 21:34:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008851236s
Jan 18 21:34:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010210479s
Jan 18 21:34:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009239893s
Jan 18 21:34:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009939229s
Jan 18 21:34:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008843271s
Jan 18 21:34:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008993629s
Jan 18 21:34:54.850: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014274052s
Jan 18 21:34:56.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008245507s
Jan 18 21:34:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010351553s
Jan 18 21:35:00.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010067171s
Jan 18 21:35:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010630299s
Jan 18 21:35:04.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008385267s
Jan 18 21:35:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008472435s
Jan 18 21:35:08.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008678704s
Jan 18 21:35:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009072775s
Jan 18 21:35:12.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008547069s
Jan 18 21:35:14.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009971356s
Jan 18 21:35:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009401018s
Jan 18 21:35:18.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010372299s
Jan 18 21:35:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009064954s
Jan 18 21:35:22.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009912873s
Jan 18 21:35:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010117884s
Jan 18 21:35:26.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009826589s
Jan 18 21:35:28.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009989198s
Jan 18 21:35:30.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009257751s
Jan 18 21:35:32.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008836507s
Jan 18 21:35:34.861: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025361875s
Jan 18 21:35:36.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009549632s
Jan 18 21:35:38.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009175695s
Jan 18 21:35:40.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010182675s
Jan 18 21:35:42.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008304521s
Jan 18 21:35:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010340616s
Jan 18 21:35:46.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009557332s
Jan 18 21:35:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00975316s
Jan 18 21:35:50.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008411169s
Jan 18 21:35:52.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008450995s
Jan 18 21:35:54.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009144592s
Jan 18 21:35:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00907308s
Jan 18 21:35:58.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008977467s
Jan 18 21:36:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00929336s
Jan 18 21:36:02.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009817936s
Jan 18 21:36:04.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009336199s
Jan 18 21:36:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008492358s
Jan 18 21:36:08.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010647754s
Jan 18 21:36:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008639782s
Jan 18 21:36:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009653948s
Jan 18 21:36:14.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008461386s
Jan 18 21:36:16.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008154035s
Jan 18 21:36:18.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010922606s
Jan 18 21:36:20.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01080384s
Jan 18 21:36:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008870152s
Jan 18 21:36:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009968402s
Jan 18 21:36:26.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009280931s
Jan 18 21:36:28.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010310057s
Jan 18 21:36:30.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008769753s
Jan 18 21:36:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008306761s
Jan 18 21:36:34.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010398847s
Jan 18 21:36:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009248776s
Jan 18 21:36:38.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010351237s
Jan 18 21:36:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009153369s
Jan 18 21:36:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009429394s
Jan 18 21:36:44.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00856172s
Jan 18 21:36:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009284876s
Jan 18 21:36:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010013731s
Jan 18 21:36:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008631568s
Jan 18 21:36:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008808397s
Jan 18 21:36:54.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009779774s
Jan 18 21:36:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00890383s
Jan 18 21:36:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01037691s
Jan 18 21:37:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008992934s
Jan 18 21:37:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011461201s
Jan 18 21:37:04.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009527618s
Jan 18 21:37:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009398199s
Jan 18 21:37:08.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010224892s
Jan 18 21:37:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008918762s
Jan 18 21:37:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009617355s
Jan 18 21:37:14.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010441289s
Jan 18 21:37:16.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007908746s
Jan 18 21:37:18.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00970075s
Jan 18 21:37:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009262129s
Jan 18 21:37:22.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009475563s
Jan 18 21:37:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010309124s
Jan 18 21:37:26.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008675557s
Jan 18 21:37:28.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009216321s
Jan 18 21:37:30.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008421996s
Jan 18 21:37:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.0080539s
Jan 18 21:37:34.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009161852s
Jan 18 21:37:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009129968s
Jan 18 21:37:38.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009144s
Jan 18 21:37:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009034989s
Jan 18 21:37:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008710475s
Jan 18 21:37:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010187382s
Jan 18 21:37:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009436394s
Jan 18 21:37:48.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009050552s
Jan 18 21:37:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008605409s
Jan 18 21:37:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008595673s
Jan 18 21:37:54.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008697956s
Jan 18 21:37:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008809481s
Jan 18 21:37:58.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.0084466s
Jan 18 21:38:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008957031s
Jan 18 21:38:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010540359s
Jan 18 21:38:04.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009892729s
Jan 18 21:38:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.00865299s
Jan 18 21:38:08.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009639086s
Jan 18 21:38:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008865472s
Jan 18 21:38:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.01003305s
Jan 18 21:38:14.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008674303s
Jan 18 21:38:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008961772s
Jan 18 21:38:18.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007550067s
Jan 18 21:38:20.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008391616s
Jan 18 21:38:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008740959s
Jan 18 21:38:24.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008485497s
Jan 18 21:38:26.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00819827s
Jan 18 21:38:28.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007776871s
Jan 18 21:38:30.848: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01192992s
Jan 18 21:38:32.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008534289s
Jan 18 21:38:34.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010544751s
Jan 18 21:38:36.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008165069s
Jan 18 21:38:38.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008389019s
Jan 18 21:38:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009395778s
Jan 18 21:38:42.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009731151s
Jan 18 21:38:44.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009035248s
Jan 18 21:38:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008865852s
Jan 18 21:38:48.849: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.0127866s
Jan 18 21:38:50.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008457837s
Jan 18 21:38:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008757988s
Jan 18 21:38:54.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007615923s
Jan 18 21:38:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008512781s
Jan 18 21:38:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009757361s
Jan 18 21:39:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008605296s
Jan 18 21:39:02.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009064849s
Jan 18 21:39:04.849: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013087141s
Jan 18 21:39:06.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008159724s
Jan 18 21:39:08.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008508418s
Jan 18 21:39:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.009147414s
Jan 18 21:39:12.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010456218s
Jan 18 21:39:14.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008046002s
Jan 18 21:39:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008510454s
Jan 18 21:39:18.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007963094s
Jan 18 21:39:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009162454s
Jan 18 21:39:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009248177s
Jan 18 21:39:24.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008535353s
Jan 18 21:39:26.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008416834s
Jan 18 21:39:28.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01071034s
Jan 18 21:39:30.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008358966s
Jan 18 21:39:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008063649s
Jan 18 21:39:34.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010361159s
Jan 18 21:39:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00893463s
Jan 18 21:39:38.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008028386s
Jan 18 21:39:40.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00805654s
Jan 18 21:39:40.848: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011521257s
STEP: removing the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 off the node test-vm-1 01/18/23 21:39:40.848
STEP: verifying the node doesn't have the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 01/18/23 21:39:40.864
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:39:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1657" for this suite. 01/18/23 21:39:40.873
------------------------------
• [SLOW TEST] [304.224 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:34:36.662
    Jan 18 21:34:36.662: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-pred 01/18/23 21:34:36.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:34:36.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:34:36.698
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 21:34:36.700: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 21:34:36.709: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 21:34:36.712: INFO: 
    Logging pods the apiserver thinks is on node test-vm-2 before test
    Jan 18 21:34:36.720: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.720: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.720: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:34:36.720: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:34:36.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.720: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 21:34:36.720: INFO: 
    Logging pods the apiserver thinks is on node test-vm-1 before test
    Jan 18 21:34:36.727: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.727: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:34:36.727: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:34:36.727: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:34:36.727: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:34:36.727: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.727: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 21:34:36.727: INFO: execpodldzjx from services-5564 started at 2023-01-18 21:34:29 +0000 UTC (1 container statuses recorded)
    Jan 18 21:34:36.727: INFO: 	Container agnhost-container ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 21:34:36.727
    Jan 18 21:34:36.744: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1657" to be "running"
    Jan 18 21:34:36.748: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.184851ms
    Jan 18 21:34:38.753: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009464175s
    Jan 18 21:34:38.754: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 21:34:38.757
    STEP: Trying to apply a random label on the found node. 01/18/23 21:34:38.786
    STEP: verifying the node has the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 95 01/18/23 21:34:38.799
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 21:34:38.803
    Jan 18 21:34:38.813: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1657" to be "not pending"
    Jan 18 21:34:38.817: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.739046ms
    Jan 18 21:34:40.823: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010237697s
    Jan 18 21:34:40.823: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.0.4 on the node which pod4 resides and expect not scheduled 01/18/23 21:34:40.823
    Jan 18 21:34:40.836: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1657" to be "not pending"
    Jan 18 21:34:40.839: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449843ms
    Jan 18 21:34:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008851236s
    Jan 18 21:34:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010210479s
    Jan 18 21:34:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009239893s
    Jan 18 21:34:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009939229s
    Jan 18 21:34:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008843271s
    Jan 18 21:34:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008993629s
    Jan 18 21:34:54.850: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014274052s
    Jan 18 21:34:56.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008245507s
    Jan 18 21:34:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010351553s
    Jan 18 21:35:00.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010067171s
    Jan 18 21:35:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010630299s
    Jan 18 21:35:04.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008385267s
    Jan 18 21:35:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008472435s
    Jan 18 21:35:08.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008678704s
    Jan 18 21:35:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009072775s
    Jan 18 21:35:12.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008547069s
    Jan 18 21:35:14.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009971356s
    Jan 18 21:35:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009401018s
    Jan 18 21:35:18.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010372299s
    Jan 18 21:35:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009064954s
    Jan 18 21:35:22.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009912873s
    Jan 18 21:35:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010117884s
    Jan 18 21:35:26.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009826589s
    Jan 18 21:35:28.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.009989198s
    Jan 18 21:35:30.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.009257751s
    Jan 18 21:35:32.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008836507s
    Jan 18 21:35:34.861: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.025361875s
    Jan 18 21:35:36.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009549632s
    Jan 18 21:35:38.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009175695s
    Jan 18 21:35:40.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010182675s
    Jan 18 21:35:42.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008304521s
    Jan 18 21:35:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010340616s
    Jan 18 21:35:46.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009557332s
    Jan 18 21:35:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.00975316s
    Jan 18 21:35:50.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008411169s
    Jan 18 21:35:52.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008450995s
    Jan 18 21:35:54.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009144592s
    Jan 18 21:35:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00907308s
    Jan 18 21:35:58.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008977467s
    Jan 18 21:36:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.00929336s
    Jan 18 21:36:02.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009817936s
    Jan 18 21:36:04.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009336199s
    Jan 18 21:36:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008492358s
    Jan 18 21:36:08.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.010647754s
    Jan 18 21:36:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008639782s
    Jan 18 21:36:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009653948s
    Jan 18 21:36:14.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008461386s
    Jan 18 21:36:16.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008154035s
    Jan 18 21:36:18.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010922606s
    Jan 18 21:36:20.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01080384s
    Jan 18 21:36:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008870152s
    Jan 18 21:36:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009968402s
    Jan 18 21:36:26.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.009280931s
    Jan 18 21:36:28.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010310057s
    Jan 18 21:36:30.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008769753s
    Jan 18 21:36:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008306761s
    Jan 18 21:36:34.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010398847s
    Jan 18 21:36:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009248776s
    Jan 18 21:36:38.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010351237s
    Jan 18 21:36:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009153369s
    Jan 18 21:36:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009429394s
    Jan 18 21:36:44.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.00856172s
    Jan 18 21:36:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.009284876s
    Jan 18 21:36:48.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010013731s
    Jan 18 21:36:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.008631568s
    Jan 18 21:36:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008808397s
    Jan 18 21:36:54.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009779774s
    Jan 18 21:36:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.00890383s
    Jan 18 21:36:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.01037691s
    Jan 18 21:37:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008992934s
    Jan 18 21:37:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011461201s
    Jan 18 21:37:04.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009527618s
    Jan 18 21:37:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.009398199s
    Jan 18 21:37:08.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010224892s
    Jan 18 21:37:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008918762s
    Jan 18 21:37:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009617355s
    Jan 18 21:37:14.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010441289s
    Jan 18 21:37:16.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007908746s
    Jan 18 21:37:18.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00970075s
    Jan 18 21:37:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009262129s
    Jan 18 21:37:22.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009475563s
    Jan 18 21:37:24.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010309124s
    Jan 18 21:37:26.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008675557s
    Jan 18 21:37:28.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009216321s
    Jan 18 21:37:30.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008421996s
    Jan 18 21:37:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.0080539s
    Jan 18 21:37:34.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009161852s
    Jan 18 21:37:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009129968s
    Jan 18 21:37:38.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009144s
    Jan 18 21:37:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.009034989s
    Jan 18 21:37:42.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008710475s
    Jan 18 21:37:44.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010187382s
    Jan 18 21:37:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.009436394s
    Jan 18 21:37:48.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009050552s
    Jan 18 21:37:50.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008605409s
    Jan 18 21:37:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008595673s
    Jan 18 21:37:54.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008697956s
    Jan 18 21:37:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008809481s
    Jan 18 21:37:58.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.0084466s
    Jan 18 21:38:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008957031s
    Jan 18 21:38:02.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010540359s
    Jan 18 21:38:04.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009892729s
    Jan 18 21:38:06.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.00865299s
    Jan 18 21:38:08.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009639086s
    Jan 18 21:38:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008865472s
    Jan 18 21:38:12.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.01003305s
    Jan 18 21:38:14.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008674303s
    Jan 18 21:38:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008961772s
    Jan 18 21:38:18.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007550067s
    Jan 18 21:38:20.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008391616s
    Jan 18 21:38:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008740959s
    Jan 18 21:38:24.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008485497s
    Jan 18 21:38:26.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00819827s
    Jan 18 21:38:28.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007776871s
    Jan 18 21:38:30.848: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01192992s
    Jan 18 21:38:32.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.008534289s
    Jan 18 21:38:34.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010544751s
    Jan 18 21:38:36.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.008165069s
    Jan 18 21:38:38.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008389019s
    Jan 18 21:38:40.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009395778s
    Jan 18 21:38:42.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.009731151s
    Jan 18 21:38:44.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009035248s
    Jan 18 21:38:46.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008865852s
    Jan 18 21:38:48.849: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.0127866s
    Jan 18 21:38:50.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008457837s
    Jan 18 21:38:52.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008757988s
    Jan 18 21:38:54.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.007615923s
    Jan 18 21:38:56.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008512781s
    Jan 18 21:38:58.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009757361s
    Jan 18 21:39:00.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008605296s
    Jan 18 21:39:02.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.009064849s
    Jan 18 21:39:04.849: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.013087141s
    Jan 18 21:39:06.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008159724s
    Jan 18 21:39:08.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008508418s
    Jan 18 21:39:10.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.009147414s
    Jan 18 21:39:12.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.010456218s
    Jan 18 21:39:14.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008046002s
    Jan 18 21:39:16.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.008510454s
    Jan 18 21:39:18.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.007963094s
    Jan 18 21:39:20.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.009162454s
    Jan 18 21:39:22.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009248177s
    Jan 18 21:39:24.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008535353s
    Jan 18 21:39:26.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008416834s
    Jan 18 21:39:28.847: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.01071034s
    Jan 18 21:39:30.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008358966s
    Jan 18 21:39:32.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008063649s
    Jan 18 21:39:34.846: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010361159s
    Jan 18 21:39:36.845: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.00893463s
    Jan 18 21:39:38.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008028386s
    Jan 18 21:39:40.844: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.00805654s
    Jan 18 21:39:40.848: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011521257s
    STEP: removing the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 off the node test-vm-1 01/18/23 21:39:40.848
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-32fe8dbb-3f56-4856-9075-f50e3ce8b522 01/18/23 21:39:40.864
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:39:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1657" for this suite. 01/18/23 21:39:40.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:39:40.888
Jan 18 21:39:40.888: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:39:40.889
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:40.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:40.931
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan 18 21:39:40.934: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: creating the pod 01/18/23 21:39:40.935
STEP: submitting the pod to kubernetes 01/18/23 21:39:40.935
Jan 18 21:39:40.952: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777" in namespace "pods-2122" to be "running and ready"
Jan 18 21:39:40.955: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655812ms
Jan 18 21:39:40.955: INFO: The phase of Pod pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:39:42.960: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777": Phase="Running", Reason="", readiness=true. Elapsed: 2.008119125s
Jan 18 21:39:42.960: INFO: The phase of Pod pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777 is Running (Ready = true)
Jan 18 21:39:42.960: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:39:43.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2122" for this suite. 01/18/23 21:39:43.069
------------------------------
• [2.195 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:39:40.888
    Jan 18 21:39:40.888: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:39:40.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:40.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:40.931
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan 18 21:39:40.934: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: creating the pod 01/18/23 21:39:40.935
    STEP: submitting the pod to kubernetes 01/18/23 21:39:40.935
    Jan 18 21:39:40.952: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777" in namespace "pods-2122" to be "running and ready"
    Jan 18 21:39:40.955: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655812ms
    Jan 18 21:39:40.955: INFO: The phase of Pod pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:39:42.960: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777": Phase="Running", Reason="", readiness=true. Elapsed: 2.008119125s
    Jan 18 21:39:42.960: INFO: The phase of Pod pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777 is Running (Ready = true)
    Jan 18 21:39:42.960: INFO: Pod "pod-exec-websocket-b6f86821-e29a-4df8-9593-db6aaef8e777" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:39:43.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2122" for this suite. 01/18/23 21:39:43.069
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:39:43.083
Jan 18 21:39:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:39:43.085
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:43.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:43.116
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6739 01/18/23 21:39:43.119
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 21:39:43.128
STEP: Creating stateful set ss in namespace statefulset-6739 01/18/23 21:39:43.142
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6739 01/18/23 21:39:43.154
Jan 18 21:39:43.160: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:39:53.166: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 21:39:53.166
Jan 18 21:39:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:39:53.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:39:53.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:39:53.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 21:39:53.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 21:40:03.365: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 21:40:03.365: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:40:03.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999997s
Jan 18 21:40:04.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995847203s
Jan 18 21:40:05.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990645094s
Jan 18 21:40:06.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984380772s
Jan 18 21:40:07.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979444066s
Jan 18 21:40:08.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972401133s
Jan 18 21:40:09.420: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967280838s
Jan 18 21:40:10.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.961897233s
Jan 18 21:40:11.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955601909s
Jan 18 21:40:12.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 949.427587ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6739 01/18/23 21:40:13.438
Jan 18 21:40:13.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:40:13.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:40:13.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:40:13.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 21:40:13.614: INFO: Found 1 stateful pods, waiting for 3
Jan 18 21:40:23.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:40:23.621: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:40:23.621: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/18/23 21:40:23.621
STEP: Scale down will halt with unhealthy stateful pod 01/18/23 21:40:23.621
Jan 18 21:40:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:40:23.800: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:40:23.800: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:40:23.800: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 21:40:23.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:40:23.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:40:23.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:40:23.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 21:40:23.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 21:40:24.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 21:40:24.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 21:40:24.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 21:40:24.138: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:40:24.142: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 18 21:40:34.152: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 21:40:34.152: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 21:40:34.152: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 21:40:34.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999996s
Jan 18 21:40:35.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995002669s
Jan 18 21:40:36.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988813822s
Jan 18 21:40:37.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982396673s
Jan 18 21:40:38.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977178338s
Jan 18 21:40:39.205: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970223482s
Jan 18 21:40:40.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964709144s
Jan 18 21:40:41.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958210093s
Jan 18 21:40:42.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953637134s
Jan 18 21:40:43.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.447396ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6739 01/18/23 21:40:44.229
Jan 18 21:40:44.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:40:44.410: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:40:44.411: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:40:44.411: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 21:40:44.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:40:44.568: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:40:44.568: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:40:44.568: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 21:40:44.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 21:40:44.741: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 21:40:44.741: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 21:40:44.741: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 21:40:44.741: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 21:40:54.761
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:40:54.761: INFO: Deleting all statefulset in ns statefulset-6739
Jan 18 21:40:54.766: INFO: Scaling statefulset ss to 0
Jan 18 21:40:54.783: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:40:54.786: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:40:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6739" for this suite. 01/18/23 21:40:54.819
------------------------------
• [SLOW TEST] [71.748 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:39:43.083
    Jan 18 21:39:43.083: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:39:43.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:39:43.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:39:43.116
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6739 01/18/23 21:39:43.119
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 21:39:43.128
    STEP: Creating stateful set ss in namespace statefulset-6739 01/18/23 21:39:43.142
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6739 01/18/23 21:39:43.154
    Jan 18 21:39:43.160: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:39:53.166: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 21:39:53.166
    Jan 18 21:39:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:39:53.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:39:53.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:39:53.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 21:39:53.360: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 21:40:03.365: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 21:40:03.365: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:40:03.386: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999997s
    Jan 18 21:40:04.392: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995847203s
    Jan 18 21:40:05.397: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.990645094s
    Jan 18 21:40:06.402: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984380772s
    Jan 18 21:40:07.409: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979444066s
    Jan 18 21:40:08.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.972401133s
    Jan 18 21:40:09.420: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.967280838s
    Jan 18 21:40:10.427: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.961897233s
    Jan 18 21:40:11.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.955601909s
    Jan 18 21:40:12.437: INFO: Verifying statefulset ss doesn't scale past 1 for another 949.427587ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6739 01/18/23 21:40:13.438
    Jan 18 21:40:13.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:40:13.610: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:40:13.610: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:40:13.610: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 21:40:13.614: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 21:40:23.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:40:23.621: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:40:23.621: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/18/23 21:40:23.621
    STEP: Scale down will halt with unhealthy stateful pod 01/18/23 21:40:23.621
    Jan 18 21:40:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:40:23.800: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:40:23.800: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:40:23.800: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 21:40:23.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:40:23.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:40:23.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:40:23.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 21:40:23.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 21:40:24.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 21:40:24.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 21:40:24.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 21:40:24.138: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:40:24.142: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 18 21:40:34.152: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 21:40:34.152: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 21:40:34.152: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 21:40:34.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999996s
    Jan 18 21:40:35.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995002669s
    Jan 18 21:40:36.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988813822s
    Jan 18 21:40:37.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982396673s
    Jan 18 21:40:38.200: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977178338s
    Jan 18 21:40:39.205: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970223482s
    Jan 18 21:40:40.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964709144s
    Jan 18 21:40:41.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958210093s
    Jan 18 21:40:42.222: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953637134s
    Jan 18 21:40:43.229: INFO: Verifying statefulset ss doesn't scale past 3 for another 947.447396ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6739 01/18/23 21:40:44.229
    Jan 18 21:40:44.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:40:44.410: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:40:44.411: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:40:44.411: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 21:40:44.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:40:44.568: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:40:44.568: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:40:44.568: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 21:40:44.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-6739 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 21:40:44.741: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 21:40:44.741: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 21:40:44.741: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 21:40:44.741: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 21:40:54.761
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:40:54.761: INFO: Deleting all statefulset in ns statefulset-6739
    Jan 18 21:40:54.766: INFO: Scaling statefulset ss to 0
    Jan 18 21:40:54.783: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:40:54.786: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:40:54.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6739" for this suite. 01/18/23 21:40:54.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:40:54.833
Jan 18 21:40:54.833: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename containers 01/18/23 21:40:54.835
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:54.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:54.868
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/18/23 21:40:54.877
Jan 18 21:40:54.904: INFO: Waiting up to 5m0s for pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d" in namespace "containers-5984" to be "Succeeded or Failed"
Jan 18 21:40:54.910: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896473ms
Jan 18 21:40:56.915: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010798096s
Jan 18 21:40:58.918: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013344836s
Jan 18 21:41:00.916: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011459945s
STEP: Saw pod success 01/18/23 21:41:00.916
Jan 18 21:41:00.916: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d" satisfied condition "Succeeded or Failed"
Jan 18 21:41:00.922: INFO: Trying to get logs from node test-vm-1 pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:41:00.943
Jan 18 21:41:00.966: INFO: Waiting for pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d to disappear
Jan 18 21:41:00.973: INFO: Pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:00.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5984" for this suite. 01/18/23 21:41:00.978
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:40:54.833
    Jan 18 21:40:54.833: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename containers 01/18/23 21:40:54.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:40:54.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:40:54.868
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/18/23 21:40:54.877
    Jan 18 21:40:54.904: INFO: Waiting up to 5m0s for pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d" in namespace "containers-5984" to be "Succeeded or Failed"
    Jan 18 21:40:54.910: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.896473ms
    Jan 18 21:40:56.915: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010798096s
    Jan 18 21:40:58.918: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Running", Reason="", readiness=false. Elapsed: 4.013344836s
    Jan 18 21:41:00.916: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011459945s
    STEP: Saw pod success 01/18/23 21:41:00.916
    Jan 18 21:41:00.916: INFO: Pod "client-containers-89200916-cb94-4ecd-96d5-40c124698c2d" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:00.922: INFO: Trying to get logs from node test-vm-1 pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:41:00.943
    Jan 18 21:41:00.966: INFO: Waiting for pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d to disappear
    Jan 18 21:41:00.973: INFO: Pod client-containers-89200916-cb94-4ecd-96d5-40c124698c2d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:00.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5984" for this suite. 01/18/23 21:41:00.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:00.991
Jan 18 21:41:00.991: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption 01/18/23 21:41:00.992
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:01.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:01.036
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/18/23 21:41:01.038
STEP: Waiting for the pdb to be processed 01/18/23 21:41:01.048
STEP: updating the pdb 01/18/23 21:41:03.057
STEP: Waiting for the pdb to be processed 01/18/23 21:41:03.073
STEP: patching the pdb 01/18/23 21:41:05.082
STEP: Waiting for the pdb to be processed 01/18/23 21:41:05.098
STEP: Waiting for the pdb to be deleted 01/18/23 21:41:07.119
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4474" for this suite. 01/18/23 21:41:07.131
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:00.991
    Jan 18 21:41:00.991: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption 01/18/23 21:41:00.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:01.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:01.036
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/18/23 21:41:01.038
    STEP: Waiting for the pdb to be processed 01/18/23 21:41:01.048
    STEP: updating the pdb 01/18/23 21:41:03.057
    STEP: Waiting for the pdb to be processed 01/18/23 21:41:03.073
    STEP: patching the pdb 01/18/23 21:41:05.082
    STEP: Waiting for the pdb to be processed 01/18/23 21:41:05.098
    STEP: Waiting for the pdb to be deleted 01/18/23 21:41:07.119
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:07.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4474" for this suite. 01/18/23 21:41:07.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:07.152
Jan 18 21:41:07.152: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:41:07.153
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:07.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:07.191
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-8eeb43f3-d6d1-4d0c-a88f-23e1bf1886ff 01/18/23 21:41:07.194
STEP: Creating a pod to test consume configMaps 01/18/23 21:41:07.205
Jan 18 21:41:07.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47" in namespace "projected-9754" to be "Succeeded or Failed"
Jan 18 21:41:07.229: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331159ms
Jan 18 21:41:09.234: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580426s
Jan 18 21:41:11.235: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010472633s
Jan 18 21:41:13.234: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009938521s
STEP: Saw pod success 01/18/23 21:41:13.234
Jan 18 21:41:13.235: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47" satisfied condition "Succeeded or Failed"
Jan 18 21:41:13.238: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:41:13.245
Jan 18 21:41:13.272: INFO: Waiting for pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 to disappear
Jan 18 21:41:13.280: INFO: Pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9754" for this suite. 01/18/23 21:41:13.285
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:07.152
    Jan 18 21:41:07.152: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:41:07.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:07.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:07.191
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-8eeb43f3-d6d1-4d0c-a88f-23e1bf1886ff 01/18/23 21:41:07.194
    STEP: Creating a pod to test consume configMaps 01/18/23 21:41:07.205
    Jan 18 21:41:07.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47" in namespace "projected-9754" to be "Succeeded or Failed"
    Jan 18 21:41:07.229: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.331159ms
    Jan 18 21:41:09.234: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009580426s
    Jan 18 21:41:11.235: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010472633s
    Jan 18 21:41:13.234: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009938521s
    STEP: Saw pod success 01/18/23 21:41:13.234
    Jan 18 21:41:13.235: INFO: Pod "pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:13.238: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:41:13.245
    Jan 18 21:41:13.272: INFO: Waiting for pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 to disappear
    Jan 18 21:41:13.280: INFO: Pod pod-projected-configmaps-a863a9e4-b0d4-4cfc-8682-e400db09bd47 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:13.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9754" for this suite. 01/18/23 21:41:13.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:13.3
Jan 18 21:41:13.300: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:41:13.301
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:13.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:13.347
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/18/23 21:41:13.35
Jan 18 21:41:13.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128" in namespace "downward-api-3315" to be "Succeeded or Failed"
Jan 18 21:41:13.368: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Pending", Reason="", readiness=false. Elapsed: 3.507845ms
Jan 18 21:41:15.373: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008477786s
Jan 18 21:41:17.375: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009944782s
STEP: Saw pod success 01/18/23 21:41:17.375
Jan 18 21:41:17.375: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128" satisfied condition "Succeeded or Failed"
Jan 18 21:41:17.382: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 container client-container: <nil>
STEP: delete the pod 01/18/23 21:41:17.395
Jan 18 21:41:17.428: INFO: Waiting for pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 to disappear
Jan 18 21:41:17.435: INFO: Pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:17.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3315" for this suite. 01/18/23 21:41:17.44
------------------------------
• [4.156 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:13.3
    Jan 18 21:41:13.300: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:41:13.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:13.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:13.347
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/18/23 21:41:13.35
    Jan 18 21:41:13.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128" in namespace "downward-api-3315" to be "Succeeded or Failed"
    Jan 18 21:41:13.368: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Pending", Reason="", readiness=false. Elapsed: 3.507845ms
    Jan 18 21:41:15.373: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008477786s
    Jan 18 21:41:17.375: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009944782s
    STEP: Saw pod success 01/18/23 21:41:17.375
    Jan 18 21:41:17.375: INFO: Pod "downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128" satisfied condition "Succeeded or Failed"
    Jan 18 21:41:17.382: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 container client-container: <nil>
    STEP: delete the pod 01/18/23 21:41:17.395
    Jan 18 21:41:17.428: INFO: Waiting for pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 to disappear
    Jan 18 21:41:17.435: INFO: Pod downwardapi-volume-a421cb4c-397c-4752-b52a-50636fa94128 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:17.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3315" for this suite. 01/18/23 21:41:17.44
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:17.456
Jan 18 21:41:17.456: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:41:17.458
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:17.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:17.517
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan 18 21:41:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:41:19.477
Jan 18 21:41:19.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 create -f -'
Jan 18 21:41:20.288: INFO: stderr: ""
Jan 18 21:41:20.288: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 21:41:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 delete e2e-test-crd-publish-openapi-6667-crds test-cr'
Jan 18 21:41:20.404: INFO: stderr: ""
Jan 18 21:41:20.404: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 21:41:20.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 apply -f -'
Jan 18 21:41:20.648: INFO: stderr: ""
Jan 18 21:41:20.648: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 21:41:20.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 delete e2e-test-crd-publish-openapi-6667-crds test-cr'
Jan 18 21:41:20.772: INFO: stderr: ""
Jan 18 21:41:20.772: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 21:41:20.772
Jan 18 21:41:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 explain e2e-test-crd-publish-openapi-6667-crds'
Jan 18 21:41:21.424: INFO: stderr: ""
Jan 18 21:41:21.424: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6667-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:23.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9096" for this suite. 01/18/23 21:41:23.436
------------------------------
• [SLOW TEST] [5.994 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:17.456
    Jan 18 21:41:17.456: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:41:17.458
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:17.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:17.517
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan 18 21:41:17.524: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 21:41:19.477
    Jan 18 21:41:19.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 create -f -'
    Jan 18 21:41:20.288: INFO: stderr: ""
    Jan 18 21:41:20.288: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 21:41:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 delete e2e-test-crd-publish-openapi-6667-crds test-cr'
    Jan 18 21:41:20.404: INFO: stderr: ""
    Jan 18 21:41:20.404: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 18 21:41:20.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 apply -f -'
    Jan 18 21:41:20.648: INFO: stderr: ""
    Jan 18 21:41:20.648: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 21:41:20.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 --namespace=crd-publish-openapi-9096 delete e2e-test-crd-publish-openapi-6667-crds test-cr'
    Jan 18 21:41:20.772: INFO: stderr: ""
    Jan 18 21:41:20.772: INFO: stdout: "e2e-test-crd-publish-openapi-6667-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 21:41:20.772
    Jan 18 21:41:20.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-9096 explain e2e-test-crd-publish-openapi-6667-crds'
    Jan 18 21:41:21.424: INFO: stderr: ""
    Jan 18 21:41:21.424: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6667-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:23.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9096" for this suite. 01/18/23 21:41:23.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:23.451
Jan 18 21:41:23.451: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:41:23.453
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:23.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:23.762
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 21:41:23.786
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 21:41:38.867
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 21:41:38.87
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 21:41:38.877
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 21:41:38.877
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 21:41:38.929
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 21:41:41.95
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 21:41:43.966
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 21:41:43.974
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 21:41:43.974
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 21:41:44.021
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 21:41:45.03
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 21:41:48.048
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 21:41:48.058
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 21:41:48.058
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-4540" for this suite. 01/18/23 21:41:48.098
------------------------------
• [SLOW TEST] [24.658 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:23.451
    Jan 18 21:41:23.451: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:41:23.453
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:23.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:23.762
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 21:41:23.786
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 21:41:38.867
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 21:41:38.87
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 21:41:38.877
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 21:41:38.877
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 21:41:38.929
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 21:41:41.95
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 21:41:43.966
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 21:41:43.974
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 21:41:43.974
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 21:41:44.021
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 21:41:45.03
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 21:41:48.048
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 21:41:48.058
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 21:41:48.058
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-4540" for this suite. 01/18/23 21:41:48.098
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:48.11
Jan 18 21:41:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 21:41:48.111
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:48.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:48.874
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-7n4kj" 01/18/23 21:41:48.877
Jan 18 21:41:49.872: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-7n4kj-1313" 01/18/23 21:41:49.872
Jan 18 21:41:49.886: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-7n4kj-1313" 01/18/23 21:41:49.886
Jan 18 21:41:49.898: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:49.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-2652" for this suite. 01/18/23 21:41:49.902
STEP: Destroying namespace "e2e-ns-7n4kj-1313" for this suite. 01/18/23 21:41:49.913
------------------------------
• [1.814 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:48.11
    Jan 18 21:41:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:41:48.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:48.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:48.874
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-7n4kj" 01/18/23 21:41:48.877
    Jan 18 21:41:49.872: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-7n4kj-1313" 01/18/23 21:41:49.872
    Jan 18 21:41:49.886: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-7n4kj-1313" 01/18/23 21:41:49.886
    Jan 18 21:41:49.898: INFO: Namespace "e2e-ns-7n4kj-1313" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:49.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-2652" for this suite. 01/18/23 21:41:49.902
    STEP: Destroying namespace "e2e-ns-7n4kj-1313" for this suite. 01/18/23 21:41:49.913
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:49.927
Jan 18 21:41:49.927: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 21:41:49.928
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:50.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:50.559
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/18/23 21:41:50.562
STEP: When the matched label of one of its pods change 01/18/23 21:41:50.573
Jan 18 21:41:50.578: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 21:41:55.583: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 21:41:55.604
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:41:56.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4603" for this suite. 01/18/23 21:41:56.621
------------------------------
• [SLOW TEST] [6.709 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:49.927
    Jan 18 21:41:49.927: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 21:41:49.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:50.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:50.559
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/18/23 21:41:50.562
    STEP: When the matched label of one of its pods change 01/18/23 21:41:50.573
    Jan 18 21:41:50.578: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 18 21:41:55.583: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 21:41:55.604
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:41:56.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4603" for this suite. 01/18/23 21:41:56.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:41:56.637
Jan 18 21:41:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:41:56.638
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:56.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:56.781
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-ea45c919-355d-4495-bf21-406e0c6672fa 01/18/23 21:41:56.784
STEP: Creating a pod to test consume secrets 01/18/23 21:41:56.794
Jan 18 21:41:56.820: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3" in namespace "projected-3742" to be "Succeeded or Failed"
Jan 18 21:41:56.825: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908961ms
Jan 18 21:41:58.830: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050165s
Jan 18 21:42:00.831: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010888515s
STEP: Saw pod success 01/18/23 21:42:00.831
Jan 18 21:42:00.831: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3" satisfied condition "Succeeded or Failed"
Jan 18 21:42:00.835: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:42:00.854
Jan 18 21:42:00.878: INFO: Waiting for pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 to disappear
Jan 18 21:42:00.884: INFO: Pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3742" for this suite. 01/18/23 21:42:00.888
------------------------------
• [4.264 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:41:56.637
    Jan 18 21:41:56.637: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:41:56.638
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:41:56.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:41:56.781
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-ea45c919-355d-4495-bf21-406e0c6672fa 01/18/23 21:41:56.784
    STEP: Creating a pod to test consume secrets 01/18/23 21:41:56.794
    Jan 18 21:41:56.820: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3" in namespace "projected-3742" to be "Succeeded or Failed"
    Jan 18 21:41:56.825: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.908961ms
    Jan 18 21:41:58.830: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010050165s
    Jan 18 21:42:00.831: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010888515s
    STEP: Saw pod success 01/18/23 21:42:00.831
    Jan 18 21:42:00.831: INFO: Pod "pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3" satisfied condition "Succeeded or Failed"
    Jan 18 21:42:00.835: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:42:00.854
    Jan 18 21:42:00.878: INFO: Waiting for pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 to disappear
    Jan 18 21:42:00.884: INFO: Pod pod-projected-secrets-ffe3f75e-5eca-435a-b1aa-1a13821cbef3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3742" for this suite. 01/18/23 21:42:00.888
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:00.903
Jan 18 21:42:00.903: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 21:42:00.904
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:01.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:01.296
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/18/23 21:42:01.299
STEP: delete the rc 01/18/23 21:42:06.349
STEP: wait for all pods to be garbage collected 01/18/23 21:42:06.365
STEP: Gathering metrics 01/18/23 21:42:11.377
W0118 21:42:11.383141      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 21:42:11.383: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:11.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8878" for this suite. 01/18/23 21:42:11.387
------------------------------
• [SLOW TEST] [10.496 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:00.903
    Jan 18 21:42:00.903: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 21:42:00.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:01.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:01.296
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/18/23 21:42:01.299
    STEP: delete the rc 01/18/23 21:42:06.349
    STEP: wait for all pods to be garbage collected 01/18/23 21:42:06.365
    STEP: Gathering metrics 01/18/23 21:42:11.377
    W0118 21:42:11.383141      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 21:42:11.383: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:11.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8878" for this suite. 01/18/23 21:42:11.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:11.4
Jan 18 21:42:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:42:11.401
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:11.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:11.874
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-700 01/18/23 21:42:11.877
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-700 01/18/23 21:42:11.895
Jan 18 21:42:11.912: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:42:21.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/18/23 21:42:21.925
STEP: Getting /status 01/18/23 21:42:21.94
Jan 18 21:42:21.944: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/18/23 21:42:21.944
Jan 18 21:42:21.960: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/18/23 21:42:21.96
Jan 18 21:42:21.963: INFO: Observed &StatefulSet event: ADDED
Jan 18 21:42:21.963: INFO: Found Statefulset ss in namespace statefulset-700 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 21:42:21.963: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/18/23 21:42:21.963
Jan 18 21:42:21.963: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 21:42:21.977: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/18/23 21:42:21.977
Jan 18 21:42:21.979: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:42:21.979: INFO: Deleting all statefulset in ns statefulset-700
Jan 18 21:42:21.982: INFO: Scaling statefulset ss to 0
Jan 18 21:42:32.008: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 21:42:32.012: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:32.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-700" for this suite. 01/18/23 21:42:32.039
------------------------------
• [SLOW TEST] [20.661 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:11.4
    Jan 18 21:42:11.400: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:42:11.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:11.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:11.874
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-700 01/18/23 21:42:11.877
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-700 01/18/23 21:42:11.895
    Jan 18 21:42:11.912: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:42:21.917: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/18/23 21:42:21.925
    STEP: Getting /status 01/18/23 21:42:21.94
    Jan 18 21:42:21.944: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/18/23 21:42:21.944
    Jan 18 21:42:21.960: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/18/23 21:42:21.96
    Jan 18 21:42:21.963: INFO: Observed &StatefulSet event: ADDED
    Jan 18 21:42:21.963: INFO: Found Statefulset ss in namespace statefulset-700 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 21:42:21.963: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/18/23 21:42:21.963
    Jan 18 21:42:21.963: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 21:42:21.977: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/18/23 21:42:21.977
    Jan 18 21:42:21.979: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:42:21.979: INFO: Deleting all statefulset in ns statefulset-700
    Jan 18 21:42:21.982: INFO: Scaling statefulset ss to 0
    Jan 18 21:42:32.008: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 21:42:32.012: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:32.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-700" for this suite. 01/18/23 21:42:32.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:32.063
Jan 18 21:42:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 21:42:32.064
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:32.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:32.876
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/18/23 21:42:32.878
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:33.871
STEP: Creating a pod in the namespace 01/18/23 21:42:33.874
STEP: Waiting for the pod to have running status 01/18/23 21:42:33.889
Jan 18 21:42:33.889: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-969" to be "running"
Jan 18 21:42:33.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34244ms
Jan 18 21:42:35.897: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007792893s
Jan 18 21:42:35.897: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/18/23 21:42:35.897
STEP: Waiting for the namespace to be removed. 01/18/23 21:42:35.91
STEP: Recreating the namespace 01/18/23 21:42:47.915
STEP: Verifying there are no pods in the namespace 01/18/23 21:42:48.557
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:48.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5341" for this suite. 01/18/23 21:42:48.567
STEP: Destroying namespace "nsdeletetest-969" for this suite. 01/18/23 21:42:48.578
Jan 18 21:42:48.581: INFO: Namespace nsdeletetest-969 was already deleted
STEP: Destroying namespace "nsdeletetest-6992" for this suite. 01/18/23 21:42:48.581
------------------------------
• [SLOW TEST] [16.530 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:32.063
    Jan 18 21:42:32.063: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 21:42:32.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:32.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:32.876
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/18/23 21:42:32.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:33.871
    STEP: Creating a pod in the namespace 01/18/23 21:42:33.874
    STEP: Waiting for the pod to have running status 01/18/23 21:42:33.889
    Jan 18 21:42:33.889: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-969" to be "running"
    Jan 18 21:42:33.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.34244ms
    Jan 18 21:42:35.897: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007792893s
    Jan 18 21:42:35.897: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/18/23 21:42:35.897
    STEP: Waiting for the namespace to be removed. 01/18/23 21:42:35.91
    STEP: Recreating the namespace 01/18/23 21:42:47.915
    STEP: Verifying there are no pods in the namespace 01/18/23 21:42:48.557
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:48.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5341" for this suite. 01/18/23 21:42:48.567
    STEP: Destroying namespace "nsdeletetest-969" for this suite. 01/18/23 21:42:48.578
    Jan 18 21:42:48.581: INFO: Namespace nsdeletetest-969 was already deleted
    STEP: Destroying namespace "nsdeletetest-6992" for this suite. 01/18/23 21:42:48.581
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:48.592
Jan 18 21:42:48.593: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename tables 01/18/23 21:42:48.594
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:48.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:48.874
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:48.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-1615" for this suite. 01/18/23 21:42:48.885
------------------------------
• [0.310 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:48.592
    Jan 18 21:42:48.593: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename tables 01/18/23 21:42:48.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:48.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:48.874
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:48.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-1615" for this suite. 01/18/23 21:42:48.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:48.906
Jan 18 21:42:48.906: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:42:48.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:49.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:49.63
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:42:49.67
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:42:49.681
Jan 18 21:42:49.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:42:49.693: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:42:50.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:42:50.703: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:42:51.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:42:51.713: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:42:52.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:42:52.705: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/18/23 21:42:52.709
Jan 18 21:42:52.714: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/18/23 21:42:52.714
Jan 18 21:42:52.728: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/18/23 21:42:52.728
Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: ADDED
Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.731: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.731: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.731: INFO: Found daemon set daemon-set in namespace daemonsets-6942 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:42:52.731: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/18/23 21:42:52.731
STEP: watching for the daemon set status to be patched 01/18/23 21:42:52.749
Jan 18 21:42:52.751: INFO: Observed &DaemonSet event: ADDED
Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.753: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.753: INFO: Observed daemon set daemon-set in namespace daemonsets-6942 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 21:42:52.753: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 21:42:52.753: INFO: Found daemon set daemon-set in namespace daemonsets-6942 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 21:42:52.753: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:42:52.757
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6942, will wait for the garbage collector to delete the pods 01/18/23 21:42:52.757
Jan 18 21:42:52.828: INFO: Deleting DaemonSet.extensions daemon-set took: 15.65708ms
Jan 18 21:42:53.928: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.10020336s
Jan 18 21:42:55.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:42:55.935: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:42:55.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19773"},"items":null}

Jan 18 21:42:55.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19773"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:42:55.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6942" for this suite. 01/18/23 21:42:55.984
------------------------------
• [SLOW TEST] [7.092 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:48.906
    Jan 18 21:42:48.906: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:42:48.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:49.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:49.63
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 21:42:49.67
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:42:49.681
    Jan 18 21:42:49.693: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:42:49.693: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:42:50.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:42:50.703: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:42:51.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:42:51.713: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:42:52.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:42:52.705: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/18/23 21:42:52.709
    Jan 18 21:42:52.714: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/18/23 21:42:52.714
    Jan 18 21:42:52.728: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/18/23 21:42:52.728
    Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: ADDED
    Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.730: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.731: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.731: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.731: INFO: Found daemon set daemon-set in namespace daemonsets-6942 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:42:52.731: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/18/23 21:42:52.731
    STEP: watching for the daemon set status to be patched 01/18/23 21:42:52.749
    Jan 18 21:42:52.751: INFO: Observed &DaemonSet event: ADDED
    Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.752: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.753: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.753: INFO: Observed daemon set daemon-set in namespace daemonsets-6942 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 21:42:52.753: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 21:42:52.753: INFO: Found daemon set daemon-set in namespace daemonsets-6942 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 18 21:42:52.753: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:42:52.757
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6942, will wait for the garbage collector to delete the pods 01/18/23 21:42:52.757
    Jan 18 21:42:52.828: INFO: Deleting DaemonSet.extensions daemon-set took: 15.65708ms
    Jan 18 21:42:53.928: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.10020336s
    Jan 18 21:42:55.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:42:55.935: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:42:55.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19773"},"items":null}

    Jan 18 21:42:55.960: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19773"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:42:55.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6942" for this suite. 01/18/23 21:42:55.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:42:56
Jan 18 21:42:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:42:56.002
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:56.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:56.643
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/18/23 21:42:56.656
STEP: watching for Pod to be ready 01/18/23 21:42:56.672
Jan 18 21:42:56.674: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 21:42:56.685: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
Jan 18 21:42:56.702: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
Jan 18 21:42:57.220: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
Jan 18 21:42:57.675: INFO: Found Pod pod-test in namespace pods-7786 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/18/23 21:42:57.679
STEP: getting the Pod and ensuring that it's patched 01/18/23 21:42:57.694
STEP: replacing the Pod's status Ready condition to False 01/18/23 21:42:57.698
STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 21:42:57.716
STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 21:42:57.716
STEP: watching for the Pod to be deleted 01/18/23 21:42:57.732
Jan 18 21:42:57.734: INFO: observed event type MODIFIED
Jan 18 21:42:59.693: INFO: observed event type MODIFIED
Jan 18 21:42:59.907: INFO: observed event type MODIFIED
Jan 18 21:43:00.694: INFO: observed event type MODIFIED
Jan 18 21:43:00.707: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:43:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7786" for this suite. 01/18/23 21:43:00.726
------------------------------
• [4.737 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:42:56
    Jan 18 21:42:56.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:42:56.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:42:56.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:42:56.643
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/18/23 21:42:56.656
    STEP: watching for Pod to be ready 01/18/23 21:42:56.672
    Jan 18 21:42:56.674: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 18 21:42:56.685: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
    Jan 18 21:42:56.702: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
    Jan 18 21:42:57.220: INFO: observed Pod pod-test in namespace pods-7786 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
    Jan 18 21:42:57.675: INFO: Found Pod pod-test in namespace pods-7786 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 21:42:56 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/18/23 21:42:57.679
    STEP: getting the Pod and ensuring that it's patched 01/18/23 21:42:57.694
    STEP: replacing the Pod's status Ready condition to False 01/18/23 21:42:57.698
    STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 21:42:57.716
    STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 21:42:57.716
    STEP: watching for the Pod to be deleted 01/18/23 21:42:57.732
    Jan 18 21:42:57.734: INFO: observed event type MODIFIED
    Jan 18 21:42:59.693: INFO: observed event type MODIFIED
    Jan 18 21:42:59.907: INFO: observed event type MODIFIED
    Jan 18 21:43:00.694: INFO: observed event type MODIFIED
    Jan 18 21:43:00.707: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:43:00.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7786" for this suite. 01/18/23 21:43:00.726
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:43:00.738
Jan 18 21:43:00.739: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:43:00.74
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:00.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:00.974
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:43:01.002
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:43:01.514
STEP: Deploying the webhook pod 01/18/23 21:43:01.535
STEP: Wait for the deployment to be ready 01/18/23 21:43:01.558
Jan 18 21:43:01.568: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:43:03.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:43:05.586
STEP: Verifying the service has paired with the endpoint 01/18/23 21:43:05.607
Jan 18 21:43:06.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 21:43:06.612
Jan 18 21:43:06.644: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 21:43:06.754
Jan 18 21:43:06.754: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:43:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6189" for this suite. 01/18/23 21:43:06.848
STEP: Destroying namespace "webhook-6189-markers" for this suite. 01/18/23 21:43:06.863
------------------------------
• [SLOW TEST] [6.140 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:43:00.738
    Jan 18 21:43:00.739: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:43:00.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:00.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:00.974
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:43:01.002
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:43:01.514
    STEP: Deploying the webhook pod 01/18/23 21:43:01.535
    STEP: Wait for the deployment to be ready 01/18/23 21:43:01.558
    Jan 18 21:43:01.568: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:43:03.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 43, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:43:05.586
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:43:05.607
    Jan 18 21:43:06.608: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 21:43:06.612
    Jan 18 21:43:06.644: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 21:43:06.754
    Jan 18 21:43:06.754: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:43:06.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6189" for this suite. 01/18/23 21:43:06.848
    STEP: Destroying namespace "webhook-6189-markers" for this suite. 01/18/23 21:43:06.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:43:06.88
Jan 18 21:43:06.880: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 21:43:06.881
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:07.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:07.199
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 in namespace container-probe-6501 01/18/23 21:43:07.202
Jan 18 21:43:07.246: INFO: Waiting up to 5m0s for pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21" in namespace "container-probe-6501" to be "not pending"
Jan 18 21:43:07.252: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.861165ms
Jan 18 21:43:09.257: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21": Phase="Running", Reason="", readiness=true. Elapsed: 2.010579967s
Jan 18 21:43:09.257: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21" satisfied condition "not pending"
Jan 18 21:43:09.257: INFO: Started pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 in namespace container-probe-6501
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:43:09.257
Jan 18 21:43:09.260: INFO: Initial restart count of pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 is 0
Jan 18 21:43:59.397: INFO: Restart count of pod container-probe-6501/busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 is now 1 (50.137014772s elapsed)
STEP: deleting the pod 01/18/23 21:43:59.397
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 21:43:59.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6501" for this suite. 01/18/23 21:43:59.454
------------------------------
• [SLOW TEST] [52.596 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:43:06.88
    Jan 18 21:43:06.880: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 21:43:06.881
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:07.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:07.199
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 in namespace container-probe-6501 01/18/23 21:43:07.202
    Jan 18 21:43:07.246: INFO: Waiting up to 5m0s for pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21" in namespace "container-probe-6501" to be "not pending"
    Jan 18 21:43:07.252: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21": Phase="Pending", Reason="", readiness=false. Elapsed: 5.861165ms
    Jan 18 21:43:09.257: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21": Phase="Running", Reason="", readiness=true. Elapsed: 2.010579967s
    Jan 18 21:43:09.257: INFO: Pod "busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21" satisfied condition "not pending"
    Jan 18 21:43:09.257: INFO: Started pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 in namespace container-probe-6501
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:43:09.257
    Jan 18 21:43:09.260: INFO: Initial restart count of pod busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 is 0
    Jan 18 21:43:59.397: INFO: Restart count of pod container-probe-6501/busybox-60e92ce2-7613-4d48-b0da-bcea2781ce21 is now 1 (50.137014772s elapsed)
    STEP: deleting the pod 01/18/23 21:43:59.397
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:43:59.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6501" for this suite. 01/18/23 21:43:59.454
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:43:59.476
Jan 18 21:43:59.476: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:43:59.478
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:59.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:59.875
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:43:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4570" for this suite. 01/18/23 21:43:59.948
------------------------------
• [0.483 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:43:59.476
    Jan 18 21:43:59.476: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:43:59.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:43:59.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:43:59.875
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:43:59.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4570" for this suite. 01/18/23 21:43:59.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:43:59.961
Jan 18 21:43:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename cronjob 01/18/23 21:43:59.962
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:00.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:00.673
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/18/23 21:44:00.676
STEP: Ensuring more than one job is running at a time 01/18/23 21:44:00.689
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 21:46:00.694
STEP: Removing cronjob 01/18/23 21:46:00.698
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:00.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3445" for this suite. 01/18/23 21:46:00.717
------------------------------
• [SLOW TEST] [120.785 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:43:59.961
    Jan 18 21:43:59.961: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:43:59.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:44:00.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:44:00.673
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/18/23 21:44:00.676
    STEP: Ensuring more than one job is running at a time 01/18/23 21:44:00.689
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 21:46:00.694
    STEP: Removing cronjob 01/18/23 21:46:00.698
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:00.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3445" for this suite. 01/18/23 21:46:00.717
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:00.748
Jan 18 21:46:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context-test 01/18/23 21:46:00.75
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:00.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:00.886
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan 18 21:46:00.918: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea" in namespace "security-context-test-7945" to be "Succeeded or Failed"
Jan 18 21:46:00.924: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.322471ms
Jan 18 21:46:02.930: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011995485s
Jan 18 21:46:04.929: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011204189s
Jan 18 21:46:04.929: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea" satisfied condition "Succeeded or Failed"
Jan 18 21:46:04.948: INFO: Got logs for pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:04.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7945" for this suite. 01/18/23 21:46:04.952
------------------------------
• [4.216 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:00.748
    Jan 18 21:46:00.749: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context-test 01/18/23 21:46:00.75
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:00.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:00.886
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan 18 21:46:00.918: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea" in namespace "security-context-test-7945" to be "Succeeded or Failed"
    Jan 18 21:46:00.924: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.322471ms
    Jan 18 21:46:02.930: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011995485s
    Jan 18 21:46:04.929: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011204189s
    Jan 18 21:46:04.929: INFO: Pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea" satisfied condition "Succeeded or Failed"
    Jan 18 21:46:04.948: INFO: Got logs for pod "busybox-privileged-false-a6d0dd88-98c4-4921-b18c-2094c61d2aea": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:04.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7945" for this suite. 01/18/23 21:46:04.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:04.971
Jan 18 21:46:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:46:04.972
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:05.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:05.603
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/18/23 21:46:05.605
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 21:46:05.615
STEP: patching the secret 01/18/23 21:46:05.619
STEP: deleting the secret using a LabelSelector 01/18/23 21:46:05.634
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 21:46:05.648
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1599" for this suite. 01/18/23 21:46:05.656
------------------------------
• [0.697 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:04.971
    Jan 18 21:46:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:46:04.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:05.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:05.603
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/18/23 21:46:05.605
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 21:46:05.615
    STEP: patching the secret 01/18/23 21:46:05.619
    STEP: deleting the secret using a LabelSelector 01/18/23 21:46:05.634
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 21:46:05.648
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:05.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1599" for this suite. 01/18/23 21:46:05.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:05.67
Jan 18 21:46:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:46:05.671
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:05.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:05.874
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-f62402c7-7a55-4631-b270-9e5349645e33 01/18/23 21:46:05.877
STEP: Creating a pod to test consume secrets 01/18/23 21:46:05.892
Jan 18 21:46:05.914: INFO: Waiting up to 5m0s for pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34" in namespace "secrets-3267" to be "Succeeded or Failed"
Jan 18 21:46:05.919: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.54995ms
Jan 18 21:46:07.926: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011251937s
Jan 18 21:46:09.924: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009349891s
STEP: Saw pod success 01/18/23 21:46:09.924
Jan 18 21:46:09.924: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34" satisfied condition "Succeeded or Failed"
Jan 18 21:46:09.928: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:46:09.935
Jan 18 21:46:09.961: INFO: Waiting for pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 to disappear
Jan 18 21:46:09.967: INFO: Pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:09.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3267" for this suite. 01/18/23 21:46:09.971
------------------------------
• [4.312 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:05.67
    Jan 18 21:46:05.670: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:46:05.671
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:05.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:05.874
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-f62402c7-7a55-4631-b270-9e5349645e33 01/18/23 21:46:05.877
    STEP: Creating a pod to test consume secrets 01/18/23 21:46:05.892
    Jan 18 21:46:05.914: INFO: Waiting up to 5m0s for pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34" in namespace "secrets-3267" to be "Succeeded or Failed"
    Jan 18 21:46:05.919: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.54995ms
    Jan 18 21:46:07.926: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011251937s
    Jan 18 21:46:09.924: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009349891s
    STEP: Saw pod success 01/18/23 21:46:09.924
    Jan 18 21:46:09.924: INFO: Pod "pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34" satisfied condition "Succeeded or Failed"
    Jan 18 21:46:09.928: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:46:09.935
    Jan 18 21:46:09.961: INFO: Waiting for pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 to disappear
    Jan 18 21:46:09.967: INFO: Pod pod-secrets-6ffb79b7-a392-49ec-890d-2c3b64ff1e34 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:09.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3267" for this suite. 01/18/23 21:46:09.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:09.984
Jan 18 21:46:09.984: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:46:09.985
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:10.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:10.185
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-acb54795-476d-49b4-90a0-f9b2e1ea345d 01/18/23 21:46:10.187
STEP: Creating a pod to test consume secrets 01/18/23 21:46:10.2
Jan 18 21:46:10.219: INFO: Waiting up to 5m0s for pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937" in namespace "secrets-296" to be "Succeeded or Failed"
Jan 18 21:46:10.224: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064855ms
Jan 18 21:46:12.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010089898s
Jan 18 21:46:14.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009690283s
STEP: Saw pod success 01/18/23 21:46:14.229
Jan 18 21:46:14.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937" satisfied condition "Succeeded or Failed"
Jan 18 21:46:14.232: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 21:46:14.239
Jan 18 21:46:14.264: INFO: Waiting for pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 to disappear
Jan 18 21:46:14.270: INFO: Pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:14.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-296" for this suite. 01/18/23 21:46:14.275
------------------------------
• [4.302 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:09.984
    Jan 18 21:46:09.984: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:46:09.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:10.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:10.185
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-acb54795-476d-49b4-90a0-f9b2e1ea345d 01/18/23 21:46:10.187
    STEP: Creating a pod to test consume secrets 01/18/23 21:46:10.2
    Jan 18 21:46:10.219: INFO: Waiting up to 5m0s for pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937" in namespace "secrets-296" to be "Succeeded or Failed"
    Jan 18 21:46:10.224: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064855ms
    Jan 18 21:46:12.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010089898s
    Jan 18 21:46:14.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009690283s
    STEP: Saw pod success 01/18/23 21:46:14.229
    Jan 18 21:46:14.229: INFO: Pod "pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937" satisfied condition "Succeeded or Failed"
    Jan 18 21:46:14.232: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 21:46:14.239
    Jan 18 21:46:14.264: INFO: Waiting for pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 to disappear
    Jan 18 21:46:14.270: INFO: Pod pod-secrets-0cccde1d-e24a-450a-9e6d-e5a9334c8937 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:14.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-296" for this suite. 01/18/23 21:46:14.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:14.288
Jan 18 21:46:14.288: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 21:46:14.289
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:14.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:14.874
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/18/23 21:46:14.879
Jan 18 21:46:14.899: INFO: Waiting up to 5m0s for pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605" in namespace "var-expansion-3252" to be "Succeeded or Failed"
Jan 18 21:46:14.903: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Pending", Reason="", readiness=false. Elapsed: 3.596439ms
Jan 18 21:46:16.908: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009028787s
Jan 18 21:46:18.909: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009387664s
STEP: Saw pod success 01/18/23 21:46:18.909
Jan 18 21:46:18.909: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605" satisfied condition "Succeeded or Failed"
Jan 18 21:46:18.913: INFO: Trying to get logs from node test-vm-1 pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:46:18.919
Jan 18 21:46:18.943: INFO: Waiting for pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 to disappear
Jan 18 21:46:18.948: INFO: Pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:18.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3252" for this suite. 01/18/23 21:46:18.954
------------------------------
• [4.678 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:14.288
    Jan 18 21:46:14.288: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 21:46:14.289
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:14.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:14.874
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/18/23 21:46:14.879
    Jan 18 21:46:14.899: INFO: Waiting up to 5m0s for pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605" in namespace "var-expansion-3252" to be "Succeeded or Failed"
    Jan 18 21:46:14.903: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Pending", Reason="", readiness=false. Elapsed: 3.596439ms
    Jan 18 21:46:16.908: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009028787s
    Jan 18 21:46:18.909: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009387664s
    STEP: Saw pod success 01/18/23 21:46:18.909
    Jan 18 21:46:18.909: INFO: Pod "var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605" satisfied condition "Succeeded or Failed"
    Jan 18 21:46:18.913: INFO: Trying to get logs from node test-vm-1 pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:46:18.919
    Jan 18 21:46:18.943: INFO: Waiting for pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 to disappear
    Jan 18 21:46:18.948: INFO: Pod var-expansion-109eb129-065b-4c30-a65d-ee7e7cabf605 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:18.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3252" for this suite. 01/18/23 21:46:18.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:18.966
Jan 18 21:46:18.966: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 21:46:18.967
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:19.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:19.874
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8950 01/18/23 21:46:19.877
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan 18 21:46:19.912: INFO: Found 0 stateful pods, waiting for 1
Jan 18 21:46:29.918: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/18/23 21:46:29.924
W0118 21:46:29.936135      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 21:46:29.943: INFO: Found 1 stateful pods, waiting for 2
Jan 18 21:46:39.949: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 21:46:39.949: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/18/23 21:46:39.958
STEP: Delete all of the StatefulSets 01/18/23 21:46:39.963
STEP: Verify that StatefulSets have been deleted 01/18/23 21:46:39.981
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 21:46:39.987: INFO: Deleting all statefulset in ns statefulset-8950
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8950" for this suite. 01/18/23 21:46:40.021
------------------------------
• [SLOW TEST] [21.087 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:18.966
    Jan 18 21:46:18.966: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 21:46:18.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:19.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:19.874
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8950 01/18/23 21:46:19.877
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan 18 21:46:19.912: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 21:46:29.918: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/18/23 21:46:29.924
    W0118 21:46:29.936135      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 21:46:29.943: INFO: Found 1 stateful pods, waiting for 2
    Jan 18 21:46:39.949: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 21:46:39.949: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/18/23 21:46:39.958
    STEP: Delete all of the StatefulSets 01/18/23 21:46:39.963
    STEP: Verify that StatefulSets have been deleted 01/18/23 21:46:39.981
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 21:46:39.987: INFO: Deleting all statefulset in ns statefulset-8950
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:40.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8950" for this suite. 01/18/23 21:46:40.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:40.056
Jan 18 21:46:40.056: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 21:46:40.057
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:40.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:40.232
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 21:46:40.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4415" for this suite. 01/18/23 21:46:40.335
------------------------------
• [0.291 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:40.056
    Jan 18 21:46:40.056: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 21:46:40.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:40.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:40.232
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:46:40.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4415" for this suite. 01/18/23 21:46:40.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:46:40.347
Jan 18 21:46:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:46:40.348
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:40.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:40.445
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/18/23 21:46:57.454
STEP: Creating a ResourceQuota 01/18/23 21:47:02.463
STEP: Ensuring resource quota status is calculated 01/18/23 21:47:02.475
STEP: Creating a ConfigMap 01/18/23 21:47:04.481
STEP: Ensuring resource quota status captures configMap creation 01/18/23 21:47:04.502
STEP: Deleting a ConfigMap 01/18/23 21:47:06.513
STEP: Ensuring resource quota status released usage 01/18/23 21:47:06.526
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:08.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8514" for this suite. 01/18/23 21:47:08.535
------------------------------
• [SLOW TEST] [28.201 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:46:40.347
    Jan 18 21:46:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:46:40.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:46:40.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:46:40.445
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/18/23 21:46:57.454
    STEP: Creating a ResourceQuota 01/18/23 21:47:02.463
    STEP: Ensuring resource quota status is calculated 01/18/23 21:47:02.475
    STEP: Creating a ConfigMap 01/18/23 21:47:04.481
    STEP: Ensuring resource quota status captures configMap creation 01/18/23 21:47:04.502
    STEP: Deleting a ConfigMap 01/18/23 21:47:06.513
    STEP: Ensuring resource quota status released usage 01/18/23 21:47:06.526
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:08.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8514" for this suite. 01/18/23 21:47:08.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:08.552
Jan 18 21:47:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:47:08.554
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:08.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:08.665
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/18/23 21:47:08.668
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local;sleep 1; done
 01/18/23 21:47:08.68
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local;sleep 1; done
 01/18/23 21:47:08.68
STEP: creating a pod to probe DNS 01/18/23 21:47:08.68
STEP: submitting the pod to kubernetes 01/18/23 21:47:08.68
Jan 18 21:47:08.695: INFO: Waiting up to 15m0s for pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf" in namespace "dns-8109" to be "running"
Jan 18 21:47:08.700: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084356ms
Jan 18 21:47:10.706: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011217237s
Jan 18 21:47:10.706: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:47:10.706
STEP: looking for the results for each expected name from probers 01/18/23 21:47:10.71
Jan 18 21:47:10.715: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.719: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.730: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.734: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.738: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.741: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
Jan 18 21:47:10.741: INFO: Lookups using dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local]

Jan 18 21:47:15.776: INFO: DNS probes using dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf succeeded

STEP: deleting the pod 01/18/23 21:47:15.776
STEP: deleting the test headless service 01/18/23 21:47:15.819
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8109" for this suite. 01/18/23 21:47:15.872
------------------------------
• [SLOW TEST] [7.354 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:08.552
    Jan 18 21:47:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:47:08.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:08.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:08.665
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/18/23 21:47:08.668
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local;sleep 1; done
     01/18/23 21:47:08.68
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8109.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local;sleep 1; done
     01/18/23 21:47:08.68
    STEP: creating a pod to probe DNS 01/18/23 21:47:08.68
    STEP: submitting the pod to kubernetes 01/18/23 21:47:08.68
    Jan 18 21:47:08.695: INFO: Waiting up to 15m0s for pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf" in namespace "dns-8109" to be "running"
    Jan 18 21:47:08.700: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.084356ms
    Jan 18 21:47:10.706: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.011217237s
    Jan 18 21:47:10.706: INFO: Pod "dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:47:10.706
    STEP: looking for the results for each expected name from probers 01/18/23 21:47:10.71
    Jan 18 21:47:10.715: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.719: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.723: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.726: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.730: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.734: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.738: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.741: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local from pod dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf: the server could not find the requested resource (get pods dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf)
    Jan 18 21:47:10.741: INFO: Lookups using dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8109.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8109.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8109.svc.cluster.local jessie_udp@dns-test-service-2.dns-8109.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8109.svc.cluster.local]

    Jan 18 21:47:15.776: INFO: DNS probes using dns-8109/dns-test-9c19cae0-aaee-4438-a7c5-f6e1b6d100bf succeeded

    STEP: deleting the pod 01/18/23 21:47:15.776
    STEP: deleting the test headless service 01/18/23 21:47:15.819
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:15.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8109" for this suite. 01/18/23 21:47:15.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:15.909
Jan 18 21:47:15.910: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 21:47:15.91
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:16.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:16.459
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/18/23 21:47:16.463
STEP: getting 01/18/23 21:47:16.493
STEP: listing in namespace 01/18/23 21:47:16.5
STEP: patching 01/18/23 21:47:16.511
STEP: deleting 01/18/23 21:47:16.53
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:16.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-3710" for this suite. 01/18/23 21:47:16.566
------------------------------
• [0.670 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:15.909
    Jan 18 21:47:15.910: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 21:47:15.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:16.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:16.459
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/18/23 21:47:16.463
    STEP: getting 01/18/23 21:47:16.493
    STEP: listing in namespace 01/18/23 21:47:16.5
    STEP: patching 01/18/23 21:47:16.511
    STEP: deleting 01/18/23 21:47:16.53
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:16.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-3710" for this suite. 01/18/23 21:47:16.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:16.581
Jan 18 21:47:16.581: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:47:16.582
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:16.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:16.728
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:47:16.732
Jan 18 21:47:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 21:47:16.829: INFO: stderr: ""
Jan 18 21:47:16.829: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 21:47:16.83
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 21:47:21.881
Jan 18 21:47:21.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 get pod e2e-test-httpd-pod -o json'
Jan 18 21:47:21.977: INFO: stderr: ""
Jan 18 21:47:21.977: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8bec13a3a6edf7f03e2d1e5f0dfefa878ad9ceb6dc62bb2d8eba25248fa9ab1a\",\n            \"cni.projectcalico.org/podIP\": \"10.1.192.7/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.1.192.7/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T21:47:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8422\",\n        \"resourceVersion\": \"20782\",\n        \"uid\": \"010c3827-831d-4552-823c-29aa081c5198\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-54zln\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"test-vm-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-54zln\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ae8887e2d3ca2847ef81611a5162fcc807a35b23882d5d1d009933338304c4c9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T21:47:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.192.7\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.1.192.7\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T21:47:16Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/18/23 21:47:21.978
Jan 18 21:47:21.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 replace -f -'
Jan 18 21:47:22.974: INFO: stderr: ""
Jan 18 21:47:22.974: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/18/23 21:47:22.974
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan 18 21:47:22.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 delete pods e2e-test-httpd-pod'
Jan 18 21:47:24.631: INFO: stderr: ""
Jan 18 21:47:24.631: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:24.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8422" for this suite. 01/18/23 21:47:24.636
------------------------------
• [SLOW TEST] [8.072 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:16.581
    Jan 18 21:47:16.581: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:47:16.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:16.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:16.728
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 21:47:16.732
    Jan 18 21:47:16.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 21:47:16.829: INFO: stderr: ""
    Jan 18 21:47:16.829: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 21:47:16.83
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 21:47:21.881
    Jan 18 21:47:21.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 get pod e2e-test-httpd-pod -o json'
    Jan 18 21:47:21.977: INFO: stderr: ""
    Jan 18 21:47:21.977: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8bec13a3a6edf7f03e2d1e5f0dfefa878ad9ceb6dc62bb2d8eba25248fa9ab1a\",\n            \"cni.projectcalico.org/podIP\": \"10.1.192.7/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.1.192.7/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T21:47:16Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8422\",\n        \"resourceVersion\": \"20782\",\n        \"uid\": \"010c3827-831d-4552-823c-29aa081c5198\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-54zln\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"test-vm-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-54zln\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:18Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:18Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T21:47:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://ae8887e2d3ca2847ef81611a5162fcc807a35b23882d5d1d009933338304c4c9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T21:47:17Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.0.4\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.192.7\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.1.192.7\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T21:47:16Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/18/23 21:47:21.978
    Jan 18 21:47:21.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 replace -f -'
    Jan 18 21:47:22.974: INFO: stderr: ""
    Jan 18 21:47:22.974: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/18/23 21:47:22.974
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan 18 21:47:22.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-8422 delete pods e2e-test-httpd-pod'
    Jan 18 21:47:24.631: INFO: stderr: ""
    Jan 18 21:47:24.631: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:24.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8422" for this suite. 01/18/23 21:47:24.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:24.654
Jan 18 21:47:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 21:47:24.655
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:25.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:25.775
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/18/23 21:47:25.778
STEP: getting 01/18/23 21:47:25.823
STEP: listing 01/18/23 21:47:25.83
STEP: deleting 01/18/23 21:47:25.833
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-970" for this suite. 01/18/23 21:47:25.88
------------------------------
• [1.244 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:24.654
    Jan 18 21:47:24.654: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename csiinlinevolumes 01/18/23 21:47:24.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:25.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:25.775
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/18/23 21:47:25.778
    STEP: getting 01/18/23 21:47:25.823
    STEP: listing 01/18/23 21:47:25.83
    STEP: deleting 01/18/23 21:47:25.833
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-970" for this suite. 01/18/23 21:47:25.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:25.901
Jan 18 21:47:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:47:25.902
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:26.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:26.809
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/18/23 21:47:26.816
STEP: waiting for available Endpoint 01/18/23 21:47:26.838
STEP: listing all Endpoints 01/18/23 21:47:26.839
STEP: updating the Endpoint 01/18/23 21:47:26.843
STEP: fetching the Endpoint 01/18/23 21:47:26.877
STEP: patching the Endpoint 01/18/23 21:47:26.882
STEP: fetching the Endpoint 01/18/23 21:47:26.904
STEP: deleting the Endpoint by Collection 01/18/23 21:47:26.908
STEP: waiting for Endpoint deletion 01/18/23 21:47:26.923
STEP: fetching the Endpoint 01/18/23 21:47:26.925
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:26.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9269" for this suite. 01/18/23 21:47:26.934
------------------------------
• [1.061 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:25.901
    Jan 18 21:47:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:47:25.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:26.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:26.809
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/18/23 21:47:26.816
    STEP: waiting for available Endpoint 01/18/23 21:47:26.838
    STEP: listing all Endpoints 01/18/23 21:47:26.839
    STEP: updating the Endpoint 01/18/23 21:47:26.843
    STEP: fetching the Endpoint 01/18/23 21:47:26.877
    STEP: patching the Endpoint 01/18/23 21:47:26.882
    STEP: fetching the Endpoint 01/18/23 21:47:26.904
    STEP: deleting the Endpoint by Collection 01/18/23 21:47:26.908
    STEP: waiting for Endpoint deletion 01/18/23 21:47:26.923
    STEP: fetching the Endpoint 01/18/23 21:47:26.925
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:26.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9269" for this suite. 01/18/23 21:47:26.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:26.964
Jan 18 21:47:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename cronjob 01/18/23 21:47:26.965
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:27.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:27.873
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/18/23 21:47:27.875
STEP: creating 01/18/23 21:47:27.876
STEP: getting 01/18/23 21:47:27.896
STEP: listing 01/18/23 21:47:27.901
STEP: watching 01/18/23 21:47:27.905
Jan 18 21:47:27.906: INFO: starting watch
STEP: cluster-wide listing 01/18/23 21:47:27.907
STEP: cluster-wide watching 01/18/23 21:47:27.91
Jan 18 21:47:27.910: INFO: starting watch
STEP: patching 01/18/23 21:47:27.912
STEP: updating 01/18/23 21:47:27.93
Jan 18 21:47:27.948: INFO: waiting for watch events with expected annotations
Jan 18 21:47:27.948: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 21:47:27.948
STEP: updating /status 01/18/23 21:47:27.965
STEP: get /status 01/18/23 21:47:27.975
STEP: deleting 01/18/23 21:47:27.978
STEP: deleting a collection 01/18/23 21:47:28.025
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:28.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5634" for this suite. 01/18/23 21:47:28.058
------------------------------
• [1.113 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:26.964
    Jan 18 21:47:26.964: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename cronjob 01/18/23 21:47:26.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:27.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:27.873
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/18/23 21:47:27.875
    STEP: creating 01/18/23 21:47:27.876
    STEP: getting 01/18/23 21:47:27.896
    STEP: listing 01/18/23 21:47:27.901
    STEP: watching 01/18/23 21:47:27.905
    Jan 18 21:47:27.906: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 21:47:27.907
    STEP: cluster-wide watching 01/18/23 21:47:27.91
    Jan 18 21:47:27.910: INFO: starting watch
    STEP: patching 01/18/23 21:47:27.912
    STEP: updating 01/18/23 21:47:27.93
    Jan 18 21:47:27.948: INFO: waiting for watch events with expected annotations
    Jan 18 21:47:27.948: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 21:47:27.948
    STEP: updating /status 01/18/23 21:47:27.965
    STEP: get /status 01/18/23 21:47:27.975
    STEP: deleting 01/18/23 21:47:27.978
    STEP: deleting a collection 01/18/23 21:47:28.025
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:28.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5634" for this suite. 01/18/23 21:47:28.058
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:28.077
Jan 18 21:47:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename watch 01/18/23 21:47:28.079
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:28.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:28.43
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/18/23 21:47:28.434
STEP: starting a background goroutine to produce watch events 01/18/23 21:47:28.441
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 21:47:28.441
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-390" for this suite. 01/18/23 21:47:31.181
------------------------------
• [3.171 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:28.077
    Jan 18 21:47:28.077: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename watch 01/18/23 21:47:28.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:28.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:28.43
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/18/23 21:47:28.434
    STEP: starting a background goroutine to produce watch events 01/18/23 21:47:28.441
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 21:47:28.441
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:31.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-390" for this suite. 01/18/23 21:47:31.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:31.25
Jan 18 21:47:31.250: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:47:31.251
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:31.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:31.461
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/18/23 21:47:31.464
STEP: Creating a ResourceQuota 01/18/23 21:47:36.478
STEP: Ensuring resource quota status is calculated 01/18/23 21:47:36.5
STEP: Creating a Service 01/18/23 21:47:38.505
STEP: Creating a NodePort Service 01/18/23 21:47:38.558
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 21:47:38.635
STEP: Ensuring resource quota status captures service creation 01/18/23 21:47:38.715
STEP: Deleting Services 01/18/23 21:47:40.72
STEP: Ensuring resource quota status released usage 01/18/23 21:47:40.803
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2553" for this suite. 01/18/23 21:47:42.812
------------------------------
• [SLOW TEST] [11.573 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:31.25
    Jan 18 21:47:31.250: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:47:31.251
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:31.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:31.461
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/18/23 21:47:31.464
    STEP: Creating a ResourceQuota 01/18/23 21:47:36.478
    STEP: Ensuring resource quota status is calculated 01/18/23 21:47:36.5
    STEP: Creating a Service 01/18/23 21:47:38.505
    STEP: Creating a NodePort Service 01/18/23 21:47:38.558
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 21:47:38.635
    STEP: Ensuring resource quota status captures service creation 01/18/23 21:47:38.715
    STEP: Deleting Services 01/18/23 21:47:40.72
    STEP: Ensuring resource quota status released usage 01/18/23 21:47:40.803
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:42.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2553" for this suite. 01/18/23 21:47:42.812
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:42.823
Jan 18 21:47:42.823: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption 01/18/23 21:47:42.825
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:43.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:43.399
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/18/23 21:47:43.413
STEP: Waiting for all pods to be running 01/18/23 21:47:45.468
Jan 18 21:47:45.473: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9266" for this suite. 01/18/23 21:47:49.492
------------------------------
• [SLOW TEST] [6.681 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:42.823
    Jan 18 21:47:42.823: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption 01/18/23 21:47:42.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:43.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:43.399
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/18/23 21:47:43.413
    STEP: Waiting for all pods to be running 01/18/23 21:47:45.468
    Jan 18 21:47:45.473: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:49.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9266" for this suite. 01/18/23 21:47:49.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:49.505
Jan 18 21:47:49.505: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:47:49.506
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:49.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:49.689
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan 18 21:47:49.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 create -f -'
Jan 18 21:47:49.976: INFO: stderr: ""
Jan 18 21:47:49.976: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 21:47:49.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 create -f -'
Jan 18 21:47:50.236: INFO: stderr: ""
Jan 18 21:47:50.236: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 21:47:50.236
Jan 18 21:47:51.243: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:47:51.243: INFO: Found 0 / 1
Jan 18 21:47:52.241: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:47:52.241: INFO: Found 1 / 1
Jan 18 21:47:52.241: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 21:47:52.245: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 21:47:52.245: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 21:47:52.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe pod agnhost-primary-vfv54'
Jan 18 21:47:52.365: INFO: stderr: ""
Jan 18 21:47:52.365: INFO: stdout: "Name:             agnhost-primary-vfv54\nNamespace:        kubectl-4753\nPriority:         0\nService Account:  default\nNode:             test-vm-2/10.0.0.5\nStart Time:       Wed, 18 Jan 2023 21:47:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 60b646f3d1376690b2f4068a3d39d910854ac58607a3767a369533b47554e0ec\n                  cni.projectcalico.org/podIP: 10.1.132.45/32\n                  cni.projectcalico.org/podIPs: 10.1.132.45/32\nStatus:           Running\nIP:               10.1.132.45\nIPs:\n  IP:           10.1.132.45\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://941db8b68953265abe05af5ab6fb9fef24e3d961371137a5cbf12383639cc4cc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 21:47:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64psm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64psm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4753/agnhost-primary-vfv54 to test-vm-2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan 18 21:47:52.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe rc agnhost-primary'
Jan 18 21:47:52.476: INFO: stderr: ""
Jan 18 21:47:52.476: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4753\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-vfv54\n"
Jan 18 21:47:52.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe service agnhost-primary'
Jan 18 21:47:52.573: INFO: stderr: ""
Jan 18 21:47:52.573: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4753\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.97\nIPs:               10.152.183.97\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.1.132.45:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 21:47:52.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe node test-vm-1'
Jan 18 21:47:52.691: INFO: stderr: ""
Jan 18 21:47:52.691: INFO: stdout: "Name:               test-vm-1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=test-vm-1\n                    kubernetes.io/os=linux\n                    microk8s.io/cluster=true\n                    node.kubernetes.io/microk8s-controlplane=microk8s-controlplane\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.4/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.1.192.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 20:55:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  test-vm-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 21:47:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 20:57:31 +0000   Wed, 18 Jan 2023 20:57:31 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:59:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.4\n  Hostname:    test-vm-1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30298176Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386908Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  29249600Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16284508Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b31434c89edd4a48a0aa91471a798676\n  System UUID:                51b337f3-c236-46e1-bdaa-a00e8d1098c0\n  Boot ID:                    ef998f90-6d69-40ff-8aa7-64826dbb809b\n  Kernel Version:             5.15.0-1031-azure\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-j9rg6                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         50m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  cronjob-3445                concurrent-27901306-w952v                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         112s\n  csiinlinevolumes-3710       pod-csi-inline-volumes                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         36s\n  disruption-9266             pod-0                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7s\n  disruption-9266             pod-2                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                250m (6%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
Jan 18 21:47:52.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe namespace kubectl-4753'
Jan 18 21:47:52.789: INFO: stderr: ""
Jan 18 21:47:52.789: INFO: stdout: "Name:         kubectl-4753\nLabels:       e2e-framework=kubectl\n              e2e-run=de0ec6bb-1b40-473e-879d-9346f84b1ea8\n              kubernetes.io/metadata.name=kubectl-4753\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:52.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4753" for this suite. 01/18/23 21:47:52.794
------------------------------
• [3.301 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:49.505
    Jan 18 21:47:49.505: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:47:49.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:49.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:49.689
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan 18 21:47:49.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 create -f -'
    Jan 18 21:47:49.976: INFO: stderr: ""
    Jan 18 21:47:49.976: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 18 21:47:49.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 create -f -'
    Jan 18 21:47:50.236: INFO: stderr: ""
    Jan 18 21:47:50.236: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 21:47:50.236
    Jan 18 21:47:51.243: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:47:51.243: INFO: Found 0 / 1
    Jan 18 21:47:52.241: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:47:52.241: INFO: Found 1 / 1
    Jan 18 21:47:52.241: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 21:47:52.245: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 21:47:52.245: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 21:47:52.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe pod agnhost-primary-vfv54'
    Jan 18 21:47:52.365: INFO: stderr: ""
    Jan 18 21:47:52.365: INFO: stdout: "Name:             agnhost-primary-vfv54\nNamespace:        kubectl-4753\nPriority:         0\nService Account:  default\nNode:             test-vm-2/10.0.0.5\nStart Time:       Wed, 18 Jan 2023 21:47:50 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 60b646f3d1376690b2f4068a3d39d910854ac58607a3767a369533b47554e0ec\n                  cni.projectcalico.org/podIP: 10.1.132.45/32\n                  cni.projectcalico.org/podIPs: 10.1.132.45/32\nStatus:           Running\nIP:               10.1.132.45\nIPs:\n  IP:           10.1.132.45\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://941db8b68953265abe05af5ab6fb9fef24e3d961371137a5cbf12383639cc4cc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 21:47:51 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-64psm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-64psm:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-4753/agnhost-primary-vfv54 to test-vm-2\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan 18 21:47:52.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe rc agnhost-primary'
    Jan 18 21:47:52.476: INFO: stderr: ""
    Jan 18 21:47:52.476: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-4753\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-vfv54\n"
    Jan 18 21:47:52.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe service agnhost-primary'
    Jan 18 21:47:52.573: INFO: stderr: ""
    Jan 18 21:47:52.573: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-4753\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.152.183.97\nIPs:               10.152.183.97\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.1.132.45:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 18 21:47:52.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe node test-vm-1'
    Jan 18 21:47:52.691: INFO: stderr: ""
    Jan 18 21:47:52.691: INFO: stdout: "Name:               test-vm-1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=test-vm-1\n                    kubernetes.io/os=linux\n                    microk8s.io/cluster=true\n                    node.kubernetes.io/microk8s-controlplane=microk8s-controlplane\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.0.4/24\n                    projectcalico.org/IPv4VXLANTunnelAddr: 10.1.192.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 20:55:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  test-vm-1\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 21:47:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 18 Jan 2023 20:57:31 +0000   Wed, 18 Jan 2023 20:57:31 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:55:35 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 21:43:43 +0000   Wed, 18 Jan 2023 20:59:16 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.0.0.4\n  Hostname:    test-vm-1\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30298176Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16386908Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  29249600Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16284508Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 b31434c89edd4a48a0aa91471a798676\n  System UUID:                51b337f3-c236-46e1-bdaa-a00e8d1098c0\n  Boot ID:                    ef998f90-6d69-40ff-8aa7-64826dbb809b\n  Kernel Version:             5.15.0-1031-azure\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.26.0\n  Kube-Proxy Version:         v1.26.0\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-j9rg6                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         50m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\n  cronjob-3445                concurrent-27901306-w952v                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         112s\n  csiinlinevolumes-3710       pod-csi-inline-volumes                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         36s\n  disruption-9266             pod-0                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7s\n  disruption-9266             pod-2                                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         7s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         47m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                250m (6%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\nEvents:              <none>\n"
    Jan 18 21:47:52.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4753 describe namespace kubectl-4753'
    Jan 18 21:47:52.789: INFO: stderr: ""
    Jan 18 21:47:52.789: INFO: stdout: "Name:         kubectl-4753\nLabels:       e2e-framework=kubectl\n              e2e-run=de0ec6bb-1b40-473e-879d-9346f84b1ea8\n              kubernetes.io/metadata.name=kubectl-4753\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:52.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4753" for this suite. 01/18/23 21:47:52.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:52.807
Jan 18 21:47:52.807: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-runtime 01/18/23 21:47:52.809
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:53.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:53.111
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/18/23 21:47:53.114
STEP: wait for the container to reach Succeeded 01/18/23 21:47:53.129
STEP: get the container status 01/18/23 21:47:56.152
STEP: the container should be terminated 01/18/23 21:47:56.155
STEP: the termination message should be set 01/18/23 21:47:56.155
Jan 18 21:47:56.155: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/18/23 21:47:56.156
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 21:47:56.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-2735" for this suite. 01/18/23 21:47:56.189
------------------------------
• [3.396 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:52.807
    Jan 18 21:47:52.807: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-runtime 01/18/23 21:47:52.809
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:53.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:53.111
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/18/23 21:47:53.114
    STEP: wait for the container to reach Succeeded 01/18/23 21:47:53.129
    STEP: get the container status 01/18/23 21:47:56.152
    STEP: the container should be terminated 01/18/23 21:47:56.155
    STEP: the termination message should be set 01/18/23 21:47:56.155
    Jan 18 21:47:56.155: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/18/23 21:47:56.156
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:47:56.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-2735" for this suite. 01/18/23 21:47:56.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:47:56.204
Jan 18 21:47:56.204: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:47:56.206
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:56.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:56.357
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 21:47:56.530: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:48:56.551: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:48:56.555
Jan 18 21:48:56.555: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 21:48:56.556
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:48:56.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:48:56.875
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/18/23 21:48:56.877
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 21:48:56.877
Jan 18 21:48:56.893: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1634" to be "running"
Jan 18 21:48:56.897: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891249ms
Jan 18 21:48:58.903: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009602075s
Jan 18 21:48:58.903: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 21:48:58.906
Jan 18 21:48:58.931: INFO: found a healthy node: test-vm-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan 18 21:49:09.067: INFO: pods created so far: [1 1 1]
Jan 18 21:49:09.067: INFO: length of pods created so far: 3
Jan 18 21:49:11.084: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan 18 21:49:18.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:49:18.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-1634" for this suite. 01/18/23 21:49:18.21
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1409" for this suite. 01/18/23 21:49:18.222
------------------------------
• [SLOW TEST] [82.031 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:47:56.204
    Jan 18 21:47:56.204: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:47:56.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:47:56.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:47:56.357
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 21:47:56.530: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:48:56.551: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:48:56.555
    Jan 18 21:48:56.555: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 21:48:56.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:48:56.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:48:56.875
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/18/23 21:48:56.877
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 21:48:56.877
    Jan 18 21:48:56.893: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1634" to be "running"
    Jan 18 21:48:56.897: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891249ms
    Jan 18 21:48:58.903: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009602075s
    Jan 18 21:48:58.903: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 21:48:58.906
    Jan 18 21:48:58.931: INFO: found a healthy node: test-vm-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan 18 21:49:09.067: INFO: pods created so far: [1 1 1]
    Jan 18 21:49:09.067: INFO: length of pods created so far: 3
    Jan 18 21:49:11.084: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:49:18.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:49:18.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-1634" for this suite. 01/18/23 21:49:18.21
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1409" for this suite. 01/18/23 21:49:18.222
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:49:18.236
Jan 18 21:49:18.236: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 21:49:18.237
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:18.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:18.877
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/18/23 21:49:18.885
Jan 18 21:49:18.911: INFO: Waiting up to 5m0s for pod "pod-2fhbb" in namespace "pods-2031" to be "running"
Jan 18 21:49:18.917: INFO: Pod "pod-2fhbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356773ms
Jan 18 21:49:20.921: INFO: Pod "pod-2fhbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009586388s
Jan 18 21:49:20.921: INFO: Pod "pod-2fhbb" satisfied condition "running"
STEP: patching /status 01/18/23 21:49:20.921
Jan 18 21:49:20.937: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 21:49:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2031" for this suite. 01/18/23 21:49:20.943
------------------------------
• [2.719 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:49:18.236
    Jan 18 21:49:18.236: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 21:49:18.237
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:18.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:18.877
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/18/23 21:49:18.885
    Jan 18 21:49:18.911: INFO: Waiting up to 5m0s for pod "pod-2fhbb" in namespace "pods-2031" to be "running"
    Jan 18 21:49:18.917: INFO: Pod "pod-2fhbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.356773ms
    Jan 18 21:49:20.921: INFO: Pod "pod-2fhbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009586388s
    Jan 18 21:49:20.921: INFO: Pod "pod-2fhbb" satisfied condition "running"
    STEP: patching /status 01/18/23 21:49:20.921
    Jan 18 21:49:20.937: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:49:20.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2031" for this suite. 01/18/23 21:49:20.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:49:20.956
Jan 18 21:49:20.956: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:49:20.957
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:21.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:21.362
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 21:49:21.392: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:50:21.412: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/18/23 21:50:21.415
Jan 18 21:50:21.457: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 21:50:21.472: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 21:50:21.507: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 21:50:21.522: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 21:50:21.522
Jan 18 21:50:21.522: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3430" to be "running"
Jan 18 21:50:21.526: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063157ms
Jan 18 21:50:23.568: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045636901s
Jan 18 21:50:25.532: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009794647s
Jan 18 21:50:25.532: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 21:50:25.532: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
Jan 18 21:50:25.535: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.338447ms
Jan 18 21:50:25.535: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 21:50:25.535: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
Jan 18 21:50:25.539: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.51515ms
Jan 18 21:50:25.539: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 21:50:25.539: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
Jan 18 21:50:25.542: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.565351ms
Jan 18 21:50:25.542: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 21:50:25.542
Jan 18 21:50:25.555: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3430" to be "running"
Jan 18 21:50:25.559: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329261ms
Jan 18 21:50:27.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011143013s
Jan 18 21:50:29.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010915166s
Jan 18 21:50:31.565: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009835307s
Jan 18 21:50:31.565: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:50:31.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3430" for this suite. 01/18/23 21:50:31.651
------------------------------
• [SLOW TEST] [70.706 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:49:20.956
    Jan 18 21:49:20.956: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 21:49:20.957
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:49:21.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:49:21.362
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 21:49:21.392: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:50:21.412: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/18/23 21:50:21.415
    Jan 18 21:50:21.457: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 21:50:21.472: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 21:50:21.507: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 21:50:21.522: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 21:50:21.522
    Jan 18 21:50:21.522: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3430" to be "running"
    Jan 18 21:50:21.526: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063157ms
    Jan 18 21:50:23.568: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045636901s
    Jan 18 21:50:25.532: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009794647s
    Jan 18 21:50:25.532: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 21:50:25.532: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
    Jan 18 21:50:25.535: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.338447ms
    Jan 18 21:50:25.535: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 21:50:25.535: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
    Jan 18 21:50:25.539: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.51515ms
    Jan 18 21:50:25.539: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 21:50:25.539: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3430" to be "running"
    Jan 18 21:50:25.542: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.565351ms
    Jan 18 21:50:25.542: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 21:50:25.542
    Jan 18 21:50:25.555: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3430" to be "running"
    Jan 18 21:50:25.559: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.329261ms
    Jan 18 21:50:27.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011143013s
    Jan 18 21:50:29.566: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010915166s
    Jan 18 21:50:31.565: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009835307s
    Jan 18 21:50:31.565: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:50:31.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3430" for this suite. 01/18/23 21:50:31.651
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:50:31.665
Jan 18 21:50:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:50:31.666
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.399
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/18/23 21:50:32.402
Jan 18 21:50:32.402: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4626 proxy --unix-socket=/tmp/kubectl-proxy-unix431895236/test'
STEP: retrieving proxy /api/ output 01/18/23 21:50:32.463
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:50:32.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4626" for this suite. 01/18/23 21:50:32.471
------------------------------
• [0.818 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:50:31.665
    Jan 18 21:50:31.665: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:50:31.666
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.399
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/18/23 21:50:32.402
    Jan 18 21:50:32.402: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4626 proxy --unix-socket=/tmp/kubectl-proxy-unix431895236/test'
    STEP: retrieving proxy /api/ output 01/18/23 21:50:32.463
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:50:32.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4626" for this suite. 01/18/23 21:50:32.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:50:32.483
Jan 18 21:50:32.484: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:50:32.485
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.874
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/18/23 21:50:32.876
Jan 18 21:50:32.895: INFO: Waiting up to 5m0s for pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456" in namespace "downward-api-2680" to be "Succeeded or Failed"
Jan 18 21:50:32.902: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Pending", Reason="", readiness=false. Elapsed: 7.705209ms
Jan 18 21:50:34.909: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014569236s
Jan 18 21:50:36.907: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012393403s
STEP: Saw pod success 01/18/23 21:50:36.907
Jan 18 21:50:36.907: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456" satisfied condition "Succeeded or Failed"
Jan 18 21:50:36.911: INFO: Trying to get logs from node test-vm-1 pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:50:36.933
Jan 18 21:50:36.974: INFO: Waiting for pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 to disappear
Jan 18 21:50:36.982: INFO: Pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:50:36.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2680" for this suite. 01/18/23 21:50:36.987
------------------------------
• [4.521 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:50:32.483
    Jan 18 21:50:32.484: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:50:32.485
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:32.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:32.874
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/18/23 21:50:32.876
    Jan 18 21:50:32.895: INFO: Waiting up to 5m0s for pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456" in namespace "downward-api-2680" to be "Succeeded or Failed"
    Jan 18 21:50:32.902: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Pending", Reason="", readiness=false. Elapsed: 7.705209ms
    Jan 18 21:50:34.909: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014569236s
    Jan 18 21:50:36.907: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012393403s
    STEP: Saw pod success 01/18/23 21:50:36.907
    Jan 18 21:50:36.907: INFO: Pod "downward-api-63aa0587-17d9-4700-8885-ee81f9d80456" satisfied condition "Succeeded or Failed"
    Jan 18 21:50:36.911: INFO: Trying to get logs from node test-vm-1 pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:50:36.933
    Jan 18 21:50:36.974: INFO: Waiting for pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 to disappear
    Jan 18 21:50:36.982: INFO: Pod downward-api-63aa0587-17d9-4700-8885-ee81f9d80456 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:50:36.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2680" for this suite. 01/18/23 21:50:36.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:50:37.007
Jan 18 21:50:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:50:37.008
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:37.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:37.44
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 18 21:50:37.462: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 21:50:42.468: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 21:50:42.468
Jan 18 21:50:42.468: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 21:50:42.486
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:50:42.498: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5629  67b08a39-d15c-4f3c-80c3-bc723bc11eab 21984 1 2023-01-18 21:50:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 21:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 21:50:42.507: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:50:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5629" for this suite. 01/18/23 21:50:42.515
------------------------------
• [SLOW TEST] [5.529 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:50:37.007
    Jan 18 21:50:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:50:37.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:37.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:37.44
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 18 21:50:37.462: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 18 21:50:42.468: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 21:50:42.468
    Jan 18 21:50:42.468: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 21:50:42.486
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:50:42.498: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5629  67b08a39-d15c-4f3c-80c3-bc723bc11eab 21984 1 2023-01-18 21:50:42 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 21:50:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d8db68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 21:50:42.507: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:50:42.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5629" for this suite. 01/18/23 21:50:42.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:50:42.538
Jan 18 21:50:42.538: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 21:50:42.539
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:42.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:42.613
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan 18 21:50:42.663: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:50:42.673
Jan 18 21:50:42.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:42.684: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:50:43.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:43.699: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 21:50:44.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:50:44.694: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:50:45.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:50:45.694: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/18/23 21:50:45.708
STEP: Check that daemon pods images are updated. 01/18/23 21:50:45.728
Jan 18 21:50:45.732: INFO: Wrong image for pod: daemon-set-gv9w8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:45.732: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:46.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:47.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:48.741: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:48.741: INFO: Pod daemon-set-q45ld is not available
Jan 18 21:50:49.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan 18 21:50:49.742: INFO: Pod daemon-set-q45ld is not available
Jan 18 21:50:51.741: INFO: Pod daemon-set-mcc9d is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 21:50:51.747
Jan 18 21:50:51.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:50:51.756: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:50:52.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 21:50:52.767: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 21:50:53.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 21:50:53.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:50:53.784
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1513, will wait for the garbage collector to delete the pods 01/18/23 21:50:53.784
Jan 18 21:50:53.851: INFO: Deleting DaemonSet.extensions daemon-set took: 13.622191ms
Jan 18 21:50:54.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.525449ms
Jan 18 21:50:56.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 21:50:56.062: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 21:50:56.068: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22148"},"items":null}

Jan 18 21:50:56.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:50:56.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1513" for this suite. 01/18/23 21:50:56.09
------------------------------
• [SLOW TEST] [13.564 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:50:42.538
    Jan 18 21:50:42.538: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 21:50:42.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:42.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:42.613
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan 18 21:50:42.663: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 21:50:42.673
    Jan 18 21:50:42.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:42.684: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:50:43.699: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:43.699: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 21:50:44.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:50:44.694: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:50:45.694: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:50:45.694: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/18/23 21:50:45.708
    STEP: Check that daemon pods images are updated. 01/18/23 21:50:45.728
    Jan 18 21:50:45.732: INFO: Wrong image for pod: daemon-set-gv9w8. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:45.732: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:46.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:47.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:48.741: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:48.741: INFO: Pod daemon-set-q45ld is not available
    Jan 18 21:50:49.742: INFO: Wrong image for pod: daemon-set-f58bc. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan 18 21:50:49.742: INFO: Pod daemon-set-q45ld is not available
    Jan 18 21:50:51.741: INFO: Pod daemon-set-mcc9d is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 21:50:51.747
    Jan 18 21:50:51.756: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:50:51.756: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:50:52.767: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 21:50:52.767: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 21:50:53.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 21:50:53.766: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 21:50:53.784
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1513, will wait for the garbage collector to delete the pods 01/18/23 21:50:53.784
    Jan 18 21:50:53.851: INFO: Deleting DaemonSet.extensions daemon-set took: 13.622191ms
    Jan 18 21:50:54.751: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.525449ms
    Jan 18 21:50:56.062: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 21:50:56.062: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 21:50:56.068: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22148"},"items":null}

    Jan 18 21:50:56.074: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22148"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:50:56.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1513" for this suite. 01/18/23 21:50:56.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:50:56.103
Jan 18 21:50:56.103: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:50:56.104
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:56.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:56.759
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/18/23 21:50:56.762
STEP: Creating a ResourceQuota 01/18/23 21:51:01.771
STEP: Ensuring resource quota status is calculated 01/18/23 21:51:01.783
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:03.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6372" for this suite. 01/18/23 21:51:03.795
------------------------------
• [SLOW TEST] [7.706 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:50:56.103
    Jan 18 21:50:56.103: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:50:56.104
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:50:56.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:50:56.759
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/18/23 21:50:56.762
    STEP: Creating a ResourceQuota 01/18/23 21:51:01.771
    STEP: Ensuring resource quota status is calculated 01/18/23 21:51:01.783
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:03.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6372" for this suite. 01/18/23 21:51:03.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:03.81
Jan 18 21:51:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 21:51:03.811
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:04.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:04.212
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/18/23 21:51:04.215
Jan 18 21:51:04.231: INFO: Waiting up to 5m0s for pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be" in namespace "downward-api-7725" to be "Succeeded or Failed"
Jan 18 21:51:04.235: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234459ms
Jan 18 21:51:06.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818202s
Jan 18 21:51:08.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009942427s
STEP: Saw pod success 01/18/23 21:51:08.241
Jan 18 21:51:08.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be" satisfied condition "Succeeded or Failed"
Jan 18 21:51:08.245: INFO: Trying to get logs from node test-vm-1 pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be container dapi-container: <nil>
STEP: delete the pod 01/18/23 21:51:08.253
Jan 18 21:51:08.279: INFO: Waiting for pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be to disappear
Jan 18 21:51:08.285: INFO: Pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7725" for this suite. 01/18/23 21:51:08.289
------------------------------
• [4.490 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:03.81
    Jan 18 21:51:03.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 21:51:03.811
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:04.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:04.212
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/18/23 21:51:04.215
    Jan 18 21:51:04.231: INFO: Waiting up to 5m0s for pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be" in namespace "downward-api-7725" to be "Succeeded or Failed"
    Jan 18 21:51:04.235: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.234459ms
    Jan 18 21:51:06.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009818202s
    Jan 18 21:51:08.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009942427s
    STEP: Saw pod success 01/18/23 21:51:08.241
    Jan 18 21:51:08.241: INFO: Pod "downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:08.245: INFO: Trying to get logs from node test-vm-1 pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be container dapi-container: <nil>
    STEP: delete the pod 01/18/23 21:51:08.253
    Jan 18 21:51:08.279: INFO: Waiting for pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be to disappear
    Jan 18 21:51:08.285: INFO: Pod downward-api-9f88af80-bbcc-4119-83f8-f1a5b01f95be no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7725" for this suite. 01/18/23 21:51:08.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:08.304
Jan 18 21:51:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:51:08.306
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:08.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:08.866
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:51:08.869
Jan 18 21:51:08.892: INFO: Waiting up to 5m0s for pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5" in namespace "emptydir-8167" to be "Succeeded or Failed"
Jan 18 21:51:08.896: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31546ms
Jan 18 21:51:10.901: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009061849s
Jan 18 21:51:12.903: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010857198s
STEP: Saw pod success 01/18/23 21:51:12.903
Jan 18 21:51:12.903: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5" satisfied condition "Succeeded or Failed"
Jan 18 21:51:12.907: INFO: Trying to get logs from node test-vm-1 pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 container test-container: <nil>
STEP: delete the pod 01/18/23 21:51:12.915
Jan 18 21:51:12.947: INFO: Waiting for pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 to disappear
Jan 18 21:51:12.955: INFO: Pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:12.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8167" for this suite. 01/18/23 21:51:12.962
------------------------------
• [4.687 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:08.304
    Jan 18 21:51:08.304: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:51:08.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:08.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:08.866
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 21:51:08.869
    Jan 18 21:51:08.892: INFO: Waiting up to 5m0s for pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5" in namespace "emptydir-8167" to be "Succeeded or Failed"
    Jan 18 21:51:08.896: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31546ms
    Jan 18 21:51:10.901: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009061849s
    Jan 18 21:51:12.903: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010857198s
    STEP: Saw pod success 01/18/23 21:51:12.903
    Jan 18 21:51:12.903: INFO: Pod "pod-77beaa91-6679-4cf8-8639-4e2550fce0b5" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:12.907: INFO: Trying to get logs from node test-vm-1 pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 container test-container: <nil>
    STEP: delete the pod 01/18/23 21:51:12.915
    Jan 18 21:51:12.947: INFO: Waiting for pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 to disappear
    Jan 18 21:51:12.955: INFO: Pod pod-77beaa91-6679-4cf8-8639-4e2550fce0b5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:12.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8167" for this suite. 01/18/23 21:51:12.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:12.995
Jan 18 21:51:12.995: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:51:12.996
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:13.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:13.339
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-5e8a474d-4fb0-4317-8502-476357920fd3 01/18/23 21:51:13.342
STEP: Creating a pod to test consume configMaps 01/18/23 21:51:13.351
Jan 18 21:51:13.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669" in namespace "projected-1111" to be "Succeeded or Failed"
Jan 18 21:51:13.371: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710552ms
Jan 18 21:51:15.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009974344s
Jan 18 21:51:17.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010374941s
STEP: Saw pod success 01/18/23 21:51:17.377
Jan 18 21:51:17.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669" satisfied condition "Succeeded or Failed"
Jan 18 21:51:17.381: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:51:17.39
Jan 18 21:51:17.415: INFO: Waiting for pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 to disappear
Jan 18 21:51:17.423: INFO: Pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1111" for this suite. 01/18/23 21:51:17.428
------------------------------
• [4.445 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:12.995
    Jan 18 21:51:12.995: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:51:12.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:13.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:13.339
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-5e8a474d-4fb0-4317-8502-476357920fd3 01/18/23 21:51:13.342
    STEP: Creating a pod to test consume configMaps 01/18/23 21:51:13.351
    Jan 18 21:51:13.367: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669" in namespace "projected-1111" to be "Succeeded or Failed"
    Jan 18 21:51:13.371: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Pending", Reason="", readiness=false. Elapsed: 3.710552ms
    Jan 18 21:51:15.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009974344s
    Jan 18 21:51:17.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010374941s
    STEP: Saw pod success 01/18/23 21:51:17.377
    Jan 18 21:51:17.377: INFO: Pod "pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:17.381: INFO: Trying to get logs from node test-vm-1 pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:51:17.39
    Jan 18 21:51:17.415: INFO: Waiting for pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 to disappear
    Jan 18 21:51:17.423: INFO: Pod pod-projected-configmaps-45c0e9c7-d143-40bc-8f68-cee8865fb669 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:17.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1111" for this suite. 01/18/23 21:51:17.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:17.44
Jan 18 21:51:17.440: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:51:17.442
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:17.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:17.861
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-818a4496-2758-4813-9102-9538d8f50e0c 01/18/23 21:51:17.863
STEP: Creating a pod to test consume configMaps 01/18/23 21:51:17.873
Jan 18 21:51:17.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1" in namespace "configmap-6538" to be "Succeeded or Failed"
Jan 18 21:51:17.891: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.676046ms
Jan 18 21:51:19.899: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011309034s
Jan 18 21:51:21.896: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008355989s
STEP: Saw pod success 01/18/23 21:51:21.896
Jan 18 21:51:21.896: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1" satisfied condition "Succeeded or Failed"
Jan 18 21:51:21.900: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:51:21.908
Jan 18 21:51:21.946: INFO: Waiting for pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 to disappear
Jan 18 21:51:21.953: INFO: Pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6538" for this suite. 01/18/23 21:51:21.959
------------------------------
• [4.537 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:17.44
    Jan 18 21:51:17.440: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:51:17.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:17.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:17.861
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-818a4496-2758-4813-9102-9538d8f50e0c 01/18/23 21:51:17.863
    STEP: Creating a pod to test consume configMaps 01/18/23 21:51:17.873
    Jan 18 21:51:17.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1" in namespace "configmap-6538" to be "Succeeded or Failed"
    Jan 18 21:51:17.891: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.676046ms
    Jan 18 21:51:19.899: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011309034s
    Jan 18 21:51:21.896: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008355989s
    STEP: Saw pod success 01/18/23 21:51:21.896
    Jan 18 21:51:21.896: INFO: Pod "pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:21.900: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:51:21.908
    Jan 18 21:51:21.946: INFO: Waiting for pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 to disappear
    Jan 18 21:51:21.953: INFO: Pod pod-configmaps-d062ab4b-5221-44aa-98bf-05754146f7c1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6538" for this suite. 01/18/23 21:51:21.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:21.978
Jan 18 21:51:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 21:51:21.979
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:22.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:22.271
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/18/23 21:51:22.278
STEP: waiting for Deployment to be created 01/18/23 21:51:22.292
STEP: waiting for all Replicas to be Ready 01/18/23 21:51:22.293
Jan 18 21:51:22.294: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.295: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.818: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.818: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 21:51:24.871: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/18/23 21:51:24.871
W0118 21:51:24.890127      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 21:51:24.891: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/18/23 21:51:24.891
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:24.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:24.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:25.456: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:25.456: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:25.503: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:25.503: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:27.403: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:27.403: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:27.442: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
STEP: listing Deployments 01/18/23 21:51:27.442
Jan 18 21:51:27.451: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/18/23 21:51:27.451
Jan 18 21:51:27.482: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/18/23 21:51:27.482
Jan 18 21:51:27.492: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:27.775: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:27.775: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:27.832: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:27.877: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:27.952: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:29.045: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 21:51:32.012: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/18/23 21:51:32.872
STEP: fetching the DeploymentStatus 01/18/23 21:51:32.885
Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3
STEP: deleting the Deployment 01/18/23 21:51:32.894
Jan 18 21:51:32.918: INFO: observed event type MODIFIED
Jan 18 21:51:32.918: INFO: observed event type MODIFIED
Jan 18 21:51:32.918: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
Jan 18 21:51:32.919: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 21:51:32.926: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 21:51:32.931: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-680  56d40cf6-1447-4cd7-9939-22d9032cc695 22443 3 2023-01-18 21:51:22 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ed97 0xc004b0ed98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0ee38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 21:51:32.936: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-680  de85e122-2a87-4d75-9cf0-2fa89fea949c 22538 2 2023-01-18 21:51:27 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ee97 0xc004b0ee98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0ef38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 21:51:32.941: INFO: pod: "test-deployment-7b7876f9d6-2f4ln":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-2f4ln test-deployment-7b7876f9d6- deployment-680  ca58281b-bcd7-412f-8ad6-014b668de2cd 22501 0 2023-01-18 21:51:27 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:19973db08a5eb844c6364e81c13d7b8775561eb2989f09e8b5fbccdd54bdd21b cni.projectcalico.org/podIP:10.1.192.47/32 cni.projectcalico.org/podIPs:10.1.192.47/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de85e122-2a87-4d75-9cf0-2fa89fea949c 0xc004b0f547 0xc004b0f548}] [] [{kubelite Update v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de85e122-2a87-4d75-9cf0-2fa89fea949c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq692,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq692,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.47,StartTime:2023-01-18 21:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:51:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bb556174065a48265f0a68cad27a4f818e040edf7b1a9ebf487fdfdc0ea93881,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 21:51:32.941: INFO: pod: "test-deployment-7b7876f9d6-9qnjp":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-9qnjp test-deployment-7b7876f9d6- deployment-680  09b34b77-56e6-48a2-9de4-9f7ee6e5f458 22537 0 2023-01-18 21:51:29 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:dc42c3e150e27e3555edbd2ebdb62a0b187f75214398896f5839182dd1b9082e cni.projectcalico.org/podIP:10.1.132.53/32 cni.projectcalico.org/podIPs:10.1.132.53/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de85e122-2a87-4d75-9cf0-2fa89fea949c 0xc004b0fc07 0xc004b0fc08}] [] [{kubelite Update v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de85e122-2a87-4d75-9cf0-2fa89fea949c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2zc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2zc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.53,StartTime:2023-01-18 21:51:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:51:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9d02ee8e467b3ec4528dc47deb4404ed227daac66e7e04a8fda553c20dcdc42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 21:51:32.942: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-680  6962fdb7-39d1-4876-99ca-be21f31b382b 22547 4 2023-01-18 21:51:25 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ef97 0xc004b0ef98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0f038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 21:51:32.956: INFO: pod: "test-deployment-7df74c55ff-svvpg":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-svvpg test-deployment-7df74c55ff- deployment-680  69385190-bff0-443c-a754-f540b1070f04 22555 0 2023-01-18 21:51:27 +0000 UTC 2023-01-18 21:51:32 +0000 UTC 0xc002eaf6f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:d0eb26d9b52007d879b7ac3817caace1d2be5db6ed91b1d3b57f8b99f26a1367 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6962fdb7-39d1-4876-99ca-be21f31b382b 0xc002eaf727 0xc002eaf728}] [] [{kubelite Update v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6962fdb7-39d1-4876-99ca-be21f31b382b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zt9x7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zt9x7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.52,StartTime:2023-01-18 21:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-18 21:51:28 +0000 UTC,FinishedAt:2023-01-18 21:51:32 +0000 UTC,ContainerID:containerd://021ed21505104345c007bd6f51e633891fdbe64f92d3d5f3dfb1e56de62100ac,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://021ed21505104345c007bd6f51e633891fdbe64f92d3d5f3dfb1e56de62100ac,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:32.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-680" for this suite. 01/18/23 21:51:32.962
------------------------------
• [SLOW TEST] [11.033 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:21.978
    Jan 18 21:51:21.978: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 21:51:21.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:22.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:22.271
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/18/23 21:51:22.278
    STEP: waiting for Deployment to be created 01/18/23 21:51:22.292
    STEP: waiting for all Replicas to be Ready 01/18/23 21:51:22.293
    Jan 18 21:51:22.294: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.295: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.818: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.818: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:22.819: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 21:51:24.871: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/18/23 21:51:24.871
    W0118 21:51:24.890127      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 21:51:24.891: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/18/23 21:51:24.891
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 0
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:24.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:24.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:24.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:25.412: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:25.456: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:25.456: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:25.503: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:25.503: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:27.403: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:27.403: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:27.442: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    STEP: listing Deployments 01/18/23 21:51:27.442
    Jan 18 21:51:27.451: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/18/23 21:51:27.451
    Jan 18 21:51:27.482: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/18/23 21:51:27.482
    Jan 18 21:51:27.492: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:27.775: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:27.775: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:27.832: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:27.877: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:27.952: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:29.045: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:29.873: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 21:51:32.012: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/18/23 21:51:32.872
    STEP: fetching the DeploymentStatus 01/18/23 21:51:32.885
    Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.893: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 1
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 2
    Jan 18 21:51:32.894: INFO: observed Deployment test-deployment in namespace deployment-680 with ReadyReplicas 3
    STEP: deleting the Deployment 01/18/23 21:51:32.894
    Jan 18 21:51:32.918: INFO: observed event type MODIFIED
    Jan 18 21:51:32.918: INFO: observed event type MODIFIED
    Jan 18 21:51:32.918: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    Jan 18 21:51:32.919: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 21:51:32.926: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 18 21:51:32.931: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-680  56d40cf6-1447-4cd7-9939-22d9032cc695 22443 3 2023-01-18 21:51:22 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ed97 0xc004b0ed98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0ee38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 21:51:32.936: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-680  de85e122-2a87-4d75-9cf0-2fa89fea949c 22538 2 2023-01-18 21:51:27 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ee97 0xc004b0ee98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0ef38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 21:51:32.941: INFO: pod: "test-deployment-7b7876f9d6-2f4ln":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-2f4ln test-deployment-7b7876f9d6- deployment-680  ca58281b-bcd7-412f-8ad6-014b668de2cd 22501 0 2023-01-18 21:51:27 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:19973db08a5eb844c6364e81c13d7b8775561eb2989f09e8b5fbccdd54bdd21b cni.projectcalico.org/podIP:10.1.192.47/32 cni.projectcalico.org/podIPs:10.1.192.47/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de85e122-2a87-4d75-9cf0-2fa89fea949c 0xc004b0f547 0xc004b0f548}] [] [{kubelite Update v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de85e122-2a87-4d75-9cf0-2fa89fea949c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.47\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qq692,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qq692,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.47,StartTime:2023-01-18 21:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:51:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bb556174065a48265f0a68cad27a4f818e040edf7b1a9ebf487fdfdc0ea93881,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.47,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 21:51:32.941: INFO: pod: "test-deployment-7b7876f9d6-9qnjp":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-9qnjp test-deployment-7b7876f9d6- deployment-680  09b34b77-56e6-48a2-9de4-9f7ee6e5f458 22537 0 2023-01-18 21:51:29 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:dc42c3e150e27e3555edbd2ebdb62a0b187f75214398896f5839182dd1b9082e cni.projectcalico.org/podIP:10.1.132.53/32 cni.projectcalico.org/podIPs:10.1.132.53/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 de85e122-2a87-4d75-9cf0-2fa89fea949c 0xc004b0fc07 0xc004b0fc08}] [] [{kubelite Update v1 2023-01-18 21:51:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"de85e122-2a87-4d75-9cf0-2fa89fea949c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2zc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2zc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.53,StartTime:2023-01-18 21:51:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 21:51:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a9d02ee8e467b3ec4528dc47deb4404ed227daac66e7e04a8fda553c20dcdc42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 21:51:32.942: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-680  6962fdb7-39d1-4876-99ca-be21f31b382b 22547 4 2023-01-18 21:51:25 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7724333c-336b-4a9b-bc11-89e3fcd92404 0xc004b0ef97 0xc004b0ef98}] [] [{kubelite Update apps/v1 2023-01-18 21:51:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7724333c-336b-4a9b-bc11-89e3fcd92404\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b0f038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 21:51:32.956: INFO: pod: "test-deployment-7df74c55ff-svvpg":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-svvpg test-deployment-7df74c55ff- deployment-680  69385190-bff0-443c-a754-f540b1070f04 22555 0 2023-01-18 21:51:27 +0000 UTC 2023-01-18 21:51:32 +0000 UTC 0xc002eaf6f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:d0eb26d9b52007d879b7ac3817caace1d2be5db6ed91b1d3b57f8b99f26a1367 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 6962fdb7-39d1-4876-99ca-be21f31b382b 0xc002eaf727 0xc002eaf728}] [] [{kubelite Update v1 2023-01-18 21:51:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6962fdb7-39d1-4876-99ca-be21f31b382b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 21:51:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zt9x7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zt9x7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 21:51:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.5,PodIP:10.1.132.52,StartTime:2023-01-18 21:51:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-01-18 21:51:28 +0000 UTC,FinishedAt:2023-01-18 21:51:32 +0000 UTC,ContainerID:containerd://021ed21505104345c007bd6f51e633891fdbe64f92d3d5f3dfb1e56de62100ac,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://021ed21505104345c007bd6f51e633891fdbe64f92d3d5f3dfb1e56de62100ac,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.132.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:32.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-680" for this suite. 01/18/23 21:51:32.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:33.019
Jan 18 21:51:33.019: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 21:51:33.02
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:33.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:33.115
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 21:51:33.119
Jan 18 21:51:33.141: INFO: Waiting up to 5m0s for pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c" in namespace "emptydir-8898" to be "Succeeded or Failed"
Jan 18 21:51:33.145: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261136ms
Jan 18 21:51:35.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958424s
Jan 18 21:51:37.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Running", Reason="", readiness=false. Elapsed: 4.009685478s
Jan 18 21:51:39.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010405244s
STEP: Saw pod success 01/18/23 21:51:39.151
Jan 18 21:51:39.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c" satisfied condition "Succeeded or Failed"
Jan 18 21:51:39.156: INFO: Trying to get logs from node test-vm-1 pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c container test-container: <nil>
STEP: delete the pod 01/18/23 21:51:39.164
Jan 18 21:51:39.195: INFO: Waiting for pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c to disappear
Jan 18 21:51:39.201: INFO: Pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8898" for this suite. 01/18/23 21:51:39.205
------------------------------
• [SLOW TEST] [6.202 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:33.019
    Jan 18 21:51:33.019: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 21:51:33.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:33.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:33.115
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 21:51:33.119
    Jan 18 21:51:33.141: INFO: Waiting up to 5m0s for pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c" in namespace "emptydir-8898" to be "Succeeded or Failed"
    Jan 18 21:51:33.145: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.261136ms
    Jan 18 21:51:35.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009958424s
    Jan 18 21:51:37.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Running", Reason="", readiness=false. Elapsed: 4.009685478s
    Jan 18 21:51:39.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010405244s
    STEP: Saw pod success 01/18/23 21:51:39.151
    Jan 18 21:51:39.151: INFO: Pod "pod-f03d180b-16d4-4cc4-b899-a600a15f257c" satisfied condition "Succeeded or Failed"
    Jan 18 21:51:39.156: INFO: Trying to get logs from node test-vm-1 pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c container test-container: <nil>
    STEP: delete the pod 01/18/23 21:51:39.164
    Jan 18 21:51:39.195: INFO: Waiting for pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c to disappear
    Jan 18 21:51:39.201: INFO: Pod pod-f03d180b-16d4-4cc4-b899-a600a15f257c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:39.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8898" for this suite. 01/18/23 21:51:39.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:39.223
Jan 18 21:51:39.223: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:51:39.224
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:39.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:39.642
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2581 01/18/23 21:51:39.646
STEP: creating service affinity-nodeport-transition in namespace services-2581 01/18/23 21:51:39.646
STEP: creating replication controller affinity-nodeport-transition in namespace services-2581 01/18/23 21:51:39.683
I0118 21:51:39.697591      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2581, replica count: 3
I0118 21:51:42.748936      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:51:42.760: INFO: Creating new exec pod
Jan 18 21:51:42.776: INFO: Waiting up to 5m0s for pod "execpod-affinitymfgtn" in namespace "services-2581" to be "running"
Jan 18 21:51:42.780: INFO: Pod "execpod-affinitymfgtn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315949ms
Jan 18 21:51:44.786: INFO: Pod "execpod-affinitymfgtn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009937737s
Jan 18 21:51:44.786: INFO: Pod "execpod-affinitymfgtn" satisfied condition "running"
Jan 18 21:51:45.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan 18 21:51:45.987: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 21:51:45.987: INFO: stdout: ""
Jan 18 21:51:45.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.152.183.195 80'
Jan 18 21:51:46.170: INFO: stderr: "+ nc -v -z -w 2 10.152.183.195 80\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
Jan 18 21:51:46.170: INFO: stdout: ""
Jan 18 21:51:46.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30852'
Jan 18 21:51:46.337: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30852\nConnection to 10.0.0.4 30852 port [tcp/*] succeeded!\n"
Jan 18 21:51:46.337: INFO: stdout: ""
Jan 18 21:51:46.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 30852'
Jan 18 21:51:46.528: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 30852\nConnection to 10.0.0.5 30852 port [tcp/*] succeeded!\n"
Jan 18 21:51:46.528: INFO: stdout: ""
Jan 18 21:51:46.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30852/ ; done'
Jan 18 21:51:46.799: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n"
Jan 18 21:51:46.799: INFO: stdout: "\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq"
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:46.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30852/ ; done'
Jan 18 21:51:47.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n"
Jan 18 21:51:47.071: INFO: stdout: "\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq"
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
Jan 18 21:51:47.071: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2581, will wait for the garbage collector to delete the pods 01/18/23 21:51:47.113
Jan 18 21:51:47.264: INFO: Deleting ReplicationController affinity-nodeport-transition took: 92.513688ms
Jan 18 21:51:47.565: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 300.890665ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:50.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2581" for this suite. 01/18/23 21:51:50.225
------------------------------
• [SLOW TEST] [11.014 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:39.223
    Jan 18 21:51:39.223: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:51:39.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:39.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:39.642
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2581 01/18/23 21:51:39.646
    STEP: creating service affinity-nodeport-transition in namespace services-2581 01/18/23 21:51:39.646
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2581 01/18/23 21:51:39.683
    I0118 21:51:39.697591      20 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2581, replica count: 3
    I0118 21:51:42.748936      20 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:51:42.760: INFO: Creating new exec pod
    Jan 18 21:51:42.776: INFO: Waiting up to 5m0s for pod "execpod-affinitymfgtn" in namespace "services-2581" to be "running"
    Jan 18 21:51:42.780: INFO: Pod "execpod-affinitymfgtn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.315949ms
    Jan 18 21:51:44.786: INFO: Pod "execpod-affinitymfgtn": Phase="Running", Reason="", readiness=true. Elapsed: 2.009937737s
    Jan 18 21:51:44.786: INFO: Pod "execpod-affinitymfgtn" satisfied condition "running"
    Jan 18 21:51:45.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan 18 21:51:45.987: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 21:51:45.987: INFO: stdout: ""
    Jan 18 21:51:45.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.152.183.195 80'
    Jan 18 21:51:46.170: INFO: stderr: "+ nc -v -z -w 2 10.152.183.195 80\nConnection to 10.152.183.195 80 port [tcp/http] succeeded!\n"
    Jan 18 21:51:46.170: INFO: stdout: ""
    Jan 18 21:51:46.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30852'
    Jan 18 21:51:46.337: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30852\nConnection to 10.0.0.4 30852 port [tcp/*] succeeded!\n"
    Jan 18 21:51:46.337: INFO: stdout: ""
    Jan 18 21:51:46.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 30852'
    Jan 18 21:51:46.528: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 30852\nConnection to 10.0.0.5 30852 port [tcp/*] succeeded!\n"
    Jan 18 21:51:46.528: INFO: stdout: ""
    Jan 18 21:51:46.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30852/ ; done'
    Jan 18 21:51:46.799: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n"
    Jan 18 21:51:46.799: INFO: stdout: "\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-9xjmv\naffinity-nodeport-transition-kq5hc\naffinity-nodeport-transition-5sjbq"
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-9xjmv
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-kq5hc
    Jan 18 21:51:46.799: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:46.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-2581 exec execpod-affinitymfgtn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:30852/ ; done'
    Jan 18 21:51:47.071: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:30852/\n"
    Jan 18 21:51:47.071: INFO: stdout: "\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq\naffinity-nodeport-transition-5sjbq"
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Received response from host: affinity-nodeport-transition-5sjbq
    Jan 18 21:51:47.071: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2581, will wait for the garbage collector to delete the pods 01/18/23 21:51:47.113
    Jan 18 21:51:47.264: INFO: Deleting ReplicationController affinity-nodeport-transition took: 92.513688ms
    Jan 18 21:51:47.565: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 300.890665ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:50.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2581" for this suite. 01/18/23 21:51:50.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:50.24
Jan 18 21:51:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 21:51:50.241
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:50.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:50.877
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/18/23 21:51:50.881
Jan 18 21:51:50.881: INFO: Creating e2e-svc-a-lklxj
Jan 18 21:51:50.901: INFO: Creating e2e-svc-b-7qch7
Jan 18 21:51:50.925: INFO: Creating e2e-svc-c-ljhlq
STEP: deleting service collection 01/18/23 21:51:50.949
Jan 18 21:51:51.007: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7967" for this suite. 01/18/23 21:51:51.012
------------------------------
• [0.784 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:50.24
    Jan 18 21:51:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 21:51:50.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:50.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:50.877
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/18/23 21:51:50.881
    Jan 18 21:51:50.881: INFO: Creating e2e-svc-a-lklxj
    Jan 18 21:51:50.901: INFO: Creating e2e-svc-b-7qch7
    Jan 18 21:51:50.925: INFO: Creating e2e-svc-c-ljhlq
    STEP: deleting service collection 01/18/23 21:51:50.949
    Jan 18 21:51:51.007: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:51.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7967" for this suite. 01/18/23 21:51:51.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:51.026
Jan 18 21:51:51.026: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:51:51.027
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:51.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:51.167
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:51:51.205
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:51:51.486
STEP: Deploying the webhook pod 01/18/23 21:51:51.503
STEP: Wait for the deployment to be ready 01/18/23 21:51:51.524
Jan 18 21:51:51.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 21:51:53.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 21:51:55.552
STEP: Verifying the service has paired with the endpoint 01/18/23 21:51:55.573
Jan 18 21:51:56.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/18/23 21:51:56.778
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:51:56.825
STEP: Deleting the collection of validation webhooks 01/18/23 21:51:56.866
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:51:56.993
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:51:57.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6467" for this suite. 01/18/23 21:51:57.094
STEP: Destroying namespace "webhook-6467-markers" for this suite. 01/18/23 21:51:57.107
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:51.026
    Jan 18 21:51:51.026: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:51:51.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:51.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:51.167
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:51:51.205
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:51:51.486
    STEP: Deploying the webhook pod 01/18/23 21:51:51.503
    STEP: Wait for the deployment to be ready 01/18/23 21:51:51.524
    Jan 18 21:51:51.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 21:51:53.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 21, 51, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 21:51:55.552
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:51:55.573
    Jan 18 21:51:56.573: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/18/23 21:51:56.778
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:51:56.825
    STEP: Deleting the collection of validation webhooks 01/18/23 21:51:56.866
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 21:51:56.993
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:51:57.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6467" for this suite. 01/18/23 21:51:57.094
    STEP: Destroying namespace "webhook-6467-markers" for this suite. 01/18/23 21:51:57.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:51:57.152
Jan 18 21:51:57.152: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename proxy 01/18/23 21:51:57.153
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:57.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:57.348
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/18/23 21:51:57.39
STEP: creating replication controller proxy-service-7xxn6 in namespace proxy-2385 01/18/23 21:51:57.39
I0118 21:51:57.402854      20 runners.go:193] Created replication controller with name: proxy-service-7xxn6, namespace: proxy-2385, replica count: 1
I0118 21:51:58.454858      20 runners.go:193] proxy-service-7xxn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 21:51:59.456014      20 runners.go:193] proxy-service-7xxn6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 21:51:59.464: INFO: Endpoint proxy-2385/proxy-service-7xxn6 is not ready yet
Jan 18 21:52:01.470: INFO: setup took 4.118435627s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 21:52:01.47
Jan 18 21:52:01.490: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 19.46246ms)
Jan 18 21:52:01.490: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 19.443659ms)
Jan 18 21:52:01.496: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.191636ms)
Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 29.339091ms)
Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 29.436792ms)
Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.252103ms)
Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 36.526186ms)
Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 36.77589ms)
Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 36.992092ms)
Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 37.136094ms)
Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 37.142495ms)
Jan 18 21:52:01.508: INFO: (0) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 37.621701ms)
Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 43.611481ms)
Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 44.225489ms)
Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 43.896785ms)
Jan 18 21:52:01.515: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 45.251103ms)
Jan 18 21:52:01.521: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 6.00338ms)
Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 12.121562ms)
Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 12.414465ms)
Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 12.74047ms)
Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 13.040074ms)
Jan 18 21:52:01.536: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 20.143668ms)
Jan 18 21:52:01.536: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.530773ms)
Jan 18 21:52:01.537: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 21.072881ms)
Jan 18 21:52:01.546: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 30.074201ms)
Jan 18 21:52:01.546: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 30.627007ms)
Jan 18 21:52:01.547: INFO: (1) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 31.670222ms)
Jan 18 21:52:01.548: INFO: (1) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 32.685336ms)
Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 38.899818ms)
Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.092233ms)
Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 39.630327ms)
Jan 18 21:52:01.557: INFO: (1) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.508152ms)
Jan 18 21:52:01.568: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 11.27885ms)
Jan 18 21:52:01.568: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 11.406852ms)
Jan 18 21:52:01.569: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 11.647955ms)
Jan 18 21:52:01.569: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 12.140162ms)
Jan 18 21:52:01.577: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.091168ms)
Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.257836ms)
Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.254836ms)
Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 25.212235ms)
Jan 18 21:52:01.586: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.395678ms)
Jan 18 21:52:01.586: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 28.431878ms)
Jan 18 21:52:01.590: INFO: (2) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.06064ms)
Jan 18 21:52:01.590: INFO: (2) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 32.929138ms)
Jan 18 21:52:01.591: INFO: (2) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 33.557347ms)
Jan 18 21:52:01.594: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 36.862791ms)
Jan 18 21:52:01.594: INFO: (2) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 36.665888ms)
Jan 18 21:52:01.598: INFO: (2) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.719243ms)
Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 18.441845ms)
Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.413346ms)
Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.313444ms)
Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 18.991853ms)
Jan 18 21:52:01.618: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 20.051467ms)
Jan 18 21:52:01.625: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 26.545154ms)
Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 27.681469ms)
Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.607068ms)
Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 27.667468ms)
Jan 18 21:52:01.627: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.586981ms)
Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 36.884191ms)
Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 36.990993ms)
Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 36.399585ms)
Jan 18 21:52:01.637: INFO: (3) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 39.411625ms)
Jan 18 21:52:01.638: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 39.526126ms)
Jan 18 21:52:01.639: INFO: (3) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.34985ms)
Jan 18 21:52:01.646: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 6.897791ms)
Jan 18 21:52:01.654: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 13.951685ms)
Jan 18 21:52:01.654: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 14.196689ms)
Jan 18 21:52:01.657: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.074728ms)
Jan 18 21:52:01.657: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.365931ms)
Jan 18 21:52:01.662: INFO: (4) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 22.4911ms)
Jan 18 21:52:01.662: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 22.669602ms)
Jan 18 21:52:01.663: INFO: (4) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 23.348011ms)
Jan 18 21:52:01.666: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 26.387051ms)
Jan 18 21:52:01.672: INFO: (4) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 32.743436ms)
Jan 18 21:52:01.673: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 33.470846ms)
Jan 18 21:52:01.673: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 33.229443ms)
Jan 18 21:52:01.676: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 36.078581ms)
Jan 18 21:52:01.681: INFO: (4) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 40.985246ms)
Jan 18 21:52:01.681: INFO: (4) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.785456ms)
Jan 18 21:52:01.684: INFO: (4) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 44.523993ms)
Jan 18 21:52:01.697: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 12.504766ms)
Jan 18 21:52:01.697: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.119962ms)
Jan 18 21:52:01.700: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 15.548907ms)
Jan 18 21:52:01.705: INFO: (5) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 20.615474ms)
Jan 18 21:52:01.706: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 21.147981ms)
Jan 18 21:52:01.706: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.247483ms)
Jan 18 21:52:01.709: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 24.567027ms)
Jan 18 21:52:01.714: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 29.438692ms)
Jan 18 21:52:01.715: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.137901ms)
Jan 18 21:52:01.719: INFO: (5) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 34.706062ms)
Jan 18 21:52:01.720: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 34.980965ms)
Jan 18 21:52:01.720: INFO: (5) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 35.425972ms)
Jan 18 21:52:01.723: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 38.510312ms)
Jan 18 21:52:01.723: INFO: (5) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 38.512912ms)
Jan 18 21:52:01.726: INFO: (5) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.32285ms)
Jan 18 21:52:01.726: INFO: (5) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 41.559153ms)
Jan 18 21:52:01.733: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 6.702789ms)
Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 18.204742ms)
Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 18.518947ms)
Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 18.243143ms)
Jan 18 21:52:01.754: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 26.772556ms)
Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 27.72477ms)
Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 27.75627ms)
Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 28.055273ms)
Jan 18 21:52:01.756: INFO: (6) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 29.25939ms)
Jan 18 21:52:01.763: INFO: (6) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 36.06648ms)
Jan 18 21:52:01.763: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 36.670988ms)
Jan 18 21:52:01.764: INFO: (6) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 37.434898ms)
Jan 18 21:52:01.764: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 37.395598ms)
Jan 18 21:52:01.765: INFO: (6) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 38.036506ms)
Jan 18 21:52:01.768: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.28715ms)
Jan 18 21:52:01.768: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 41.381651ms)
Jan 18 21:52:01.781: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.160662ms)
Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 19.220856ms)
Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 19.387458ms)
Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 19.581861ms)
Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 19.942365ms)
Jan 18 21:52:01.790: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 21.188682ms)
Jan 18 21:52:01.796: INFO: (7) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 27.427565ms)
Jan 18 21:52:01.796: INFO: (7) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 27.186462ms)
Jan 18 21:52:01.797: INFO: (7) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 28.077674ms)
Jan 18 21:52:01.800: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 31.55662ms)
Jan 18 21:52:01.800: INFO: (7) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 31.364717ms)
Jan 18 21:52:01.805: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 36.10388ms)
Jan 18 21:52:01.805: INFO: (7) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 36.133081ms)
Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 41.042447ms)
Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.129047ms)
Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.334951ms)
Jan 18 21:52:01.819: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 9.434226ms)
Jan 18 21:52:01.820: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 10.48344ms)
Jan 18 21:52:01.823: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 13.091775ms)
Jan 18 21:52:01.828: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 17.970239ms)
Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 24.03402ms)
Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 23.974219ms)
Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 24.06752ms)
Jan 18 21:52:01.835: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 25.240236ms)
Jan 18 21:52:01.836: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 26.324451ms)
Jan 18 21:52:01.843: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 32.635535ms)
Jan 18 21:52:01.843: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 32.570034ms)
Jan 18 21:52:01.844: INFO: (8) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.686048ms)
Jan 18 21:52:01.845: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 35.023066ms)
Jan 18 21:52:01.850: INFO: (8) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 39.427825ms)
Jan 18 21:52:01.850: INFO: (8) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.267236ms)
Jan 18 21:52:01.851: INFO: (8) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.423051ms)
Jan 18 21:52:01.862: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 10.418639ms)
Jan 18 21:52:01.862: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 10.436339ms)
Jan 18 21:52:01.867: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 15.109102ms)
Jan 18 21:52:01.872: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 20.093067ms)
Jan 18 21:52:01.872: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 20.887278ms)
Jan 18 21:52:01.877: INFO: (9) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 25.475539ms)
Jan 18 21:52:01.877: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 25.710642ms)
Jan 18 21:52:01.880: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.693182ms)
Jan 18 21:52:01.881: INFO: (9) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 29.189888ms)
Jan 18 21:52:01.884: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 32.565433ms)
Jan 18 21:52:01.884: INFO: (9) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 32.851437ms)
Jan 18 21:52:01.889: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 37.765203ms)
Jan 18 21:52:01.889: INFO: (9) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 37.575ms)
Jan 18 21:52:01.890: INFO: (9) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 38.417512ms)
Jan 18 21:52:01.893: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 41.235349ms)
Jan 18 21:52:01.893: INFO: (9) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.381251ms)
Jan 18 21:52:01.906: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 12.470866ms)
Jan 18 21:52:01.906: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 13.220776ms)
Jan 18 21:52:01.911: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.190042ms)
Jan 18 21:52:01.916: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 22.768904ms)
Jan 18 21:52:01.916: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 23.072408ms)
Jan 18 21:52:01.917: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 23.386911ms)
Jan 18 21:52:01.920: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 26.475552ms)
Jan 18 21:52:01.921: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 27.249763ms)
Jan 18 21:52:01.926: INFO: (10) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 32.608134ms)
Jan 18 21:52:01.926: INFO: (10) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 33.239443ms)
Jan 18 21:52:01.929: INFO: (10) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 35.839077ms)
Jan 18 21:52:01.930: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 36.263083ms)
Jan 18 21:52:01.934: INFO: (10) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.776842ms)
Jan 18 21:52:01.934: INFO: (10) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.054547ms)
Jan 18 21:52:01.935: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 41.426552ms)
Jan 18 21:52:01.936: INFO: (10) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 43.199775ms)
Jan 18 21:52:01.943: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 6.446685ms)
Jan 18 21:52:01.948: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 11.504853ms)
Jan 18 21:52:01.954: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 16.882924ms)
Jan 18 21:52:01.954: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.073727ms)
Jan 18 21:52:01.959: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 22.074094ms)
Jan 18 21:52:01.970: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 33.403445ms)
Jan 18 21:52:01.970: INFO: (11) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.487546ms)
Jan 18 21:52:01.971: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 33.976052ms)
Jan 18 21:52:01.971: INFO: (11) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 34.730462ms)
Jan 18 21:52:01.972: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 35.356771ms)
Jan 18 21:52:01.973: INFO: (11) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 36.249183ms)
Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 42.82647ms)
Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 43.417078ms)
Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 43.686081ms)
Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 43.383778ms)
Jan 18 21:52:01.982: INFO: (11) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 44.689195ms)
Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 12.146762ms)
Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 12.02166ms)
Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.597267ms)
Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.436366ms)
Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 12.498967ms)
Jan 18 21:52:02.002: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.764676ms)
Jan 18 21:52:02.009: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 27.148861ms)
Jan 18 21:52:02.009: INFO: (12) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.689869ms)
Jan 18 21:52:02.014: INFO: (12) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 32.043926ms)
Jan 18 21:52:02.015: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 32.277229ms)
Jan 18 21:52:02.015: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 32.491132ms)
Jan 18 21:52:02.018: INFO: (12) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 35.727176ms)
Jan 18 21:52:02.018: INFO: (12) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 35.676775ms)
Jan 18 21:52:02.024: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.32165ms)
Jan 18 21:52:02.024: INFO: (12) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.916658ms)
Jan 18 21:52:02.025: INFO: (12) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 43.388678ms)
Jan 18 21:52:02.037: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 11.558954ms)
Jan 18 21:52:02.037: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 11.888359ms)
Jan 18 21:52:02.038: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.363365ms)
Jan 18 21:52:02.042: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 15.885312ms)
Jan 18 21:52:02.042: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 15.847511ms)
Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 20.698075ms)
Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 20.842178ms)
Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.473686ms)
Jan 18 21:52:02.050: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 24.375125ms)
Jan 18 21:52:02.055: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 29.26549ms)
Jan 18 21:52:02.056: INFO: (13) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 30.460905ms)
Jan 18 21:52:02.061: INFO: (13) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 34.709662ms)
Jan 18 21:52:02.061: INFO: (13) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.36137ms)
Jan 18 21:52:02.064: INFO: (13) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 38.739216ms)
Jan 18 21:52:02.067: INFO: (13) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.756042ms)
Jan 18 21:52:02.068: INFO: (13) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 42.491166ms)
Jan 18 21:52:02.082: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 12.861772ms)
Jan 18 21:52:02.083: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 13.738983ms)
Jan 18 21:52:02.083: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.371191ms)
Jan 18 21:52:02.091: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.400985ms)
Jan 18 21:52:02.091: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 22.130294ms)
Jan 18 21:52:02.092: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 23.093407ms)
Jan 18 21:52:02.100: INFO: (14) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 31.367017ms)
Jan 18 21:52:02.100: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 31.464519ms)
Jan 18 21:52:02.101: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 31.818523ms)
Jan 18 21:52:02.104: INFO: (14) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.005066ms)
Jan 18 21:52:02.104: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 35.370371ms)
Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 40.456439ms)
Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.680841ms)
Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 40.821643ms)
Jan 18 21:52:02.113: INFO: (14) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 44.022086ms)
Jan 18 21:52:02.113: INFO: (14) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 44.681395ms)
Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.083727ms)
Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 17.573933ms)
Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 17.403532ms)
Jan 18 21:52:02.133: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 19.320657ms)
Jan 18 21:52:02.139: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 25.736843ms)
Jan 18 21:52:02.139: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.828844ms)
Jan 18 21:52:02.140: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 26.660355ms)
Jan 18 21:52:02.140: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 26.828557ms)
Jan 18 21:52:02.152: INFO: (15) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 38.31541ms)
Jan 18 21:52:02.152: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 38.406611ms)
Jan 18 21:52:02.153: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 39.245622ms)
Jan 18 21:52:02.156: INFO: (15) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 42.543767ms)
Jan 18 21:52:02.164: INFO: (15) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 50.921478ms)
Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 50.923178ms)
Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 51.14438ms)
Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 50.881377ms)
Jan 18 21:52:02.172: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 7.5979ms)
Jan 18 21:52:02.173: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 8.075907ms)
Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 13.533779ms)
Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 13.441477ms)
Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 13.69898ms)
Jan 18 21:52:02.183: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 18.615846ms)
Jan 18 21:52:02.183: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 18.811448ms)
Jan 18 21:52:02.191: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.976343ms)
Jan 18 21:52:02.191: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 26.53985ms)
Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 41.316644ms)
Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.425345ms)
Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 41.319745ms)
Jan 18 21:52:02.211: INFO: (16) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 46.717816ms)
Jan 18 21:52:02.211: INFO: (16) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 46.693215ms)
Jan 18 21:52:02.216: INFO: (16) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 51.429477ms)
Jan 18 21:52:02.217: INFO: (16) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 51.913684ms)
Jan 18 21:52:02.222: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 5.594974ms)
Jan 18 21:52:02.232: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 14.589492ms)
Jan 18 21:52:02.232: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.024785ms)
Jan 18 21:52:02.237: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 19.614958ms)
Jan 18 21:52:02.238: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 19.643359ms)
Jan 18 21:52:02.247: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 30.202398ms)
Jan 18 21:52:02.247: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.142197ms)
Jan 18 21:52:02.252: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 34.92306ms)
Jan 18 21:52:02.253: INFO: (17) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.107663ms)
Jan 18 21:52:02.253: INFO: (17) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 35.67867ms)
Jan 18 21:52:02.258: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 41.161942ms)
Jan 18 21:52:02.263: INFO: (17) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 45.741903ms)
Jan 18 21:52:02.263: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 46.212209ms)
Jan 18 21:52:02.269: INFO: (17) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 52.014385ms)
Jan 18 21:52:02.273: INFO: (17) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 55.296028ms)
Jan 18 21:52:02.273: INFO: (17) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 55.059425ms)
Jan 18 21:52:02.295: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.221479ms)
Jan 18 21:52:02.295: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 22.01479ms)
Jan 18 21:52:02.296: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 22.734599ms)
Jan 18 21:52:02.299: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.471435ms)
Jan 18 21:52:02.300: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 26.412248ms)
Jan 18 21:52:02.310: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 36.621982ms)
Jan 18 21:52:02.310: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 36.924686ms)
Jan 18 21:52:02.311: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 38.071001ms)
Jan 18 21:52:02.313: INFO: (18) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 40.122528ms)
Jan 18 21:52:02.314: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 40.808838ms)
Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 49.495152ms)
Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 49.449351ms)
Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 49.689054ms)
Jan 18 21:52:02.326: INFO: (18) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 53.053699ms)
Jan 18 21:52:02.326: INFO: (18) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 53.218201ms)
Jan 18 21:52:02.331: INFO: (18) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 58.013964ms)
Jan 18 21:52:02.346: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.086385ms)
Jan 18 21:52:02.346: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 14.719594ms)
Jan 18 21:52:02.350: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 18.856348ms)
Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 27.119757ms)
Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 27.181658ms)
Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.37186ms)
Jan 18 21:52:02.364: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 32.67943ms)
Jan 18 21:52:02.364: INFO: (19) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 32.536628ms)
Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 38.139402ms)
Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 38.001201ms)
Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 38.419906ms)
Jan 18 21:52:02.377: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 44.905891ms)
Jan 18 21:52:02.377: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 45.085394ms)
Jan 18 21:52:02.382: INFO: (19) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 50.200961ms)
Jan 18 21:52:02.382: INFO: (19) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 50.443565ms)
Jan 18 21:52:02.387: INFO: (19) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 54.989024ms)
STEP: deleting ReplicationController proxy-service-7xxn6 in namespace proxy-2385, will wait for the garbage collector to delete the pods 01/18/23 21:52:02.387
Jan 18 21:52:02.463: INFO: Deleting ReplicationController proxy-service-7xxn6 took: 20.46567ms
Jan 18 21:52:02.864: INFO: Terminating ReplicationController proxy-service-7xxn6 pods took: 400.427973ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2385" for this suite. 01/18/23 21:52:04.37
------------------------------
• [SLOW TEST] [7.230 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:51:57.152
    Jan 18 21:51:57.152: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename proxy 01/18/23 21:51:57.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:51:57.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:51:57.348
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/18/23 21:51:57.39
    STEP: creating replication controller proxy-service-7xxn6 in namespace proxy-2385 01/18/23 21:51:57.39
    I0118 21:51:57.402854      20 runners.go:193] Created replication controller with name: proxy-service-7xxn6, namespace: proxy-2385, replica count: 1
    I0118 21:51:58.454858      20 runners.go:193] proxy-service-7xxn6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 21:51:59.456014      20 runners.go:193] proxy-service-7xxn6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 21:51:59.464: INFO: Endpoint proxy-2385/proxy-service-7xxn6 is not ready yet
    Jan 18 21:52:01.470: INFO: setup took 4.118435627s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 21:52:01.47
    Jan 18 21:52:01.490: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 19.46246ms)
    Jan 18 21:52:01.490: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 19.443659ms)
    Jan 18 21:52:01.496: INFO: (0) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.191636ms)
    Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 29.339091ms)
    Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 29.436792ms)
    Jan 18 21:52:01.500: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.252103ms)
    Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 36.526186ms)
    Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 36.77589ms)
    Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 36.992092ms)
    Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 37.136094ms)
    Jan 18 21:52:01.507: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 37.142495ms)
    Jan 18 21:52:01.508: INFO: (0) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 37.621701ms)
    Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 43.611481ms)
    Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 44.225489ms)
    Jan 18 21:52:01.514: INFO: (0) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 43.896785ms)
    Jan 18 21:52:01.515: INFO: (0) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 45.251103ms)
    Jan 18 21:52:01.521: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 6.00338ms)
    Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 12.121562ms)
    Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 12.414465ms)
    Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 12.74047ms)
    Jan 18 21:52:01.528: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 13.040074ms)
    Jan 18 21:52:01.536: INFO: (1) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 20.143668ms)
    Jan 18 21:52:01.536: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.530773ms)
    Jan 18 21:52:01.537: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 21.072881ms)
    Jan 18 21:52:01.546: INFO: (1) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 30.074201ms)
    Jan 18 21:52:01.546: INFO: (1) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 30.627007ms)
    Jan 18 21:52:01.547: INFO: (1) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 31.670222ms)
    Jan 18 21:52:01.548: INFO: (1) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 32.685336ms)
    Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 38.899818ms)
    Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.092233ms)
    Jan 18 21:52:01.555: INFO: (1) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 39.630327ms)
    Jan 18 21:52:01.557: INFO: (1) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.508152ms)
    Jan 18 21:52:01.568: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 11.27885ms)
    Jan 18 21:52:01.568: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 11.406852ms)
    Jan 18 21:52:01.569: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 11.647955ms)
    Jan 18 21:52:01.569: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 12.140162ms)
    Jan 18 21:52:01.577: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.091168ms)
    Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.257836ms)
    Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.254836ms)
    Jan 18 21:52:01.582: INFO: (2) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 25.212235ms)
    Jan 18 21:52:01.586: INFO: (2) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.395678ms)
    Jan 18 21:52:01.586: INFO: (2) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 28.431878ms)
    Jan 18 21:52:01.590: INFO: (2) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.06064ms)
    Jan 18 21:52:01.590: INFO: (2) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 32.929138ms)
    Jan 18 21:52:01.591: INFO: (2) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 33.557347ms)
    Jan 18 21:52:01.594: INFO: (2) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 36.862791ms)
    Jan 18 21:52:01.594: INFO: (2) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 36.665888ms)
    Jan 18 21:52:01.598: INFO: (2) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.719243ms)
    Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 18.441845ms)
    Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.413346ms)
    Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.313444ms)
    Jan 18 21:52:01.617: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 18.991853ms)
    Jan 18 21:52:01.618: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 20.051467ms)
    Jan 18 21:52:01.625: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 26.545154ms)
    Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 27.681469ms)
    Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.607068ms)
    Jan 18 21:52:01.626: INFO: (3) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 27.667468ms)
    Jan 18 21:52:01.627: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.586981ms)
    Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 36.884191ms)
    Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 36.990993ms)
    Jan 18 21:52:01.635: INFO: (3) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 36.399585ms)
    Jan 18 21:52:01.637: INFO: (3) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 39.411625ms)
    Jan 18 21:52:01.638: INFO: (3) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 39.526126ms)
    Jan 18 21:52:01.639: INFO: (3) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.34985ms)
    Jan 18 21:52:01.646: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 6.897791ms)
    Jan 18 21:52:01.654: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 13.951685ms)
    Jan 18 21:52:01.654: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 14.196689ms)
    Jan 18 21:52:01.657: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.074728ms)
    Jan 18 21:52:01.657: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.365931ms)
    Jan 18 21:52:01.662: INFO: (4) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 22.4911ms)
    Jan 18 21:52:01.662: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 22.669602ms)
    Jan 18 21:52:01.663: INFO: (4) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 23.348011ms)
    Jan 18 21:52:01.666: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 26.387051ms)
    Jan 18 21:52:01.672: INFO: (4) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 32.743436ms)
    Jan 18 21:52:01.673: INFO: (4) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 33.470846ms)
    Jan 18 21:52:01.673: INFO: (4) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 33.229443ms)
    Jan 18 21:52:01.676: INFO: (4) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 36.078581ms)
    Jan 18 21:52:01.681: INFO: (4) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 40.985246ms)
    Jan 18 21:52:01.681: INFO: (4) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.785456ms)
    Jan 18 21:52:01.684: INFO: (4) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 44.523993ms)
    Jan 18 21:52:01.697: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 12.504766ms)
    Jan 18 21:52:01.697: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.119962ms)
    Jan 18 21:52:01.700: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 15.548907ms)
    Jan 18 21:52:01.705: INFO: (5) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 20.615474ms)
    Jan 18 21:52:01.706: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 21.147981ms)
    Jan 18 21:52:01.706: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.247483ms)
    Jan 18 21:52:01.709: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 24.567027ms)
    Jan 18 21:52:01.714: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 29.438692ms)
    Jan 18 21:52:01.715: INFO: (5) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.137901ms)
    Jan 18 21:52:01.719: INFO: (5) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 34.706062ms)
    Jan 18 21:52:01.720: INFO: (5) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 34.980965ms)
    Jan 18 21:52:01.720: INFO: (5) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 35.425972ms)
    Jan 18 21:52:01.723: INFO: (5) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 38.510312ms)
    Jan 18 21:52:01.723: INFO: (5) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 38.512912ms)
    Jan 18 21:52:01.726: INFO: (5) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.32285ms)
    Jan 18 21:52:01.726: INFO: (5) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 41.559153ms)
    Jan 18 21:52:01.733: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 6.702789ms)
    Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 18.204742ms)
    Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 18.518947ms)
    Jan 18 21:52:01.745: INFO: (6) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 18.243143ms)
    Jan 18 21:52:01.754: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 26.772556ms)
    Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 27.72477ms)
    Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 27.75627ms)
    Jan 18 21:52:01.755: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 28.055273ms)
    Jan 18 21:52:01.756: INFO: (6) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 29.25939ms)
    Jan 18 21:52:01.763: INFO: (6) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 36.06648ms)
    Jan 18 21:52:01.763: INFO: (6) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 36.670988ms)
    Jan 18 21:52:01.764: INFO: (6) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 37.434898ms)
    Jan 18 21:52:01.764: INFO: (6) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 37.395598ms)
    Jan 18 21:52:01.765: INFO: (6) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 38.036506ms)
    Jan 18 21:52:01.768: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.28715ms)
    Jan 18 21:52:01.768: INFO: (6) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 41.381651ms)
    Jan 18 21:52:01.781: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.160662ms)
    Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 19.220856ms)
    Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 19.387458ms)
    Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 19.581861ms)
    Jan 18 21:52:01.788: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 19.942365ms)
    Jan 18 21:52:01.790: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 21.188682ms)
    Jan 18 21:52:01.796: INFO: (7) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 27.427565ms)
    Jan 18 21:52:01.796: INFO: (7) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 27.186462ms)
    Jan 18 21:52:01.797: INFO: (7) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 28.077674ms)
    Jan 18 21:52:01.800: INFO: (7) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 31.55662ms)
    Jan 18 21:52:01.800: INFO: (7) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 31.364717ms)
    Jan 18 21:52:01.805: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 36.10388ms)
    Jan 18 21:52:01.805: INFO: (7) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 36.133081ms)
    Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 41.042447ms)
    Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 41.129047ms)
    Jan 18 21:52:01.810: INFO: (7) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.334951ms)
    Jan 18 21:52:01.819: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 9.434226ms)
    Jan 18 21:52:01.820: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 10.48344ms)
    Jan 18 21:52:01.823: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 13.091775ms)
    Jan 18 21:52:01.828: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 17.970239ms)
    Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 24.03402ms)
    Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 23.974219ms)
    Jan 18 21:52:01.834: INFO: (8) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 24.06752ms)
    Jan 18 21:52:01.835: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 25.240236ms)
    Jan 18 21:52:01.836: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 26.324451ms)
    Jan 18 21:52:01.843: INFO: (8) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 32.635535ms)
    Jan 18 21:52:01.843: INFO: (8) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 32.570034ms)
    Jan 18 21:52:01.844: INFO: (8) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.686048ms)
    Jan 18 21:52:01.845: INFO: (8) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 35.023066ms)
    Jan 18 21:52:01.850: INFO: (8) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 39.427825ms)
    Jan 18 21:52:01.850: INFO: (8) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 40.267236ms)
    Jan 18 21:52:01.851: INFO: (8) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.423051ms)
    Jan 18 21:52:01.862: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 10.418639ms)
    Jan 18 21:52:01.862: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 10.436339ms)
    Jan 18 21:52:01.867: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 15.109102ms)
    Jan 18 21:52:01.872: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 20.093067ms)
    Jan 18 21:52:01.872: INFO: (9) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 20.887278ms)
    Jan 18 21:52:01.877: INFO: (9) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 25.475539ms)
    Jan 18 21:52:01.877: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 25.710642ms)
    Jan 18 21:52:01.880: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 28.693182ms)
    Jan 18 21:52:01.881: INFO: (9) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 29.189888ms)
    Jan 18 21:52:01.884: INFO: (9) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 32.565433ms)
    Jan 18 21:52:01.884: INFO: (9) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 32.851437ms)
    Jan 18 21:52:01.889: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 37.765203ms)
    Jan 18 21:52:01.889: INFO: (9) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 37.575ms)
    Jan 18 21:52:01.890: INFO: (9) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 38.417512ms)
    Jan 18 21:52:01.893: INFO: (9) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 41.235349ms)
    Jan 18 21:52:01.893: INFO: (9) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.381251ms)
    Jan 18 21:52:01.906: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 12.470866ms)
    Jan 18 21:52:01.906: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 13.220776ms)
    Jan 18 21:52:01.911: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 18.190042ms)
    Jan 18 21:52:01.916: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 22.768904ms)
    Jan 18 21:52:01.916: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 23.072408ms)
    Jan 18 21:52:01.917: INFO: (10) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 23.386911ms)
    Jan 18 21:52:01.920: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 26.475552ms)
    Jan 18 21:52:01.921: INFO: (10) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 27.249763ms)
    Jan 18 21:52:01.926: INFO: (10) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 32.608134ms)
    Jan 18 21:52:01.926: INFO: (10) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 33.239443ms)
    Jan 18 21:52:01.929: INFO: (10) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 35.839077ms)
    Jan 18 21:52:01.930: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 36.263083ms)
    Jan 18 21:52:01.934: INFO: (10) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.776842ms)
    Jan 18 21:52:01.934: INFO: (10) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 41.054547ms)
    Jan 18 21:52:01.935: INFO: (10) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 41.426552ms)
    Jan 18 21:52:01.936: INFO: (10) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 43.199775ms)
    Jan 18 21:52:01.943: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 6.446685ms)
    Jan 18 21:52:01.948: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 11.504853ms)
    Jan 18 21:52:01.954: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 16.882924ms)
    Jan 18 21:52:01.954: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.073727ms)
    Jan 18 21:52:01.959: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 22.074094ms)
    Jan 18 21:52:01.970: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 33.403445ms)
    Jan 18 21:52:01.970: INFO: (11) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 33.487546ms)
    Jan 18 21:52:01.971: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 33.976052ms)
    Jan 18 21:52:01.971: INFO: (11) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 34.730462ms)
    Jan 18 21:52:01.972: INFO: (11) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 35.356771ms)
    Jan 18 21:52:01.973: INFO: (11) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 36.249183ms)
    Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 42.82647ms)
    Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 43.417078ms)
    Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 43.686081ms)
    Jan 18 21:52:01.980: INFO: (11) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 43.383778ms)
    Jan 18 21:52:01.982: INFO: (11) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 44.689195ms)
    Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 12.146762ms)
    Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 12.02166ms)
    Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.597267ms)
    Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.436366ms)
    Jan 18 21:52:01.994: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 12.498967ms)
    Jan 18 21:52:02.002: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 20.764676ms)
    Jan 18 21:52:02.009: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 27.148861ms)
    Jan 18 21:52:02.009: INFO: (12) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.689869ms)
    Jan 18 21:52:02.014: INFO: (12) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 32.043926ms)
    Jan 18 21:52:02.015: INFO: (12) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 32.277229ms)
    Jan 18 21:52:02.015: INFO: (12) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 32.491132ms)
    Jan 18 21:52:02.018: INFO: (12) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 35.727176ms)
    Jan 18 21:52:02.018: INFO: (12) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 35.676775ms)
    Jan 18 21:52:02.024: INFO: (12) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 41.32165ms)
    Jan 18 21:52:02.024: INFO: (12) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.916658ms)
    Jan 18 21:52:02.025: INFO: (12) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 43.388678ms)
    Jan 18 21:52:02.037: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 11.558954ms)
    Jan 18 21:52:02.037: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 11.888359ms)
    Jan 18 21:52:02.038: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 12.363365ms)
    Jan 18 21:52:02.042: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 15.885312ms)
    Jan 18 21:52:02.042: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 15.847511ms)
    Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 20.698075ms)
    Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 20.842178ms)
    Jan 18 21:52:02.047: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.473686ms)
    Jan 18 21:52:02.050: INFO: (13) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 24.375125ms)
    Jan 18 21:52:02.055: INFO: (13) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 29.26549ms)
    Jan 18 21:52:02.056: INFO: (13) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 30.460905ms)
    Jan 18 21:52:02.061: INFO: (13) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 34.709662ms)
    Jan 18 21:52:02.061: INFO: (13) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.36137ms)
    Jan 18 21:52:02.064: INFO: (13) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 38.739216ms)
    Jan 18 21:52:02.067: INFO: (13) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.756042ms)
    Jan 18 21:52:02.068: INFO: (13) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 42.491166ms)
    Jan 18 21:52:02.082: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 12.861772ms)
    Jan 18 21:52:02.083: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 13.738983ms)
    Jan 18 21:52:02.083: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.371191ms)
    Jan 18 21:52:02.091: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.400985ms)
    Jan 18 21:52:02.091: INFO: (14) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 22.130294ms)
    Jan 18 21:52:02.092: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 23.093407ms)
    Jan 18 21:52:02.100: INFO: (14) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 31.367017ms)
    Jan 18 21:52:02.100: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 31.464519ms)
    Jan 18 21:52:02.101: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 31.818523ms)
    Jan 18 21:52:02.104: INFO: (14) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.005066ms)
    Jan 18 21:52:02.104: INFO: (14) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 35.370371ms)
    Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 40.456439ms)
    Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 40.680841ms)
    Jan 18 21:52:02.109: INFO: (14) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 40.821643ms)
    Jan 18 21:52:02.113: INFO: (14) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 44.022086ms)
    Jan 18 21:52:02.113: INFO: (14) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 44.681395ms)
    Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 17.083727ms)
    Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 17.573933ms)
    Jan 18 21:52:02.131: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 17.403532ms)
    Jan 18 21:52:02.133: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 19.320657ms)
    Jan 18 21:52:02.139: INFO: (15) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 25.736843ms)
    Jan 18 21:52:02.139: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.828844ms)
    Jan 18 21:52:02.140: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 26.660355ms)
    Jan 18 21:52:02.140: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 26.828557ms)
    Jan 18 21:52:02.152: INFO: (15) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 38.31541ms)
    Jan 18 21:52:02.152: INFO: (15) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 38.406611ms)
    Jan 18 21:52:02.153: INFO: (15) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 39.245622ms)
    Jan 18 21:52:02.156: INFO: (15) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 42.543767ms)
    Jan 18 21:52:02.164: INFO: (15) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 50.921478ms)
    Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 50.923178ms)
    Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 51.14438ms)
    Jan 18 21:52:02.165: INFO: (15) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 50.881377ms)
    Jan 18 21:52:02.172: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 7.5979ms)
    Jan 18 21:52:02.173: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 8.075907ms)
    Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 13.533779ms)
    Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 13.441477ms)
    Jan 18 21:52:02.178: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 13.69898ms)
    Jan 18 21:52:02.183: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 18.615846ms)
    Jan 18 21:52:02.183: INFO: (16) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 18.811448ms)
    Jan 18 21:52:02.191: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 25.976343ms)
    Jan 18 21:52:02.191: INFO: (16) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 26.53985ms)
    Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 41.316644ms)
    Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 41.425345ms)
    Jan 18 21:52:02.206: INFO: (16) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 41.319745ms)
    Jan 18 21:52:02.211: INFO: (16) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 46.717816ms)
    Jan 18 21:52:02.211: INFO: (16) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 46.693215ms)
    Jan 18 21:52:02.216: INFO: (16) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 51.429477ms)
    Jan 18 21:52:02.217: INFO: (16) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 51.913684ms)
    Jan 18 21:52:02.222: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 5.594974ms)
    Jan 18 21:52:02.232: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 14.589492ms)
    Jan 18 21:52:02.232: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.024785ms)
    Jan 18 21:52:02.237: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 19.614958ms)
    Jan 18 21:52:02.238: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 19.643359ms)
    Jan 18 21:52:02.247: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 30.202398ms)
    Jan 18 21:52:02.247: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 30.142197ms)
    Jan 18 21:52:02.252: INFO: (17) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 34.92306ms)
    Jan 18 21:52:02.253: INFO: (17) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 35.107663ms)
    Jan 18 21:52:02.253: INFO: (17) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 35.67867ms)
    Jan 18 21:52:02.258: INFO: (17) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 41.161942ms)
    Jan 18 21:52:02.263: INFO: (17) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 45.741903ms)
    Jan 18 21:52:02.263: INFO: (17) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 46.212209ms)
    Jan 18 21:52:02.269: INFO: (17) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 52.014385ms)
    Jan 18 21:52:02.273: INFO: (17) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 55.296028ms)
    Jan 18 21:52:02.273: INFO: (17) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 55.059425ms)
    Jan 18 21:52:02.295: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 21.221479ms)
    Jan 18 21:52:02.295: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 22.01479ms)
    Jan 18 21:52:02.296: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 22.734599ms)
    Jan 18 21:52:02.299: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 25.471435ms)
    Jan 18 21:52:02.300: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 26.412248ms)
    Jan 18 21:52:02.310: INFO: (18) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 36.621982ms)
    Jan 18 21:52:02.310: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 36.924686ms)
    Jan 18 21:52:02.311: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 38.071001ms)
    Jan 18 21:52:02.313: INFO: (18) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 40.122528ms)
    Jan 18 21:52:02.314: INFO: (18) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 40.808838ms)
    Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 49.495152ms)
    Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 49.449351ms)
    Jan 18 21:52:02.323: INFO: (18) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 49.689054ms)
    Jan 18 21:52:02.326: INFO: (18) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 53.053699ms)
    Jan 18 21:52:02.326: INFO: (18) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 53.218201ms)
    Jan 18 21:52:02.331: INFO: (18) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 58.013964ms)
    Jan 18 21:52:02.346: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 14.086385ms)
    Jan 18 21:52:02.346: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">... (200; 14.719594ms)
    Jan 18 21:52:02.350: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:462/proxy/: tls qux (200; 18.856348ms)
    Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/pods/http:proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 27.119757ms)
    Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:1080/proxy/rewriteme">test<... (200; 27.181658ms)
    Jan 18 21:52:02.359: INFO: (19) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname2/proxy/: tls qux (200; 27.37186ms)
    Jan 18 21:52:02.364: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:443/proxy/tlsrewritem... (200; 32.67943ms)
    Jan 18 21:52:02.364: INFO: (19) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname1/proxy/: foo (200; 32.536628ms)
    Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/: <a href="/api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf/proxy/rewriteme">test</a> (200; 38.139402ms)
    Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname1/proxy/: foo (200; 38.001201ms)
    Jan 18 21:52:02.370: INFO: (19) /api/v1/namespaces/proxy-2385/pods/https:proxy-service-7xxn6-rdbbf:460/proxy/: tls baz (200; 38.419906ms)
    Jan 18 21:52:02.377: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:160/proxy/: foo (200; 44.905891ms)
    Jan 18 21:52:02.377: INFO: (19) /api/v1/namespaces/proxy-2385/pods/proxy-service-7xxn6-rdbbf:162/proxy/: bar (200; 45.085394ms)
    Jan 18 21:52:02.382: INFO: (19) /api/v1/namespaces/proxy-2385/services/proxy-service-7xxn6:portname2/proxy/: bar (200; 50.200961ms)
    Jan 18 21:52:02.382: INFO: (19) /api/v1/namespaces/proxy-2385/services/https:proxy-service-7xxn6:tlsportname1/proxy/: tls baz (200; 50.443565ms)
    Jan 18 21:52:02.387: INFO: (19) /api/v1/namespaces/proxy-2385/services/http:proxy-service-7xxn6:portname2/proxy/: bar (200; 54.989024ms)
    STEP: deleting ReplicationController proxy-service-7xxn6 in namespace proxy-2385, will wait for the garbage collector to delete the pods 01/18/23 21:52:02.387
    Jan 18 21:52:02.463: INFO: Deleting ReplicationController proxy-service-7xxn6 took: 20.46567ms
    Jan 18 21:52:02.864: INFO: Terminating ReplicationController proxy-service-7xxn6 pods took: 400.427973ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:04.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2385" for this suite. 01/18/23 21:52:04.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:04.384
Jan 18 21:52:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename subpath 01/18/23 21:52:04.385
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:04.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:04.854
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 21:52:04.857
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-cdlj 01/18/23 21:52:04.88
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 21:52:04.88
Jan 18 21:52:04.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cdlj" in namespace "subpath-1597" to be "Succeeded or Failed"
Jan 18 21:52:04.907: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737589ms
Jan 18 21:52:06.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011614691s
Jan 18 21:52:08.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013212949s
Jan 18 21:52:10.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 6.011976033s
Jan 18 21:52:12.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 8.011941097s
Jan 18 21:52:14.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012388466s
Jan 18 21:52:16.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012701149s
Jan 18 21:52:18.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 14.012217747s
Jan 18 21:52:20.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 16.012780907s
Jan 18 21:52:22.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 18.01199665s
Jan 18 21:52:24.927: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 20.026995989s
Jan 18 21:52:26.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=false. Elapsed: 22.011736412s
Jan 18 21:52:28.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012360882s
STEP: Saw pod success 01/18/23 21:52:28.912
Jan 18 21:52:28.913: INFO: Pod "pod-subpath-test-secret-cdlj" satisfied condition "Succeeded or Failed"
Jan 18 21:52:28.916: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-secret-cdlj container test-container-subpath-secret-cdlj: <nil>
STEP: delete the pod 01/18/23 21:52:28.925
Jan 18 21:52:28.962: INFO: Waiting for pod pod-subpath-test-secret-cdlj to disappear
Jan 18 21:52:28.970: INFO: Pod pod-subpath-test-secret-cdlj no longer exists
STEP: Deleting pod pod-subpath-test-secret-cdlj 01/18/23 21:52:28.97
Jan 18 21:52:28.970: INFO: Deleting pod "pod-subpath-test-secret-cdlj" in namespace "subpath-1597"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1597" for this suite. 01/18/23 21:52:28.979
------------------------------
• [SLOW TEST] [24.636 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:04.384
    Jan 18 21:52:04.384: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename subpath 01/18/23 21:52:04.385
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:04.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:04.854
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 21:52:04.857
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-cdlj 01/18/23 21:52:04.88
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 21:52:04.88
    Jan 18 21:52:04.900: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-cdlj" in namespace "subpath-1597" to be "Succeeded or Failed"
    Jan 18 21:52:04.907: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737589ms
    Jan 18 21:52:06.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011614691s
    Jan 18 21:52:08.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013212949s
    Jan 18 21:52:10.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 6.011976033s
    Jan 18 21:52:12.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 8.011941097s
    Jan 18 21:52:14.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 10.012388466s
    Jan 18 21:52:16.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 12.012701149s
    Jan 18 21:52:18.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 14.012217747s
    Jan 18 21:52:20.913: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 16.012780907s
    Jan 18 21:52:22.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 18.01199665s
    Jan 18 21:52:24.927: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=true. Elapsed: 20.026995989s
    Jan 18 21:52:26.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Running", Reason="", readiness=false. Elapsed: 22.011736412s
    Jan 18 21:52:28.912: INFO: Pod "pod-subpath-test-secret-cdlj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012360882s
    STEP: Saw pod success 01/18/23 21:52:28.912
    Jan 18 21:52:28.913: INFO: Pod "pod-subpath-test-secret-cdlj" satisfied condition "Succeeded or Failed"
    Jan 18 21:52:28.916: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-secret-cdlj container test-container-subpath-secret-cdlj: <nil>
    STEP: delete the pod 01/18/23 21:52:28.925
    Jan 18 21:52:28.962: INFO: Waiting for pod pod-subpath-test-secret-cdlj to disappear
    Jan 18 21:52:28.970: INFO: Pod pod-subpath-test-secret-cdlj no longer exists
    STEP: Deleting pod pod-subpath-test-secret-cdlj 01/18/23 21:52:28.97
    Jan 18 21:52:28.970: INFO: Deleting pod "pod-subpath-test-secret-cdlj" in namespace "subpath-1597"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1597" for this suite. 01/18/23 21:52:28.979
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:29.02
Jan 18 21:52:29.020: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename events 01/18/23 21:52:29.022
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:29.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:29.198
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/18/23 21:52:29.2
Jan 18 21:52:29.220: INFO: created test-event-1
Jan 18 21:52:29.237: INFO: created test-event-2
Jan 18 21:52:29.254: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/18/23 21:52:29.254
STEP: delete collection of events 01/18/23 21:52:29.26
Jan 18 21:52:29.260: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 21:52:29.345
Jan 18 21:52:29.345: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:29.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1078" for this suite. 01/18/23 21:52:29.356
------------------------------
• [0.348 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:29.02
    Jan 18 21:52:29.020: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename events 01/18/23 21:52:29.022
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:29.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:29.198
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/18/23 21:52:29.2
    Jan 18 21:52:29.220: INFO: created test-event-1
    Jan 18 21:52:29.237: INFO: created test-event-2
    Jan 18 21:52:29.254: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/18/23 21:52:29.254
    STEP: delete collection of events 01/18/23 21:52:29.26
    Jan 18 21:52:29.260: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 21:52:29.345
    Jan 18 21:52:29.345: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:29.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1078" for this suite. 01/18/23 21:52:29.356
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:29.368
Jan 18 21:52:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename podtemplate 01/18/23 21:52:29.37
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:29.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:29.663
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:29.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3128" for this suite. 01/18/23 21:52:29.756
------------------------------
• [0.415 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:29.368
    Jan 18 21:52:29.368: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename podtemplate 01/18/23 21:52:29.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:29.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:29.663
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:29.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3128" for this suite. 01/18/23 21:52:29.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:29.785
Jan 18 21:52:29.785: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-pred 01/18/23 21:52:29.786
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:30.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:30.021
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 21:52:30.024: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 21:52:30.037: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 21:52:30.043: INFO: 
Logging pods the apiserver thinks is on node test-vm-1 before test
Jan 18 21:52:30.055: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
Jan 18 21:52:30.055: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:52:30.055: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:52:30.055: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:52:30.055: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:52:30.055: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
Jan 18 21:52:30.055: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 21:52:30.055: INFO: 
Logging pods the apiserver thinks is on node test-vm-2 before test
Jan 18 21:52:30.068: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
Jan 18 21:52:30.069: INFO: 	Container coredns ready: true, restart count 0
Jan 18 21:52:30.069: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:52:30.069: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 21:52:30.069: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:52:30.069: INFO: 	Container e2e ready: true, restart count 0
Jan 18 21:52:30.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:52:30.069: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 21:52:30.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 21:52:30.069: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 21:52:30.069: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
Jan 18 21:52:30.069: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node test-vm-1 01/18/23 21:52:30.099
STEP: verifying the node has the label node test-vm-2 01/18/23 21:52:30.136
Jan 18 21:52:30.153: INFO: Pod coredns-6f5f9b5d74-j2j8v requesting resource cpu=100m on Node test-vm-2
Jan 18 21:52:30.153: INFO: Pod calico-node-j9rg6 requesting resource cpu=250m on Node test-vm-1
Jan 18 21:52:30.153: INFO: Pod calico-node-kv49n requesting resource cpu=250m on Node test-vm-2
Jan 18 21:52:30.153: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf requesting resource cpu=0m on Node test-vm-1
Jan 18 21:52:30.153: INFO: Pod sonobuoy-e2e-job-0fc7392783254edf requesting resource cpu=0m on Node test-vm-2
Jan 18 21:52:30.153: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 requesting resource cpu=0m on Node test-vm-2
Jan 18 21:52:30.153: INFO: Pod calico-kube-controllers-5b6fd6b6d-mmvrs requesting resource cpu=0m on Node test-vm-2
Jan 18 21:52:30.153: INFO: Pod sonobuoy requesting resource cpu=0m on Node test-vm-1
STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 21:52:30.153
Jan 18 21:52:30.153: INFO: Creating a pod which consumes cpu=2625m on Node test-vm-1
Jan 18 21:52:30.181: INFO: Creating a pod which consumes cpu=2555m on Node test-vm-2
Jan 18 21:52:30.218: INFO: Waiting up to 5m0s for pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556" in namespace "sched-pred-9321" to be "running"
Jan 18 21:52:30.225: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059081ms
Jan 18 21:52:32.231: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013015113s
Jan 18 21:52:34.232: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Running", Reason="", readiness=true. Elapsed: 4.013876487s
Jan 18 21:52:34.232: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556" satisfied condition "running"
Jan 18 21:52:34.232: INFO: Waiting up to 5m0s for pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781" in namespace "sched-pred-9321" to be "running"
Jan 18 21:52:34.236: INFO: Pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781": Phase="Running", Reason="", readiness=true. Elapsed: 4.206049ms
Jan 18 21:52:34.237: INFO: Pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 21:52:34.237
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c0f0f31823], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-9155d908-ccfb-4288-891a-78a217553556 to test-vm-1] 01/18/23 21:52:34.245
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c0f2b6d015], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-13a5792c-edbd-4d88-b951-77da57503781 to test-vm-2] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c128ca34d2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c12c5e5248], Reason = [Created], Message = [Created container filler-pod-9155d908-ccfb-4288-891a-78a217553556] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c12eb56df6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c131470103], Reason = [Created], Message = [Created container filler-pod-13a5792c-edbd-4d88-b951-77da57503781] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c134e0ac65], Reason = [Started], Message = [Started container filler-pod-9155d908-ccfb-4288-891a-78a217553556] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c1381ee0cb], Reason = [Started], Message = [Started container filler-pod-13a5792c-edbd-4d88-b951-77da57503781] 01/18/23 21:52:34.246
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b85c1e213089d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/18/23 21:52:34.296
STEP: removing the label node off the node test-vm-1 01/18/23 21:52:35.271
STEP: verifying the node doesn't have the label node 01/18/23 21:52:35.293
STEP: removing the label node off the node test-vm-2 01/18/23 21:52:35.301
STEP: verifying the node doesn't have the label node 01/18/23 21:52:35.345
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:35.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9321" for this suite. 01/18/23 21:52:35.358
------------------------------
• [SLOW TEST] [5.589 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:29.785
    Jan 18 21:52:29.785: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-pred 01/18/23 21:52:29.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:30.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:30.021
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 21:52:30.024: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 21:52:30.037: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 21:52:30.043: INFO: 
    Logging pods the apiserver thinks is on node test-vm-1 before test
    Jan 18 21:52:30.055: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
    Jan 18 21:52:30.055: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:52:30.055: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:52:30.055: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:52:30.055: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:52:30.055: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
    Jan 18 21:52:30.055: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 21:52:30.055: INFO: 
    Logging pods the apiserver thinks is on node test-vm-2 before test
    Jan 18 21:52:30.068: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
    Jan 18 21:52:30.069: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:52:30.069: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:52:30.069: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 21:52:30.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 21:52:30.069: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
    Jan 18 21:52:30.069: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node test-vm-1 01/18/23 21:52:30.099
    STEP: verifying the node has the label node test-vm-2 01/18/23 21:52:30.136
    Jan 18 21:52:30.153: INFO: Pod coredns-6f5f9b5d74-j2j8v requesting resource cpu=100m on Node test-vm-2
    Jan 18 21:52:30.153: INFO: Pod calico-node-j9rg6 requesting resource cpu=250m on Node test-vm-1
    Jan 18 21:52:30.153: INFO: Pod calico-node-kv49n requesting resource cpu=250m on Node test-vm-2
    Jan 18 21:52:30.153: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf requesting resource cpu=0m on Node test-vm-1
    Jan 18 21:52:30.153: INFO: Pod sonobuoy-e2e-job-0fc7392783254edf requesting resource cpu=0m on Node test-vm-2
    Jan 18 21:52:30.153: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 requesting resource cpu=0m on Node test-vm-2
    Jan 18 21:52:30.153: INFO: Pod calico-kube-controllers-5b6fd6b6d-mmvrs requesting resource cpu=0m on Node test-vm-2
    Jan 18 21:52:30.153: INFO: Pod sonobuoy requesting resource cpu=0m on Node test-vm-1
    STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 21:52:30.153
    Jan 18 21:52:30.153: INFO: Creating a pod which consumes cpu=2625m on Node test-vm-1
    Jan 18 21:52:30.181: INFO: Creating a pod which consumes cpu=2555m on Node test-vm-2
    Jan 18 21:52:30.218: INFO: Waiting up to 5m0s for pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556" in namespace "sched-pred-9321" to be "running"
    Jan 18 21:52:30.225: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Pending", Reason="", readiness=false. Elapsed: 7.059081ms
    Jan 18 21:52:32.231: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013015113s
    Jan 18 21:52:34.232: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556": Phase="Running", Reason="", readiness=true. Elapsed: 4.013876487s
    Jan 18 21:52:34.232: INFO: Pod "filler-pod-9155d908-ccfb-4288-891a-78a217553556" satisfied condition "running"
    Jan 18 21:52:34.232: INFO: Waiting up to 5m0s for pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781" in namespace "sched-pred-9321" to be "running"
    Jan 18 21:52:34.236: INFO: Pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781": Phase="Running", Reason="", readiness=true. Elapsed: 4.206049ms
    Jan 18 21:52:34.237: INFO: Pod "filler-pod-13a5792c-edbd-4d88-b951-77da57503781" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 21:52:34.237
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c0f0f31823], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-9155d908-ccfb-4288-891a-78a217553556 to test-vm-1] 01/18/23 21:52:34.245
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c0f2b6d015], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-13a5792c-edbd-4d88-b951-77da57503781 to test-vm-2] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c128ca34d2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c12c5e5248], Reason = [Created], Message = [Created container filler-pod-9155d908-ccfb-4288-891a-78a217553556] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c12eb56df6], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c131470103], Reason = [Created], Message = [Created container filler-pod-13a5792c-edbd-4d88-b951-77da57503781] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9155d908-ccfb-4288-891a-78a217553556.173b85c134e0ac65], Reason = [Started], Message = [Started container filler-pod-9155d908-ccfb-4288-891a-78a217553556] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-13a5792c-edbd-4d88-b951-77da57503781.173b85c1381ee0cb], Reason = [Started], Message = [Started container filler-pod-13a5792c-edbd-4d88-b951-77da57503781] 01/18/23 21:52:34.246
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b85c1e213089d], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 01/18/23 21:52:34.296
    STEP: removing the label node off the node test-vm-1 01/18/23 21:52:35.271
    STEP: verifying the node doesn't have the label node 01/18/23 21:52:35.293
    STEP: removing the label node off the node test-vm-2 01/18/23 21:52:35.301
    STEP: verifying the node doesn't have the label node 01/18/23 21:52:35.345
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:35.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9321" for this suite. 01/18/23 21:52:35.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:35.376
Jan 18 21:52:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 21:52:35.377
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:35.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:35.73
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/18/23 21:52:35.732
Jan 18 21:52:35.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 create -f -'
Jan 18 21:52:36.447: INFO: stderr: ""
Jan 18 21:52:36.447: INFO: stdout: "pod/pause created\n"
Jan 18 21:52:36.447: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 21:52:36.447: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2290" to be "running and ready"
Jan 18 21:52:36.453: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.736867ms
Jan 18 21:52:36.453: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
Jan 18 21:52:38.457: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010550005s
Jan 18 21:52:38.457: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 21:52:38.457: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 21:52:38.458
Jan 18 21:52:38.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 label pods pause testing-label=testing-label-value'
Jan 18 21:52:38.551: INFO: stderr: ""
Jan 18 21:52:38.551: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 21:52:38.551
Jan 18 21:52:38.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pod pause -L testing-label'
Jan 18 21:52:38.633: INFO: stderr: ""
Jan 18 21:52:38.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/18/23 21:52:38.633
Jan 18 21:52:38.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 label pods pause testing-label-'
Jan 18 21:52:38.726: INFO: stderr: ""
Jan 18 21:52:38.726: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/18/23 21:52:38.726
Jan 18 21:52:38.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pod pause -L testing-label'
Jan 18 21:52:38.810: INFO: stderr: ""
Jan 18 21:52:38.810: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/18/23 21:52:38.81
Jan 18 21:52:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 delete --grace-period=0 --force -f -'
Jan 18 21:52:38.923: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 21:52:38.923: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 21:52:38.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get rc,svc -l name=pause --no-headers'
Jan 18 21:52:39.010: INFO: stderr: "No resources found in kubectl-2290 namespace.\n"
Jan 18 21:52:39.010: INFO: stdout: ""
Jan 18 21:52:39.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 21:52:39.095: INFO: stderr: ""
Jan 18 21:52:39.095: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:39.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2290" for this suite. 01/18/23 21:52:39.1
------------------------------
• [3.748 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:35.376
    Jan 18 21:52:35.376: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 21:52:35.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:35.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:35.73
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/18/23 21:52:35.732
    Jan 18 21:52:35.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 create -f -'
    Jan 18 21:52:36.447: INFO: stderr: ""
    Jan 18 21:52:36.447: INFO: stdout: "pod/pause created\n"
    Jan 18 21:52:36.447: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 18 21:52:36.447: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2290" to be "running and ready"
    Jan 18 21:52:36.453: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.736867ms
    Jan 18 21:52:36.453: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '' to be 'Running' but was 'Pending'
    Jan 18 21:52:38.457: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010550005s
    Jan 18 21:52:38.457: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 18 21:52:38.457: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 21:52:38.458
    Jan 18 21:52:38.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 label pods pause testing-label=testing-label-value'
    Jan 18 21:52:38.551: INFO: stderr: ""
    Jan 18 21:52:38.551: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 21:52:38.551
    Jan 18 21:52:38.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pod pause -L testing-label'
    Jan 18 21:52:38.633: INFO: stderr: ""
    Jan 18 21:52:38.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/18/23 21:52:38.633
    Jan 18 21:52:38.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 label pods pause testing-label-'
    Jan 18 21:52:38.726: INFO: stderr: ""
    Jan 18 21:52:38.726: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/18/23 21:52:38.726
    Jan 18 21:52:38.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pod pause -L testing-label'
    Jan 18 21:52:38.810: INFO: stderr: ""
    Jan 18 21:52:38.810: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/18/23 21:52:38.81
    Jan 18 21:52:38.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 delete --grace-period=0 --force -f -'
    Jan 18 21:52:38.923: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 21:52:38.923: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 18 21:52:38.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get rc,svc -l name=pause --no-headers'
    Jan 18 21:52:39.010: INFO: stderr: "No resources found in kubectl-2290 namespace.\n"
    Jan 18 21:52:39.010: INFO: stdout: ""
    Jan 18 21:52:39.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2290 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 21:52:39.095: INFO: stderr: ""
    Jan 18 21:52:39.095: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:39.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2290" for this suite. 01/18/23 21:52:39.1
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:39.124
Jan 18 21:52:39.124: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename proxy 01/18/23 21:52:39.127
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:39.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:39.77
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 18 21:52:39.773: INFO: Creating pod...
Jan 18 21:52:39.789: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2610" to be "running"
Jan 18 21:52:39.793: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356951ms
Jan 18 21:52:41.798: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009209889s
Jan 18 21:52:41.798: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 21:52:41.798: INFO: Creating service...
Jan 18 21:52:41.823: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=DELETE
Jan 18 21:52:41.830: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 21:52:41.830: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=OPTIONS
Jan 18 21:52:41.835: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 21:52:41.835: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=PATCH
Jan 18 21:52:41.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 21:52:41.839: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=POST
Jan 18 21:52:41.843: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 21:52:41.843: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=PUT
Jan 18 21:52:41.847: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 21:52:41.847: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 21:52:41.852: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.866: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.876: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.887: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.896: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.907: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.916: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.928: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.937: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 21:52:41.951: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 21:52:41.951: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 21:52:41.959: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 21:52:41.959: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 21:52:41.970: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 21:52:41.970: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 21:52:41.977: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 21:52:41.977: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 21:52:41.984: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 21:52:41.984: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=GET
Jan 18 21:52:41.988: INFO: http.Client request:GET StatusCode:301
Jan 18 21:52:41.988: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 21:52:41.994: INFO: http.Client request:GET StatusCode:301
Jan 18 21:52:41.994: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=HEAD
Jan 18 21:52:41.998: INFO: http.Client request:HEAD StatusCode:301
Jan 18 21:52:41.998: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 21:52:42.005: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:42.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2610" for this suite. 01/18/23 21:52:42.01
------------------------------
• [2.901 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:39.124
    Jan 18 21:52:39.124: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename proxy 01/18/23 21:52:39.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:39.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:39.77
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 18 21:52:39.773: INFO: Creating pod...
    Jan 18 21:52:39.789: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2610" to be "running"
    Jan 18 21:52:39.793: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356951ms
    Jan 18 21:52:41.798: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.009209889s
    Jan 18 21:52:41.798: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 21:52:41.798: INFO: Creating service...
    Jan 18 21:52:41.823: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=DELETE
    Jan 18 21:52:41.830: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 21:52:41.830: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=OPTIONS
    Jan 18 21:52:41.835: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 21:52:41.835: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=PATCH
    Jan 18 21:52:41.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 21:52:41.839: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=POST
    Jan 18 21:52:41.843: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 21:52:41.843: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=PUT
    Jan 18 21:52:41.847: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 21:52:41.847: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 18 21:52:41.852: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.866: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.876: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.887: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.896: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.907: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.916: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.928: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.937: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 21:52:41.951: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 21:52:41.951: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 18 21:52:41.959: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 21:52:41.959: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 18 21:52:41.970: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 21:52:41.970: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=POST
    Jan 18 21:52:41.977: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 21:52:41.977: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 18 21:52:41.984: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 21:52:41.984: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=GET
    Jan 18 21:52:41.988: INFO: http.Client request:GET StatusCode:301
    Jan 18 21:52:41.988: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=GET
    Jan 18 21:52:41.994: INFO: http.Client request:GET StatusCode:301
    Jan 18 21:52:41.994: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/pods/agnhost/proxy?method=HEAD
    Jan 18 21:52:41.998: INFO: http.Client request:HEAD StatusCode:301
    Jan 18 21:52:41.998: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-2610/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 18 21:52:42.005: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:42.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2610" for this suite. 01/18/23 21:52:42.01
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:42.026
Jan 18 21:52:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename endpointslice 01/18/23 21:52:42.027
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:42.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:42.304
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 21:52:46.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-392" for this suite. 01/18/23 21:52:46.405
------------------------------
• [4.392 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:42.026
    Jan 18 21:52:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename endpointslice 01/18/23 21:52:42.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:42.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:42.304
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:52:46.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-392" for this suite. 01/18/23 21:52:46.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:52:46.42
Jan 18 21:52:46.420: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename taint-single-pod 01/18/23 21:52:46.421
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:46.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:46.818
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan 18 21:52:46.821: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 21:53:46.836: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan 18 21:53:46.840: INFO: Starting informer...
STEP: Starting pod... 01/18/23 21:53:46.84
Jan 18 21:53:47.065: INFO: Pod is running on test-vm-1. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 21:53:47.065
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:53:47.081
STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 21:53:47.087
Jan 18 21:53:47.087: INFO: Pod wasn't evicted. Proceeding
Jan 18 21:53:47.087: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:53:47.102
STEP: Waiting some time to make sure that toleration time passed. 01/18/23 21:53:47.113
Jan 18 21:55:02.114: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:55:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-2647" for this suite. 01/18/23 21:55:02.121
------------------------------
• [SLOW TEST] [135.714 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:52:46.42
    Jan 18 21:52:46.420: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename taint-single-pod 01/18/23 21:52:46.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:52:46.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:52:46.818
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan 18 21:52:46.821: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 21:53:46.836: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan 18 21:53:46.840: INFO: Starting informer...
    STEP: Starting pod... 01/18/23 21:53:46.84
    Jan 18 21:53:47.065: INFO: Pod is running on test-vm-1. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 21:53:47.065
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:53:47.081
    STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 21:53:47.087
    Jan 18 21:53:47.087: INFO: Pod wasn't evicted. Proceeding
    Jan 18 21:53:47.087: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 21:53:47.102
    STEP: Waiting some time to make sure that toleration time passed. 01/18/23 21:53:47.113
    Jan 18 21:55:02.114: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:55:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-2647" for this suite. 01/18/23 21:55:02.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:55:02.135
Jan 18 21:55:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 21:55:02.136
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:02.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:02.857
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb in namespace container-probe-1008 01/18/23 21:55:02.86
Jan 18 21:55:02.887: INFO: Waiting up to 5m0s for pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb" in namespace "container-probe-1008" to be "not pending"
Jan 18 21:55:02.891: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081042ms
Jan 18 21:55:04.896: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009440046s
Jan 18 21:55:04.896: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb" satisfied condition "not pending"
Jan 18 21:55:04.896: INFO: Started pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb in namespace container-probe-1008
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:55:04.896
Jan 18 21:55:04.901: INFO: Initial restart count of pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb is 0
STEP: deleting the pod 01/18/23 21:59:05.605
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:05.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1008" for this suite. 01/18/23 21:59:05.666
------------------------------
• [SLOW TEST] [243.572 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:55:02.135
    Jan 18 21:55:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 21:55:02.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:55:02.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:55:02.857
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb in namespace container-probe-1008 01/18/23 21:55:02.86
    Jan 18 21:55:02.887: INFO: Waiting up to 5m0s for pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb" in namespace "container-probe-1008" to be "not pending"
    Jan 18 21:55:02.891: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081042ms
    Jan 18 21:55:04.896: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb": Phase="Running", Reason="", readiness=true. Elapsed: 2.009440046s
    Jan 18 21:55:04.896: INFO: Pod "liveness-48659123-58c6-4d30-a146-041f0a45e8fb" satisfied condition "not pending"
    Jan 18 21:55:04.896: INFO: Started pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb in namespace container-probe-1008
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 21:55:04.896
    Jan 18 21:55:04.901: INFO: Initial restart count of pod liveness-48659123-58c6-4d30-a146-041f0a45e8fb is 0
    STEP: deleting the pod 01/18/23 21:59:05.605
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:05.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1008" for this suite. 01/18/23 21:59:05.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:05.711
Jan 18 21:59:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 21:59:05.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:05.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:05.987
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/18/23 21:59:06.224
Jan 18 21:59:06.244: INFO: Waiting up to 5m0s for pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a" in namespace "projected-7482" to be "running and ready"
Jan 18 21:59:06.248: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583646ms
Jan 18 21:59:06.248: INFO: The phase of Pod labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:59:08.253: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008648977s
Jan 18 21:59:08.253: INFO: The phase of Pod labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a is Running (Ready = true)
Jan 18 21:59:08.253: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a" satisfied condition "running and ready"
Jan 18 21:59:08.793: INFO: Successfully updated pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:12.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7482" for this suite. 01/18/23 21:59:12.825
------------------------------
• [SLOW TEST] [7.126 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:05.711
    Jan 18 21:59:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 21:59:05.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:05.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:05.987
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/18/23 21:59:06.224
    Jan 18 21:59:06.244: INFO: Waiting up to 5m0s for pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a" in namespace "projected-7482" to be "running and ready"
    Jan 18 21:59:06.248: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583646ms
    Jan 18 21:59:06.248: INFO: The phase of Pod labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:59:08.253: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a": Phase="Running", Reason="", readiness=true. Elapsed: 2.008648977s
    Jan 18 21:59:08.253: INFO: The phase of Pod labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a is Running (Ready = true)
    Jan 18 21:59:08.253: INFO: Pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a" satisfied condition "running and ready"
    Jan 18 21:59:08.793: INFO: Successfully updated pod "labelsupdate3df5cce0-c9e3-4774-86e0-4dd598a8e74a"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:12.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7482" for this suite. 01/18/23 21:59:12.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:12.838
Jan 18 21:59:12.839: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename containers 01/18/23 21:59:12.84
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:13.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:13.163
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/18/23 21:59:13.168
Jan 18 21:59:13.186: INFO: Waiting up to 5m0s for pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36" in namespace "containers-1864" to be "Succeeded or Failed"
Jan 18 21:59:13.189: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669443ms
Jan 18 21:59:15.194: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Running", Reason="", readiness=true. Elapsed: 2.008710752s
Jan 18 21:59:17.194: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Running", Reason="", readiness=false. Elapsed: 4.008018272s
Jan 18 21:59:19.193: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007566595s
STEP: Saw pod success 01/18/23 21:59:19.193
Jan 18 21:59:19.193: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36" satisfied condition "Succeeded or Failed"
Jan 18 21:59:19.196: INFO: Trying to get logs from node test-vm-1 pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 21:59:19.202
Jan 18 21:59:19.232: INFO: Waiting for pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 to disappear
Jan 18 21:59:19.237: INFO: Pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:19.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1864" for this suite. 01/18/23 21:59:19.242
------------------------------
• [SLOW TEST] [6.415 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:12.838
    Jan 18 21:59:12.839: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename containers 01/18/23 21:59:12.84
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:13.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:13.163
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/18/23 21:59:13.168
    Jan 18 21:59:13.186: INFO: Waiting up to 5m0s for pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36" in namespace "containers-1864" to be "Succeeded or Failed"
    Jan 18 21:59:13.189: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669443ms
    Jan 18 21:59:15.194: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Running", Reason="", readiness=true. Elapsed: 2.008710752s
    Jan 18 21:59:17.194: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Running", Reason="", readiness=false. Elapsed: 4.008018272s
    Jan 18 21:59:19.193: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007566595s
    STEP: Saw pod success 01/18/23 21:59:19.193
    Jan 18 21:59:19.193: INFO: Pod "client-containers-18bffb0e-d508-4ca4-80e4-963459458b36" satisfied condition "Succeeded or Failed"
    Jan 18 21:59:19.196: INFO: Trying to get logs from node test-vm-1 pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 21:59:19.202
    Jan 18 21:59:19.232: INFO: Waiting for pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 to disappear
    Jan 18 21:59:19.237: INFO: Pod client-containers-18bffb0e-d508-4ca4-80e4-963459458b36 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:19.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1864" for this suite. 01/18/23 21:59:19.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:19.262
Jan 18 21:59:19.262: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 21:59:19.263
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:19.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:19.874
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-c72724a9-fb02-4e67-8e6e-081815ee2180 01/18/23 21:59:19.881
STEP: Creating configMap with name cm-test-opt-upd-a39c9817-dc60-499b-a4fb-5abb3f6d3144 01/18/23 21:59:19.89
STEP: Creating the pod 01/18/23 21:59:19.9
Jan 18 21:59:19.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3" in namespace "configmap-673" to be "running and ready"
Jan 18 21:59:19.925: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575839ms
Jan 18 21:59:19.925: INFO: The phase of Pod pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 21:59:21.930: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008621604s
Jan 18 21:59:21.930: INFO: The phase of Pod pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3 is Running (Ready = true)
Jan 18 21:59:21.930: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-c72724a9-fb02-4e67-8e6e-081815ee2180 01/18/23 21:59:21.955
STEP: Updating configmap cm-test-opt-upd-a39c9817-dc60-499b-a4fb-5abb3f6d3144 01/18/23 21:59:21.968
STEP: Creating configMap with name cm-test-opt-create-5a24fe6c-93be-4e67-8b02-170aee3829f4 01/18/23 21:59:21.977
STEP: waiting to observe update in volume 01/18/23 21:59:21.986
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:24.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-673" for this suite. 01/18/23 21:59:24.017
------------------------------
• [4.766 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:19.262
    Jan 18 21:59:19.262: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 21:59:19.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:19.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:19.874
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-c72724a9-fb02-4e67-8e6e-081815ee2180 01/18/23 21:59:19.881
    STEP: Creating configMap with name cm-test-opt-upd-a39c9817-dc60-499b-a4fb-5abb3f6d3144 01/18/23 21:59:19.89
    STEP: Creating the pod 01/18/23 21:59:19.9
    Jan 18 21:59:19.921: INFO: Waiting up to 5m0s for pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3" in namespace "configmap-673" to be "running and ready"
    Jan 18 21:59:19.925: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.575839ms
    Jan 18 21:59:19.925: INFO: The phase of Pod pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 21:59:21.930: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008621604s
    Jan 18 21:59:21.930: INFO: The phase of Pod pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3 is Running (Ready = true)
    Jan 18 21:59:21.930: INFO: Pod "pod-configmaps-2103f831-e694-43f8-8877-b4e84b14f6f3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-c72724a9-fb02-4e67-8e6e-081815ee2180 01/18/23 21:59:21.955
    STEP: Updating configmap cm-test-opt-upd-a39c9817-dc60-499b-a4fb-5abb3f6d3144 01/18/23 21:59:21.968
    STEP: Creating configMap with name cm-test-opt-create-5a24fe6c-93be-4e67-8b02-170aee3829f4 01/18/23 21:59:21.977
    STEP: waiting to observe update in volume 01/18/23 21:59:21.986
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:24.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-673" for this suite. 01/18/23 21:59:24.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:24.03
Jan 18 21:59:24.030: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:59:24.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:24.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:24.874
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan 18 21:59:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 21:59:26.963
Jan 18 21:59:26.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
Jan 18 21:59:27.750: INFO: stderr: ""
Jan 18 21:59:27.750: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 21:59:27.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 delete e2e-test-crd-publish-openapi-1764-crds test-foo'
Jan 18 21:59:27.842: INFO: stderr: ""
Jan 18 21:59:27.842: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 21:59:27.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
Jan 18 21:59:28.573: INFO: stderr: ""
Jan 18 21:59:28.573: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 21:59:28.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 delete e2e-test-crd-publish-openapi-1764-crds test-foo'
Jan 18 21:59:28.663: INFO: stderr: ""
Jan 18 21:59:28.663: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 21:59:28.663
Jan 18 21:59:28.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
Jan 18 21:59:28.865: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 21:59:28.865
Jan 18 21:59:28.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
Jan 18 21:59:29.132: INFO: rc: 1
Jan 18 21:59:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
Jan 18 21:59:29.363: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 21:59:29.363
Jan 18 21:59:29.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
Jan 18 21:59:29.591: INFO: rc: 1
Jan 18 21:59:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
Jan 18 21:59:29.825: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/18/23 21:59:29.825
Jan 18 21:59:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds'
Jan 18 21:59:30.065: INFO: stderr: ""
Jan 18 21:59:30.065: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/18/23 21:59:30.066
Jan 18 21:59:30.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.metadata'
Jan 18 21:59:30.306: INFO: stderr: ""
Jan 18 21:59:30.307: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 21:59:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec'
Jan 18 21:59:30.519: INFO: stderr: ""
Jan 18 21:59:30.519: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 21:59:30.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec.bars'
Jan 18 21:59:30.733: INFO: stderr: ""
Jan 18 21:59:30.733: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 21:59:30.733
Jan 18 21:59:30.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec.bars2'
Jan 18 21:59:30.979: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:32.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6073" for this suite. 01/18/23 21:59:32.873
------------------------------
• [SLOW TEST] [8.861 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:24.03
    Jan 18 21:59:24.030: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 21:59:24.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:24.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:24.874
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan 18 21:59:24.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 21:59:26.963
    Jan 18 21:59:26.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
    Jan 18 21:59:27.750: INFO: stderr: ""
    Jan 18 21:59:27.750: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 21:59:27.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 delete e2e-test-crd-publish-openapi-1764-crds test-foo'
    Jan 18 21:59:27.842: INFO: stderr: ""
    Jan 18 21:59:27.842: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 18 21:59:27.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
    Jan 18 21:59:28.573: INFO: stderr: ""
    Jan 18 21:59:28.573: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 21:59:28.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 delete e2e-test-crd-publish-openapi-1764-crds test-foo'
    Jan 18 21:59:28.663: INFO: stderr: ""
    Jan 18 21:59:28.663: INFO: stdout: "e2e-test-crd-publish-openapi-1764-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 21:59:28.663
    Jan 18 21:59:28.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
    Jan 18 21:59:28.865: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 21:59:28.865
    Jan 18 21:59:28.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
    Jan 18 21:59:29.132: INFO: rc: 1
    Jan 18 21:59:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
    Jan 18 21:59:29.363: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 21:59:29.363
    Jan 18 21:59:29.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 create -f -'
    Jan 18 21:59:29.591: INFO: rc: 1
    Jan 18 21:59:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 --namespace=crd-publish-openapi-6073 apply -f -'
    Jan 18 21:59:29.825: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/18/23 21:59:29.825
    Jan 18 21:59:29.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds'
    Jan 18 21:59:30.065: INFO: stderr: ""
    Jan 18 21:59:30.065: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/18/23 21:59:30.066
    Jan 18 21:59:30.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.metadata'
    Jan 18 21:59:30.306: INFO: stderr: ""
    Jan 18 21:59:30.307: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 18 21:59:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec'
    Jan 18 21:59:30.519: INFO: stderr: ""
    Jan 18 21:59:30.519: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 18 21:59:30.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec.bars'
    Jan 18 21:59:30.733: INFO: stderr: ""
    Jan 18 21:59:30.733: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1764-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 21:59:30.733
    Jan 18 21:59:30.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-6073 explain e2e-test-crd-publish-openapi-1764-crds.spec.bars2'
    Jan 18 21:59:30.979: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:32.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6073" for this suite. 01/18/23 21:59:32.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:32.891
Jan 18 21:59:32.891: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 21:59:32.893
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:33.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:33.443
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan 18 21:59:33.463: INFO: Waiting up to 2m0s for pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" in namespace "var-expansion-9711" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 21:59:33.467: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430733ms
Jan 18 21:59:35.472: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009833793s
Jan 18 21:59:35.472: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 21:59:35.473: INFO: Deleting pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" in namespace "var-expansion-9711"
Jan 18 21:59:35.486: INFO: Wait up to 5m0s for pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:37.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-9711" for this suite. 01/18/23 21:59:37.501
------------------------------
• [4.622 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:32.891
    Jan 18 21:59:32.891: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 21:59:32.893
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:33.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:33.443
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan 18 21:59:33.463: INFO: Waiting up to 2m0s for pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" in namespace "var-expansion-9711" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 21:59:33.467: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.430733ms
    Jan 18 21:59:35.472: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009833793s
    Jan 18 21:59:35.472: INFO: Pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 21:59:35.473: INFO: Deleting pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" in namespace "var-expansion-9711"
    Jan 18 21:59:35.486: INFO: Wait up to 5m0s for pod "var-expansion-82be786a-5a63-4346-beed-3993bce6b60a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:37.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-9711" for this suite. 01/18/23 21:59:37.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:37.514
Jan 18 21:59:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 21:59:37.515
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:37.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:37.874
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/18/23 21:59:37.877
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_tcp@PTR;sleep 1; done
 01/18/23 21:59:37.912
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_tcp@PTR;sleep 1; done
 01/18/23 21:59:37.912
STEP: creating a pod to probe DNS 01/18/23 21:59:37.912
STEP: submitting the pod to kubernetes 01/18/23 21:59:37.913
Jan 18 21:59:37.932: INFO: Waiting up to 15m0s for pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e" in namespace "dns-1350" to be "running"
Jan 18 21:59:37.939: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411091ms
Jan 18 21:59:39.945: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013315576s
Jan 18 21:59:39.945: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e" satisfied condition "running"
STEP: retrieving the pod 01/18/23 21:59:39.945
STEP: looking for the results for each expected name from probers 01/18/23 21:59:39.95
Jan 18 21:59:39.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.962: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.966: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.984: INFO: Unable to read jessie_udp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.987: INFO: Unable to read jessie_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.993: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:39.998: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:40.013: INFO: Lookups using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e failed for: [wheezy_udp@dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local jessie_udp@dns-test-service.dns-1350.svc.cluster.local jessie_tcp@dns-test-service.dns-1350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local]

Jan 18 21:59:45.023: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:45.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:45.032: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
Jan 18 21:59:45.079: INFO: Lookups using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e failed for: [wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local]

Jan 18 21:59:50.072: INFO: DNS probes using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e succeeded

STEP: deleting the pod 01/18/23 21:59:50.072
STEP: deleting the test service 01/18/23 21:59:50.112
STEP: deleting the test headless service 01/18/23 21:59:50.233
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1350" for this suite. 01/18/23 21:59:50.27
------------------------------
• [SLOW TEST] [12.767 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:37.514
    Jan 18 21:59:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 21:59:37.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:37.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:37.874
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/18/23 21:59:37.877
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_tcp@PTR;sleep 1; done
     01/18/23 21:59:37.912
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_udp@PTR;check="$$(dig +tcp +noall +answer +search 101.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.101_tcp@PTR;sleep 1; done
     01/18/23 21:59:37.912
    STEP: creating a pod to probe DNS 01/18/23 21:59:37.912
    STEP: submitting the pod to kubernetes 01/18/23 21:59:37.913
    Jan 18 21:59:37.932: INFO: Waiting up to 15m0s for pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e" in namespace "dns-1350" to be "running"
    Jan 18 21:59:37.939: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.411091ms
    Jan 18 21:59:39.945: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013315576s
    Jan 18 21:59:39.945: INFO: Pod "dns-test-9c8736a4-050f-4756-ad9b-d2886588022e" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 21:59:39.945
    STEP: looking for the results for each expected name from probers 01/18/23 21:59:39.95
    Jan 18 21:59:39.955: INFO: Unable to read wheezy_udp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.958: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.962: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.966: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.984: INFO: Unable to read jessie_udp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.987: INFO: Unable to read jessie_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.993: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:39.998: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:40.013: INFO: Lookups using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e failed for: [wheezy_udp@dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local jessie_udp@dns-test-service.dns-1350.svc.cluster.local jessie_tcp@dns-test-service.dns-1350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local]

    Jan 18 21:59:45.023: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:45.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:45.032: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local from pod dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e: the server could not find the requested resource (get pods dns-test-9c8736a4-050f-4756-ad9b-d2886588022e)
    Jan 18 21:59:45.079: INFO: Lookups using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e failed for: [wheezy_tcp@dns-test-service.dns-1350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1350.svc.cluster.local]

    Jan 18 21:59:50.072: INFO: DNS probes using dns-1350/dns-test-9c8736a4-050f-4756-ad9b-d2886588022e succeeded

    STEP: deleting the pod 01/18/23 21:59:50.072
    STEP: deleting the test service 01/18/23 21:59:50.112
    STEP: deleting the test headless service 01/18/23 21:59:50.233
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:50.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1350" for this suite. 01/18/23 21:59:50.27
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:50.281
Jan 18 21:59:50.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 21:59:50.283
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:50.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:50.877
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 21:59:50.908
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:59:51.451
STEP: Deploying the webhook pod 01/18/23 21:59:51.474
STEP: Wait for the deployment to be ready 01/18/23 21:59:51.584
Jan 18 21:59:51.597: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 21:59:53.61
STEP: Verifying the service has paired with the endpoint 01/18/23 21:59:53.63
Jan 18 21:59:54.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 21:59:54.635
STEP: create a configmap that should be updated by the webhook 01/18/23 21:59:54.655
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:54.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1422" for this suite. 01/18/23 21:59:54.759
STEP: Destroying namespace "webhook-1422-markers" for this suite. 01/18/23 21:59:54.773
------------------------------
• [4.504 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:50.281
    Jan 18 21:59:50.281: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 21:59:50.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:50.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:50.877
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 21:59:50.908
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 21:59:51.451
    STEP: Deploying the webhook pod 01/18/23 21:59:51.474
    STEP: Wait for the deployment to be ready 01/18/23 21:59:51.584
    Jan 18 21:59:51.597: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 21:59:53.61
    STEP: Verifying the service has paired with the endpoint 01/18/23 21:59:53.63
    Jan 18 21:59:54.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 21:59:54.635
    STEP: create a configmap that should be updated by the webhook 01/18/23 21:59:54.655
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:54.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1422" for this suite. 01/18/23 21:59:54.759
    STEP: Destroying namespace "webhook-1422-markers" for this suite. 01/18/23 21:59:54.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:54.787
Jan 18 21:59:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:59:54.788
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:55.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:55.783
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 18 21:59:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 21:59:56.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1833" for this suite. 01/18/23 21:59:56.845
------------------------------
• [2.077 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:54.787
    Jan 18 21:59:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 21:59:54.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:55.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:55.783
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 18 21:59:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 21:59:56.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1833" for this suite. 01/18/23 21:59:56.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 21:59:56.865
Jan 18 21:59:56.866: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 21:59:56.866
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:57.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:57.795
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/18/23 21:59:57.798
STEP: Counting existing ResourceQuota 01/18/23 22:00:02.804
STEP: Creating a ResourceQuota 01/18/23 22:00:07.811
STEP: Ensuring resource quota status is calculated 01/18/23 22:00:07.821
STEP: Creating a Secret 01/18/23 22:00:09.826
STEP: Ensuring resource quota status captures secret creation 01/18/23 22:00:09.847
STEP: Deleting a secret 01/18/23 22:00:11.853
STEP: Ensuring resource quota status released usage 01/18/23 22:00:11.863
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:13.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9195" for this suite. 01/18/23 22:00:13.873
------------------------------
• [SLOW TEST] [17.018 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 21:59:56.865
    Jan 18 21:59:56.866: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 21:59:56.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 21:59:57.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 21:59:57.795
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/18/23 21:59:57.798
    STEP: Counting existing ResourceQuota 01/18/23 22:00:02.804
    STEP: Creating a ResourceQuota 01/18/23 22:00:07.811
    STEP: Ensuring resource quota status is calculated 01/18/23 22:00:07.821
    STEP: Creating a Secret 01/18/23 22:00:09.826
    STEP: Ensuring resource quota status captures secret creation 01/18/23 22:00:09.847
    STEP: Deleting a secret 01/18/23 22:00:11.853
    STEP: Ensuring resource quota status released usage 01/18/23 22:00:11.863
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:13.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9195" for this suite. 01/18/23 22:00:13.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:13.885
Jan 18 22:00:13.885: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:00:13.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:14.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:14.875
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/18/23 22:00:14.878
STEP: wait for the container to reach Succeeded 01/18/23 22:00:14.896
STEP: get the container status 01/18/23 22:00:18.968
STEP: the container should be terminated 01/18/23 22:00:18.971
STEP: the termination message should be set 01/18/23 22:00:18.971
Jan 18 22:00:18.971: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 22:00:18.971
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:19.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7824" for this suite. 01/18/23 22:00:19.005
------------------------------
• [SLOW TEST] [5.132 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:13.885
    Jan 18 22:00:13.885: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:00:13.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:14.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:14.875
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/18/23 22:00:14.878
    STEP: wait for the container to reach Succeeded 01/18/23 22:00:14.896
    STEP: get the container status 01/18/23 22:00:18.968
    STEP: the container should be terminated 01/18/23 22:00:18.971
    STEP: the termination message should be set 01/18/23 22:00:18.971
    Jan 18 22:00:18.971: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 22:00:18.971
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:19.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7824" for this suite. 01/18/23 22:00:19.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:19.018
Jan 18 22:00:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:00:19.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:19.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:19.811
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-733c03f3-42b9-4fcd-8be2-456ad87817e5 01/18/23 22:00:19.813
STEP: Creating a pod to test consume configMaps 01/18/23 22:00:19.828
Jan 18 22:00:19.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12" in namespace "configmap-8155" to be "Succeeded or Failed"
Jan 18 22:00:19.850: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359579ms
Jan 18 22:00:21.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009000137s
Jan 18 22:00:23.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009060814s
STEP: Saw pod success 01/18/23 22:00:23.855
Jan 18 22:00:23.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12" satisfied condition "Succeeded or Failed"
Jan 18 22:00:23.859: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:00:23.866
Jan 18 22:00:23.893: INFO: Waiting for pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 to disappear
Jan 18 22:00:23.898: INFO: Pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:23.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8155" for this suite. 01/18/23 22:00:23.903
------------------------------
• [4.898 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:19.018
    Jan 18 22:00:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:00:19.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:19.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:19.811
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-733c03f3-42b9-4fcd-8be2-456ad87817e5 01/18/23 22:00:19.813
    STEP: Creating a pod to test consume configMaps 01/18/23 22:00:19.828
    Jan 18 22:00:19.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12" in namespace "configmap-8155" to be "Succeeded or Failed"
    Jan 18 22:00:19.850: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359579ms
    Jan 18 22:00:21.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009000137s
    Jan 18 22:00:23.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009060814s
    STEP: Saw pod success 01/18/23 22:00:23.855
    Jan 18 22:00:23.855: INFO: Pod "pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12" satisfied condition "Succeeded or Failed"
    Jan 18 22:00:23.859: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:00:23.866
    Jan 18 22:00:23.893: INFO: Waiting for pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 to disappear
    Jan 18 22:00:23.898: INFO: Pod pod-configmaps-64c9a764-3d9f-42c9-9e10-5f7563be0d12 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:23.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8155" for this suite. 01/18/23 22:00:23.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:23.917
Jan 18 22:00:23.917: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:00:23.918
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:24.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:24.875
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:00:24.904
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:00:25.598
STEP: Deploying the webhook pod 01/18/23 22:00:25.613
STEP: Wait for the deployment to be ready 01/18/23 22:00:25.632
Jan 18 22:00:25.643: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:00:27.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:00:29.66
STEP: Verifying the service has paired with the endpoint 01/18/23 22:00:29.68
Jan 18 22:00:30.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:00:30.684
Jan 18 22:00:30.722: INFO: Waiting for webhook configuration to be ready...
STEP: create a pod 01/18/23 22:00:30.832
Jan 18 22:00:30.851: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5111" to be "running"
Jan 18 22:00:30.856: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.443548ms
Jan 18 22:00:32.861: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009471952s
Jan 18 22:00:32.861: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:00:32.861
Jan 18 22:00:32.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=webhook-5111 attach --namespace=webhook-5111 to-be-attached-pod -i -c=container1'
Jan 18 22:00:34.000: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:34.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5111" for this suite. 01/18/23 22:00:34.112
STEP: Destroying namespace "webhook-5111-markers" for this suite. 01/18/23 22:00:34.133
------------------------------
• [SLOW TEST] [10.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:23.917
    Jan 18 22:00:23.917: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:00:23.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:24.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:24.875
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:00:24.904
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:00:25.598
    STEP: Deploying the webhook pod 01/18/23 22:00:25.613
    STEP: Wait for the deployment to be ready 01/18/23 22:00:25.632
    Jan 18 22:00:25.643: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:00:27.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 0, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:00:29.66
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:00:29.68
    Jan 18 22:00:30.680: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 22:00:30.684
    Jan 18 22:00:30.722: INFO: Waiting for webhook configuration to be ready...
    STEP: create a pod 01/18/23 22:00:30.832
    Jan 18 22:00:30.851: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-5111" to be "running"
    Jan 18 22:00:30.856: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.443548ms
    Jan 18 22:00:32.861: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009471952s
    Jan 18 22:00:32.861: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 22:00:32.861
    Jan 18 22:00:32.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=webhook-5111 attach --namespace=webhook-5111 to-be-attached-pod -i -c=container1'
    Jan 18 22:00:34.000: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:34.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5111" for this suite. 01/18/23 22:00:34.112
    STEP: Destroying namespace "webhook-5111-markers" for this suite. 01/18/23 22:00:34.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:34.149
Jan 18 22:00:34.149: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:00:34.15
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:34.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:34.479
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7792" for this suite. 01/18/23 22:00:34.51
------------------------------
• [0.376 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:34.149
    Jan 18 22:00:34.149: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:00:34.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:34.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:34.479
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7792" for this suite. 01/18/23 22:00:34.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:34.526
Jan 18 22:00:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:00:34.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:34.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:34.774
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/18/23 22:00:34.777
STEP: Creating a ResourceQuota 01/18/23 22:00:39.797
STEP: Ensuring resource quota status is calculated 01/18/23 22:00:39.823
STEP: Creating a ReplicaSet 01/18/23 22:00:41.829
STEP: Ensuring resource quota status captures replicaset creation 01/18/23 22:00:41.855
STEP: Deleting a ReplicaSet 01/18/23 22:00:43.861
STEP: Ensuring resource quota status released usage 01/18/23 22:00:43.873
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:45.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7496" for this suite. 01/18/23 22:00:45.883
------------------------------
• [SLOW TEST] [11.368 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:34.526
    Jan 18 22:00:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:00:34.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:34.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:34.774
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/18/23 22:00:34.777
    STEP: Creating a ResourceQuota 01/18/23 22:00:39.797
    STEP: Ensuring resource quota status is calculated 01/18/23 22:00:39.823
    STEP: Creating a ReplicaSet 01/18/23 22:00:41.829
    STEP: Ensuring resource quota status captures replicaset creation 01/18/23 22:00:41.855
    STEP: Deleting a ReplicaSet 01/18/23 22:00:43.861
    STEP: Ensuring resource quota status released usage 01/18/23 22:00:43.873
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:45.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7496" for this suite. 01/18/23 22:00:45.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:45.896
Jan 18 22:00:45.896: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:00:45.897
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:46.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:46.16
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 18 22:00:46.162: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:46.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-271" for this suite. 01/18/23 22:00:46.739
------------------------------
• [0.857 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:45.896
    Jan 18 22:00:45.896: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 22:00:45.897
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:46.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:46.16
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 18 22:00:46.162: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:46.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-271" for this suite. 01/18/23 22:00:46.739
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:46.755
Jan 18 22:00:46.755: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:00:46.756
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:47.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:47.875
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1932 01/18/23 22:00:47.878
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:00:47.897
STEP: creating service externalsvc in namespace services-1932 01/18/23 22:00:47.897
STEP: creating replication controller externalsvc in namespace services-1932 01/18/23 22:00:47.919
I0118 22:00:47.929566      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1932, replica count: 2
I0118 22:00:50.981731      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:00:50.985
Jan 18 22:00:51.012: INFO: Creating new exec pod
Jan 18 22:00:51.030: INFO: Waiting up to 5m0s for pod "execpod2kx9h" in namespace "services-1932" to be "running"
Jan 18 22:00:51.035: INFO: Pod "execpod2kx9h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735363ms
Jan 18 22:00:53.040: INFO: Pod "execpod2kx9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010632767s
Jan 18 22:00:53.040: INFO: Pod "execpod2kx9h" satisfied condition "running"
Jan 18 22:00:53.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1932 exec execpod2kx9h -- /bin/sh -x -c nslookup clusterip-service.services-1932.svc.cluster.local'
Jan 18 22:00:53.248: INFO: stderr: "+ nslookup clusterip-service.services-1932.svc.cluster.local\n"
Jan 18 22:00:53.248: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nclusterip-service.services-1932.svc.cluster.local\tcanonical name = externalsvc.services-1932.svc.cluster.local.\nName:\texternalsvc.services-1932.svc.cluster.local\nAddress: 10.152.183.73\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1932, will wait for the garbage collector to delete the pods 01/18/23 22:00:53.248
Jan 18 22:00:53.316: INFO: Deleting ReplicationController externalsvc took: 12.652639ms
Jan 18 22:00:53.917: INFO: Terminating ReplicationController externalsvc pods took: 600.42739ms
Jan 18 22:00:56.450: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:00:56.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1932" for this suite. 01/18/23 22:00:56.491
------------------------------
• [SLOW TEST] [9.751 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:46.755
    Jan 18 22:00:46.755: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:00:46.756
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:47.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:47.875
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1932 01/18/23 22:00:47.878
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:00:47.897
    STEP: creating service externalsvc in namespace services-1932 01/18/23 22:00:47.897
    STEP: creating replication controller externalsvc in namespace services-1932 01/18/23 22:00:47.919
    I0118 22:00:47.929566      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1932, replica count: 2
    I0118 22:00:50.981731      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/18/23 22:00:50.985
    Jan 18 22:00:51.012: INFO: Creating new exec pod
    Jan 18 22:00:51.030: INFO: Waiting up to 5m0s for pod "execpod2kx9h" in namespace "services-1932" to be "running"
    Jan 18 22:00:51.035: INFO: Pod "execpod2kx9h": Phase="Pending", Reason="", readiness=false. Elapsed: 5.735363ms
    Jan 18 22:00:53.040: INFO: Pod "execpod2kx9h": Phase="Running", Reason="", readiness=true. Elapsed: 2.010632767s
    Jan 18 22:00:53.040: INFO: Pod "execpod2kx9h" satisfied condition "running"
    Jan 18 22:00:53.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1932 exec execpod2kx9h -- /bin/sh -x -c nslookup clusterip-service.services-1932.svc.cluster.local'
    Jan 18 22:00:53.248: INFO: stderr: "+ nslookup clusterip-service.services-1932.svc.cluster.local\n"
    Jan 18 22:00:53.248: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nclusterip-service.services-1932.svc.cluster.local\tcanonical name = externalsvc.services-1932.svc.cluster.local.\nName:\texternalsvc.services-1932.svc.cluster.local\nAddress: 10.152.183.73\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1932, will wait for the garbage collector to delete the pods 01/18/23 22:00:53.248
    Jan 18 22:00:53.316: INFO: Deleting ReplicationController externalsvc took: 12.652639ms
    Jan 18 22:00:53.917: INFO: Terminating ReplicationController externalsvc pods took: 600.42739ms
    Jan 18 22:00:56.450: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:00:56.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1932" for this suite. 01/18/23 22:00:56.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:00:56.508
Jan 18 22:00:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:00:56.509
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:56.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:56.876
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan 18 22:00:56.899: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab" in namespace "security-context-test-308" to be "Succeeded or Failed"
Jan 18 22:00:56.906: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.150278ms
Jan 18 22:00:58.912: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012957594s
Jan 18 22:01:00.910: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011766469s
Jan 18 22:01:00.911: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:01:00.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-308" for this suite. 01/18/23 22:01:00.915
------------------------------
• [4.419 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:00:56.508
    Jan 18 22:00:56.508: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:00:56.509
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:00:56.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:00:56.876
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan 18 22:00:56.899: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab" in namespace "security-context-test-308" to be "Succeeded or Failed"
    Jan 18 22:00:56.906: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Pending", Reason="", readiness=false. Elapsed: 7.150278ms
    Jan 18 22:00:58.912: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012957594s
    Jan 18 22:01:00.910: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011766469s
    Jan 18 22:01:00.911: INFO: Pod "busybox-readonly-false-3dbf05ba-f68f-4bb1-9110-355ad41e35ab" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:01:00.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-308" for this suite. 01/18/23 22:01:00.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:01:00.928
Jan 18 22:01:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:01:00.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:01:01.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:01:01.356
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-fda7facb-6a16-4f1a-a802-1fe311b004de 01/18/23 22:01:01.364
STEP: Creating configMap with name cm-test-opt-upd-c03980d0-2b64-4923-a08c-78b3987f0517 01/18/23 22:01:01.374
STEP: Creating the pod 01/18/23 22:01:01.398
Jan 18 22:01:01.415: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3" in namespace "projected-3897" to be "running and ready"
Jan 18 22:01:01.421: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.629974ms
Jan 18 22:01:01.421: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:01:03.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012580927s
Jan 18 22:01:03.427: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:01:05.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.011926308s
Jan 18 22:01:05.427: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Running (Ready = true)
Jan 18 22:01:05.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-fda7facb-6a16-4f1a-a802-1fe311b004de 01/18/23 22:01:05.451
STEP: Updating configmap cm-test-opt-upd-c03980d0-2b64-4923-a08c-78b3987f0517 01/18/23 22:01:05.466
STEP: Creating configMap with name cm-test-opt-create-ba87b667-44f8-4585-8456-2f0b951681c6 01/18/23 22:01:05.476
STEP: waiting to observe update in volume 01/18/23 22:01:05.487
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:35.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3897" for this suite. 01/18/23 22:02:35.983
------------------------------
• [SLOW TEST] [95.074 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:01:00.928
    Jan 18 22:01:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:01:00.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:01:01.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:01:01.356
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-fda7facb-6a16-4f1a-a802-1fe311b004de 01/18/23 22:01:01.364
    STEP: Creating configMap with name cm-test-opt-upd-c03980d0-2b64-4923-a08c-78b3987f0517 01/18/23 22:01:01.374
    STEP: Creating the pod 01/18/23 22:01:01.398
    Jan 18 22:01:01.415: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3" in namespace "projected-3897" to be "running and ready"
    Jan 18 22:01:01.421: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.629974ms
    Jan 18 22:01:01.421: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:01:03.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012580927s
    Jan 18 22:01:03.427: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:01:05.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.011926308s
    Jan 18 22:01:05.427: INFO: The phase of Pod pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3 is Running (Ready = true)
    Jan 18 22:01:05.427: INFO: Pod "pod-projected-configmaps-8c0b3f6c-9bae-4522-a5a3-c032dbb374d3" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-fda7facb-6a16-4f1a-a802-1fe311b004de 01/18/23 22:01:05.451
    STEP: Updating configmap cm-test-opt-upd-c03980d0-2b64-4923-a08c-78b3987f0517 01/18/23 22:01:05.466
    STEP: Creating configMap with name cm-test-opt-create-ba87b667-44f8-4585-8456-2f0b951681c6 01/18/23 22:01:05.476
    STEP: waiting to observe update in volume 01/18/23 22:01:05.487
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:35.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3897" for this suite. 01/18/23 22:02:35.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:36.007
Jan 18 22:02:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:02:36.008
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:36.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:36.557
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:02:36.56
Jan 18 22:02:36.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b" in namespace "projected-6837" to be "Succeeded or Failed"
Jan 18 22:02:36.585: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848843ms
Jan 18 22:02:38.591: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00956547s
Jan 18 22:02:40.593: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01197406s
STEP: Saw pod success 01/18/23 22:02:40.593
Jan 18 22:02:40.593: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b" satisfied condition "Succeeded or Failed"
Jan 18 22:02:40.597: INFO: Trying to get logs from node test-vm-2 pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b container client-container: <nil>
STEP: delete the pod 01/18/23 22:02:40.618
Jan 18 22:02:40.642: INFO: Waiting for pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b to disappear
Jan 18 22:02:40.647: INFO: Pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6837" for this suite. 01/18/23 22:02:40.653
------------------------------
• [4.658 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:36.007
    Jan 18 22:02:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:02:36.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:36.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:36.557
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:02:36.56
    Jan 18 22:02:36.581: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b" in namespace "projected-6837" to be "Succeeded or Failed"
    Jan 18 22:02:36.585: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848843ms
    Jan 18 22:02:38.591: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00956547s
    Jan 18 22:02:40.593: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01197406s
    STEP: Saw pod success 01/18/23 22:02:40.593
    Jan 18 22:02:40.593: INFO: Pod "downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b" satisfied condition "Succeeded or Failed"
    Jan 18 22:02:40.597: INFO: Trying to get logs from node test-vm-2 pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b container client-container: <nil>
    STEP: delete the pod 01/18/23 22:02:40.618
    Jan 18 22:02:40.642: INFO: Waiting for pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b to disappear
    Jan 18 22:02:40.647: INFO: Pod downwardapi-volume-38379297-9273-4924-997b-ded134f22f4b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:40.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6837" for this suite. 01/18/23 22:02:40.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:40.667
Jan 18 22:02:40.667: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:02:40.669
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:40.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:40.757
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1754 01/18/23 22:02:40.76
STEP: changing the ExternalName service to type=ClusterIP 01/18/23 22:02:40.772
STEP: creating replication controller externalname-service in namespace services-1754 01/18/23 22:02:40.803
I0118 22:02:40.818201      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1754, replica count: 2
I0118 22:02:43.873041      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:02:43.873: INFO: Creating new exec pod
Jan 18 22:02:43.890: INFO: Waiting up to 5m0s for pod "execpod7tpfz" in namespace "services-1754" to be "running"
Jan 18 22:02:43.893: INFO: Pod "execpod7tpfz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70694ms
Jan 18 22:02:45.899: INFO: Pod "execpod7tpfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.0096004s
Jan 18 22:02:45.899: INFO: Pod "execpod7tpfz" satisfied condition "running"
Jan 18 22:02:46.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1754 exec execpod7tpfz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 18 22:02:47.104: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 22:02:47.104: INFO: stdout: ""
Jan 18 22:02:47.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1754 exec execpod7tpfz -- /bin/sh -x -c nc -v -z -w 2 10.152.183.66 80'
Jan 18 22:02:47.296: INFO: stderr: "+ nc -v -z -w 2 10.152.183.66 80\nConnection to 10.152.183.66 80 port [tcp/http] succeeded!\n"
Jan 18 22:02:47.296: INFO: stdout: ""
Jan 18 22:02:47.296: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:47.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1754" for this suite. 01/18/23 22:02:47.341
------------------------------
• [SLOW TEST] [6.687 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:40.667
    Jan 18 22:02:40.667: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:02:40.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:40.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:40.757
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1754 01/18/23 22:02:40.76
    STEP: changing the ExternalName service to type=ClusterIP 01/18/23 22:02:40.772
    STEP: creating replication controller externalname-service in namespace services-1754 01/18/23 22:02:40.803
    I0118 22:02:40.818201      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1754, replica count: 2
    I0118 22:02:43.873041      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:02:43.873: INFO: Creating new exec pod
    Jan 18 22:02:43.890: INFO: Waiting up to 5m0s for pod "execpod7tpfz" in namespace "services-1754" to be "running"
    Jan 18 22:02:43.893: INFO: Pod "execpod7tpfz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70694ms
    Jan 18 22:02:45.899: INFO: Pod "execpod7tpfz": Phase="Running", Reason="", readiness=true. Elapsed: 2.0096004s
    Jan 18 22:02:45.899: INFO: Pod "execpod7tpfz" satisfied condition "running"
    Jan 18 22:02:46.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1754 exec execpod7tpfz -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 18 22:02:47.104: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 22:02:47.104: INFO: stdout: ""
    Jan 18 22:02:47.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-1754 exec execpod7tpfz -- /bin/sh -x -c nc -v -z -w 2 10.152.183.66 80'
    Jan 18 22:02:47.296: INFO: stderr: "+ nc -v -z -w 2 10.152.183.66 80\nConnection to 10.152.183.66 80 port [tcp/http] succeeded!\n"
    Jan 18 22:02:47.296: INFO: stdout: ""
    Jan 18 22:02:47.296: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:47.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1754" for this suite. 01/18/23 22:02:47.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:47.355
Jan 18 22:02:47.356: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:02:47.356
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:47.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:47.874
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:47.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1944" for this suite. 01/18/23 22:02:47.885
------------------------------
• [0.542 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:47.355
    Jan 18 22:02:47.356: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:02:47.356
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:47.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:47.874
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:47.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1944" for this suite. 01/18/23 22:02:47.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:47.898
Jan 18 22:02:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 22:02:47.9
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:48.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:48.875
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 22:02:48.878
Jan 18 22:02:48.894: INFO: Waiting up to 5m0s for pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9" in namespace "emptydir-6004" to be "Succeeded or Failed"
Jan 18 22:02:48.898: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70044ms
Jan 18 22:02:50.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008775592s
Jan 18 22:02:52.913: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.018653567s
Jan 18 22:02:54.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00839845s
STEP: Saw pod success 01/18/23 22:02:54.903
Jan 18 22:02:54.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9" satisfied condition "Succeeded or Failed"
Jan 18 22:02:54.907: INFO: Trying to get logs from node test-vm-1 pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 container test-container: <nil>
STEP: delete the pod 01/18/23 22:02:54.921
Jan 18 22:02:54.973: INFO: Waiting for pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 to disappear
Jan 18 22:02:54.980: INFO: Pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:54.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6004" for this suite. 01/18/23 22:02:54.987
------------------------------
• [SLOW TEST] [7.103 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:47.898
    Jan 18 22:02:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:02:47.9
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:48.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:48.875
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 22:02:48.878
    Jan 18 22:02:48.894: INFO: Waiting up to 5m0s for pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9" in namespace "emptydir-6004" to be "Succeeded or Failed"
    Jan 18 22:02:48.898: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70044ms
    Jan 18 22:02:50.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.008775592s
    Jan 18 22:02:52.913: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Running", Reason="", readiness=false. Elapsed: 4.018653567s
    Jan 18 22:02:54.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00839845s
    STEP: Saw pod success 01/18/23 22:02:54.903
    Jan 18 22:02:54.903: INFO: Pod "pod-b863721a-5f20-41d6-9e37-2508c79056f9" satisfied condition "Succeeded or Failed"
    Jan 18 22:02:54.907: INFO: Trying to get logs from node test-vm-1 pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:02:54.921
    Jan 18 22:02:54.973: INFO: Waiting for pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 to disappear
    Jan 18 22:02:54.980: INFO: Pod pod-b863721a-5f20-41d6-9e37-2508c79056f9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:54.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6004" for this suite. 01/18/23 22:02:54.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:55.002
Jan 18 22:02:55.002: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:02:55.003
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:55.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:55.327
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:02:55.33
Jan 18 22:02:55.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf" in namespace "projected-2524" to be "Succeeded or Failed"
Jan 18 22:02:55.349: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.858742ms
Jan 18 22:02:57.355: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009761502s
Jan 18 22:02:59.357: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011730141s
STEP: Saw pod success 01/18/23 22:02:59.357
Jan 18 22:02:59.357: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf" satisfied condition "Succeeded or Failed"
Jan 18 22:02:59.362: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf container client-container: <nil>
STEP: delete the pod 01/18/23 22:02:59.37
Jan 18 22:02:59.395: INFO: Waiting for pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf to disappear
Jan 18 22:02:59.401: INFO: Pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:02:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2524" for this suite. 01/18/23 22:02:59.406
------------------------------
• [4.417 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:55.002
    Jan 18 22:02:55.002: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:02:55.003
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:55.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:55.327
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:02:55.33
    Jan 18 22:02:55.345: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf" in namespace "projected-2524" to be "Succeeded or Failed"
    Jan 18 22:02:55.349: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.858742ms
    Jan 18 22:02:57.355: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009761502s
    Jan 18 22:02:59.357: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011730141s
    STEP: Saw pod success 01/18/23 22:02:59.357
    Jan 18 22:02:59.357: INFO: Pod "downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf" satisfied condition "Succeeded or Failed"
    Jan 18 22:02:59.362: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf container client-container: <nil>
    STEP: delete the pod 01/18/23 22:02:59.37
    Jan 18 22:02:59.395: INFO: Waiting for pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf to disappear
    Jan 18 22:02:59.401: INFO: Pod downwardapi-volume-80a2743f-e884-477e-bbd8-f4b443b855bf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:02:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2524" for this suite. 01/18/23 22:02:59.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:02:59.419
Jan 18 22:02:59.419: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename subpath 01/18/23 22:02:59.42
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:59.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:59.818
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:02:59.82
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-49qs 01/18/23 22:02:59.84
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:02:59.84
Jan 18 22:02:59.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-49qs" in namespace "subpath-6053" to be "Succeeded or Failed"
Jan 18 22:02:59.859: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508339ms
Jan 18 22:03:01.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00779253s
Jan 18 22:03:03.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 4.008232278s
Jan 18 22:03:05.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 6.008255122s
Jan 18 22:03:07.868: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 8.01227778s
Jan 18 22:03:09.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 10.008973713s
Jan 18 22:03:11.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 12.008170374s
Jan 18 22:03:13.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 14.009193254s
Jan 18 22:03:15.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 16.009814418s
Jan 18 22:03:17.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 18.010224561s
Jan 18 22:03:19.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 20.009784895s
Jan 18 22:03:21.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=false. Elapsed: 22.008802823s
Jan 18 22:03:23.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008321262s
STEP: Saw pod success 01/18/23 22:03:23.864
Jan 18 22:03:23.864: INFO: Pod "pod-subpath-test-projected-49qs" satisfied condition "Succeeded or Failed"
Jan 18 22:03:23.868: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-projected-49qs container test-container-subpath-projected-49qs: <nil>
STEP: delete the pod 01/18/23 22:03:23.876
Jan 18 22:03:23.900: INFO: Waiting for pod pod-subpath-test-projected-49qs to disappear
Jan 18 22:03:23.906: INFO: Pod pod-subpath-test-projected-49qs no longer exists
STEP: Deleting pod pod-subpath-test-projected-49qs 01/18/23 22:03:23.906
Jan 18 22:03:23.906: INFO: Deleting pod "pod-subpath-test-projected-49qs" in namespace "subpath-6053"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:03:23.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6053" for this suite. 01/18/23 22:03:23.915
------------------------------
• [SLOW TEST] [24.508 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:02:59.419
    Jan 18 22:02:59.419: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename subpath 01/18/23 22:02:59.42
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:02:59.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:02:59.818
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:02:59.82
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-49qs 01/18/23 22:02:59.84
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:02:59.84
    Jan 18 22:02:59.855: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-49qs" in namespace "subpath-6053" to be "Succeeded or Failed"
    Jan 18 22:02:59.859: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Pending", Reason="", readiness=false. Elapsed: 3.508339ms
    Jan 18 22:03:01.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 2.00779253s
    Jan 18 22:03:03.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 4.008232278s
    Jan 18 22:03:05.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 6.008255122s
    Jan 18 22:03:07.868: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 8.01227778s
    Jan 18 22:03:09.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 10.008973713s
    Jan 18 22:03:11.863: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 12.008170374s
    Jan 18 22:03:13.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 14.009193254s
    Jan 18 22:03:15.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 16.009814418s
    Jan 18 22:03:17.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 18.010224561s
    Jan 18 22:03:19.865: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=true. Elapsed: 20.009784895s
    Jan 18 22:03:21.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Running", Reason="", readiness=false. Elapsed: 22.008802823s
    Jan 18 22:03:23.864: INFO: Pod "pod-subpath-test-projected-49qs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008321262s
    STEP: Saw pod success 01/18/23 22:03:23.864
    Jan 18 22:03:23.864: INFO: Pod "pod-subpath-test-projected-49qs" satisfied condition "Succeeded or Failed"
    Jan 18 22:03:23.868: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-projected-49qs container test-container-subpath-projected-49qs: <nil>
    STEP: delete the pod 01/18/23 22:03:23.876
    Jan 18 22:03:23.900: INFO: Waiting for pod pod-subpath-test-projected-49qs to disappear
    Jan 18 22:03:23.906: INFO: Pod pod-subpath-test-projected-49qs no longer exists
    STEP: Deleting pod pod-subpath-test-projected-49qs 01/18/23 22:03:23.906
    Jan 18 22:03:23.906: INFO: Deleting pod "pod-subpath-test-projected-49qs" in namespace "subpath-6053"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:03:23.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6053" for this suite. 01/18/23 22:03:23.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:03:23.929
Jan 18 22:03:23.929: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 22:03:23.93
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:24.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:24.019
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:04:24.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7242" for this suite. 01/18/23 22:04:24.051
------------------------------
• [SLOW TEST] [60.139 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:03:23.929
    Jan 18 22:03:23.929: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:03:23.93
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:03:24.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:03:24.019
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:04:24.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7242" for this suite. 01/18/23 22:04:24.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:04:24.068
Jan 18 22:04:24.069: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:04:24.07
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:24.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:24.293
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:04:24.379
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:04:24.762
STEP: Deploying the webhook pod 01/18/23 22:04:24.778
STEP: Wait for the deployment to be ready 01/18/23 22:04:24.801
Jan 18 22:04:24.810: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:04:26.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:04:28.828
STEP: Verifying the service has paired with the endpoint 01/18/23 22:04:28.852
Jan 18 22:04:29.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan 18 22:04:29.859: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 22:04:30.375
STEP: Creating a custom resource that should be denied by the webhook 01/18/23 22:04:30.395
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 22:04:32.427
STEP: Updating the custom resource with disallowed data should be denied 01/18/23 22:04:32.439
STEP: Deleting the custom resource should be denied 01/18/23 22:04:32.448
STEP: Remove the offending key and value from the custom resource data 01/18/23 22:04:32.456
STEP: Deleting the updated custom resource should be successful 01/18/23 22:04:32.472
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:04:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8081" for this suite. 01/18/23 22:04:33.109
STEP: Destroying namespace "webhook-8081-markers" for this suite. 01/18/23 22:04:33.125
------------------------------
• [SLOW TEST] [9.076 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:04:24.068
    Jan 18 22:04:24.069: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:04:24.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:24.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:24.293
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:04:24.379
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:04:24.762
    STEP: Deploying the webhook pod 01/18/23 22:04:24.778
    STEP: Wait for the deployment to be ready 01/18/23 22:04:24.801
    Jan 18 22:04:24.810: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:04:26.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 4, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:04:28.828
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:04:28.852
    Jan 18 22:04:29.853: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan 18 22:04:29.859: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 22:04:30.375
    STEP: Creating a custom resource that should be denied by the webhook 01/18/23 22:04:30.395
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 22:04:32.427
    STEP: Updating the custom resource with disallowed data should be denied 01/18/23 22:04:32.439
    STEP: Deleting the custom resource should be denied 01/18/23 22:04:32.448
    STEP: Remove the offending key and value from the custom resource data 01/18/23 22:04:32.456
    STEP: Deleting the updated custom resource should be successful 01/18/23 22:04:32.472
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:04:33.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8081" for this suite. 01/18/23 22:04:33.109
    STEP: Destroying namespace "webhook-8081-markers" for this suite. 01/18/23 22:04:33.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:04:33.146
Jan 18 22:04:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:04:33.147
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:33.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:33.767
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/18/23 22:04:33.772
Jan 18 22:04:33.798: INFO: Waiting up to 5m0s for pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9" in namespace "svcaccounts-5783" to be "Succeeded or Failed"
Jan 18 22:04:33.805: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280578ms
Jan 18 22:04:35.810: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012150532s
Jan 18 22:04:37.812: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938702s
STEP: Saw pod success 01/18/23 22:04:37.812
Jan 18 22:04:37.812: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9" satisfied condition "Succeeded or Failed"
Jan 18 22:04:37.815: INFO: Trying to get logs from node test-vm-1 pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:04:37.823
Jan 18 22:04:37.852: INFO: Waiting for pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 to disappear
Jan 18 22:04:37.858: INFO: Pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:04:37.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5783" for this suite. 01/18/23 22:04:37.862
------------------------------
• [4.733 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:04:33.146
    Jan 18 22:04:33.146: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:04:33.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:33.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:33.767
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/18/23 22:04:33.772
    Jan 18 22:04:33.798: INFO: Waiting up to 5m0s for pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9" in namespace "svcaccounts-5783" to be "Succeeded or Failed"
    Jan 18 22:04:33.805: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.280578ms
    Jan 18 22:04:35.810: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012150532s
    Jan 18 22:04:37.812: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013938702s
    STEP: Saw pod success 01/18/23 22:04:37.812
    Jan 18 22:04:37.812: INFO: Pod "test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9" satisfied condition "Succeeded or Failed"
    Jan 18 22:04:37.815: INFO: Trying to get logs from node test-vm-1 pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:04:37.823
    Jan 18 22:04:37.852: INFO: Waiting for pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 to disappear
    Jan 18 22:04:37.858: INFO: Pod test-pod-cc34cb96-30bc-4c2c-afd3-62f2115972a9 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:04:37.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5783" for this suite. 01/18/23 22:04:37.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:04:37.88
Jan 18 22:04:37.880: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:04:37.882
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:38.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:38.288
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan 18 22:04:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4768 version'
Jan 18 22:04:38.367: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 22:04:38.367: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-09T15:09:52Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:04:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4768" for this suite. 01/18/23 22:04:38.374
------------------------------
• [0.513 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:04:37.88
    Jan 18 22:04:37.880: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:04:37.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:38.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:38.288
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan 18 22:04:38.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4768 version'
    Jan 18 22:04:38.367: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 18 22:04:38.367: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-09T15:09:52Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:04:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4768" for this suite. 01/18/23 22:04:38.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:04:38.395
Jan 18 22:04:38.395: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:04:38.396
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:38.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:38.654
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:04:38.657
Jan 18 22:04:38.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1" in namespace "projected-3467" to be "Succeeded or Failed"
Jan 18 22:04:38.695: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30823ms
Jan 18 22:04:40.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010664876s
Jan 18 22:04:42.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01068528s
STEP: Saw pod success 01/18/23 22:04:42.7
Jan 18 22:04:42.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1" satisfied condition "Succeeded or Failed"
Jan 18 22:04:42.704: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 container client-container: <nil>
STEP: delete the pod 01/18/23 22:04:42.711
Jan 18 22:04:42.740: INFO: Waiting for pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 to disappear
Jan 18 22:04:42.745: INFO: Pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:04:42.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3467" for this suite. 01/18/23 22:04:42.749
------------------------------
• [4.370 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:04:38.395
    Jan 18 22:04:38.395: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:04:38.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:38.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:38.654
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:04:38.657
    Jan 18 22:04:38.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1" in namespace "projected-3467" to be "Succeeded or Failed"
    Jan 18 22:04:38.695: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30823ms
    Jan 18 22:04:40.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010664876s
    Jan 18 22:04:42.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01068528s
    STEP: Saw pod success 01/18/23 22:04:42.7
    Jan 18 22:04:42.700: INFO: Pod "downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1" satisfied condition "Succeeded or Failed"
    Jan 18 22:04:42.704: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:04:42.711
    Jan 18 22:04:42.740: INFO: Waiting for pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 to disappear
    Jan 18 22:04:42.745: INFO: Pod downwardapi-volume-2c37a2a0-abdb-41a7-b116-143b6189a4f1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:04:42.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3467" for this suite. 01/18/23 22:04:42.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:04:42.768
Jan 18 22:04:42.768: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-watch 01/18/23 22:04:42.769
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:42.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:42.874
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 18 22:04:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Creating first CR  01/18/23 22:04:45.453
Jan 18 22:04:45.465: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:04:45Z]] name:name1 resourceVersion:25697 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/18/23 22:04:55.466
Jan 18 22:04:55.481: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:04:55Z]] name:name2 resourceVersion:25724 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/18/23 22:05:05.482
Jan 18 22:05:05.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:05Z]] name:name1 resourceVersion:25741 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/18/23 22:05:15.496
Jan 18 22:05:15.509: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:15Z]] name:name2 resourceVersion:25758 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/18/23 22:05:25.51
Jan 18 22:05:25.564: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:05Z]] name:name1 resourceVersion:25775 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/18/23 22:05:35.565
Jan 18 22:05:35.586: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:15Z]] name:name2 resourceVersion:25792 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:05:46.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-2698" for this suite. 01/18/23 22:05:46.109
------------------------------
• [SLOW TEST] [63.354 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:04:42.768
    Jan 18 22:04:42.768: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-watch 01/18/23 22:04:42.769
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:04:42.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:04:42.874
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 18 22:04:42.876: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Creating first CR  01/18/23 22:04:45.453
    Jan 18 22:04:45.465: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:04:45Z]] name:name1 resourceVersion:25697 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/18/23 22:04:55.466
    Jan 18 22:04:55.481: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:04:55Z]] name:name2 resourceVersion:25724 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/18/23 22:05:05.482
    Jan 18 22:05:05.495: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:05Z]] name:name1 resourceVersion:25741 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/18/23 22:05:15.496
    Jan 18 22:05:15.509: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:15Z]] name:name2 resourceVersion:25758 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/18/23 22:05:25.51
    Jan 18 22:05:25.564: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:45Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:05Z]] name:name1 resourceVersion:25775 uid:fff318d6-62c4-4ec7-a006-bf25bf594ce0] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/18/23 22:05:35.565
    Jan 18 22:05:35.586: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T22:04:55Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T22:05:15Z]] name:name2 resourceVersion:25792 uid:f4bc9178-3402-4a4c-99ce-88af55bea729] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:05:46.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-2698" for this suite. 01/18/23 22:05:46.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:05:46.124
Jan 18 22:05:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:05:46.125
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:46.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:46.64
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3621-delete-me 01/18/23 22:05:46.654
STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:05:46.67
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:05:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3621" for this suite. 01/18/23 22:05:46.699
------------------------------
• [0.589 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:05:46.124
    Jan 18 22:05:46.124: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:05:46.125
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:46.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:46.64
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3621-delete-me 01/18/23 22:05:46.654
    STEP: Waiting for the RuntimeClass to disappear 01/18/23 22:05:46.67
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:05:46.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3621" for this suite. 01/18/23 22:05:46.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:05:46.714
Jan 18 22:05:46.714: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:05:46.716
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:46.952
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:46.955
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-vldrc"  01/18/23 22:05:46.958
Jan 18 22:05:46.974: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-vldrc"  01/18/23 22:05:46.974
Jan 18 22:05:46.987: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:05:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7006" for this suite. 01/18/23 22:05:46.992
------------------------------
• [0.290 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:05:46.714
    Jan 18 22:05:46.714: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:05:46.716
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:46.952
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:46.955
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-vldrc"  01/18/23 22:05:46.958
    Jan 18 22:05:46.974: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-vldrc"  01/18/23 22:05:46.974
    Jan 18 22:05:46.987: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:05:46.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7006" for this suite. 01/18/23 22:05:46.992
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:05:47.004
Jan 18 22:05:47.005: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename namespaces 01/18/23 22:05:47.006
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:47.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:47.873
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-5837" 01/18/23 22:05:47.876
Jan 18 22:05:47.898: INFO: Namespace "namespaces-5837" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"de0ec6bb-1b40-473e-879d-9346f84b1ea8", "kubernetes.io/metadata.name":"namespaces-5837", "namespaces-5837":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:05:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5837" for this suite. 01/18/23 22:05:47.905
------------------------------
• [0.913 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:05:47.004
    Jan 18 22:05:47.005: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename namespaces 01/18/23 22:05:47.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:47.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:47.873
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-5837" 01/18/23 22:05:47.876
    Jan 18 22:05:47.898: INFO: Namespace "namespaces-5837" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"de0ec6bb-1b40-473e-879d-9346f84b1ea8", "kubernetes.io/metadata.name":"namespaces-5837", "namespaces-5837":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:05:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5837" for this suite. 01/18/23 22:05:47.905
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:05:47.918
Jan 18 22:05:47.918: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:05:47.919
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:48.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:48.877
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/18/23 22:05:48.88
STEP: fetching the ConfigMap 01/18/23 22:05:48.893
STEP: patching the ConfigMap 01/18/23 22:05:48.896
STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 22:05:48.907
STEP: deleting the ConfigMap by collection with a label selector 01/18/23 22:05:48.912
STEP: listing all ConfigMaps in test namespace 01/18/23 22:05:48.929
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:05:48.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5304" for this suite. 01/18/23 22:05:48.938
------------------------------
• [1.032 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:05:47.918
    Jan 18 22:05:47.918: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:05:47.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:48.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:48.877
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/18/23 22:05:48.88
    STEP: fetching the ConfigMap 01/18/23 22:05:48.893
    STEP: patching the ConfigMap 01/18/23 22:05:48.896
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 22:05:48.907
    STEP: deleting the ConfigMap by collection with a label selector 01/18/23 22:05:48.912
    STEP: listing all ConfigMaps in test namespace 01/18/23 22:05:48.929
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:05:48.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5304" for this suite. 01/18/23 22:05:48.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:05:48.951
Jan 18 22:05:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:05:48.952
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:49.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:49.284
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8190 01/18/23 22:05:49.286
STEP: creating a selector 01/18/23 22:05:49.286
STEP: Creating the service pods in kubernetes 01/18/23 22:05:49.286
Jan 18 22:05:49.287: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:05:49.324: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8190" to be "running and ready"
Jan 18 22:05:49.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930279ms
Jan 18 22:05:49.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:05:51.335: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010492222s
Jan 18 22:05:51.335: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:05:53.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009979245s
Jan 18 22:05:53.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:05:55.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009957583s
Jan 18 22:05:55.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:05:57.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009965622s
Jan 18 22:05:57.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:05:59.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008901045s
Jan 18 22:05:59.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:01.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009593695s
Jan 18 22:06:01.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:03.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008751152s
Jan 18 22:06:03.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:05.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009021587s
Jan 18 22:06:05.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:07.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009663227s
Jan 18 22:06:07.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:09.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00907715s
Jan 18 22:06:09.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:06:11.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.00953528s
Jan 18 22:06:11.334: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:06:11.334: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:06:11.337: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8190" to be "running and ready"
Jan 18 22:06:11.341: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.694852ms
Jan 18 22:06:11.341: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:06:11.341: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:06:11.344
Jan 18 22:06:11.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8190" to be "running"
Jan 18 22:06:11.358: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679251ms
Jan 18 22:06:13.365: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009930201s
Jan 18 22:06:13.365: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:06:13.368: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:06:13.368: INFO: Breadth first check of 10.1.192.35 on host 10.0.0.4...
Jan 18 22:06:13.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.43:9080/dial?request=hostname&protocol=http&host=10.1.192.35&port=8083&tries=1'] Namespace:pod-network-test-8190 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:06:13.372: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:06:13.372: INFO: ExecWithOptions: Clientset creation
Jan 18 22:06:13.372: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8190/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.192.35%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 22:06:13.458: INFO: Waiting for responses: map[]
Jan 18 22:06:13.458: INFO: reached 10.1.192.35 after 0/1 tries
Jan 18 22:06:13.458: INFO: Breadth first check of 10.1.132.27 on host 10.0.0.5...
Jan 18 22:06:13.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.43:9080/dial?request=hostname&protocol=http&host=10.1.132.27&port=8083&tries=1'] Namespace:pod-network-test-8190 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:06:13.462: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:06:13.463: INFO: ExecWithOptions: Clientset creation
Jan 18 22:06:13.463: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8190/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.132.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 22:06:13.555: INFO: Waiting for responses: map[]
Jan 18 22:06:13.555: INFO: reached 10.1.132.27 after 0/1 tries
Jan 18 22:06:13.555: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8190" for this suite. 01/18/23 22:06:13.56
------------------------------
• [SLOW TEST] [24.622 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:05:48.951
    Jan 18 22:05:48.951: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:05:48.952
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:05:49.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:05:49.284
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8190 01/18/23 22:05:49.286
    STEP: creating a selector 01/18/23 22:05:49.286
    STEP: Creating the service pods in kubernetes 01/18/23 22:05:49.286
    Jan 18 22:05:49.287: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:05:49.324: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8190" to be "running and ready"
    Jan 18 22:05:49.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.930279ms
    Jan 18 22:05:49.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:05:51.335: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010492222s
    Jan 18 22:05:51.335: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:05:53.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009979245s
    Jan 18 22:05:53.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:05:55.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009957583s
    Jan 18 22:05:55.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:05:57.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009965622s
    Jan 18 22:05:57.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:05:59.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008901045s
    Jan 18 22:05:59.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:01.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009593695s
    Jan 18 22:06:01.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:03.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008751152s
    Jan 18 22:06:03.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:05.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009021587s
    Jan 18 22:06:05.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:07.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009663227s
    Jan 18 22:06:07.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:09.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00907715s
    Jan 18 22:06:09.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:06:11.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.00953528s
    Jan 18 22:06:11.334: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:06:11.334: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:06:11.337: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8190" to be "running and ready"
    Jan 18 22:06:11.341: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.694852ms
    Jan 18 22:06:11.341: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:06:11.341: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:06:11.344
    Jan 18 22:06:11.355: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8190" to be "running"
    Jan 18 22:06:11.358: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679251ms
    Jan 18 22:06:13.365: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009930201s
    Jan 18 22:06:13.365: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:06:13.368: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:06:13.368: INFO: Breadth first check of 10.1.192.35 on host 10.0.0.4...
    Jan 18 22:06:13.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.43:9080/dial?request=hostname&protocol=http&host=10.1.192.35&port=8083&tries=1'] Namespace:pod-network-test-8190 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:06:13.372: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:06:13.372: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:06:13.372: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8190/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.192.35%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 22:06:13.458: INFO: Waiting for responses: map[]
    Jan 18 22:06:13.458: INFO: reached 10.1.192.35 after 0/1 tries
    Jan 18 22:06:13.458: INFO: Breadth first check of 10.1.132.27 on host 10.0.0.5...
    Jan 18 22:06:13.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.192.43:9080/dial?request=hostname&protocol=http&host=10.1.132.27&port=8083&tries=1'] Namespace:pod-network-test-8190 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:06:13.462: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:06:13.463: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:06:13.463: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-8190/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.1.192.43%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.1.132.27%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 22:06:13.555: INFO: Waiting for responses: map[]
    Jan 18 22:06:13.555: INFO: reached 10.1.132.27 after 0/1 tries
    Jan 18 22:06:13.555: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:13.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8190" for this suite. 01/18/23 22:06:13.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:13.576
Jan 18 22:06:13.576: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:06:13.577
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:13.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:13.875
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/18/23 22:06:13.88
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9146" for this suite. 01/18/23 22:06:13.888
------------------------------
• [0.325 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:13.576
    Jan 18 22:06:13.576: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:06:13.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:13.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:13.875
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/18/23 22:06:13.88
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9146" for this suite. 01/18/23 22:06:13.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:13.903
Jan 18 22:06:13.903: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:13.904
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:14.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:14.391
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/18/23 22:06:14.396
Jan 18 22:06:14.397: INFO: namespace kubectl-1558
Jan 18 22:06:14.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 create -f -'
Jan 18 22:06:15.457: INFO: stderr: ""
Jan 18 22:06:15.457: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:06:15.457
Jan 18 22:06:16.464: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:06:16.464: INFO: Found 0 / 1
Jan 18 22:06:17.464: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:06:17.464: INFO: Found 0 / 1
Jan 18 22:06:18.464: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:06:18.464: INFO: Found 1 / 1
Jan 18 22:06:18.464: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 22:06:18.467: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:06:18.467: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:06:18.467: INFO: wait on agnhost-primary startup in kubectl-1558 
Jan 18 22:06:18.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 logs agnhost-primary-g7dsl agnhost-primary'
Jan 18 22:06:18.577: INFO: stderr: ""
Jan 18 22:06:18.578: INFO: stdout: "Paused\n"
STEP: exposing RC 01/18/23 22:06:18.578
Jan 18 22:06:18.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 22:06:18.687: INFO: stderr: ""
Jan 18 22:06:18.687: INFO: stdout: "service/rm2 exposed\n"
Jan 18 22:06:18.692: INFO: Service rm2 in namespace kubectl-1558 found.
STEP: exposing service 01/18/23 22:06:20.701
Jan 18 22:06:20.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 22:06:20.844: INFO: stderr: ""
Jan 18 22:06:20.844: INFO: stdout: "service/rm3 exposed\n"
Jan 18 22:06:20.849: INFO: Service rm3 in namespace kubectl-1558 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:22.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1558" for this suite. 01/18/23 22:06:22.861
------------------------------
• [SLOW TEST] [8.969 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:13.903
    Jan 18 22:06:13.903: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:13.904
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:14.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:14.391
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/18/23 22:06:14.396
    Jan 18 22:06:14.397: INFO: namespace kubectl-1558
    Jan 18 22:06:14.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 create -f -'
    Jan 18 22:06:15.457: INFO: stderr: ""
    Jan 18 22:06:15.457: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:06:15.457
    Jan 18 22:06:16.464: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:06:16.464: INFO: Found 0 / 1
    Jan 18 22:06:17.464: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:06:17.464: INFO: Found 0 / 1
    Jan 18 22:06:18.464: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:06:18.464: INFO: Found 1 / 1
    Jan 18 22:06:18.464: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 22:06:18.467: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:06:18.467: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:06:18.467: INFO: wait on agnhost-primary startup in kubectl-1558 
    Jan 18 22:06:18.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 logs agnhost-primary-g7dsl agnhost-primary'
    Jan 18 22:06:18.577: INFO: stderr: ""
    Jan 18 22:06:18.578: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/18/23 22:06:18.578
    Jan 18 22:06:18.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 18 22:06:18.687: INFO: stderr: ""
    Jan 18 22:06:18.687: INFO: stdout: "service/rm2 exposed\n"
    Jan 18 22:06:18.692: INFO: Service rm2 in namespace kubectl-1558 found.
    STEP: exposing service 01/18/23 22:06:20.701
    Jan 18 22:06:20.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-1558 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 18 22:06:20.844: INFO: stderr: ""
    Jan 18 22:06:20.844: INFO: stdout: "service/rm3 exposed\n"
    Jan 18 22:06:20.849: INFO: Service rm3 in namespace kubectl-1558 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:22.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1558" for this suite. 01/18/23 22:06:22.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:22.873
Jan 18 22:06:22.873: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:06:22.874
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:23.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:23.331
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-147de1cd-b0e6-45a0-9e85-3d1f9f62c8b6 01/18/23 22:06:23.334
STEP: Creating secret with name secret-projected-all-test-volume-a8c1e1f1-dbc9-4a83-afd4-0da44361d8c0 01/18/23 22:06:23.346
STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 22:06:23.356
Jan 18 22:06:23.372: INFO: Waiting up to 5m0s for pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc" in namespace "projected-5513" to be "Succeeded or Failed"
Jan 18 22:06:23.376: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.920856ms
Jan 18 22:06:25.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009711168s
Jan 18 22:06:27.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009182598s
STEP: Saw pod success 01/18/23 22:06:27.381
Jan 18 22:06:27.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc" satisfied condition "Succeeded or Failed"
Jan 18 22:06:27.385: INFO: Trying to get logs from node test-vm-1 pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc container projected-all-volume-test: <nil>
STEP: delete the pod 01/18/23 22:06:27.393
Jan 18 22:06:27.415: INFO: Waiting for pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc to disappear
Jan 18 22:06:27.421: INFO: Pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:27.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5513" for this suite. 01/18/23 22:06:27.425
------------------------------
• [4.567 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:22.873
    Jan 18 22:06:22.873: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:06:22.874
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:23.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:23.331
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-147de1cd-b0e6-45a0-9e85-3d1f9f62c8b6 01/18/23 22:06:23.334
    STEP: Creating secret with name secret-projected-all-test-volume-a8c1e1f1-dbc9-4a83-afd4-0da44361d8c0 01/18/23 22:06:23.346
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 22:06:23.356
    Jan 18 22:06:23.372: INFO: Waiting up to 5m0s for pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc" in namespace "projected-5513" to be "Succeeded or Failed"
    Jan 18 22:06:23.376: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.920856ms
    Jan 18 22:06:25.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009711168s
    Jan 18 22:06:27.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009182598s
    STEP: Saw pod success 01/18/23 22:06:27.381
    Jan 18 22:06:27.381: INFO: Pod "projected-volume-3f944977-860c-4520-84e3-71d67958cebc" satisfied condition "Succeeded or Failed"
    Jan 18 22:06:27.385: INFO: Trying to get logs from node test-vm-1 pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc container projected-all-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:06:27.393
    Jan 18 22:06:27.415: INFO: Waiting for pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc to disappear
    Jan 18 22:06:27.421: INFO: Pod projected-volume-3f944977-860c-4520-84e3-71d67958cebc no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:27.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5513" for this suite. 01/18/23 22:06:27.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:27.441
Jan 18 22:06:27.441: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:27.442
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:27.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:27.875
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/18/23 22:06:27.877
Jan 18 22:06:27.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 create -f -'
Jan 18 22:06:28.993: INFO: stderr: ""
Jan 18 22:06:28.993: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/18/23 22:06:28.993
Jan 18 22:06:28.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 diff -f -'
Jan 18 22:06:29.236: INFO: rc: 1
Jan 18 22:06:29.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 delete -f -'
Jan 18 22:06:29.347: INFO: stderr: ""
Jan 18 22:06:29.347: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:29.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3628" for this suite. 01/18/23 22:06:29.353
------------------------------
• [1.930 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:27.441
    Jan 18 22:06:27.441: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:06:27.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:27.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:27.875
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/18/23 22:06:27.877
    Jan 18 22:06:27.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 create -f -'
    Jan 18 22:06:28.993: INFO: stderr: ""
    Jan 18 22:06:28.993: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/18/23 22:06:28.993
    Jan 18 22:06:28.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 diff -f -'
    Jan 18 22:06:29.236: INFO: rc: 1
    Jan 18 22:06:29.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3628 delete -f -'
    Jan 18 22:06:29.347: INFO: stderr: ""
    Jan 18 22:06:29.347: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:29.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3628" for this suite. 01/18/23 22:06:29.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:29.371
Jan 18 22:06:29.372: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:06:29.373
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:29.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:29.513
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 22:06:29.546
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:06:29.559
Jan 18 22:06:29.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:06:29.569: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:06:30.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:06:30.581: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:06:31.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:06:31.578: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 22:06:32.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:06:32.578: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 22:06:32.582
Jan 18 22:06:32.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:06:32.610: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 22:06:32.61
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:06:33.63
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3097, will wait for the garbage collector to delete the pods 01/18/23 22:06:33.63
Jan 18 22:06:33.705: INFO: Deleting DaemonSet.extensions daemon-set took: 18.221061ms
Jan 18 22:06:34.605: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.34467ms
Jan 18 22:06:37.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:06:37.612: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:06:37.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26234"},"items":null}

Jan 18 22:06:37.624: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26234"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:37.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3097" for this suite. 01/18/23 22:06:37.641
------------------------------
• [SLOW TEST] [8.281 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:29.371
    Jan 18 22:06:29.372: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:06:29.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:29.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:29.513
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 22:06:29.546
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:06:29.559
    Jan 18 22:06:29.569: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:06:29.569: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:06:30.581: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:06:30.581: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:06:31.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:06:31.578: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 22:06:32.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:06:32.578: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 22:06:32.582
    Jan 18 22:06:32.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:06:32.610: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 22:06:32.61
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:06:33.63
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3097, will wait for the garbage collector to delete the pods 01/18/23 22:06:33.63
    Jan 18 22:06:33.705: INFO: Deleting DaemonSet.extensions daemon-set took: 18.221061ms
    Jan 18 22:06:34.605: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.34467ms
    Jan 18 22:06:37.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:06:37.612: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:06:37.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26234"},"items":null}

    Jan 18 22:06:37.624: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26234"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:37.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3097" for this suite. 01/18/23 22:06:37.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:37.654
Jan 18 22:06:37.654: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename containers 01/18/23 22:06:37.655
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:37.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:37.874
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/18/23 22:06:37.879
Jan 18 22:06:37.897: INFO: Waiting up to 5m0s for pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7" in namespace "containers-1434" to be "Succeeded or Failed"
Jan 18 22:06:37.901: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019557ms
Jan 18 22:06:39.907: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010014303s
Jan 18 22:06:41.906: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00906085s
STEP: Saw pod success 01/18/23 22:06:41.906
Jan 18 22:06:41.907: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7" satisfied condition "Succeeded or Failed"
Jan 18 22:06:41.911: INFO: Trying to get logs from node test-vm-1 pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:06:41.918
Jan 18 22:06:41.942: INFO: Waiting for pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 to disappear
Jan 18 22:06:41.949: INFO: Pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1434" for this suite. 01/18/23 22:06:41.954
------------------------------
• [4.312 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:37.654
    Jan 18 22:06:37.654: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename containers 01/18/23 22:06:37.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:37.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:37.874
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/18/23 22:06:37.879
    Jan 18 22:06:37.897: INFO: Waiting up to 5m0s for pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7" in namespace "containers-1434" to be "Succeeded or Failed"
    Jan 18 22:06:37.901: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019557ms
    Jan 18 22:06:39.907: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010014303s
    Jan 18 22:06:41.906: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00906085s
    STEP: Saw pod success 01/18/23 22:06:41.906
    Jan 18 22:06:41.907: INFO: Pod "client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7" satisfied condition "Succeeded or Failed"
    Jan 18 22:06:41.911: INFO: Trying to get logs from node test-vm-1 pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:06:41.918
    Jan 18 22:06:41.942: INFO: Waiting for pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 to disappear
    Jan 18 22:06:41.949: INFO: Pod client-containers-2005729a-ec6b-42ad-bdb3-90cad613e3f7 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1434" for this suite. 01/18/23 22:06:41.954
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:41.967
Jan 18 22:06:41.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename events 01/18/23 22:06:41.968
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:42.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:42.569
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/18/23 22:06:42.572
STEP: listing events in all namespaces 01/18/23 22:06:42.581
STEP: listing events in test namespace 01/18/23 22:06:42.586
STEP: listing events with field selection filtering on source 01/18/23 22:06:42.589
STEP: listing events with field selection filtering on reportingController 01/18/23 22:06:42.593
STEP: getting the test event 01/18/23 22:06:42.597
STEP: patching the test event 01/18/23 22:06:42.6
STEP: getting the test event 01/18/23 22:06:42.625
STEP: updating the test event 01/18/23 22:06:42.629
STEP: getting the test event 01/18/23 22:06:42.641
STEP: deleting the test event 01/18/23 22:06:42.644
STEP: listing events in all namespaces 01/18/23 22:06:42.66
STEP: listing events in test namespace 01/18/23 22:06:42.665
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:42.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6759" for this suite. 01/18/23 22:06:42.676
------------------------------
• [0.725 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:41.967
    Jan 18 22:06:41.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename events 01/18/23 22:06:41.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:42.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:42.569
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/18/23 22:06:42.572
    STEP: listing events in all namespaces 01/18/23 22:06:42.581
    STEP: listing events in test namespace 01/18/23 22:06:42.586
    STEP: listing events with field selection filtering on source 01/18/23 22:06:42.589
    STEP: listing events with field selection filtering on reportingController 01/18/23 22:06:42.593
    STEP: getting the test event 01/18/23 22:06:42.597
    STEP: patching the test event 01/18/23 22:06:42.6
    STEP: getting the test event 01/18/23 22:06:42.625
    STEP: updating the test event 01/18/23 22:06:42.629
    STEP: getting the test event 01/18/23 22:06:42.641
    STEP: deleting the test event 01/18/23 22:06:42.644
    STEP: listing events in all namespaces 01/18/23 22:06:42.66
    STEP: listing events in test namespace 01/18/23 22:06:42.665
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:42.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6759" for this suite. 01/18/23 22:06:42.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:42.692
Jan 18 22:06:42.692: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename watch 01/18/23 22:06:42.693
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:42.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:42.874
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/18/23 22:06:42.877
STEP: creating a new configmap 01/18/23 22:06:42.878
STEP: modifying the configmap once 01/18/23 22:06:42.89
STEP: closing the watch once it receives two notifications 01/18/23 22:06:42.907
Jan 18 22:06:42.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26281 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:06:42.907: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26283 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/18/23 22:06:42.907
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 22:06:42.923
STEP: deleting the configmap 01/18/23 22:06:42.925
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 22:06:42.94
Jan 18 22:06:42.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26285 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:06:42.941: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26287 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:42.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9112" for this suite. 01/18/23 22:06:42.947
------------------------------
• [0.270 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:42.692
    Jan 18 22:06:42.692: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename watch 01/18/23 22:06:42.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:42.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:42.874
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/18/23 22:06:42.877
    STEP: creating a new configmap 01/18/23 22:06:42.878
    STEP: modifying the configmap once 01/18/23 22:06:42.89
    STEP: closing the watch once it receives two notifications 01/18/23 22:06:42.907
    Jan 18 22:06:42.907: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26281 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:06:42.907: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26283 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/18/23 22:06:42.907
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 22:06:42.923
    STEP: deleting the configmap 01/18/23 22:06:42.925
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 22:06:42.94
    Jan 18 22:06:42.940: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26285 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:06:42.941: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9112  9bdadebe-0ab1-4945-bdef-cf2dbd048f43 26287 0 2023-01-18 22:06:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 22:06:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:42.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9112" for this suite. 01/18/23 22:06:42.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:42.964
Jan 18 22:06:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 22:06:42.965
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:43.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:43.25
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/18/23 22:06:43.253
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:06:43.264
STEP: delete the deployment 01/18/23 22:06:43.783
STEP: wait for all rs to be garbage collected 01/18/23 22:06:43.815
STEP: expected 0 pods, got 2 pods 01/18/23 22:06:43.821
STEP: expected 0 rs, got 1 rs 01/18/23 22:06:43.833
STEP: Gathering metrics 01/18/23 22:06:44.383
W0118 22:06:44.393619      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 22:06:44.393: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4145" for this suite. 01/18/23 22:06:44.4
------------------------------
• [1.457 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:42.964
    Jan 18 22:06:42.965: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 22:06:42.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:43.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:43.25
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/18/23 22:06:43.253
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 22:06:43.264
    STEP: delete the deployment 01/18/23 22:06:43.783
    STEP: wait for all rs to be garbage collected 01/18/23 22:06:43.815
    STEP: expected 0 pods, got 2 pods 01/18/23 22:06:43.821
    STEP: expected 0 rs, got 1 rs 01/18/23 22:06:43.833
    STEP: Gathering metrics 01/18/23 22:06:44.383
    W0118 22:06:44.393619      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 22:06:44.393: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:44.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4145" for this suite. 01/18/23 22:06:44.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:44.423
Jan 18 22:06:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:06:44.424
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:44.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:44.874
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/18/23 22:06:44.877
Jan 18 22:06:44.896: INFO: Waiting up to 5m0s for pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1" in namespace "downward-api-170" to be "Succeeded or Failed"
Jan 18 22:06:44.900: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155259ms
Jan 18 22:06:46.904: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008453639s
Jan 18 22:06:48.906: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010577287s
STEP: Saw pod success 01/18/23 22:06:48.906
Jan 18 22:06:48.907: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1" satisfied condition "Succeeded or Failed"
Jan 18 22:06:48.911: INFO: Trying to get logs from node test-vm-1 pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:06:48.92
Jan 18 22:06:48.948: INFO: Waiting for pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 to disappear
Jan 18 22:06:48.954: INFO: Pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-170" for this suite. 01/18/23 22:06:48.959
------------------------------
• [4.549 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:44.423
    Jan 18 22:06:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:06:44.424
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:44.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:44.874
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/18/23 22:06:44.877
    Jan 18 22:06:44.896: INFO: Waiting up to 5m0s for pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1" in namespace "downward-api-170" to be "Succeeded or Failed"
    Jan 18 22:06:44.900: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155259ms
    Jan 18 22:06:46.904: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008453639s
    Jan 18 22:06:48.906: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010577287s
    STEP: Saw pod success 01/18/23 22:06:48.906
    Jan 18 22:06:48.907: INFO: Pod "downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1" satisfied condition "Succeeded or Failed"
    Jan 18 22:06:48.911: INFO: Trying to get logs from node test-vm-1 pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:06:48.92
    Jan 18 22:06:48.948: INFO: Waiting for pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 to disappear
    Jan 18 22:06:48.954: INFO: Pod downward-api-c14a1511-e774-41b3-98eb-0dc2521e7de1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-170" for this suite. 01/18/23 22:06:48.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:48.973
Jan 18 22:06:48.974: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:06:48.975
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:49.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:49.607
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/18/23 22:06:49.61
Jan 18 22:06:49.632: INFO: Waiting up to 5m0s for pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36" in namespace "var-expansion-7170" to be "Succeeded or Failed"
Jan 18 22:06:49.637: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.656067ms
Jan 18 22:06:51.642: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Running", Reason="", readiness=false. Elapsed: 2.009762052s
Jan 18 22:06:53.646: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013372379s
STEP: Saw pod success 01/18/23 22:06:53.646
Jan 18 22:06:53.646: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36" satisfied condition "Succeeded or Failed"
Jan 18 22:06:53.651: INFO: Trying to get logs from node test-vm-1 pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:06:53.66
Jan 18 22:06:53.685: INFO: Waiting for pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 to disappear
Jan 18 22:06:53.691: INFO: Pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:53.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7170" for this suite. 01/18/23 22:06:53.696
------------------------------
• [4.734 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:48.973
    Jan 18 22:06:48.974: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:06:48.975
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:49.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:49.607
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/18/23 22:06:49.61
    Jan 18 22:06:49.632: INFO: Waiting up to 5m0s for pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36" in namespace "var-expansion-7170" to be "Succeeded or Failed"
    Jan 18 22:06:49.637: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Pending", Reason="", readiness=false. Elapsed: 4.656067ms
    Jan 18 22:06:51.642: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Running", Reason="", readiness=false. Elapsed: 2.009762052s
    Jan 18 22:06:53.646: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013372379s
    STEP: Saw pod success 01/18/23 22:06:53.646
    Jan 18 22:06:53.646: INFO: Pod "var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36" satisfied condition "Succeeded or Failed"
    Jan 18 22:06:53.651: INFO: Trying to get logs from node test-vm-1 pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:06:53.66
    Jan 18 22:06:53.685: INFO: Waiting for pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 to disappear
    Jan 18 22:06:53.691: INFO: Pod var-expansion-9887d551-ea91-4bd0-8bda-d3fa9736be36 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:53.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7170" for this suite. 01/18/23 22:06:53.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:53.711
Jan 18 22:06:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename events 01/18/23 22:06:53.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:53.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:53.875
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/18/23 22:06:53.878
STEP: get a list of Events with a label in the current namespace 01/18/23 22:06:53.917
STEP: delete a list of events 01/18/23 22:06:53.922
Jan 18 22:06:53.922: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 22:06:53.976
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan 18 22:06:53.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4905" for this suite. 01/18/23 22:06:53.988
------------------------------
• [0.290 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:53.711
    Jan 18 22:06:53.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename events 01/18/23 22:06:53.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:53.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:53.875
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/18/23 22:06:53.878
    STEP: get a list of Events with a label in the current namespace 01/18/23 22:06:53.917
    STEP: delete a list of events 01/18/23 22:06:53.922
    Jan 18 22:06:53.922: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 22:06:53.976
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:06:53.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4905" for this suite. 01/18/23 22:06:53.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:06:54.005
Jan 18 22:06:54.005: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 22:06:54.006
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:54.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:54.874
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/18/23 22:06:54.877
STEP: Ensuring active pods == parallelism 01/18/23 22:06:54.892
STEP: Orphaning one of the Job's Pods 01/18/23 22:06:58.897
Jan 18 22:06:59.432: INFO: Successfully updated pod "adopt-release-5dlgl"
STEP: Checking that the Job readopts the Pod 01/18/23 22:06:59.432
Jan 18 22:06:59.433: INFO: Waiting up to 15m0s for pod "adopt-release-5dlgl" in namespace "job-7691" to be "adopted"
Jan 18 22:06:59.442: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 9.506836ms
Jan 18 22:07:01.448: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.015657733s
Jan 18 22:07:01.448: INFO: Pod "adopt-release-5dlgl" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/18/23 22:07:01.448
Jan 18 22:07:01.968: INFO: Successfully updated pod "adopt-release-5dlgl"
STEP: Checking that the Job releases the Pod 01/18/23 22:07:01.968
Jan 18 22:07:01.968: INFO: Waiting up to 15m0s for pod "adopt-release-5dlgl" in namespace "job-7691" to be "released"
Jan 18 22:07:01.977: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 9.131631ms
Jan 18 22:07:03.983: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01472882s
Jan 18 22:07:03.983: INFO: Pod "adopt-release-5dlgl" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:07:03.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7691" for this suite. 01/18/23 22:07:03.988
------------------------------
• [SLOW TEST] [9.995 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:06:54.005
    Jan 18 22:06:54.005: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 22:06:54.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:06:54.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:06:54.874
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/18/23 22:06:54.877
    STEP: Ensuring active pods == parallelism 01/18/23 22:06:54.892
    STEP: Orphaning one of the Job's Pods 01/18/23 22:06:58.897
    Jan 18 22:06:59.432: INFO: Successfully updated pod "adopt-release-5dlgl"
    STEP: Checking that the Job readopts the Pod 01/18/23 22:06:59.432
    Jan 18 22:06:59.433: INFO: Waiting up to 15m0s for pod "adopt-release-5dlgl" in namespace "job-7691" to be "adopted"
    Jan 18 22:06:59.442: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 9.506836ms
    Jan 18 22:07:01.448: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.015657733s
    Jan 18 22:07:01.448: INFO: Pod "adopt-release-5dlgl" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/18/23 22:07:01.448
    Jan 18 22:07:01.968: INFO: Successfully updated pod "adopt-release-5dlgl"
    STEP: Checking that the Job releases the Pod 01/18/23 22:07:01.968
    Jan 18 22:07:01.968: INFO: Waiting up to 15m0s for pod "adopt-release-5dlgl" in namespace "job-7691" to be "released"
    Jan 18 22:07:01.977: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 9.131631ms
    Jan 18 22:07:03.983: INFO: Pod "adopt-release-5dlgl": Phase="Running", Reason="", readiness=true. Elapsed: 2.01472882s
    Jan 18 22:07:03.983: INFO: Pod "adopt-release-5dlgl" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:07:03.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7691" for this suite. 01/18/23 22:07:03.988
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:07:04
Jan 18 22:07:04.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:07:04.002
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:04.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:04.877
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5873 01/18/23 22:07:04.88
STEP: creating a selector 01/18/23 22:07:04.881
STEP: Creating the service pods in kubernetes 01/18/23 22:07:04.881
Jan 18 22:07:04.881: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:07:04.928: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5873" to be "running and ready"
Jan 18 22:07:04.933: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440964ms
Jan 18 22:07:04.933: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:07:06.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010335557s
Jan 18 22:07:06.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:07:08.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010450948s
Jan 18 22:07:08.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:07:10.940: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011856624s
Jan 18 22:07:10.940: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:07:12.940: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011100569s
Jan 18 22:07:12.940: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:07:14.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01216944s
Jan 18 22:07:14.941: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:07:16.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012167407s
Jan 18 22:07:16.941: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:07:16.941: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:07:16.947: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5873" to be "running and ready"
Jan 18 22:07:16.953: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.77828ms
Jan 18 22:07:16.953: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:07:16.953: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:07:16.958
Jan 18 22:07:16.989: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5873" to be "running"
Jan 18 22:07:16.998: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705021ms
Jan 18 22:07:19.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013930518s
Jan 18 22:07:19.003: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:07:19.007: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5873" to be "running"
Jan 18 22:07:19.011: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.715652ms
Jan 18 22:07:19.011: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 22:07:19.014: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:07:19.014: INFO: Going to poll 10.1.192.8 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:07:19.018: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.192.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:07:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:07:19.019: INFO: ExecWithOptions: Clientset creation
Jan 18 22:07:19.019: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5873/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.192.8+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:07:20.115: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 22:07:20.115: INFO: Going to poll 10.1.132.55 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:07:20.119: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.132.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:07:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:07:20.120: INFO: ExecWithOptions: Clientset creation
Jan 18 22:07:20.120: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5873/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.132.55+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:07:21.215: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 22:07:21.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5873" for this suite. 01/18/23 22:07:21.221
------------------------------
• [SLOW TEST] [17.235 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:07:04
    Jan 18 22:07:04.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:07:04.002
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:04.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:04.877
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5873 01/18/23 22:07:04.88
    STEP: creating a selector 01/18/23 22:07:04.881
    STEP: Creating the service pods in kubernetes 01/18/23 22:07:04.881
    Jan 18 22:07:04.881: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:07:04.928: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5873" to be "running and ready"
    Jan 18 22:07:04.933: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.440964ms
    Jan 18 22:07:04.933: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:07:06.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010335557s
    Jan 18 22:07:06.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:07:08.939: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010450948s
    Jan 18 22:07:08.939: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:07:10.940: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011856624s
    Jan 18 22:07:10.940: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:07:12.940: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011100569s
    Jan 18 22:07:12.940: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:07:14.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.01216944s
    Jan 18 22:07:14.941: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:07:16.941: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012167407s
    Jan 18 22:07:16.941: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:07:16.941: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:07:16.947: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5873" to be "running and ready"
    Jan 18 22:07:16.953: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.77828ms
    Jan 18 22:07:16.953: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:07:16.953: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:07:16.958
    Jan 18 22:07:16.989: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5873" to be "running"
    Jan 18 22:07:16.998: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705021ms
    Jan 18 22:07:19.003: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013930518s
    Jan 18 22:07:19.003: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:07:19.007: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5873" to be "running"
    Jan 18 22:07:19.011: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.715652ms
    Jan 18 22:07:19.011: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 22:07:19.014: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:07:19.014: INFO: Going to poll 10.1.192.8 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:07:19.018: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.192.8 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:07:19.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:07:19.019: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:07:19.019: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5873/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.192.8+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:07:20.115: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 22:07:20.115: INFO: Going to poll 10.1.132.55 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:07:20.119: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.132.55 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5873 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:07:20.119: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:07:20.120: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:07:20.120: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-5873/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.1.132.55+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:07:21.215: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:07:21.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5873" for this suite. 01/18/23 22:07:21.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:07:21.237
Jan 18 22:07:21.237: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:07:21.24
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:21.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:21.649
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/18/23 22:07:21.652
STEP: waiting for pod running 01/18/23 22:07:21.67
Jan 18 22:07:21.670: INFO: Waiting up to 2m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016" to be "running"
Jan 18 22:07:21.675: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723966ms
Jan 18 22:07:23.680: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010188113s
Jan 18 22:07:23.680: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" satisfied condition "running"
STEP: creating a file in subpath 01/18/23 22:07:23.68
Jan 18 22:07:23.684: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6016 PodName:var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:07:23.684: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:07:23.685: INFO: ExecWithOptions: Clientset creation
Jan 18 22:07:23.685: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6016/pods/var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/18/23 22:07:23.768
Jan 18 22:07:23.774: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6016 PodName:var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:07:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:07:23.775: INFO: ExecWithOptions: Clientset creation
Jan 18 22:07:23.775: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6016/pods/var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/18/23 22:07:23.862
Jan 18 22:07:24.382: INFO: Successfully updated pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d"
STEP: waiting for annotated pod running 01/18/23 22:07:24.382
Jan 18 22:07:24.382: INFO: Waiting up to 2m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016" to be "running"
Jan 18 22:07:24.387: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Running", Reason="", readiness=true. Elapsed: 4.524864ms
Jan 18 22:07:24.387: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 22:07:24.387
Jan 18 22:07:24.387: INFO: Deleting pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016"
Jan 18 22:07:24.400: INFO: Wait up to 5m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:07:58.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6016" for this suite. 01/18/23 22:07:58.416
------------------------------
• [SLOW TEST] [37.192 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:07:21.237
    Jan 18 22:07:21.237: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:07:21.24
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:21.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:21.649
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/18/23 22:07:21.652
    STEP: waiting for pod running 01/18/23 22:07:21.67
    Jan 18 22:07:21.670: INFO: Waiting up to 2m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016" to be "running"
    Jan 18 22:07:21.675: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.723966ms
    Jan 18 22:07:23.680: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010188113s
    Jan 18 22:07:23.680: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" satisfied condition "running"
    STEP: creating a file in subpath 01/18/23 22:07:23.68
    Jan 18 22:07:23.684: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6016 PodName:var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:07:23.684: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:07:23.685: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:07:23.685: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6016/pods/var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/18/23 22:07:23.768
    Jan 18 22:07:23.774: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6016 PodName:var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:07:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:07:23.775: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:07:23.775: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/var-expansion-6016/pods/var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/18/23 22:07:23.862
    Jan 18 22:07:24.382: INFO: Successfully updated pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d"
    STEP: waiting for annotated pod running 01/18/23 22:07:24.382
    Jan 18 22:07:24.382: INFO: Waiting up to 2m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016" to be "running"
    Jan 18 22:07:24.387: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d": Phase="Running", Reason="", readiness=true. Elapsed: 4.524864ms
    Jan 18 22:07:24.387: INFO: Pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 22:07:24.387
    Jan 18 22:07:24.387: INFO: Deleting pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" in namespace "var-expansion-6016"
    Jan 18 22:07:24.400: INFO: Wait up to 5m0s for pod "var-expansion-ac9760c2-ff4b-4cd6-9d79-ed7b0b5a271d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:07:58.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6016" for this suite. 01/18/23 22:07:58.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:07:58.431
Jan 18 22:07:58.431: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename subpath 01/18/23 22:07:58.432
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:58.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:58.608
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:07:58.61
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-ddd5 01/18/23 22:07:58.63
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:07:58.63
Jan 18 22:07:58.645: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ddd5" in namespace "subpath-3758" to be "Succeeded or Failed"
Jan 18 22:07:58.649: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566649ms
Jan 18 22:08:00.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357489s
Jan 18 22:08:02.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008932243s
Jan 18 22:08:04.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 6.010038095s
Jan 18 22:08:06.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 8.009176539s
Jan 18 22:08:08.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 10.008989991s
Jan 18 22:08:10.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008830444s
Jan 18 22:08:12.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009270654s
Jan 18 22:08:14.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.009122767s
Jan 18 22:08:16.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 18.009378623s
Jan 18 22:08:18.657: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 20.011503379s
Jan 18 22:08:20.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=false. Elapsed: 22.009181465s
Jan 18 22:08:22.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009849495s
STEP: Saw pod success 01/18/23 22:08:22.655
Jan 18 22:08:22.655: INFO: Pod "pod-subpath-test-configmap-ddd5" satisfied condition "Succeeded or Failed"
Jan 18 22:08:22.659: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-configmap-ddd5 container test-container-subpath-configmap-ddd5: <nil>
STEP: delete the pod 01/18/23 22:08:22.666
Jan 18 22:08:22.689: INFO: Waiting for pod pod-subpath-test-configmap-ddd5 to disappear
Jan 18 22:08:22.696: INFO: Pod pod-subpath-test-configmap-ddd5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ddd5 01/18/23 22:08:22.696
Jan 18 22:08:22.696: INFO: Deleting pod "pod-subpath-test-configmap-ddd5" in namespace "subpath-3758"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:08:22.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3758" for this suite. 01/18/23 22:08:22.705
------------------------------
• [SLOW TEST] [24.291 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:07:58.431
    Jan 18 22:07:58.431: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename subpath 01/18/23 22:07:58.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:07:58.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:07:58.608
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:07:58.61
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-ddd5 01/18/23 22:07:58.63
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:07:58.63
    Jan 18 22:07:58.645: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ddd5" in namespace "subpath-3758" to be "Succeeded or Failed"
    Jan 18 22:07:58.649: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.566649ms
    Jan 18 22:08:00.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009357489s
    Jan 18 22:08:02.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008932243s
    Jan 18 22:08:04.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 6.010038095s
    Jan 18 22:08:06.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 8.009176539s
    Jan 18 22:08:08.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 10.008989991s
    Jan 18 22:08:10.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 12.008830444s
    Jan 18 22:08:12.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 14.009270654s
    Jan 18 22:08:14.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 16.009122767s
    Jan 18 22:08:16.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 18.009378623s
    Jan 18 22:08:18.657: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=true. Elapsed: 20.011503379s
    Jan 18 22:08:20.654: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Running", Reason="", readiness=false. Elapsed: 22.009181465s
    Jan 18 22:08:22.655: INFO: Pod "pod-subpath-test-configmap-ddd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009849495s
    STEP: Saw pod success 01/18/23 22:08:22.655
    Jan 18 22:08:22.655: INFO: Pod "pod-subpath-test-configmap-ddd5" satisfied condition "Succeeded or Failed"
    Jan 18 22:08:22.659: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-configmap-ddd5 container test-container-subpath-configmap-ddd5: <nil>
    STEP: delete the pod 01/18/23 22:08:22.666
    Jan 18 22:08:22.689: INFO: Waiting for pod pod-subpath-test-configmap-ddd5 to disappear
    Jan 18 22:08:22.696: INFO: Pod pod-subpath-test-configmap-ddd5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-ddd5 01/18/23 22:08:22.696
    Jan 18 22:08:22.696: INFO: Deleting pod "pod-subpath-test-configmap-ddd5" in namespace "subpath-3758"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:08:22.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3758" for this suite. 01/18/23 22:08:22.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:08:22.723
Jan 18 22:08:22.723: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:08:22.724
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:23.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:23.875
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:08:23.905
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:08:24.139
STEP: Deploying the webhook pod 01/18/23 22:08:24.155
STEP: Wait for the deployment to be ready 01/18/23 22:08:24.177
Jan 18 22:08:24.185: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:08:26.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:08:28.204
STEP: Verifying the service has paired with the endpoint 01/18/23 22:08:28.231
Jan 18 22:08:29.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/18/23 22:08:29.236
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 22:08:29.258
STEP: Creating a configMap that should not be mutated 01/18/23 22:08:29.269
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 22:08:29.293
STEP: Creating a configMap that should be mutated 01/18/23 22:08:29.305
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:08:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8829" for this suite. 01/18/23 22:08:29.477
STEP: Destroying namespace "webhook-8829-markers" for this suite. 01/18/23 22:08:29.49
------------------------------
• [SLOW TEST] [6.779 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:08:22.723
    Jan 18 22:08:22.723: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:08:22.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:23.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:23.875
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:08:23.905
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:08:24.139
    STEP: Deploying the webhook pod 01/18/23 22:08:24.155
    STEP: Wait for the deployment to be ready 01/18/23 22:08:24.177
    Jan 18 22:08:24.185: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:08:26.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:08:28.204
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:08:28.231
    Jan 18 22:08:29.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/18/23 22:08:29.236
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 22:08:29.258
    STEP: Creating a configMap that should not be mutated 01/18/23 22:08:29.269
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 22:08:29.293
    STEP: Creating a configMap that should be mutated 01/18/23 22:08:29.305
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:08:29.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8829" for this suite. 01/18/23 22:08:29.477
    STEP: Destroying namespace "webhook-8829-markers" for this suite. 01/18/23 22:08:29.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:08:29.504
Jan 18 22:08:29.505: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:08:29.505
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:29.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:29.879
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-4279/configmap-test-76d627fc-aca8-49b8-9b4f-f7ac9362a573 01/18/23 22:08:29.885
STEP: Creating a pod to test consume configMaps 01/18/23 22:08:29.903
Jan 18 22:08:29.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16" in namespace "configmap-4279" to be "Succeeded or Failed"
Jan 18 22:08:29.933: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.625669ms
Jan 18 22:08:31.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Running", Reason="", readiness=false. Elapsed: 2.011205351s
Jan 18 22:08:33.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Running", Reason="", readiness=false. Elapsed: 4.010528457s
Jan 18 22:08:35.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010606747s
STEP: Saw pod success 01/18/23 22:08:35.938
Jan 18 22:08:35.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16" satisfied condition "Succeeded or Failed"
Jan 18 22:08:35.941: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 container env-test: <nil>
STEP: delete the pod 01/18/23 22:08:35.949
Jan 18 22:08:35.994: INFO: Waiting for pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 to disappear
Jan 18 22:08:36.000: INFO: Pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:08:36.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4279" for this suite. 01/18/23 22:08:36.004
------------------------------
• [SLOW TEST] [6.512 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:08:29.504
    Jan 18 22:08:29.505: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:08:29.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:29.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:29.879
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-4279/configmap-test-76d627fc-aca8-49b8-9b4f-f7ac9362a573 01/18/23 22:08:29.885
    STEP: Creating a pod to test consume configMaps 01/18/23 22:08:29.903
    Jan 18 22:08:29.927: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16" in namespace "configmap-4279" to be "Succeeded or Failed"
    Jan 18 22:08:29.933: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.625669ms
    Jan 18 22:08:31.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Running", Reason="", readiness=false. Elapsed: 2.011205351s
    Jan 18 22:08:33.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Running", Reason="", readiness=false. Elapsed: 4.010528457s
    Jan 18 22:08:35.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010606747s
    STEP: Saw pod success 01/18/23 22:08:35.938
    Jan 18 22:08:35.938: INFO: Pod "pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16" satisfied condition "Succeeded or Failed"
    Jan 18 22:08:35.941: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 container env-test: <nil>
    STEP: delete the pod 01/18/23 22:08:35.949
    Jan 18 22:08:35.994: INFO: Waiting for pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 to disappear
    Jan 18 22:08:36.000: INFO: Pod pod-configmaps-0d3a570a-4831-4245-b9c4-be7bb4cb7b16 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:08:36.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4279" for this suite. 01/18/23 22:08:36.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:08:36.017
Jan 18 22:08:36.017: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:08:36.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:36.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:36.291
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan 18 22:08:36.294: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:08:38.142
Jan 18 22:08:38.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 create -f -'
Jan 18 22:08:38.958: INFO: stderr: ""
Jan 18 22:08:38.958: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 22:08:38.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 delete e2e-test-crd-publish-openapi-9072-crds test-cr'
Jan 18 22:08:39.111: INFO: stderr: ""
Jan 18 22:08:39.111: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 22:08:39.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 apply -f -'
Jan 18 22:08:39.334: INFO: stderr: ""
Jan 18 22:08:39.334: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 22:08:39.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 delete e2e-test-crd-publish-openapi-9072-crds test-cr'
Jan 18 22:08:39.428: INFO: stderr: ""
Jan 18 22:08:39.428: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 22:08:39.428
Jan 18 22:08:39.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 explain e2e-test-crd-publish-openapi-9072-crds'
Jan 18 22:08:40.127: INFO: stderr: ""
Jan 18 22:08:40.127: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9072-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:08:43.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2857" for this suite. 01/18/23 22:08:43.106
------------------------------
• [SLOW TEST] [7.108 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:08:36.017
    Jan 18 22:08:36.017: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:08:36.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:36.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:36.291
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan 18 22:08:36.294: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 22:08:38.142
    Jan 18 22:08:38.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 create -f -'
    Jan 18 22:08:38.958: INFO: stderr: ""
    Jan 18 22:08:38.958: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 22:08:38.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 delete e2e-test-crd-publish-openapi-9072-crds test-cr'
    Jan 18 22:08:39.111: INFO: stderr: ""
    Jan 18 22:08:39.111: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 18 22:08:39.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 apply -f -'
    Jan 18 22:08:39.334: INFO: stderr: ""
    Jan 18 22:08:39.334: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 22:08:39.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 --namespace=crd-publish-openapi-2857 delete e2e-test-crd-publish-openapi-9072-crds test-cr'
    Jan 18 22:08:39.428: INFO: stderr: ""
    Jan 18 22:08:39.428: INFO: stdout: "e2e-test-crd-publish-openapi-9072-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 22:08:39.428
    Jan 18 22:08:39.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=crd-publish-openapi-2857 explain e2e-test-crd-publish-openapi-9072-crds'
    Jan 18 22:08:40.127: INFO: stderr: ""
    Jan 18 22:08:40.127: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9072-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:08:43.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2857" for this suite. 01/18/23 22:08:43.106
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:08:43.125
Jan 18 22:08:43.125: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename init-container 01/18/23 22:08:43.127
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:43.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:43.709
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/18/23 22:08:43.712
Jan 18 22:08:43.712: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 22:09:27.088: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-58470476-305f-4b68-9db7-9a3fdd4b9f58", GenerateName:"", Namespace:"init-container-4523", SelfLink:"", UID:"ec322cde-7bf6-4db4-94b5-0ccc2bd99568", ResourceVersion:"27123", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"712696869"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"15de05ff651b5a45081518266ba568b2d553e8e6a38e1fc8d1e914549ebc7bab", "cni.projectcalico.org/podIP":"10.1.192.23/32", "cni.projectcalico.org/podIPs":"10.1.192.23/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbf80), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 8, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbfb0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelite", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 9, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbfe0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h9pff", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0050fa0e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040a5478), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"test-vm-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c6cbd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040a5500)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040a5520)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0040a5528), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0040a552c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000cbcfc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.4", PodIP:"10.1.192.23", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.1.192.23"}}, StartTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c6cd20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c6ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://0016d168c666b857981308f07039944262c64f9ee8504eae0bff3a08a43d1d04", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0050fa180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0050fa160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040a558c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:09:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4523" for this suite. 01/18/23 22:09:27.097
------------------------------
• [SLOW TEST] [43.988 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:08:43.125
    Jan 18 22:08:43.125: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename init-container 01/18/23 22:08:43.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:08:43.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:08:43.709
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/18/23 22:08:43.712
    Jan 18 22:08:43.712: INFO: PodSpec: initContainers in spec.initContainers
    Jan 18 22:09:27.088: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-58470476-305f-4b68-9db7-9a3fdd4b9f58", GenerateName:"", Namespace:"init-container-4523", SelfLink:"", UID:"ec322cde-7bf6-4db4-94b5-0ccc2bd99568", ResourceVersion:"27123", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"712696869"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"15de05ff651b5a45081518266ba568b2d553e8e6a38e1fc8d1e914549ebc7bab", "cni.projectcalico.org/podIP":"10.1.192.23/32", "cni.projectcalico.org/podIPs":"10.1.192.23/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbf80), Subresource:""}, v1.ManagedFieldsEntry{Manager:"Go-http-client", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 8, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbfb0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelite", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 22, 9, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004fdbfe0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-h9pff", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0050fa0e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-h9pff", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0040a5478), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"test-vm-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c6cbd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040a5500)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0040a5520)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0040a5528), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0040a552c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000cbcfc0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.0.4", PodIP:"10.1.192.23", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.1.192.23"}}, StartTime:time.Date(2023, time.January, 18, 22, 8, 43, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c6cd20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c6ce00)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://0016d168c666b857981308f07039944262c64f9ee8504eae0bff3a08a43d1d04", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0050fa180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0050fa160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0040a558c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:09:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4523" for this suite. 01/18/23 22:09:27.097
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:27.115
Jan 18 22:09:27.115: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename subpath 01/18/23 22:09:27.116
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:27.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:27.449
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:09:27.452
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-s7mx 01/18/23 22:09:27.479
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:09:27.48
Jan 18 22:09:27.495: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s7mx" in namespace "subpath-2834" to be "Succeeded or Failed"
Jan 18 22:09:27.499: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764948ms
Jan 18 22:09:29.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.008300992s
Jan 18 22:09:31.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 4.00843268s
Jan 18 22:09:33.506: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011290432s
Jan 18 22:09:35.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 8.009690594s
Jan 18 22:09:37.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009609174s
Jan 18 22:09:39.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 12.009832258s
Jan 18 22:09:41.506: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 14.01078417s
Jan 18 22:09:43.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 16.010383801s
Jan 18 22:09:45.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 18.008746116s
Jan 18 22:09:47.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 20.008385747s
Jan 18 22:09:49.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=false. Elapsed: 22.009525516s
Jan 18 22:09:51.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008945098s
STEP: Saw pod success 01/18/23 22:09:51.504
Jan 18 22:09:51.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx" satisfied condition "Succeeded or Failed"
Jan 18 22:09:51.509: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-downwardapi-s7mx container test-container-subpath-downwardapi-s7mx: <nil>
STEP: delete the pod 01/18/23 22:09:51.518
Jan 18 22:09:51.552: INFO: Waiting for pod pod-subpath-test-downwardapi-s7mx to disappear
Jan 18 22:09:51.561: INFO: Pod pod-subpath-test-downwardapi-s7mx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-s7mx 01/18/23 22:09:51.561
Jan 18 22:09:51.561: INFO: Deleting pod "pod-subpath-test-downwardapi-s7mx" in namespace "subpath-2834"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:09:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2834" for this suite. 01/18/23 22:09:51.571
------------------------------
• [SLOW TEST] [24.470 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:27.115
    Jan 18 22:09:27.115: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename subpath 01/18/23 22:09:27.116
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:27.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:27.449
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:09:27.452
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-s7mx 01/18/23 22:09:27.479
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:09:27.48
    Jan 18 22:09:27.495: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-s7mx" in namespace "subpath-2834" to be "Succeeded or Failed"
    Jan 18 22:09:27.499: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Pending", Reason="", readiness=false. Elapsed: 3.764948ms
    Jan 18 22:09:29.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.008300992s
    Jan 18 22:09:31.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 4.00843268s
    Jan 18 22:09:33.506: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 6.011290432s
    Jan 18 22:09:35.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 8.009690594s
    Jan 18 22:09:37.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 10.009609174s
    Jan 18 22:09:39.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 12.009832258s
    Jan 18 22:09:41.506: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 14.01078417s
    Jan 18 22:09:43.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 16.010383801s
    Jan 18 22:09:45.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 18.008746116s
    Jan 18 22:09:47.503: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=true. Elapsed: 20.008385747s
    Jan 18 22:09:49.505: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Running", Reason="", readiness=false. Elapsed: 22.009525516s
    Jan 18 22:09:51.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008945098s
    STEP: Saw pod success 01/18/23 22:09:51.504
    Jan 18 22:09:51.504: INFO: Pod "pod-subpath-test-downwardapi-s7mx" satisfied condition "Succeeded or Failed"
    Jan 18 22:09:51.509: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-downwardapi-s7mx container test-container-subpath-downwardapi-s7mx: <nil>
    STEP: delete the pod 01/18/23 22:09:51.518
    Jan 18 22:09:51.552: INFO: Waiting for pod pod-subpath-test-downwardapi-s7mx to disappear
    Jan 18 22:09:51.561: INFO: Pod pod-subpath-test-downwardapi-s7mx no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-s7mx 01/18/23 22:09:51.561
    Jan 18 22:09:51.561: INFO: Deleting pod "pod-subpath-test-downwardapi-s7mx" in namespace "subpath-2834"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:09:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2834" for this suite. 01/18/23 22:09:51.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:51.586
Jan 18 22:09:51.586: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:09:51.587
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:51.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:51.875
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-d1967fee-c633-43ec-a5ba-ecdae6e44fe6 01/18/23 22:09:51.878
STEP: Creating a pod to test consume configMaps 01/18/23 22:09:51.889
Jan 18 22:09:51.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a" in namespace "configmap-247" to be "Succeeded or Failed"
Jan 18 22:09:51.918: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022364ms
Jan 18 22:09:53.925: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011684098s
Jan 18 22:09:55.924: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010664066s
STEP: Saw pod success 01/18/23 22:09:55.924
Jan 18 22:09:55.924: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a" satisfied condition "Succeeded or Failed"
Jan 18 22:09:55.928: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a container agnhost-container: <nil>
STEP: delete the pod 01/18/23 22:09:55.935
Jan 18 22:09:55.959: INFO: Waiting for pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a to disappear
Jan 18 22:09:55.973: INFO: Pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:09:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-247" for this suite. 01/18/23 22:09:55.978
------------------------------
• [4.413 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:51.586
    Jan 18 22:09:51.586: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:09:51.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:51.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:51.875
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-d1967fee-c633-43ec-a5ba-ecdae6e44fe6 01/18/23 22:09:51.878
    STEP: Creating a pod to test consume configMaps 01/18/23 22:09:51.889
    Jan 18 22:09:51.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a" in namespace "configmap-247" to be "Succeeded or Failed"
    Jan 18 22:09:51.918: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022364ms
    Jan 18 22:09:53.925: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011684098s
    Jan 18 22:09:55.924: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010664066s
    STEP: Saw pod success 01/18/23 22:09:55.924
    Jan 18 22:09:55.924: INFO: Pod "pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a" satisfied condition "Succeeded or Failed"
    Jan 18 22:09:55.928: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 22:09:55.935
    Jan 18 22:09:55.959: INFO: Waiting for pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a to disappear
    Jan 18 22:09:55.973: INFO: Pod pod-configmaps-4d1369e7-0169-4d46-8652-14db3d87ed1a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:09:55.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-247" for this suite. 01/18/23 22:09:55.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:09:56.003
Jan 18 22:09:56.003: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 22:09:56.004
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:56.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:56.875
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/18/23 22:09:56.908
Jan 18 22:09:56.930: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/18/23 22:09:58.934
Jan 18 22:09:58.948: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/18/23 22:10:00.953
Jan 18 22:10:00.969: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan 18 22:10:02.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-9341" for this suite. 01/18/23 22:10:02.983
------------------------------
• [SLOW TEST] [6.996 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:09:56.003
    Jan 18 22:09:56.003: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 22:09:56.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:09:56.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:09:56.875
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/18/23 22:09:56.908
    Jan 18 22:09:56.930: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/18/23 22:09:58.934
    Jan 18 22:09:58.948: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/18/23 22:10:00.953
    Jan 18 22:10:00.969: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:10:02.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-9341" for this suite. 01/18/23 22:10:02.983
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:10:03
Jan 18 22:10:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 22:10:03.001
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:03.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:03.767
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 22:10:03.771
Jan 18 22:10:03.788: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4416" to be "running and ready"
Jan 18 22:10:03.792: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414255ms
Jan 18 22:10:03.792: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:10:05.797: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009277286s
Jan 18 22:10:05.797: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 18 22:10:05.797: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/18/23 22:10:05.8
STEP: Then the orphan pod is adopted 01/18/23 22:10:05.814
STEP: When the matched label of one of its pods change 01/18/23 22:10:06.827
Jan 18 22:10:06.832: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 22:10:06.851
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:10:07.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4416" for this suite. 01/18/23 22:10:07.87
------------------------------
• [4.884 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:10:03
    Jan 18 22:10:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:10:03.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:03.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:03.767
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 22:10:03.771
    Jan 18 22:10:03.788: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4416" to be "running and ready"
    Jan 18 22:10:03.792: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.414255ms
    Jan 18 22:10:03.792: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:10:05.797: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.009277286s
    Jan 18 22:10:05.797: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 18 22:10:05.797: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/18/23 22:10:05.8
    STEP: Then the orphan pod is adopted 01/18/23 22:10:05.814
    STEP: When the matched label of one of its pods change 01/18/23 22:10:06.827
    Jan 18 22:10:06.832: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 22:10:06.851
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:10:07.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4416" for this suite. 01/18/23 22:10:07.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:10:07.885
Jan 18 22:10:07.885: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 22:10:07.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:08.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:08.184
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4211 01/18/23 22:10:08.187
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-4211 01/18/23 22:10:08.209
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4211 01/18/23 22:10:08.233
Jan 18 22:10:08.241: INFO: Found 0 stateful pods, waiting for 1
Jan 18 22:10:18.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:10:18.247
Jan 18 22:10:18.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:10:18.443: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:10:18.443: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:10:18.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:10:18.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 22:10:28.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:10:28.455: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:10:28.480: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jan 18 22:10:28.480: INFO: ss-0  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
Jan 18 22:10:28.480: INFO: 
Jan 18 22:10:28.480: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 18 22:10:29.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99558251s
Jan 18 22:10:30.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973577314s
Jan 18 22:10:31.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967715507s
Jan 18 22:10:32.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962191104s
Jan 18 22:10:33.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956279497s
Jan 18 22:10:34.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95043009s
Jan 18 22:10:35.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943019665s
Jan 18 22:10:36.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934892364s
Jan 18 22:10:37.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.298375ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4211 01/18/23 22:10:38.555
Jan 18 22:10:38.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:10:38.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 22:10:38.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:10:38.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:10:38.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:10:38.936: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:10:38.936: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:10:38.936: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:10:38.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 22:10:39.113: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 22:10:39.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 22:10:39.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 22:10:39.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 18 22:10:49.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:10:49.126: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 22:10:49.126: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:10:49.126
Jan 18 22:10:49.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:10:49.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:10:49.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:10:49.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:10:49.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:10:49.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:10:49.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:10:49.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:10:49.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 22:10:49.673: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 22:10:49.674: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 22:10:49.674: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 22:10:49.674: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:10:49.678: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 22:10:59.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:10:59.688: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:10:59.688: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 22:10:59.709: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jan 18 22:10:59.709: INFO: ss-2  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
Jan 18 22:10:59.709: INFO: ss-1  test-vm-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
Jan 18 22:10:59.709: INFO: ss-0  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
Jan 18 22:10:59.709: INFO: 
Jan 18 22:10:59.709: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:11:00.718: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
Jan 18 22:11:00.718: INFO: ss-1  test-vm-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
Jan 18 22:11:00.718: INFO: ss-2  test-vm-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
Jan 18 22:11:00.718: INFO: ss-0  test-vm-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
Jan 18 22:11:00.718: INFO: 
Jan 18 22:11:00.718: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 22:11:01.725: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986357858s
Jan 18 22:11:02.732: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979385103s
Jan 18 22:11:03.739: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972126145s
Jan 18 22:11:04.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.965015789s
Jan 18 22:11:05.754: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957390527s
Jan 18 22:11:06.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.950505373s
Jan 18 22:11:07.768: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.94327143s
Jan 18 22:11:08.774: INFO: Verifying statefulset ss doesn't scale past 0 for another 936.784445ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4211 01/18/23 22:11:09.775
Jan 18 22:11:09.782: INFO: Scaling statefulset ss to 0
Jan 18 22:11:09.797: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:11:09.801: INFO: Deleting all statefulset in ns statefulset-4211
Jan 18 22:11:09.804: INFO: Scaling statefulset ss to 0
Jan 18 22:11:09.820: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:11:09.823: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:11:09.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4211" for this suite. 01/18/23 22:11:09.85
------------------------------
• [SLOW TEST] [61.978 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:10:07.885
    Jan 18 22:10:07.885: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:10:07.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:10:08.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:10:08.184
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4211 01/18/23 22:10:08.187
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-4211 01/18/23 22:10:08.209
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4211 01/18/23 22:10:08.233
    Jan 18 22:10:08.241: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 22:10:18.247: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 22:10:18.247
    Jan 18 22:10:18.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:10:18.443: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:10:18.443: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:10:18.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:10:18.449: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 22:10:28.455: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:10:28.455: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:10:28.480: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    Jan 18 22:10:28.480: INFO: ss-0  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
    Jan 18 22:10:28.480: INFO: 
    Jan 18 22:10:28.480: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 18 22:10:29.501: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99558251s
    Jan 18 22:10:30.508: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973577314s
    Jan 18 22:10:31.513: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.967715507s
    Jan 18 22:10:32.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962191104s
    Jan 18 22:10:33.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956279497s
    Jan 18 22:10:34.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95043009s
    Jan 18 22:10:35.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.943019665s
    Jan 18 22:10:36.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.934892364s
    Jan 18 22:10:37.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 927.298375ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4211 01/18/23 22:10:38.555
    Jan 18 22:10:38.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:10:38.748: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 22:10:38.748: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:10:38.748: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:10:38.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:10:38.936: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:10:38.936: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:10:38.936: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:10:38.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 22:10:39.113: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 22:10:39.114: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 22:10:39.114: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 22:10:39.120: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 18 22:10:49.126: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:10:49.126: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 22:10:49.126: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 22:10:49.126
    Jan 18 22:10:49.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:10:49.305: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:10:49.305: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:10:49.305: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:10:49.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:10:49.489: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:10:49.489: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:10:49.489: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:10:49.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=statefulset-4211 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 22:10:49.673: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 22:10:49.674: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 22:10:49.674: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 22:10:49.674: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:10:49.678: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 22:10:59.688: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:10:59.688: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:10:59.688: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 22:10:59.709: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    Jan 18 22:10:59.709: INFO: ss-2  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
    Jan 18 22:10:59.709: INFO: ss-1  test-vm-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
    Jan 18 22:10:59.709: INFO: ss-0  test-vm-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
    Jan 18 22:10:59.709: INFO: 
    Jan 18 22:10:59.709: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 22:11:00.718: INFO: POD   NODE       PHASE    GRACE  CONDITIONS
    Jan 18 22:11:00.718: INFO: ss-1  test-vm-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
    Jan 18 22:11:00.718: INFO: ss-2  test-vm-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:28 +0000 UTC  }]
    Jan 18 22:11:00.718: INFO: ss-0  test-vm-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:10:08 +0000 UTC  }]
    Jan 18 22:11:00.718: INFO: 
    Jan 18 22:11:00.718: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 22:11:01.725: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986357858s
    Jan 18 22:11:02.732: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979385103s
    Jan 18 22:11:03.739: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.972126145s
    Jan 18 22:11:04.746: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.965015789s
    Jan 18 22:11:05.754: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.957390527s
    Jan 18 22:11:06.761: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.950505373s
    Jan 18 22:11:07.768: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.94327143s
    Jan 18 22:11:08.774: INFO: Verifying statefulset ss doesn't scale past 0 for another 936.784445ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4211 01/18/23 22:11:09.775
    Jan 18 22:11:09.782: INFO: Scaling statefulset ss to 0
    Jan 18 22:11:09.797: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:11:09.801: INFO: Deleting all statefulset in ns statefulset-4211
    Jan 18 22:11:09.804: INFO: Scaling statefulset ss to 0
    Jan 18 22:11:09.820: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:11:09.823: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:11:09.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4211" for this suite. 01/18/23 22:11:09.85
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:11:09.863
Jan 18 22:11:09.863: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:11:09.864
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:10.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:10.385
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/18/23 22:11:10.388
Jan 18 22:11:10.404: INFO: Waiting up to 2m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060" to be "running"
Jan 18 22:11:10.411: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 7.249187ms
Jan 18 22:11:12.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013190771s
Jan 18 22:11:14.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013088282s
Jan 18 22:11:16.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013106506s
Jan 18 22:11:18.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013663885s
Jan 18 22:11:20.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013677912s
Jan 18 22:11:22.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01248212s
Jan 18 22:11:24.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013703582s
Jan 18 22:11:26.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012680422s
Jan 18 22:11:28.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012107967s
Jan 18 22:11:30.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.014573144s
Jan 18 22:11:32.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013943453s
Jan 18 22:11:34.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013398745s
Jan 18 22:11:36.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013846749s
Jan 18 22:11:38.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013421242s
Jan 18 22:11:40.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013472467s
Jan 18 22:11:42.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013963342s
Jan 18 22:11:44.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011783384s
Jan 18 22:11:46.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013840077s
Jan 18 22:11:48.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013750106s
Jan 18 22:11:50.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013206249s
Jan 18 22:11:52.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012665292s
Jan 18 22:11:54.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013178547s
Jan 18 22:11:56.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013995301s
Jan 18 22:11:58.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01251322s
Jan 18 22:12:00.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013676668s
Jan 18 22:12:02.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014413912s
Jan 18 22:12:04.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014059175s
Jan 18 22:12:06.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012164188s
Jan 18 22:12:08.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014377448s
Jan 18 22:12:10.419: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014789588s
Jan 18 22:12:12.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013853081s
Jan 18 22:12:14.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013643662s
Jan 18 22:12:16.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012907438s
Jan 18 22:12:18.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013887333s
Jan 18 22:12:20.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01364729s
Jan 18 22:12:22.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013373038s
Jan 18 22:12:24.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013572691s
Jan 18 22:12:26.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012672032s
Jan 18 22:12:28.446: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.042142517s
Jan 18 22:12:30.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013778623s
Jan 18 22:12:32.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014222101s
Jan 18 22:12:34.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014243774s
Jan 18 22:12:36.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012374655s
Jan 18 22:12:38.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013793821s
Jan 18 22:12:40.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012830859s
Jan 18 22:12:42.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013276413s
Jan 18 22:12:44.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012077215s
Jan 18 22:12:46.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012067552s
Jan 18 22:12:48.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013338801s
Jan 18 22:12:50.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01452605s
Jan 18 22:12:52.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012117191s
Jan 18 22:12:54.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013717441s
Jan 18 22:12:56.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013013466s
Jan 18 22:12:58.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012999499s
Jan 18 22:13:00.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012431392s
Jan 18 22:13:02.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014132407s
Jan 18 22:13:04.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013426298s
Jan 18 22:13:06.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012516587s
Jan 18 22:13:08.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012804858s
Jan 18 22:13:10.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013306323s
Jan 18 22:13:10.420: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016283855s
STEP: updating the pod 01/18/23 22:13:10.42
Jan 18 22:13:10.940: INFO: Successfully updated pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48"
STEP: waiting for pod running 01/18/23 22:13:10.94
Jan 18 22:13:10.940: INFO: Waiting up to 2m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060" to be "running"
Jan 18 22:13:10.946: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991964ms
Jan 18 22:13:12.953: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Running", Reason="", readiness=true. Elapsed: 2.012805196s
Jan 18 22:13:12.953: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 22:13:12.953
Jan 18 22:13:12.953: INFO: Deleting pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060"
Jan 18 22:13:12.980: INFO: Wait up to 5m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:13:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1060" for this suite. 01/18/23 22:13:45.002
------------------------------
• [SLOW TEST] [155.153 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:11:09.863
    Jan 18 22:11:09.863: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:11:09.864
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:11:10.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:11:10.385
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/18/23 22:11:10.388
    Jan 18 22:11:10.404: INFO: Waiting up to 2m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060" to be "running"
    Jan 18 22:11:10.411: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 7.249187ms
    Jan 18 22:11:12.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013190771s
    Jan 18 22:11:14.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013088282s
    Jan 18 22:11:16.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013106506s
    Jan 18 22:11:18.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013663885s
    Jan 18 22:11:20.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013677912s
    Jan 18 22:11:22.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 12.01248212s
    Jan 18 22:11:24.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 14.013703582s
    Jan 18 22:11:26.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012680422s
    Jan 18 22:11:28.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012107967s
    Jan 18 22:11:30.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.014573144s
    Jan 18 22:11:32.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 22.013943453s
    Jan 18 22:11:34.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 24.013398745s
    Jan 18 22:11:36.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 26.013846749s
    Jan 18 22:11:38.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 28.013421242s
    Jan 18 22:11:40.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 30.013472467s
    Jan 18 22:11:42.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 32.013963342s
    Jan 18 22:11:44.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 34.011783384s
    Jan 18 22:11:46.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 36.013840077s
    Jan 18 22:11:48.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013750106s
    Jan 18 22:11:50.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 40.013206249s
    Jan 18 22:11:52.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 42.012665292s
    Jan 18 22:11:54.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 44.013178547s
    Jan 18 22:11:56.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 46.013995301s
    Jan 18 22:11:58.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 48.01251322s
    Jan 18 22:12:00.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 50.013676668s
    Jan 18 22:12:02.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014413912s
    Jan 18 22:12:04.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 54.014059175s
    Jan 18 22:12:06.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 56.012164188s
    Jan 18 22:12:08.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 58.014377448s
    Jan 18 22:12:10.419: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014789588s
    Jan 18 22:12:12.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.013853081s
    Jan 18 22:12:14.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.013643662s
    Jan 18 22:12:16.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.012907438s
    Jan 18 22:12:18.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013887333s
    Jan 18 22:12:20.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01364729s
    Jan 18 22:12:22.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.013373038s
    Jan 18 22:12:24.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.013572691s
    Jan 18 22:12:26.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012672032s
    Jan 18 22:12:28.446: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.042142517s
    Jan 18 22:12:30.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.013778623s
    Jan 18 22:12:32.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.014222101s
    Jan 18 22:12:34.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.014243774s
    Jan 18 22:12:36.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012374655s
    Jan 18 22:12:38.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.013793821s
    Jan 18 22:12:40.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.012830859s
    Jan 18 22:12:42.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.013276413s
    Jan 18 22:12:44.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012077215s
    Jan 18 22:12:46.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.012067552s
    Jan 18 22:12:48.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013338801s
    Jan 18 22:12:50.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01452605s
    Jan 18 22:12:52.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.012117191s
    Jan 18 22:12:54.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.013717441s
    Jan 18 22:12:56.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.013013466s
    Jan 18 22:12:58.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.012999499s
    Jan 18 22:13:00.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012431392s
    Jan 18 22:13:02.418: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.014132407s
    Jan 18 22:13:04.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.013426298s
    Jan 18 22:13:06.416: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.012516587s
    Jan 18 22:13:08.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.012804858s
    Jan 18 22:13:10.417: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013306323s
    Jan 18 22:13:10.420: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016283855s
    STEP: updating the pod 01/18/23 22:13:10.42
    Jan 18 22:13:10.940: INFO: Successfully updated pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48"
    STEP: waiting for pod running 01/18/23 22:13:10.94
    Jan 18 22:13:10.940: INFO: Waiting up to 2m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060" to be "running"
    Jan 18 22:13:10.946: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Pending", Reason="", readiness=false. Elapsed: 5.991964ms
    Jan 18 22:13:12.953: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48": Phase="Running", Reason="", readiness=true. Elapsed: 2.012805196s
    Jan 18 22:13:12.953: INFO: Pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 22:13:12.953
    Jan 18 22:13:12.953: INFO: Deleting pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" in namespace "var-expansion-1060"
    Jan 18 22:13:12.980: INFO: Wait up to 5m0s for pod "var-expansion-1e7e4122-b839-4618-bdb9-3da0f69d8e48" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:13:44.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1060" for this suite. 01/18/23 22:13:45.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:13:45.019
Jan 18 22:13:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 22:13:45.02
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:45.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:45.418
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/18/23 22:13:45.421
Jan 18 22:13:45.439: INFO: created test-pod-1
Jan 18 22:13:45.457: INFO: created test-pod-2
Jan 18 22:13:45.484: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/18/23 22:13:45.484
Jan 18 22:13:45.484: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4782' to be running and ready
Jan 18 22:13:45.505: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:45.505: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:45.505: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:45.505: INFO: 0 / 3 pods in namespace 'pods-4782' are running and ready (0 seconds elapsed)
Jan 18 22:13:45.505: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
Jan 18 22:13:45.505: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
Jan 18 22:13:45.505: INFO: test-pod-1  test-vm-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
Jan 18 22:13:45.505: INFO: test-pod-2  test-vm-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
Jan 18 22:13:45.505: INFO: test-pod-3             Pending         []
Jan 18 22:13:45.505: INFO: 
Jan 18 22:13:47.520: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:47.520: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:47.520: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 22:13:47.520: INFO: 0 / 3 pods in namespace 'pods-4782' are running and ready (2 seconds elapsed)
Jan 18 22:13:47.520: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
Jan 18 22:13:47.520: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
Jan 18 22:13:47.520: INFO: test-pod-1  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
Jan 18 22:13:47.520: INFO: test-pod-2  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
Jan 18 22:13:47.520: INFO: test-pod-3  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
Jan 18 22:13:47.520: INFO: 
Jan 18 22:13:49.521: INFO: 3 / 3 pods in namespace 'pods-4782' are running and ready (4 seconds elapsed)
Jan 18 22:13:49.521: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/18/23 22:13:49.562
Jan 18 22:13:49.576: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:13:50.582: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 22:13:51.581: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:13:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4782" for this suite. 01/18/23 22:13:52.589
------------------------------
• [SLOW TEST] [7.581 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:13:45.019
    Jan 18 22:13:45.019: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 22:13:45.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:45.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:45.418
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/18/23 22:13:45.421
    Jan 18 22:13:45.439: INFO: created test-pod-1
    Jan 18 22:13:45.457: INFO: created test-pod-2
    Jan 18 22:13:45.484: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/18/23 22:13:45.484
    Jan 18 22:13:45.484: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4782' to be running and ready
    Jan 18 22:13:45.505: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:45.505: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:45.505: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:45.505: INFO: 0 / 3 pods in namespace 'pods-4782' are running and ready (0 seconds elapsed)
    Jan 18 22:13:45.505: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
    Jan 18 22:13:45.505: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
    Jan 18 22:13:45.505: INFO: test-pod-1  test-vm-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
    Jan 18 22:13:45.505: INFO: test-pod-2  test-vm-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
    Jan 18 22:13:45.505: INFO: test-pod-3             Pending         []
    Jan 18 22:13:45.505: INFO: 
    Jan 18 22:13:47.520: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:47.520: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:47.520: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 22:13:47.520: INFO: 0 / 3 pods in namespace 'pods-4782' are running and ready (2 seconds elapsed)
    Jan 18 22:13:47.520: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
    Jan 18 22:13:47.520: INFO: POD         NODE       PHASE    GRACE  CONDITIONS
    Jan 18 22:13:47.520: INFO: test-pod-1  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
    Jan 18 22:13:47.520: INFO: test-pod-2  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
    Jan 18 22:13:47.520: INFO: test-pod-3  test-vm-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 22:13:45 +0000 UTC  }]
    Jan 18 22:13:47.520: INFO: 
    Jan 18 22:13:49.521: INFO: 3 / 3 pods in namespace 'pods-4782' are running and ready (4 seconds elapsed)
    Jan 18 22:13:49.521: INFO: expected 0 pod replicas in namespace 'pods-4782', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/18/23 22:13:49.562
    Jan 18 22:13:49.576: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:13:50.582: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 22:13:51.581: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:13:52.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4782" for this suite. 01/18/23 22:13:52.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:13:52.604
Jan 18 22:13:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:13:52.605
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:52.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:52.875
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/18/23 22:13:52.878
Jan 18 22:13:52.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2748 api-versions'
Jan 18 22:13:52.980: INFO: stderr: ""
Jan 18 22:13:52.980: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:13:52.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2748" for this suite. 01/18/23 22:13:52.987
------------------------------
• [0.398 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:13:52.604
    Jan 18 22:13:52.604: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:13:52.605
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:52.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:52.875
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/18/23 22:13:52.878
    Jan 18 22:13:52.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-2748 api-versions'
    Jan 18 22:13:52.980: INFO: stderr: ""
    Jan 18 22:13:52.980: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:13:52.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2748" for this suite. 01/18/23 22:13:52.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:13:53.004
Jan 18 22:13:53.004: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:13:53.005
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:53.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:53.875
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:13:53.907
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:13:54.361
STEP: Deploying the webhook pod 01/18/23 22:13:54.383
STEP: Wait for the deployment to be ready 01/18/23 22:13:54.404
Jan 18 22:13:54.412: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 22:13:56.425
STEP: Verifying the service has paired with the endpoint 01/18/23 22:13:56.447
Jan 18 22:13:57.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/18/23 22:13:57.452
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.471
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 22:13:57.478
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.493
STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 22:13:57.514
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:13:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4366" for this suite. 01/18/23 22:13:57.626
STEP: Destroying namespace "webhook-4366-markers" for this suite. 01/18/23 22:13:57.642
------------------------------
• [4.702 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:13:53.004
    Jan 18 22:13:53.004: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:13:53.005
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:53.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:53.875
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:13:53.907
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:13:54.361
    STEP: Deploying the webhook pod 01/18/23 22:13:54.383
    STEP: Wait for the deployment to be ready 01/18/23 22:13:54.404
    Jan 18 22:13:54.412: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 22:13:56.425
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:13:56.447
    Jan 18 22:13:57.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/18/23 22:13:57.452
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.471
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 22:13:57.478
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.493
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 22:13:57.514
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 22:13:57.528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:13:57.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4366" for this suite. 01/18/23 22:13:57.626
    STEP: Destroying namespace "webhook-4366-markers" for this suite. 01/18/23 22:13:57.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:13:57.71
Jan 18 22:13:57.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:13:57.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:57.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:57.877
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-90037119-1ba9-4a60-8892-3d1239e2febb 01/18/23 22:13:57.888
STEP: Creating the pod 01/18/23 22:13:57.901
Jan 18 22:13:57.923: INFO: Waiting up to 5m0s for pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929" in namespace "configmap-8180" to be "running"
Jan 18 22:13:57.927: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436848ms
Jan 18 22:13:59.933: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929": Phase="Running", Reason="", readiness=false. Elapsed: 2.010183239s
Jan 18 22:13:59.933: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929" satisfied condition "running"
STEP: Waiting for pod with text data 01/18/23 22:13:59.933
STEP: Waiting for pod with binary data 01/18/23 22:13:59.95
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:13:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8180" for this suite. 01/18/23 22:13:59.963
------------------------------
• [2.271 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:13:57.71
    Jan 18 22:13:57.711: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:13:57.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:13:57.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:13:57.877
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-90037119-1ba9-4a60-8892-3d1239e2febb 01/18/23 22:13:57.888
    STEP: Creating the pod 01/18/23 22:13:57.901
    Jan 18 22:13:57.923: INFO: Waiting up to 5m0s for pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929" in namespace "configmap-8180" to be "running"
    Jan 18 22:13:57.927: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929": Phase="Pending", Reason="", readiness=false. Elapsed: 4.436848ms
    Jan 18 22:13:59.933: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929": Phase="Running", Reason="", readiness=false. Elapsed: 2.010183239s
    Jan 18 22:13:59.933: INFO: Pod "pod-configmaps-3af64478-5c1c-4544-ac0a-e71c19086929" satisfied condition "running"
    STEP: Waiting for pod with text data 01/18/23 22:13:59.933
    STEP: Waiting for pod with binary data 01/18/23 22:13:59.95
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:13:59.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8180" for this suite. 01/18/23 22:13:59.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:13:59.982
Jan 18 22:13:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 22:13:59.983
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:00.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:00.126
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/18/23 22:14:00.129
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:00.631
Jan 18 22:14:00.656: INFO: Pod name wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1: Found 0 pods out of 5
Jan 18 22:14:05.675: INFO: Pod name wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:14:05.675
Jan 18 22:14:05.675: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:05.682: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400593ms
Jan 18 22:14:07.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013113759s
Jan 18 22:14:09.690: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015514684s
Jan 18 22:14:11.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01315925s
Jan 18 22:14:13.691: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015650784s
Jan 18 22:14:15.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Running", Reason="", readiness=true. Elapsed: 10.013051066s
Jan 18 22:14:15.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8" satisfied condition "running"
Jan 18 22:14:15.688: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:15.693: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p": Phase="Running", Reason="", readiness=true. Elapsed: 4.533052ms
Jan 18 22:14:15.693: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p" satisfied condition "running"
Jan 18 22:14:15.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:15.697: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t": Phase="Running", Reason="", readiness=true. Elapsed: 4.628353ms
Jan 18 22:14:15.697: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t" satisfied condition "running"
Jan 18 22:14:15.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:15.702: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.210248ms
Jan 18 22:14:15.702: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b" satisfied condition "running"
Jan 18 22:14:15.702: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:15.706: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7": Phase="Running", Reason="", readiness=true. Elapsed: 4.607152ms
Jan 18 22:14:15.706: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:15.706
Jan 18 22:14:15.778: INFO: Deleting ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 took: 16.357285ms
Jan 18 22:14:16.279: INFO: Terminating ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 pods took: 501.254693ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:19.385
Jan 18 22:14:19.406: INFO: Pod name wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a: Found 0 pods out of 5
Jan 18 22:14:24.418: INFO: Pod name wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:14:24.418
Jan 18 22:14:24.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:24.424: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.628664ms
Jan 18 22:14:26.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013282148s
Jan 18 22:14:28.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01335226s
Jan 18 22:14:30.430: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012116309s
Jan 18 22:14:32.430: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011939126s
Jan 18 22:14:34.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013544736s
Jan 18 22:14:36.431: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Running", Reason="", readiness=true. Elapsed: 12.01294716s
Jan 18 22:14:36.431: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc" satisfied condition "running"
Jan 18 22:14:36.431: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:36.436: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.963364ms
Jan 18 22:14:36.436: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv" satisfied condition "running"
Jan 18 22:14:36.436: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:36.441: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4": Phase="Running", Reason="", readiness=true. Elapsed: 4.813362ms
Jan 18 22:14:36.441: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4" satisfied condition "running"
Jan 18 22:14:36.441: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:36.447: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4": Phase="Running", Reason="", readiness=true. Elapsed: 5.653273ms
Jan 18 22:14:36.447: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4" satisfied condition "running"
Jan 18 22:14:36.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:36.453: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg": Phase="Running", Reason="", readiness=true. Elapsed: 5.840875ms
Jan 18 22:14:36.453: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:36.453
Jan 18 22:14:36.577: INFO: Deleting ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a took: 66.211958ms
Jan 18 22:14:36.977: INFO: Terminating ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a pods took: 400.575394ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:39.986
Jan 18 22:14:40.011: INFO: Pod name wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3: Found 0 pods out of 5
Jan 18 22:14:45.023: INFO: Pod name wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 22:14:45.023
Jan 18 22:14:45.023: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:45.028: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530651ms
Jan 18 22:14:47.036: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012762008s
Jan 18 22:14:49.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011839261s
Jan 18 22:14:51.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01140572s
Jan 18 22:14:53.036: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013014213s
Jan 18 22:14:55.037: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014043257s
Jan 18 22:14:57.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Running", Reason="", readiness=true. Elapsed: 12.011733951s
Jan 18 22:14:57.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv" satisfied condition "running"
Jan 18 22:14:57.035: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:57.040: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.804771ms
Jan 18 22:14:57.040: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f" satisfied condition "running"
Jan 18 22:14:57.040: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:57.044: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r": Phase="Running", Reason="", readiness=true. Elapsed: 4.530767ms
Jan 18 22:14:57.044: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r" satisfied condition "running"
Jan 18 22:14:57.044: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:57.049: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65": Phase="Running", Reason="", readiness=true. Elapsed: 5.019275ms
Jan 18 22:14:57.049: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65" satisfied condition "running"
Jan 18 22:14:57.049: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm" in namespace "emptydir-wrapper-1856" to be "running"
Jan 18 22:14:57.055: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm": Phase="Running", Reason="", readiness=true. Elapsed: 5.606683ms
Jan 18 22:14:57.055: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:57.055
Jan 18 22:14:57.128: INFO: Deleting ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 took: 16.588445ms
Jan 18 22:14:57.929: INFO: Terminating ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 pods took: 801.26157ms
STEP: Cleaning up the configMaps 01/18/23 22:15:00.93
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1856" for this suite. 01/18/23 22:15:01.598
------------------------------
• [SLOW TEST] [61.628 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:13:59.982
    Jan 18 22:13:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 22:13:59.983
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:14:00.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:14:00.126
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/18/23 22:14:00.129
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:00.631
    Jan 18 22:14:00.656: INFO: Pod name wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1: Found 0 pods out of 5
    Jan 18 22:14:05.675: INFO: Pod name wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:14:05.675
    Jan 18 22:14:05.675: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:05.682: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.400593ms
    Jan 18 22:14:07.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013113759s
    Jan 18 22:14:09.690: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015514684s
    Jan 18 22:14:11.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01315925s
    Jan 18 22:14:13.691: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015650784s
    Jan 18 22:14:15.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8": Phase="Running", Reason="", readiness=true. Elapsed: 10.013051066s
    Jan 18 22:14:15.688: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5dzh8" satisfied condition "running"
    Jan 18 22:14:15.688: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:15.693: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p": Phase="Running", Reason="", readiness=true. Elapsed: 4.533052ms
    Jan 18 22:14:15.693: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-xs58p" satisfied condition "running"
    Jan 18 22:14:15.693: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:15.697: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t": Phase="Running", Reason="", readiness=true. Elapsed: 4.628353ms
    Jan 18 22:14:15.697: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-5bd8t" satisfied condition "running"
    Jan 18 22:14:15.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:15.702: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.210248ms
    Jan 18 22:14:15.702: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-h4k5b" satisfied condition "running"
    Jan 18 22:14:15.702: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:15.706: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7": Phase="Running", Reason="", readiness=true. Elapsed: 4.607152ms
    Jan 18 22:14:15.706: INFO: Pod "wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1-97wk7" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:15.706
    Jan 18 22:14:15.778: INFO: Deleting ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 took: 16.357285ms
    Jan 18 22:14:16.279: INFO: Terminating ReplicationController wrapped-volume-race-1088cd96-63cf-4f25-8d21-9215234497b1 pods took: 501.254693ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:19.385
    Jan 18 22:14:19.406: INFO: Pod name wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a: Found 0 pods out of 5
    Jan 18 22:14:24.418: INFO: Pod name wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:14:24.418
    Jan 18 22:14:24.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:24.424: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.628664ms
    Jan 18 22:14:26.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013282148s
    Jan 18 22:14:28.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01335226s
    Jan 18 22:14:30.430: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012116309s
    Jan 18 22:14:32.430: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011939126s
    Jan 18 22:14:34.432: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.013544736s
    Jan 18 22:14:36.431: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc": Phase="Running", Reason="", readiness=true. Elapsed: 12.01294716s
    Jan 18 22:14:36.431: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-bm9gc" satisfied condition "running"
    Jan 18 22:14:36.431: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:36.436: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv": Phase="Running", Reason="", readiness=true. Elapsed: 4.963364ms
    Jan 18 22:14:36.436: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-ffkkv" satisfied condition "running"
    Jan 18 22:14:36.436: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:36.441: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4": Phase="Running", Reason="", readiness=true. Elapsed: 4.813362ms
    Jan 18 22:14:36.441: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-4crr4" satisfied condition "running"
    Jan 18 22:14:36.441: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:36.447: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4": Phase="Running", Reason="", readiness=true. Elapsed: 5.653273ms
    Jan 18 22:14:36.447: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-lvtl4" satisfied condition "running"
    Jan 18 22:14:36.447: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:36.453: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg": Phase="Running", Reason="", readiness=true. Elapsed: 5.840875ms
    Jan 18 22:14:36.453: INFO: Pod "wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a-vmbtg" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:36.453
    Jan 18 22:14:36.577: INFO: Deleting ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a took: 66.211958ms
    Jan 18 22:14:36.977: INFO: Terminating ReplicationController wrapped-volume-race-13b26236-042d-4e99-ba07-23619bb1e14a pods took: 400.575394ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 22:14:39.986
    Jan 18 22:14:40.011: INFO: Pod name wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3: Found 0 pods out of 5
    Jan 18 22:14:45.023: INFO: Pod name wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 22:14:45.023
    Jan 18 22:14:45.023: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:45.028: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.530651ms
    Jan 18 22:14:47.036: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012762008s
    Jan 18 22:14:49.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011839261s
    Jan 18 22:14:51.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01140572s
    Jan 18 22:14:53.036: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013014213s
    Jan 18 22:14:55.037: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014043257s
    Jan 18 22:14:57.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv": Phase="Running", Reason="", readiness=true. Elapsed: 12.011733951s
    Jan 18 22:14:57.035: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z2zpv" satisfied condition "running"
    Jan 18 22:14:57.035: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:57.040: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.804771ms
    Jan 18 22:14:57.040: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-zpx5f" satisfied condition "running"
    Jan 18 22:14:57.040: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:57.044: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r": Phase="Running", Reason="", readiness=true. Elapsed: 4.530767ms
    Jan 18 22:14:57.044: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-z5l2r" satisfied condition "running"
    Jan 18 22:14:57.044: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:57.049: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65": Phase="Running", Reason="", readiness=true. Elapsed: 5.019275ms
    Jan 18 22:14:57.049: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-fcl65" satisfied condition "running"
    Jan 18 22:14:57.049: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm" in namespace "emptydir-wrapper-1856" to be "running"
    Jan 18 22:14:57.055: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm": Phase="Running", Reason="", readiness=true. Elapsed: 5.606683ms
    Jan 18 22:14:57.055: INFO: Pod "wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3-bp6jm" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 in namespace emptydir-wrapper-1856, will wait for the garbage collector to delete the pods 01/18/23 22:14:57.055
    Jan 18 22:14:57.128: INFO: Deleting ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 took: 16.588445ms
    Jan 18 22:14:57.929: INFO: Terminating ReplicationController wrapped-volume-race-506de9af-6fd6-4bcf-bba2-b1eb11e092b3 pods took: 801.26157ms
    STEP: Cleaning up the configMaps 01/18/23 22:15:00.93
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:01.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1856" for this suite. 01/18/23 22:15:01.598
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:01.611
Jan 18 22:15:01.611: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:15:01.612
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:01.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:01.716
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-40e98c69-8203-477c-85aa-2b3e2e92a7ef 01/18/23 22:15:01.719
STEP: Creating a pod to test consume secrets 01/18/23 22:15:01.732
Jan 18 22:15:01.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa" in namespace "projected-5914" to be "Succeeded or Failed"
Jan 18 22:15:01.753: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269956ms
Jan 18 22:15:03.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010337184s
Jan 18 22:15:05.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009837926s
STEP: Saw pod success 01/18/23 22:15:05.759
Jan 18 22:15:05.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa" satisfied condition "Succeeded or Failed"
Jan 18 22:15:05.763: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:15:05.771
Jan 18 22:15:05.798: INFO: Waiting for pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa to disappear
Jan 18 22:15:05.807: INFO: Pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5914" for this suite. 01/18/23 22:15:05.812
------------------------------
• [4.215 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:01.611
    Jan 18 22:15:01.611: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:15:01.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:01.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:01.716
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-40e98c69-8203-477c-85aa-2b3e2e92a7ef 01/18/23 22:15:01.719
    STEP: Creating a pod to test consume secrets 01/18/23 22:15:01.732
    Jan 18 22:15:01.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa" in namespace "projected-5914" to be "Succeeded or Failed"
    Jan 18 22:15:01.753: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269956ms
    Jan 18 22:15:03.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010337184s
    Jan 18 22:15:05.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009837926s
    STEP: Saw pod success 01/18/23 22:15:05.759
    Jan 18 22:15:05.759: INFO: Pod "pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa" satisfied condition "Succeeded or Failed"
    Jan 18 22:15:05.763: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:15:05.771
    Jan 18 22:15:05.798: INFO: Waiting for pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa to disappear
    Jan 18 22:15:05.807: INFO: Pod pod-projected-secrets-5c2c1bca-86c6-459c-98a1-8f139fe649aa no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:05.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5914" for this suite. 01/18/23 22:15:05.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:05.827
Jan 18 22:15:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-runtime 01/18/23 22:15:05.828
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:05.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:05.996
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/18/23 22:15:06.003
STEP: wait for the container to reach Succeeded 01/18/23 22:15:06.027
STEP: get the container status 01/18/23 22:15:11.065
STEP: the container should be terminated 01/18/23 22:15:11.071
STEP: the termination message should be set 01/18/23 22:15:11.071
Jan 18 22:15:11.071: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/18/23 22:15:11.071
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:11.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1427" for this suite. 01/18/23 22:15:11.115
------------------------------
• [SLOW TEST] [5.306 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:05.827
    Jan 18 22:15:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-runtime 01/18/23 22:15:05.828
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:05.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:05.996
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/18/23 22:15:06.003
    STEP: wait for the container to reach Succeeded 01/18/23 22:15:06.027
    STEP: get the container status 01/18/23 22:15:11.065
    STEP: the container should be terminated 01/18/23 22:15:11.071
    STEP: the termination message should be set 01/18/23 22:15:11.071
    Jan 18 22:15:11.071: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/18/23 22:15:11.071
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:11.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1427" for this suite. 01/18/23 22:15:11.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:11.145
Jan 18 22:15:11.145: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:15:11.147
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:11.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:11.503
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/18/23 22:15:11.506
STEP: Creating a ResourceQuota 01/18/23 22:15:16.516
STEP: Ensuring resource quota status is calculated 01/18/23 22:15:16.534
STEP: Creating a Pod that fits quota 01/18/23 22:15:18.54
STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 22:15:18.571
STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 22:15:20.578
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 22:15:20.581
STEP: Ensuring a pod cannot update its resource requirements 01/18/23 22:15:20.583
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 22:15:20.588
STEP: Deleting the pod 01/18/23 22:15:22.594
STEP: Ensuring resource quota status released the pod usage 01/18/23 22:15:22.627
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:24.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3793" for this suite. 01/18/23 22:15:24.638
------------------------------
• [SLOW TEST] [13.508 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:11.145
    Jan 18 22:15:11.145: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:15:11.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:11.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:11.503
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/18/23 22:15:11.506
    STEP: Creating a ResourceQuota 01/18/23 22:15:16.516
    STEP: Ensuring resource quota status is calculated 01/18/23 22:15:16.534
    STEP: Creating a Pod that fits quota 01/18/23 22:15:18.54
    STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 22:15:18.571
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 22:15:20.578
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 22:15:20.581
    STEP: Ensuring a pod cannot update its resource requirements 01/18/23 22:15:20.583
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 22:15:20.588
    STEP: Deleting the pod 01/18/23 22:15:22.594
    STEP: Ensuring resource quota status released the pod usage 01/18/23 22:15:22.627
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:24.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3793" for this suite. 01/18/23 22:15:24.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:24.655
Jan 18 22:15:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:15:24.657
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:24.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:24.742
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:15:24.774
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:15:25.038
STEP: Deploying the webhook pod 01/18/23 22:15:25.055
STEP: Wait for the deployment to be ready 01/18/23 22:15:25.079
Jan 18 22:15:25.094: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:15:27.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:15:29.114
STEP: Verifying the service has paired with the endpoint 01/18/23 22:15:29.147
Jan 18 22:15:30.147: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/18/23 22:15:30.28
STEP: Creating a configMap that should be mutated 01/18/23 22:15:30.294
STEP: Deleting the collection of validation webhooks 01/18/23 22:15:30.331
STEP: Creating a configMap that should not be mutated 01/18/23 22:15:30.451
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:30.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9713" for this suite. 01/18/23 22:15:30.556
STEP: Destroying namespace "webhook-9713-markers" for this suite. 01/18/23 22:15:30.571
------------------------------
• [SLOW TEST] [5.929 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:24.655
    Jan 18 22:15:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:15:24.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:24.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:24.742
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:15:24.774
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:15:25.038
    STEP: Deploying the webhook pod 01/18/23 22:15:25.055
    STEP: Wait for the deployment to be ready 01/18/23 22:15:25.079
    Jan 18 22:15:25.094: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:15:27.107: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 15, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:15:29.114
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:15:29.147
    Jan 18 22:15:30.147: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/18/23 22:15:30.28
    STEP: Creating a configMap that should be mutated 01/18/23 22:15:30.294
    STEP: Deleting the collection of validation webhooks 01/18/23 22:15:30.331
    STEP: Creating a configMap that should not be mutated 01/18/23 22:15:30.451
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:30.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9713" for this suite. 01/18/23 22:15:30.556
    STEP: Destroying namespace "webhook-9713-markers" for this suite. 01/18/23 22:15:30.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:30.586
Jan 18 22:15:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:15:30.587
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:30.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:30.84
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:15:30.845
Jan 18 22:15:30.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19" in namespace "downward-api-3240" to be "Succeeded or Failed"
Jan 18 22:15:30.910: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893665ms
Jan 18 22:15:32.915: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011833003s
Jan 18 22:15:34.916: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012825159s
STEP: Saw pod success 01/18/23 22:15:34.916
Jan 18 22:15:34.916: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19" satisfied condition "Succeeded or Failed"
Jan 18 22:15:34.920: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 container client-container: <nil>
STEP: delete the pod 01/18/23 22:15:34.929
Jan 18 22:15:34.954: INFO: Waiting for pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 to disappear
Jan 18 22:15:34.960: INFO: Pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3240" for this suite. 01/18/23 22:15:34.965
------------------------------
• [4.392 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:30.586
    Jan 18 22:15:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:15:30.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:30.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:30.84
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:15:30.845
    Jan 18 22:15:30.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19" in namespace "downward-api-3240" to be "Succeeded or Failed"
    Jan 18 22:15:30.910: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.893665ms
    Jan 18 22:15:32.915: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011833003s
    Jan 18 22:15:34.916: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012825159s
    STEP: Saw pod success 01/18/23 22:15:34.916
    Jan 18 22:15:34.916: INFO: Pod "downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19" satisfied condition "Succeeded or Failed"
    Jan 18 22:15:34.920: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:15:34.929
    Jan 18 22:15:34.954: INFO: Waiting for pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 to disappear
    Jan 18 22:15:34.960: INFO: Pod downwardapi-volume-8c2f47b5-5a29-4a2e-9692-d8bac5effd19 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:34.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3240" for this suite. 01/18/23 22:15:34.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:34.98
Jan 18 22:15:34.980: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename proxy 01/18/23 22:15:34.982
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:35.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:35.081
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 18 22:15:35.084: INFO: Creating pod...
Jan 18 22:15:35.114: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7047" to be "running"
Jan 18 22:15:35.119: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.988258ms
Jan 18 22:15:37.124: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864872s
Jan 18 22:15:37.124: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 22:15:37.124: INFO: Creating service...
Jan 18 22:15:37.148: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 22:15:37.155: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:15:37.155: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/GET
Jan 18 22:15:37.162: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 22:15:37.162: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 22:15:37.168: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 22:15:37.168: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 22:15:37.172: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:15:37.172: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 22:15:37.176: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:15:37.176: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/POST
Jan 18 22:15:37.180: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:15:37.180: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/PUT
Jan 18 22:15:37.192: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 22:15:37.192: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/DELETE
Jan 18 22:15:37.204: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 22:15:37.220: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
Jan 18 22:15:37.238: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 22:15:37.239: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/GET
Jan 18 22:15:37.248: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 22:15:37.248: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/HEAD
Jan 18 22:15:37.255: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 22:15:37.255: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 22:15:37.261: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 22:15:37.261: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/PATCH
Jan 18 22:15:37.267: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 22:15:37.267: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/POST
Jan 18 22:15:37.273: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 22:15:37.273: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/PUT
Jan 18 22:15:37.279: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:37.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7047" for this suite. 01/18/23 22:15:37.285
------------------------------
• [2.318 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:34.98
    Jan 18 22:15:34.980: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename proxy 01/18/23 22:15:34.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:35.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:35.081
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 18 22:15:35.084: INFO: Creating pod...
    Jan 18 22:15:35.114: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7047" to be "running"
    Jan 18 22:15:35.119: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.988258ms
    Jan 18 22:15:37.124: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.010864872s
    Jan 18 22:15:37.124: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 22:15:37.124: INFO: Creating service...
    Jan 18 22:15:37.148: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/DELETE
    Jan 18 22:15:37.155: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:15:37.155: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/GET
    Jan 18 22:15:37.162: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 22:15:37.162: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/HEAD
    Jan 18 22:15:37.168: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 22:15:37.168: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 18 22:15:37.172: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:15:37.172: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/PATCH
    Jan 18 22:15:37.176: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:15:37.176: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/POST
    Jan 18 22:15:37.180: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:15:37.180: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/pods/agnhost/proxy/some/path/with/PUT
    Jan 18 22:15:37.192: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 22:15:37.192: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/DELETE
    Jan 18 22:15:37.204: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 22:15:37.220: INFO: http.Client request:DELETE | StatusCode:404 | Response: | Method:
    Jan 18 22:15:37.238: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 22:15:37.239: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/GET
    Jan 18 22:15:37.248: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 22:15:37.248: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/HEAD
    Jan 18 22:15:37.255: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 22:15:37.255: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/OPTIONS
    Jan 18 22:15:37.261: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 22:15:37.261: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/PATCH
    Jan 18 22:15:37.267: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 22:15:37.267: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/POST
    Jan 18 22:15:37.273: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 22:15:37.273: INFO: Starting http.Client for https://10.152.183.1:443/api/v1/namespaces/proxy-7047/services/test-service/proxy/some/path/with/PUT
    Jan 18 22:15:37.279: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:37.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7047" for this suite. 01/18/23 22:15:37.285
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:37.298
Jan 18 22:15:37.299: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sysctl 01/18/23 22:15:37.3
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:37.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:37.875
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 22:15:37.878
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:37.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-3246" for this suite. 01/18/23 22:15:37.892
------------------------------
• [0.606 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:37.298
    Jan 18 22:15:37.299: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sysctl 01/18/23 22:15:37.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:37.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:37.875
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 22:15:37.878
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:37.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-3246" for this suite. 01/18/23 22:15:37.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:37.907
Jan 18 22:15:37.907: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:15:37.908
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:38.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:38.244
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/18/23 22:15:38.25
Jan 18 22:15:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3840 cluster-info'
Jan 18 22:15:38.337: INFO: stderr: ""
Jan 18 22:15:38.337: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:15:38.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3840" for this suite. 01/18/23 22:15:38.343
------------------------------
• [0.449 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:37.907
    Jan 18 22:15:37.907: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:15:37.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:38.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:38.244
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/18/23 22:15:38.25
    Jan 18 22:15:38.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-3840 cluster-info'
    Jan 18 22:15:38.337: INFO: stderr: ""
    Jan 18 22:15:38.337: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:15:38.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3840" for this suite. 01/18/23 22:15:38.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:15:38.357
Jan 18 22:15:38.357: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 22:15:38.358
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:38.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:38.876
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan 18 22:15:38.896: INFO: Waiting up to 5m0s for pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f" in namespace "container-probe-310" to be "running and ready"
Jan 18 22:15:38.904: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.72139ms
Jan 18 22:15:38.904: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:15:40.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 2.013530718s
Jan 18 22:15:40.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:42.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 4.015755809s
Jan 18 22:15:42.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:44.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 6.015318567s
Jan 18 22:15:44.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:46.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 8.013841313s
Jan 18 22:15:46.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:48.911: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 10.014831348s
Jan 18 22:15:48.911: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:50.909: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 12.012813933s
Jan 18 22:15:50.909: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:52.909: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 14.012879248s
Jan 18 22:15:52.909: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:54.911: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 16.014749189s
Jan 18 22:15:54.911: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:56.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 18.013648803s
Jan 18 22:15:56.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:15:58.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 20.015497199s
Jan 18 22:15:58.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
Jan 18 22:16:00.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=true. Elapsed: 22.015435574s
Jan 18 22:16:00.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = true)
Jan 18 22:16:00.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f" satisfied condition "running and ready"
Jan 18 22:16:00.916: INFO: Container started at 2023-01-18 22:15:39 +0000 UTC, pod became ready at 2023-01-18 22:15:59 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:00.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-310" for this suite. 01/18/23 22:16:00.921
------------------------------
• [SLOW TEST] [22.578 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:15:38.357
    Jan 18 22:15:38.357: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:15:38.358
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:15:38.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:15:38.876
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan 18 22:15:38.896: INFO: Waiting up to 5m0s for pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f" in namespace "container-probe-310" to be "running and ready"
    Jan 18 22:15:38.904: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.72139ms
    Jan 18 22:15:38.904: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:15:40.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 2.013530718s
    Jan 18 22:15:40.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:42.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 4.015755809s
    Jan 18 22:15:42.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:44.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 6.015318567s
    Jan 18 22:15:44.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:46.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 8.013841313s
    Jan 18 22:15:46.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:48.911: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 10.014831348s
    Jan 18 22:15:48.911: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:50.909: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 12.012813933s
    Jan 18 22:15:50.909: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:52.909: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 14.012879248s
    Jan 18 22:15:52.909: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:54.911: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 16.014749189s
    Jan 18 22:15:54.911: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:56.910: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 18.013648803s
    Jan 18 22:15:56.910: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:15:58.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=false. Elapsed: 20.015497199s
    Jan 18 22:15:58.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = false)
    Jan 18 22:16:00.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f": Phase="Running", Reason="", readiness=true. Elapsed: 22.015435574s
    Jan 18 22:16:00.912: INFO: The phase of Pod test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f is Running (Ready = true)
    Jan 18 22:16:00.912: INFO: Pod "test-webserver-fce7a416-3ac2-4e90-9a5a-a87e43b5b60f" satisfied condition "running and ready"
    Jan 18 22:16:00.916: INFO: Container started at 2023-01-18 22:15:39 +0000 UTC, pod became ready at 2023-01-18 22:15:59 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:00.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-310" for this suite. 01/18/23 22:16:00.921
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:00.936
Jan 18 22:16:00.936: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:16:00.937
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:01.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:01.156
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/18/23 22:16:01.158
Jan 18 22:16:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: rename a version 01/18/23 22:16:05.393
STEP: check the new version name is served 01/18/23 22:16:05.416
STEP: check the old version name is removed 01/18/23 22:16:07.561
STEP: check the other version is not changed 01/18/23 22:16:08.297
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:11.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9795" for this suite. 01/18/23 22:16:11.783
------------------------------
• [SLOW TEST] [10.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:00.936
    Jan 18 22:16:00.936: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:16:00.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:01.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:01.156
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/18/23 22:16:01.158
    Jan 18 22:16:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: rename a version 01/18/23 22:16:05.393
    STEP: check the new version name is served 01/18/23 22:16:05.416
    STEP: check the old version name is removed 01/18/23 22:16:07.561
    STEP: check the other version is not changed 01/18/23 22:16:08.297
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:11.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9795" for this suite. 01/18/23 22:16:11.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:11.797
Jan 18 22:16:11.797: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename server-version 01/18/23 22:16:11.798
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:12.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:12.175
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/18/23 22:16:12.178
STEP: Confirm major version 01/18/23 22:16:12.179
Jan 18 22:16:12.180: INFO: Major version: 1
STEP: Confirm minor version 01/18/23 22:16:12.18
Jan 18 22:16:12.180: INFO: cleanMinorVersion: 26
Jan 18 22:16:12.180: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:12.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8101" for this suite. 01/18/23 22:16:12.187
------------------------------
• [0.407 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:11.797
    Jan 18 22:16:11.797: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename server-version 01/18/23 22:16:11.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:12.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:12.175
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/18/23 22:16:12.178
    STEP: Confirm major version 01/18/23 22:16:12.179
    Jan 18 22:16:12.180: INFO: Major version: 1
    STEP: Confirm minor version 01/18/23 22:16:12.18
    Jan 18 22:16:12.180: INFO: cleanMinorVersion: 26
    Jan 18 22:16:12.180: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:12.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8101" for this suite. 01/18/23 22:16:12.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:12.205
Jan 18 22:16:12.206: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename certificates 01/18/23 22:16:12.207
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:12.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:12.331
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/18/23 22:16:13.21
STEP: getting /apis/certificates.k8s.io 01/18/23 22:16:13.213
STEP: getting /apis/certificates.k8s.io/v1 01/18/23 22:16:13.214
STEP: creating 01/18/23 22:16:13.215
STEP: getting 01/18/23 22:16:13.247
STEP: listing 01/18/23 22:16:13.251
STEP: watching 01/18/23 22:16:13.255
Jan 18 22:16:13.255: INFO: starting watch
STEP: patching 01/18/23 22:16:13.256
STEP: updating 01/18/23 22:16:13.27
Jan 18 22:16:13.288: INFO: waiting for watch events with expected annotations
Jan 18 22:16:13.288: INFO: saw patched and updated annotations
STEP: getting /approval 01/18/23 22:16:13.288
STEP: patching /approval 01/18/23 22:16:13.293
STEP: updating /approval 01/18/23 22:16:13.305
STEP: getting /status 01/18/23 22:16:13.317
STEP: patching /status 01/18/23 22:16:13.321
STEP: updating /status 01/18/23 22:16:13.335
STEP: deleting 01/18/23 22:16:13.349
STEP: deleting a collection 01/18/23 22:16:13.37
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-6150" for this suite. 01/18/23 22:16:13.411
------------------------------
• [1.224 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:12.205
    Jan 18 22:16:12.206: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename certificates 01/18/23 22:16:12.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:12.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:12.331
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/18/23 22:16:13.21
    STEP: getting /apis/certificates.k8s.io 01/18/23 22:16:13.213
    STEP: getting /apis/certificates.k8s.io/v1 01/18/23 22:16:13.214
    STEP: creating 01/18/23 22:16:13.215
    STEP: getting 01/18/23 22:16:13.247
    STEP: listing 01/18/23 22:16:13.251
    STEP: watching 01/18/23 22:16:13.255
    Jan 18 22:16:13.255: INFO: starting watch
    STEP: patching 01/18/23 22:16:13.256
    STEP: updating 01/18/23 22:16:13.27
    Jan 18 22:16:13.288: INFO: waiting for watch events with expected annotations
    Jan 18 22:16:13.288: INFO: saw patched and updated annotations
    STEP: getting /approval 01/18/23 22:16:13.288
    STEP: patching /approval 01/18/23 22:16:13.293
    STEP: updating /approval 01/18/23 22:16:13.305
    STEP: getting /status 01/18/23 22:16:13.317
    STEP: patching /status 01/18/23 22:16:13.321
    STEP: updating /status 01/18/23 22:16:13.335
    STEP: deleting 01/18/23 22:16:13.349
    STEP: deleting a collection 01/18/23 22:16:13.37
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-6150" for this suite. 01/18/23 22:16:13.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:13.43
Jan 18 22:16:13.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 22:16:13.432
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:13.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:13.877
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 18 22:16:13.880: INFO: Creating ReplicaSet my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87
Jan 18 22:16:13.898: INFO: Pod name my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Found 0 pods out of 1
Jan 18 22:16:18.905: INFO: Pod name my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Found 1 pods out of 1
Jan 18 22:16:18.905: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87" is running
Jan 18 22:16:18.905: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" in namespace "replicaset-5315" to be "running"
Jan 18 22:16:18.910: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch": Phase="Running", Reason="", readiness=true. Elapsed: 5.86887ms
Jan 18 22:16:18.911: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" satisfied condition "running"
Jan 18 22:16:18.911: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:14 +0000 UTC Reason: Message:}])
Jan 18 22:16:18.911: INFO: Trying to dial the pod
Jan 18 22:16:23.929: INFO: Controller my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Got expected result from replica 1 [my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch]: "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:23.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5315" for this suite. 01/18/23 22:16:23.935
------------------------------
• [SLOW TEST] [10.516 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:13.43
    Jan 18 22:16:13.430: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:16:13.432
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:13.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:13.877
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 18 22:16:13.880: INFO: Creating ReplicaSet my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87
    Jan 18 22:16:13.898: INFO: Pod name my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Found 0 pods out of 1
    Jan 18 22:16:18.905: INFO: Pod name my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Found 1 pods out of 1
    Jan 18 22:16:18.905: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87" is running
    Jan 18 22:16:18.905: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" in namespace "replicaset-5315" to be "running"
    Jan 18 22:16:18.910: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch": Phase="Running", Reason="", readiness=true. Elapsed: 5.86887ms
    Jan 18 22:16:18.911: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" satisfied condition "running"
    Jan 18 22:16:18.911: INFO: Pod "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:16 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:16 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:16:14 +0000 UTC Reason: Message:}])
    Jan 18 22:16:18.911: INFO: Trying to dial the pod
    Jan 18 22:16:23.929: INFO: Controller my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87: Got expected result from replica 1 [my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch]: "my-hostname-basic-f95a0cd4-d7d7-4f96-b9bd-d85e513bac87-f88ch", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:23.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5315" for this suite. 01/18/23 22:16:23.935
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:23.948
Jan 18 22:16:23.948: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:16:23.949
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:24.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:24.309
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/18/23 22:16:24.311
STEP: getting /apis/discovery.k8s.io 01/18/23 22:16:24.314
STEP: getting /apis/discovery.k8s.iov1 01/18/23 22:16:24.315
STEP: creating 01/18/23 22:16:24.316
STEP: getting 01/18/23 22:16:24.352
STEP: listing 01/18/23 22:16:24.356
STEP: watching 01/18/23 22:16:24.362
Jan 18 22:16:24.362: INFO: starting watch
STEP: cluster-wide listing 01/18/23 22:16:24.363
STEP: cluster-wide watching 01/18/23 22:16:24.368
Jan 18 22:16:24.368: INFO: starting watch
STEP: patching 01/18/23 22:16:24.369
STEP: updating 01/18/23 22:16:24.382
Jan 18 22:16:24.406: INFO: waiting for watch events with expected annotations
Jan 18 22:16:24.406: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 22:16:24.406
STEP: deleting a collection 01/18/23 22:16:24.43
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:24.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1507" for this suite. 01/18/23 22:16:24.478
------------------------------
• [0.547 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:23.948
    Jan 18 22:16:23.948: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:16:23.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:24.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:24.309
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/18/23 22:16:24.311
    STEP: getting /apis/discovery.k8s.io 01/18/23 22:16:24.314
    STEP: getting /apis/discovery.k8s.iov1 01/18/23 22:16:24.315
    STEP: creating 01/18/23 22:16:24.316
    STEP: getting 01/18/23 22:16:24.352
    STEP: listing 01/18/23 22:16:24.356
    STEP: watching 01/18/23 22:16:24.362
    Jan 18 22:16:24.362: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 22:16:24.363
    STEP: cluster-wide watching 01/18/23 22:16:24.368
    Jan 18 22:16:24.368: INFO: starting watch
    STEP: patching 01/18/23 22:16:24.369
    STEP: updating 01/18/23 22:16:24.382
    Jan 18 22:16:24.406: INFO: waiting for watch events with expected annotations
    Jan 18 22:16:24.406: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 22:16:24.406
    STEP: deleting a collection 01/18/23 22:16:24.43
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:24.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1507" for this suite. 01/18/23 22:16:24.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:24.495
Jan 18 22:16:24.495: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:16:24.497
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:24.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:24.874
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 18 22:16:24.898: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16" in namespace "kubelet-test-5881" to be "running and ready"
Jan 18 22:16:24.903: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664955ms
Jan 18 22:16:24.903: INFO: The phase of Pod busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:16:26.908: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16": Phase="Running", Reason="", readiness=true. Elapsed: 2.010302047s
Jan 18 22:16:26.908: INFO: The phase of Pod busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16 is Running (Ready = true)
Jan 18 22:16:26.908: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5881" for this suite. 01/18/23 22:16:26.926
------------------------------
• [2.444 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:24.495
    Jan 18 22:16:24.495: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 22:16:24.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:24.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:24.874
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 18 22:16:24.898: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16" in namespace "kubelet-test-5881" to be "running and ready"
    Jan 18 22:16:24.903: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664955ms
    Jan 18 22:16:24.903: INFO: The phase of Pod busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:16:26.908: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16": Phase="Running", Reason="", readiness=true. Elapsed: 2.010302047s
    Jan 18 22:16:26.908: INFO: The phase of Pod busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16 is Running (Ready = true)
    Jan 18 22:16:26.908: INFO: Pod "busybox-readonly-fs7a22c7e0-e81f-4d9e-bedb-23b6227a8f16" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5881" for this suite. 01/18/23 22:16:26.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:26.941
Jan 18 22:16:26.941: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:16:26.942
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:27.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:27.525
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-73d38ea5-e560-4570-b6da-f8beac69a66d 01/18/23 22:16:27.532
STEP: Creating the pod 01/18/23 22:16:27.542
Jan 18 22:16:27.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579" in namespace "projected-8575" to be "running and ready"
Jan 18 22:16:27.565: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579": Phase="Pending", Reason="", readiness=false. Elapsed: 6.698879ms
Jan 18 22:16:27.565: INFO: The phase of Pod pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:16:29.571: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579": Phase="Running", Reason="", readiness=true. Elapsed: 2.01281245s
Jan 18 22:16:29.571: INFO: The phase of Pod pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579 is Running (Ready = true)
Jan 18 22:16:29.571: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-73d38ea5-e560-4570-b6da-f8beac69a66d 01/18/23 22:16:29.59
STEP: waiting to observe update in volume 01/18/23 22:16:29.604
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:33.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8575" for this suite. 01/18/23 22:16:33.642
------------------------------
• [SLOW TEST] [6.716 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:26.941
    Jan 18 22:16:26.941: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:16:26.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:27.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:27.525
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-73d38ea5-e560-4570-b6da-f8beac69a66d 01/18/23 22:16:27.532
    STEP: Creating the pod 01/18/23 22:16:27.542
    Jan 18 22:16:27.558: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579" in namespace "projected-8575" to be "running and ready"
    Jan 18 22:16:27.565: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579": Phase="Pending", Reason="", readiness=false. Elapsed: 6.698879ms
    Jan 18 22:16:27.565: INFO: The phase of Pod pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:16:29.571: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579": Phase="Running", Reason="", readiness=true. Elapsed: 2.01281245s
    Jan 18 22:16:29.571: INFO: The phase of Pod pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579 is Running (Ready = true)
    Jan 18 22:16:29.571: INFO: Pod "pod-projected-configmaps-7b79afc1-5012-430d-87a3-d74e018de579" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-73d38ea5-e560-4570-b6da-f8beac69a66d 01/18/23 22:16:29.59
    STEP: waiting to observe update in volume 01/18/23 22:16:29.604
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:33.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8575" for this suite. 01/18/23 22:16:33.642
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:33.657
Jan 18 22:16:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename watch 01/18/23 22:16:33.658
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:33.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:33.875
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/18/23 22:16:33.878
STEP: creating a watch on configmaps with label B 01/18/23 22:16:33.879
STEP: creating a watch on configmaps with label A or B 01/18/23 22:16:33.88
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.881
Jan 18 22:16:33.893: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29835 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:33.893: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29835 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.893
Jan 18 22:16:33.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29836 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:33.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29836 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 22:16:33.922
Jan 18 22:16:33.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29837 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:33.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29837 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.942
Jan 18 22:16:33.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29838 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:33.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29838 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 22:16:33.964
Jan 18 22:16:33.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29839 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:33.977: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29839 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 22:16:43.978
Jan 18 22:16:43.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29871 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:16:43.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29871 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:53.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9168" for this suite. 01/18/23 22:16:53.999
------------------------------
• [SLOW TEST] [20.358 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:33.657
    Jan 18 22:16:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename watch 01/18/23 22:16:33.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:33.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:33.875
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/18/23 22:16:33.878
    STEP: creating a watch on configmaps with label B 01/18/23 22:16:33.879
    STEP: creating a watch on configmaps with label A or B 01/18/23 22:16:33.88
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.881
    Jan 18 22:16:33.893: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29835 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:33.893: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29835 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.893
    Jan 18 22:16:33.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29836 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:33.922: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29836 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 22:16:33.922
    Jan 18 22:16:33.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29837 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:33.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29837 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 22:16:33.942
    Jan 18 22:16:33.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29838 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:33.964: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9168  9d253764-93ba-459b-be8e-e0c4469237b6 29838 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 22:16:33.964
    Jan 18 22:16:33.976: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29839 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:33.977: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29839 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 22:16:43.978
    Jan 18 22:16:43.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29871 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:16:43.991: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9168  94f1270e-eb47-4050-9b04-49e840cdd78d 29871 0 2023-01-18 22:16:33 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 22:16:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:53.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9168" for this suite. 01/18/23 22:16:53.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:54.018
Jan 18 22:16:54.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename configmap 01/18/23 22:16:54.019
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:54.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:54.875
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-52d41dae-971b-43b2-8ca8-4e400a6d4bb9 01/18/23 22:16:54.886
STEP: Creating the pod 01/18/23 22:16:54.898
Jan 18 22:16:54.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b" in namespace "configmap-9056" to be "running and ready"
Jan 18 22:16:54.924: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.19815ms
Jan 18 22:16:54.924: INFO: The phase of Pod pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:16:56.930: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010064502s
Jan 18 22:16:56.930: INFO: The phase of Pod pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b is Running (Ready = true)
Jan 18 22:16:56.930: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-52d41dae-971b-43b2-8ca8-4e400a6d4bb9 01/18/23 22:16:56.941
STEP: waiting to observe update in volume 01/18/23 22:16:56.953
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan 18 22:16:58.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9056" for this suite. 01/18/23 22:16:58.975
------------------------------
• [4.970 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:54.018
    Jan 18 22:16:54.018: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename configmap 01/18/23 22:16:54.019
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:54.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:54.875
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-52d41dae-971b-43b2-8ca8-4e400a6d4bb9 01/18/23 22:16:54.886
    STEP: Creating the pod 01/18/23 22:16:54.898
    Jan 18 22:16:54.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b" in namespace "configmap-9056" to be "running and ready"
    Jan 18 22:16:54.924: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.19815ms
    Jan 18 22:16:54.924: INFO: The phase of Pod pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:16:56.930: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.010064502s
    Jan 18 22:16:56.930: INFO: The phase of Pod pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b is Running (Ready = true)
    Jan 18 22:16:56.930: INFO: Pod "pod-configmaps-5a97b1fd-c381-4f33-93b0-9ebe94ef0d9b" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-52d41dae-971b-43b2-8ca8-4e400a6d4bb9 01/18/23 22:16:56.941
    STEP: waiting to observe update in volume 01/18/23 22:16:56.953
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:16:58.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9056" for this suite. 01/18/23 22:16:58.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:16:58.989
Jan 18 22:16:58.989: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:16:58.99
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:59.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:59.694
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 18 22:16:59.739: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2024 to be scheduled
Jan 18 22:16:59.745: INFO: 1 pods are not scheduled: [runtimeclass-2024/test-runtimeclass-runtimeclass-2024-preconfigured-handler-h48w4(0d53d4e2-2152-4a18-9684-93c0b2c823fd)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:01.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2024" for this suite. 01/18/23 22:17:01.767
------------------------------
• [2.790 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:16:58.989
    Jan 18 22:16:58.989: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 22:16:58.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:16:59.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:16:59.694
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 18 22:16:59.739: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2024 to be scheduled
    Jan 18 22:16:59.745: INFO: 1 pods are not scheduled: [runtimeclass-2024/test-runtimeclass-runtimeclass-2024-preconfigured-handler-h48w4(0d53d4e2-2152-4a18-9684-93c0b2c823fd)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:01.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2024" for this suite. 01/18/23 22:17:01.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:01.78
Jan 18 22:17:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename resourcequota 01/18/23 22:17:01.781
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:02.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:02.107
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/18/23 22:17:02.109
STEP: Getting a ResourceQuota 01/18/23 22:17:02.121
STEP: Updating a ResourceQuota 01/18/23 22:17:02.125
STEP: Verifying a ResourceQuota was modified 01/18/23 22:17:02.136
STEP: Deleting a ResourceQuota 01/18/23 22:17:02.139
STEP: Verifying the deleted ResourceQuota 01/18/23 22:17:02.152
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:02.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3521" for this suite. 01/18/23 22:17:02.16
------------------------------
• [0.395 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:01.78
    Jan 18 22:17:01.780: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename resourcequota 01/18/23 22:17:01.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:02.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:02.107
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/18/23 22:17:02.109
    STEP: Getting a ResourceQuota 01/18/23 22:17:02.121
    STEP: Updating a ResourceQuota 01/18/23 22:17:02.125
    STEP: Verifying a ResourceQuota was modified 01/18/23 22:17:02.136
    STEP: Deleting a ResourceQuota 01/18/23 22:17:02.139
    STEP: Verifying the deleted ResourceQuota 01/18/23 22:17:02.152
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:02.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3521" for this suite. 01/18/23 22:17:02.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:02.176
Jan 18 22:17:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename discovery 01/18/23 22:17:02.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:02.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:02.858
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/18/23 22:17:02.861
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 18 22:17:03.272: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 22:17:03.273: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 22:17:03.273: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 22:17:03.273: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 22:17:03.273: INFO: Checking APIGroup: apps
Jan 18 22:17:03.274: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 22:17:03.274: INFO: Versions found [{apps/v1 v1}]
Jan 18 22:17:03.274: INFO: apps/v1 matches apps/v1
Jan 18 22:17:03.274: INFO: Checking APIGroup: events.k8s.io
Jan 18 22:17:03.275: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 22:17:03.275: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 18 22:17:03.275: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 22:17:03.275: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 22:17:03.276: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 22:17:03.276: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 22:17:03.276: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 22:17:03.276: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 22:17:03.277: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 22:17:03.277: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 22:17:03.277: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 22:17:03.277: INFO: Checking APIGroup: autoscaling
Jan 18 22:17:03.280: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 22:17:03.280: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan 18 22:17:03.280: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 22:17:03.280: INFO: Checking APIGroup: batch
Jan 18 22:17:03.281: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 22:17:03.281: INFO: Versions found [{batch/v1 v1}]
Jan 18 22:17:03.281: INFO: batch/v1 matches batch/v1
Jan 18 22:17:03.281: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 22:17:03.282: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 22:17:03.282: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 22:17:03.282: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 22:17:03.282: INFO: Checking APIGroup: networking.k8s.io
Jan 18 22:17:03.283: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 22:17:03.283: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 18 22:17:03.283: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 22:17:03.283: INFO: Checking APIGroup: policy
Jan 18 22:17:03.283: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 22:17:03.283: INFO: Versions found [{policy/v1 v1}]
Jan 18 22:17:03.284: INFO: policy/v1 matches policy/v1
Jan 18 22:17:03.284: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 22:17:03.284: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 22:17:03.284: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 22:17:03.284: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 22:17:03.284: INFO: Checking APIGroup: storage.k8s.io
Jan 18 22:17:03.285: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 22:17:03.285: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 22:17:03.285: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 22:17:03.285: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 22:17:03.286: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 22:17:03.286: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 22:17:03.286: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 22:17:03.286: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 22:17:03.287: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 22:17:03.287: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 22:17:03.287: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 22:17:03.287: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 22:17:03.287: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 22:17:03.288: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 22:17:03.288: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 22:17:03.288: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 22:17:03.288: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 22:17:03.288: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 22:17:03.288: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 22:17:03.288: INFO: Checking APIGroup: node.k8s.io
Jan 18 22:17:03.289: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 22:17:03.289: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 18 22:17:03.289: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 22:17:03.289: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 22:17:03.290: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 22:17:03.290: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 18 22:17:03.290: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 22:17:03.290: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 22:17:03.291: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan 18 22:17:03.291: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan 18 22:17:03.291: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan 18 22:17:03.291: INFO: Checking APIGroup: crd.projectcalico.org
Jan 18 22:17:03.292: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 18 22:17:03.292: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 18 22:17:03.292: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:03.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-2841" for this suite. 01/18/23 22:17:03.297
------------------------------
• [1.141 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:02.176
    Jan 18 22:17:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename discovery 01/18/23 22:17:02.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:02.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:02.858
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/18/23 22:17:02.861
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 18 22:17:03.272: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 18 22:17:03.273: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 18 22:17:03.273: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 18 22:17:03.273: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 18 22:17:03.273: INFO: Checking APIGroup: apps
    Jan 18 22:17:03.274: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 18 22:17:03.274: INFO: Versions found [{apps/v1 v1}]
    Jan 18 22:17:03.274: INFO: apps/v1 matches apps/v1
    Jan 18 22:17:03.274: INFO: Checking APIGroup: events.k8s.io
    Jan 18 22:17:03.275: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 18 22:17:03.275: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 18 22:17:03.275: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 18 22:17:03.275: INFO: Checking APIGroup: authentication.k8s.io
    Jan 18 22:17:03.276: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 18 22:17:03.276: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 18 22:17:03.276: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 18 22:17:03.276: INFO: Checking APIGroup: authorization.k8s.io
    Jan 18 22:17:03.277: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 18 22:17:03.277: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 18 22:17:03.277: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 18 22:17:03.277: INFO: Checking APIGroup: autoscaling
    Jan 18 22:17:03.280: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 18 22:17:03.280: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan 18 22:17:03.280: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 18 22:17:03.280: INFO: Checking APIGroup: batch
    Jan 18 22:17:03.281: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 18 22:17:03.281: INFO: Versions found [{batch/v1 v1}]
    Jan 18 22:17:03.281: INFO: batch/v1 matches batch/v1
    Jan 18 22:17:03.281: INFO: Checking APIGroup: certificates.k8s.io
    Jan 18 22:17:03.282: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 18 22:17:03.282: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 18 22:17:03.282: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 18 22:17:03.282: INFO: Checking APIGroup: networking.k8s.io
    Jan 18 22:17:03.283: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 18 22:17:03.283: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 18 22:17:03.283: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 18 22:17:03.283: INFO: Checking APIGroup: policy
    Jan 18 22:17:03.283: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 18 22:17:03.283: INFO: Versions found [{policy/v1 v1}]
    Jan 18 22:17:03.284: INFO: policy/v1 matches policy/v1
    Jan 18 22:17:03.284: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 18 22:17:03.284: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 18 22:17:03.284: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 18 22:17:03.284: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 18 22:17:03.284: INFO: Checking APIGroup: storage.k8s.io
    Jan 18 22:17:03.285: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 18 22:17:03.285: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 22:17:03.285: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 18 22:17:03.285: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 18 22:17:03.286: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 18 22:17:03.286: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 18 22:17:03.286: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 18 22:17:03.286: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 18 22:17:03.287: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 18 22:17:03.287: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 18 22:17:03.287: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 18 22:17:03.287: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 18 22:17:03.287: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 18 22:17:03.288: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 18 22:17:03.288: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 18 22:17:03.288: INFO: Checking APIGroup: coordination.k8s.io
    Jan 18 22:17:03.288: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 18 22:17:03.288: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 18 22:17:03.288: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 18 22:17:03.288: INFO: Checking APIGroup: node.k8s.io
    Jan 18 22:17:03.289: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 18 22:17:03.289: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 18 22:17:03.289: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 18 22:17:03.289: INFO: Checking APIGroup: discovery.k8s.io
    Jan 18 22:17:03.290: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 18 22:17:03.290: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 18 22:17:03.290: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 18 22:17:03.290: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 18 22:17:03.291: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan 18 22:17:03.291: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan 18 22:17:03.291: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan 18 22:17:03.291: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 18 22:17:03.292: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 18 22:17:03.292: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 18 22:17:03.292: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:03.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-2841" for this suite. 01/18/23 22:17:03.297
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:03.317
Jan 18 22:17:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:17:03.318
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:03.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:03.734
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-8942 01/18/23 22:17:03.736
STEP: creating service affinity-nodeport in namespace services-8942 01/18/23 22:17:03.737
STEP: creating replication controller affinity-nodeport in namespace services-8942 01/18/23 22:17:03.77
I0118 22:17:03.782893      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8942, replica count: 3
I0118 22:17:06.834422      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:17:06.850: INFO: Creating new exec pod
Jan 18 22:17:06.875: INFO: Waiting up to 5m0s for pod "execpod-affinity4mh65" in namespace "services-8942" to be "running"
Jan 18 22:17:06.880: INFO: Pod "execpod-affinity4mh65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771158ms
Jan 18 22:17:08.886: INFO: Pod "execpod-affinity4mh65": Phase="Running", Reason="", readiness=true. Elapsed: 2.010436978s
Jan 18 22:17:08.886: INFO: Pod "execpod-affinity4mh65" satisfied condition "running"
Jan 18 22:17:09.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan 18 22:17:10.079: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 22:17:10.079: INFO: stdout: ""
Jan 18 22:17:10.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.152.183.31 80'
Jan 18 22:17:10.248: INFO: stderr: "+ nc -v -z -w 2 10.152.183.31 80\nConnection to 10.152.183.31 80 port [tcp/http] succeeded!\n"
Jan 18 22:17:10.248: INFO: stdout: ""
Jan 18 22:17:10.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32521'
Jan 18 22:17:10.412: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32521\nConnection to 10.0.0.4 32521 port [tcp/*] succeeded!\n"
Jan 18 22:17:10.412: INFO: stdout: ""
Jan 18 22:17:10.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 32521'
Jan 18 22:17:10.610: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 32521\nConnection to 10.0.0.5 32521 port [tcp/*] succeeded!\n"
Jan 18 22:17:10.610: INFO: stdout: ""
Jan 18 22:17:10.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:32521/ ; done'
Jan 18 22:17:10.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n"
Jan 18 22:17:10.868: INFO: stdout: "\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp"
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
Jan 18 22:17:10.868: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-8942, will wait for the garbage collector to delete the pods 01/18/23 22:17:11.009
Jan 18 22:17:11.163: INFO: Deleting ReplicationController affinity-nodeport took: 76.325501ms
Jan 18 22:17:11.864: INFO: Terminating ReplicationController affinity-nodeport pods took: 700.993071ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:14.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8942" for this suite. 01/18/23 22:17:14.621
------------------------------
• [SLOW TEST] [11.317 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:03.317
    Jan 18 22:17:03.317: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:17:03.318
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:03.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:03.734
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-8942 01/18/23 22:17:03.736
    STEP: creating service affinity-nodeport in namespace services-8942 01/18/23 22:17:03.737
    STEP: creating replication controller affinity-nodeport in namespace services-8942 01/18/23 22:17:03.77
    I0118 22:17:03.782893      20 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-8942, replica count: 3
    I0118 22:17:06.834422      20 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:17:06.850: INFO: Creating new exec pod
    Jan 18 22:17:06.875: INFO: Waiting up to 5m0s for pod "execpod-affinity4mh65" in namespace "services-8942" to be "running"
    Jan 18 22:17:06.880: INFO: Pod "execpod-affinity4mh65": Phase="Pending", Reason="", readiness=false. Elapsed: 4.771158ms
    Jan 18 22:17:08.886: INFO: Pod "execpod-affinity4mh65": Phase="Running", Reason="", readiness=true. Elapsed: 2.010436978s
    Jan 18 22:17:08.886: INFO: Pod "execpod-affinity4mh65" satisfied condition "running"
    Jan 18 22:17:09.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan 18 22:17:10.079: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 18 22:17:10.079: INFO: stdout: ""
    Jan 18 22:17:10.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.152.183.31 80'
    Jan 18 22:17:10.248: INFO: stderr: "+ nc -v -z -w 2 10.152.183.31 80\nConnection to 10.152.183.31 80 port [tcp/http] succeeded!\n"
    Jan 18 22:17:10.248: INFO: stdout: ""
    Jan 18 22:17:10.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 32521'
    Jan 18 22:17:10.412: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 32521\nConnection to 10.0.0.4 32521 port [tcp/*] succeeded!\n"
    Jan 18 22:17:10.412: INFO: stdout: ""
    Jan 18 22:17:10.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 32521'
    Jan 18 22:17:10.610: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 32521\nConnection to 10.0.0.5 32521 port [tcp/*] succeeded!\n"
    Jan 18 22:17:10.610: INFO: stdout: ""
    Jan 18 22:17:10.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-8942 exec execpod-affinity4mh65 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.0.4:32521/ ; done'
    Jan 18 22:17:10.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.0.4:32521/\n"
    Jan 18 22:17:10.868: INFO: stdout: "\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp\naffinity-nodeport-zwqrp"
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Received response from host: affinity-nodeport-zwqrp
    Jan 18 22:17:10.868: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-8942, will wait for the garbage collector to delete the pods 01/18/23 22:17:11.009
    Jan 18 22:17:11.163: INFO: Deleting ReplicationController affinity-nodeport took: 76.325501ms
    Jan 18 22:17:11.864: INFO: Terminating ReplicationController affinity-nodeport pods took: 700.993071ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:14.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8942" for this suite. 01/18/23 22:17:14.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:14.635
Jan 18 22:17:14.635: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:17:14.636
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:14.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:14.875
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/18/23 22:17:14.878
Jan 18 22:17:14.896: INFO: Waiting up to 5m0s for pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5" in namespace "projected-2968" to be "running and ready"
Jan 18 22:17:14.902: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652067ms
Jan 18 22:17:14.902: INFO: The phase of Pod annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:17:16.909: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012437709s
Jan 18 22:17:16.909: INFO: The phase of Pod annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5 is Running (Ready = true)
Jan 18 22:17:16.909: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5" satisfied condition "running and ready"
Jan 18 22:17:17.445: INFO: Successfully updated pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2968" for this suite. 01/18/23 22:17:19.477
------------------------------
• [4.855 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:14.635
    Jan 18 22:17:14.635: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:17:14.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:14.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:14.875
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/18/23 22:17:14.878
    Jan 18 22:17:14.896: INFO: Waiting up to 5m0s for pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5" in namespace "projected-2968" to be "running and ready"
    Jan 18 22:17:14.902: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652067ms
    Jan 18 22:17:14.902: INFO: The phase of Pod annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:17:16.909: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.012437709s
    Jan 18 22:17:16.909: INFO: The phase of Pod annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5 is Running (Ready = true)
    Jan 18 22:17:16.909: INFO: Pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5" satisfied condition "running and ready"
    Jan 18 22:17:17.445: INFO: Successfully updated pod "annotationupdated694d87d-3d02-4be3-a648-4a867b514dc5"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2968" for this suite. 01/18/23 22:17:19.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:19.491
Jan 18 22:17:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 22:17:19.492
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:19.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:19.875
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/18/23 22:17:19.883
STEP: Verify that the required pods have come up. 01/18/23 22:17:19.922
Jan 18 22:17:19.929: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 22:17:24.935: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 22:17:24.935
STEP: Getting /status 01/18/23 22:17:24.935
Jan 18 22:17:24.940: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/18/23 22:17:24.94
Jan 18 22:17:24.959: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/18/23 22:17:24.959
Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.962: INFO: Found replicaset test-rs in namespace replicaset-2298 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 22:17:24.962: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/18/23 22:17:24.962
Jan 18 22:17:24.962: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 22:17:24.983: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/18/23 22:17:24.983
Jan 18 22:17:24.985: INFO: Observed &ReplicaSet event: ADDED
Jan 18 22:17:24.985: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.986: INFO: Observed replicaset test-rs in namespace replicaset-2298 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 22:17:24.986: INFO: Found replicaset test-rs in namespace replicaset-2298 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 22:17:24.986: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:24.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2298" for this suite. 01/18/23 22:17:24.992
------------------------------
• [SLOW TEST] [5.529 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:19.491
    Jan 18 22:17:19.491: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:17:19.492
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:19.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:19.875
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/18/23 22:17:19.883
    STEP: Verify that the required pods have come up. 01/18/23 22:17:19.922
    Jan 18 22:17:19.929: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 22:17:24.935: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 22:17:24.935
    STEP: Getting /status 01/18/23 22:17:24.935
    Jan 18 22:17:24.940: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/18/23 22:17:24.94
    Jan 18 22:17:24.959: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/18/23 22:17:24.959
    Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.962: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.962: INFO: Found replicaset test-rs in namespace replicaset-2298 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 22:17:24.962: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/18/23 22:17:24.962
    Jan 18 22:17:24.962: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 22:17:24.983: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/18/23 22:17:24.983
    Jan 18 22:17:24.985: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 22:17:24.985: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.986: INFO: Observed replicaset test-rs in namespace replicaset-2298 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 22:17:24.986: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 22:17:24.986: INFO: Found replicaset test-rs in namespace replicaset-2298 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 22:17:24.986: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:24.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2298" for this suite. 01/18/23 22:17:24.992
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:25.02
Jan 18 22:17:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 22:17:25.021
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:25.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:25.35
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-2793/secret-test-eb383d9e-f389-4516-8b0f-6b24a1b6ab39 01/18/23 22:17:25.352
STEP: Creating a pod to test consume secrets 01/18/23 22:17:25.372
Jan 18 22:17:25.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a" in namespace "secrets-2793" to be "Succeeded or Failed"
Jan 18 22:17:25.440: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.798229ms
Jan 18 22:17:27.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042760462s
Jan 18 22:17:29.447: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.044118042s
Jan 18 22:17:31.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042914091s
STEP: Saw pod success 01/18/23 22:17:31.446
Jan 18 22:17:31.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a" satisfied condition "Succeeded or Failed"
Jan 18 22:17:31.452: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a container env-test: <nil>
STEP: delete the pod 01/18/23 22:17:31.47
Jan 18 22:17:31.515: INFO: Waiting for pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a to disappear
Jan 18 22:17:31.522: INFO: Pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:17:31.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2793" for this suite. 01/18/23 22:17:31.527
------------------------------
• [SLOW TEST] [6.530 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:25.02
    Jan 18 22:17:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 22:17:25.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:25.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:25.35
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-2793/secret-test-eb383d9e-f389-4516-8b0f-6b24a1b6ab39 01/18/23 22:17:25.352
    STEP: Creating a pod to test consume secrets 01/18/23 22:17:25.372
    Jan 18 22:17:25.403: INFO: Waiting up to 5m0s for pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a" in namespace "secrets-2793" to be "Succeeded or Failed"
    Jan 18 22:17:25.440: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Pending", Reason="", readiness=false. Elapsed: 36.798229ms
    Jan 18 22:17:27.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Running", Reason="", readiness=true. Elapsed: 2.042760462s
    Jan 18 22:17:29.447: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Running", Reason="", readiness=false. Elapsed: 4.044118042s
    Jan 18 22:17:31.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042914091s
    STEP: Saw pod success 01/18/23 22:17:31.446
    Jan 18 22:17:31.446: INFO: Pod "pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a" satisfied condition "Succeeded or Failed"
    Jan 18 22:17:31.452: INFO: Trying to get logs from node test-vm-1 pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a container env-test: <nil>
    STEP: delete the pod 01/18/23 22:17:31.47
    Jan 18 22:17:31.515: INFO: Waiting for pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a to disappear
    Jan 18 22:17:31.522: INFO: Pod pod-configmaps-2964948e-e210-4149-9574-669b82d54b7a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:17:31.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2793" for this suite. 01/18/23 22:17:31.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:17:31.551
Jan 18 22:17:31.551: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 22:17:31.552
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:31.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:31.874
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/18/23 22:17:31.876
STEP: Ensuring active pods == parallelism 01/18/23 22:17:31.893
STEP: delete a job 01/18/23 22:17:35.899
STEP: deleting Job.batch foo in namespace job-7195, will wait for the garbage collector to delete the pods 01/18/23 22:17:35.899
Jan 18 22:17:36.017: INFO: Deleting Job.batch foo took: 62.763737ms
Jan 18 22:17:36.618: INFO: Terminating Job.batch foo pods took: 600.168652ms
STEP: Ensuring job was deleted 01/18/23 22:18:07.818
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:18:07.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7195" for this suite. 01/18/23 22:18:07.829
------------------------------
• [SLOW TEST] [36.291 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:17:31.551
    Jan 18 22:17:31.551: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 22:17:31.552
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:17:31.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:17:31.874
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/18/23 22:17:31.876
    STEP: Ensuring active pods == parallelism 01/18/23 22:17:31.893
    STEP: delete a job 01/18/23 22:17:35.899
    STEP: deleting Job.batch foo in namespace job-7195, will wait for the garbage collector to delete the pods 01/18/23 22:17:35.899
    Jan 18 22:17:36.017: INFO: Deleting Job.batch foo took: 62.763737ms
    Jan 18 22:17:36.618: INFO: Terminating Job.batch foo pods took: 600.168652ms
    STEP: Ensuring job was deleted 01/18/23 22:18:07.818
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:18:07.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7195" for this suite. 01/18/23 22:18:07.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:18:07.843
Jan 18 22:18:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 22:18:07.844
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:08.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:08.451
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 18 22:18:08.611: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4dcdb457-8c00-4d21-887d-1e5df3f4dbe1", Controller:(*bool)(0xc002b25386), BlockOwnerDeletion:(*bool)(0xc002b25387)}}
Jan 18 22:18:08.638: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5a617bef-83e7-4f0c-b29f-b9850f37d3af", Controller:(*bool)(0xc00160c36e), BlockOwnerDeletion:(*bool)(0xc00160c36f)}}
Jan 18 22:18:08.650: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4585a46d-ec71-4891-a84f-5673eecc7fdb", Controller:(*bool)(0xc00160ca3a), BlockOwnerDeletion:(*bool)(0xc00160ca3b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 22:18:13.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8176" for this suite. 01/18/23 22:18:13.675
------------------------------
• [SLOW TEST] [5.854 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:18:07.843
    Jan 18 22:18:07.843: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 22:18:07.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:08.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:08.451
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 18 22:18:08.611: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"4dcdb457-8c00-4d21-887d-1e5df3f4dbe1", Controller:(*bool)(0xc002b25386), BlockOwnerDeletion:(*bool)(0xc002b25387)}}
    Jan 18 22:18:08.638: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5a617bef-83e7-4f0c-b29f-b9850f37d3af", Controller:(*bool)(0xc00160c36e), BlockOwnerDeletion:(*bool)(0xc00160c36f)}}
    Jan 18 22:18:08.650: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"4585a46d-ec71-4891-a84f-5673eecc7fdb", Controller:(*bool)(0xc00160ca3a), BlockOwnerDeletion:(*bool)(0xc00160ca3b)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:18:13.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8176" for this suite. 01/18/23 22:18:13.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:18:13.712
Jan 18 22:18:13.712: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:18:13.713
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:13.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:13.874
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/18/23 22:18:13.877
Jan 18 22:18:13.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 22:18:14.008: INFO: stderr: ""
Jan 18 22:18:14.008: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/18/23 22:18:14.008
Jan 18 22:18:14.008: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 22:18:14.008: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4951" to be "running and ready, or succeeded"
Jan 18 22:18:14.012: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.267149ms
Jan 18 22:18:14.012: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
Jan 18 22:18:16.017: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009063379s
Jan 18 22:18:16.017: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 22:18:16.017: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/18/23 22:18:16.017
Jan 18 22:18:16.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator'
Jan 18 22:18:16.121: INFO: stderr: ""
Jan 18 22:18:16.121: INFO: stdout: "I0118 22:18:14.908593       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8b2 430\nI0118 22:18:15.108734       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/khdg 533\nI0118 22:18:15.309385       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/2vk 389\nI0118 22:18:15.508750       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/bkzt 389\nI0118 22:18:15.709127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/r2w5 527\nI0118 22:18:15.908713       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vwp 294\nI0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
STEP: limiting log lines 01/18/23 22:18:16.121
Jan 18 22:18:16.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --tail=1'
Jan 18 22:18:16.233: INFO: stderr: ""
Jan 18 22:18:16.233: INFO: stdout: "I0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
Jan 18 22:18:16.233: INFO: got output "I0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
STEP: limiting log bytes 01/18/23 22:18:16.233
Jan 18 22:18:16.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 22:18:16.321: INFO: stderr: ""
Jan 18 22:18:16.321: INFO: stdout: "I"
Jan 18 22:18:16.321: INFO: got output "I"
STEP: exposing timestamps 01/18/23 22:18:16.321
Jan 18 22:18:16.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 22:18:16.413: INFO: stderr: ""
Jan 18 22:18:16.413: INFO: stdout: "2023-01-18T22:18:16.309818872Z I0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\n"
Jan 18 22:18:16.413: INFO: got output "2023-01-18T22:18:16.309818872Z I0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\n"
STEP: restricting to a time range 01/18/23 22:18:16.413
Jan 18 22:18:18.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --since=1s'
Jan 18 22:18:19.021: INFO: stderr: ""
Jan 18 22:18:19.021: INFO: stdout: "I0118 22:18:18.109060       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/w2xl 420\nI0118 22:18:18.309536       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/sdw 441\nI0118 22:18:18.508839       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/97n 537\nI0118 22:18:18.709290       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/47w 344\nI0118 22:18:18.908625       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f49 477\n"
Jan 18 22:18:19.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --since=24h'
Jan 18 22:18:19.136: INFO: stderr: ""
Jan 18 22:18:19.136: INFO: stdout: "I0118 22:18:14.908593       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8b2 430\nI0118 22:18:15.108734       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/khdg 533\nI0118 22:18:15.309385       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/2vk 389\nI0118 22:18:15.508750       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/bkzt 389\nI0118 22:18:15.709127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/r2w5 527\nI0118 22:18:15.908713       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vwp 294\nI0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\nI0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\nI0118 22:18:16.508947       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/5w5 438\nI0118 22:18:16.709243       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/8qhf 314\nI0118 22:18:16.909657       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/cqj2 282\nI0118 22:18:17.109120       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/f92 334\nI0118 22:18:17.309550       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/fk8 246\nI0118 22:18:17.508890       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/c7r 410\nI0118 22:18:17.709265       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6c8 311\nI0118 22:18:17.908656       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/rlb 288\nI0118 22:18:18.109060       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/w2xl 420\nI0118 22:18:18.309536       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/sdw 441\nI0118 22:18:18.508839       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/97n 537\nI0118 22:18:18.709290       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/47w 344\nI0118 22:18:18.908625       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f49 477\nI0118 22:18:19.109104       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/fdr 486\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan 18 22:18:19.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 delete pod logs-generator'
Jan 18 22:18:20.781: INFO: stderr: ""
Jan 18 22:18:20.781: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:18:20.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4951" for this suite. 01/18/23 22:18:20.788
------------------------------
• [SLOW TEST] [7.094 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:18:13.712
    Jan 18 22:18:13.712: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:18:13.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:13.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:13.874
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/18/23 22:18:13.877
    Jan 18 22:18:13.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 18 22:18:14.008: INFO: stderr: ""
    Jan 18 22:18:14.008: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/18/23 22:18:14.008
    Jan 18 22:18:14.008: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 18 22:18:14.008: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4951" to be "running and ready, or succeeded"
    Jan 18 22:18:14.012: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.267149ms
    Jan 18 22:18:14.012: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '' to be 'Running' but was 'Pending'
    Jan 18 22:18:16.017: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009063379s
    Jan 18 22:18:16.017: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 18 22:18:16.017: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/18/23 22:18:16.017
    Jan 18 22:18:16.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator'
    Jan 18 22:18:16.121: INFO: stderr: ""
    Jan 18 22:18:16.121: INFO: stdout: "I0118 22:18:14.908593       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8b2 430\nI0118 22:18:15.108734       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/khdg 533\nI0118 22:18:15.309385       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/2vk 389\nI0118 22:18:15.508750       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/bkzt 389\nI0118 22:18:15.709127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/r2w5 527\nI0118 22:18:15.908713       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vwp 294\nI0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
    STEP: limiting log lines 01/18/23 22:18:16.121
    Jan 18 22:18:16.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --tail=1'
    Jan 18 22:18:16.233: INFO: stderr: ""
    Jan 18 22:18:16.233: INFO: stdout: "I0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
    Jan 18 22:18:16.233: INFO: got output "I0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\n"
    STEP: limiting log bytes 01/18/23 22:18:16.233
    Jan 18 22:18:16.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --limit-bytes=1'
    Jan 18 22:18:16.321: INFO: stderr: ""
    Jan 18 22:18:16.321: INFO: stdout: "I"
    Jan 18 22:18:16.321: INFO: got output "I"
    STEP: exposing timestamps 01/18/23 22:18:16.321
    Jan 18 22:18:16.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 18 22:18:16.413: INFO: stderr: ""
    Jan 18 22:18:16.413: INFO: stdout: "2023-01-18T22:18:16.309818872Z I0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\n"
    Jan 18 22:18:16.413: INFO: got output "2023-01-18T22:18:16.309818872Z I0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\n"
    STEP: restricting to a time range 01/18/23 22:18:16.413
    Jan 18 22:18:18.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --since=1s'
    Jan 18 22:18:19.021: INFO: stderr: ""
    Jan 18 22:18:19.021: INFO: stdout: "I0118 22:18:18.109060       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/w2xl 420\nI0118 22:18:18.309536       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/sdw 441\nI0118 22:18:18.508839       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/97n 537\nI0118 22:18:18.709290       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/47w 344\nI0118 22:18:18.908625       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f49 477\n"
    Jan 18 22:18:19.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 logs logs-generator logs-generator --since=24h'
    Jan 18 22:18:19.136: INFO: stderr: ""
    Jan 18 22:18:19.136: INFO: stdout: "I0118 22:18:14.908593       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/8b2 430\nI0118 22:18:15.108734       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/khdg 533\nI0118 22:18:15.309385       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/2vk 389\nI0118 22:18:15.508750       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/bkzt 389\nI0118 22:18:15.709127       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/r2w5 527\nI0118 22:18:15.908713       1 logs_generator.go:76] 5 GET /api/v1/namespaces/default/pods/vwp 294\nI0118 22:18:16.109196       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9dt 435\nI0118 22:18:16.309595       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/h6w 238\nI0118 22:18:16.508947       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/5w5 438\nI0118 22:18:16.709243       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/8qhf 314\nI0118 22:18:16.909657       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/cqj2 282\nI0118 22:18:17.109120       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/f92 334\nI0118 22:18:17.309550       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/fk8 246\nI0118 22:18:17.508890       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/c7r 410\nI0118 22:18:17.709265       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/6c8 311\nI0118 22:18:17.908656       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/rlb 288\nI0118 22:18:18.109060       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/w2xl 420\nI0118 22:18:18.309536       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/sdw 441\nI0118 22:18:18.508839       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/97n 537\nI0118 22:18:18.709290       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/47w 344\nI0118 22:18:18.908625       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/f49 477\nI0118 22:18:19.109104       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/fdr 486\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan 18 22:18:19.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4951 delete pod logs-generator'
    Jan 18 22:18:20.781: INFO: stderr: ""
    Jan 18 22:18:20.781: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:18:20.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4951" for this suite. 01/18/23 22:18:20.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:18:20.81
Jan 18 22:18:20.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename cronjob 01/18/23 22:18:20.816
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:20.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:20.975
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/18/23 22:18:20.978
STEP: Ensuring no jobs are scheduled 01/18/23 22:18:20.993
STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:23:21
STEP: Removing cronjob 01/18/23 22:23:21.008
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:21.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5733" for this suite. 01/18/23 22:23:21.023
------------------------------
• [SLOW TEST] [300.225 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:18:20.81
    Jan 18 22:18:20.810: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename cronjob 01/18/23 22:18:20.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:18:20.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:18:20.975
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/18/23 22:18:20.978
    STEP: Ensuring no jobs are scheduled 01/18/23 22:18:20.993
    STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 22:23:21
    STEP: Removing cronjob 01/18/23 22:23:21.008
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:21.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5733" for this suite. 01/18/23 22:23:21.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:21.036
Jan 18 22:23:21.036: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption 01/18/23 22:23:21.037
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:21.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:21.383
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:21.386
Jan 18 22:23:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename disruption-2 01/18/23 22:23:21.387
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:21.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:21.874
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/18/23 22:23:21.895
STEP: Waiting for the pdb to be processed 01/18/23 22:23:23.915
STEP: Waiting for the pdb to be processed 01/18/23 22:23:25.938
STEP: listing a collection of PDBs across all namespaces 01/18/23 22:23:27.947
STEP: listing a collection of PDBs in namespace disruption-9359 01/18/23 22:23:27.95
STEP: deleting a collection of PDBs 01/18/23 22:23:27.953
STEP: Waiting for the PDB collection to be deleted 01/18/23 22:23:27.998
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:28.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:28.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1754" for this suite. 01/18/23 22:23:28.011
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9359" for this suite. 01/18/23 22:23:28.025
------------------------------
• [SLOW TEST] [7.000 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:21.036
    Jan 18 22:23:21.036: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption 01/18/23 22:23:21.037
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:21.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:21.383
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:21.386
    Jan 18 22:23:21.386: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename disruption-2 01/18/23 22:23:21.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:21.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:21.874
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/18/23 22:23:21.895
    STEP: Waiting for the pdb to be processed 01/18/23 22:23:23.915
    STEP: Waiting for the pdb to be processed 01/18/23 22:23:25.938
    STEP: listing a collection of PDBs across all namespaces 01/18/23 22:23:27.947
    STEP: listing a collection of PDBs in namespace disruption-9359 01/18/23 22:23:27.95
    STEP: deleting a collection of PDBs 01/18/23 22:23:27.953
    STEP: Waiting for the PDB collection to be deleted 01/18/23 22:23:27.998
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:28.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:28.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1754" for this suite. 01/18/23 22:23:28.011
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9359" for this suite. 01/18/23 22:23:28.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:28.039
Jan 18 22:23:28.039: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:23:28.04
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:28.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:28.683
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/18/23 22:23:28.686
Jan 18 22:23:28.703: INFO: Waiting up to 5m0s for pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724" in namespace "downward-api-9463" to be "running and ready"
Jan 18 22:23:28.706: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426732ms
Jan 18 22:23:28.706: INFO: The phase of Pod annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:23:30.711: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724": Phase="Running", Reason="", readiness=true. Elapsed: 2.00858706s
Jan 18 22:23:30.711: INFO: The phase of Pod annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724 is Running (Ready = true)
Jan 18 22:23:30.711: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724" satisfied condition "running and ready"
Jan 18 22:23:31.262: INFO: Successfully updated pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9463" for this suite. 01/18/23 22:23:33.288
------------------------------
• [SLOW TEST] [5.267 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:28.039
    Jan 18 22:23:28.039: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:23:28.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:28.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:28.683
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/18/23 22:23:28.686
    Jan 18 22:23:28.703: INFO: Waiting up to 5m0s for pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724" in namespace "downward-api-9463" to be "running and ready"
    Jan 18 22:23:28.706: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724": Phase="Pending", Reason="", readiness=false. Elapsed: 3.426732ms
    Jan 18 22:23:28.706: INFO: The phase of Pod annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:23:30.711: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724": Phase="Running", Reason="", readiness=true. Elapsed: 2.00858706s
    Jan 18 22:23:30.711: INFO: The phase of Pod annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724 is Running (Ready = true)
    Jan 18 22:23:30.711: INFO: Pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724" satisfied condition "running and ready"
    Jan 18 22:23:31.262: INFO: Successfully updated pod "annotationupdatebb098b27-c49a-43c7-adc0-e84dab48b724"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:33.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9463" for this suite. 01/18/23 22:23:33.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:33.307
Jan 18 22:23:33.307: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:23:33.308
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:33.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:33.478
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan 18 22:23:33.516: INFO: Waiting up to 5m0s for pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93" in namespace "svcaccounts-2600" to be "running"
Jan 18 22:23:33.522: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97369ms
Jan 18 22:23:35.526: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93": Phase="Running", Reason="", readiness=true. Elapsed: 2.010396515s
Jan 18 22:23:35.526: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93" satisfied condition "running"
STEP: reading a file in the container 01/18/23 22:23:35.526
Jan 18 22:23:35.526: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/18/23 22:23:35.712
Jan 18 22:23:35.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/18/23 22:23:35.874
Jan 18 22:23:35.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 22:23:36.037: INFO: Got root ca configmap in namespace "svcaccounts-2600"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:36.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2600" for this suite. 01/18/23 22:23:36.044
------------------------------
• [2.749 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:33.307
    Jan 18 22:23:33.307: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 22:23:33.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:33.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:33.478
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan 18 22:23:33.516: INFO: Waiting up to 5m0s for pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93" in namespace "svcaccounts-2600" to be "running"
    Jan 18 22:23:33.522: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.97369ms
    Jan 18 22:23:35.526: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93": Phase="Running", Reason="", readiness=true. Elapsed: 2.010396515s
    Jan 18 22:23:35.526: INFO: Pod "pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93" satisfied condition "running"
    STEP: reading a file in the container 01/18/23 22:23:35.526
    Jan 18 22:23:35.526: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/18/23 22:23:35.712
    Jan 18 22:23:35.712: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/18/23 22:23:35.874
    Jan 18 22:23:35.874: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2600 pod-service-account-d58470dc-1d45-49cb-b4bf-3ccafd34ae93 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 18 22:23:36.037: INFO: Got root ca configmap in namespace "svcaccounts-2600"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:36.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2600" for this suite. 01/18/23 22:23:36.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:36.056
Jan 18 22:23:36.056: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:23:36.057
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:36.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:36.433
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6000 01/18/23 22:23:36.436
STEP: creating service affinity-clusterip in namespace services-6000 01/18/23 22:23:36.436
STEP: creating replication controller affinity-clusterip in namespace services-6000 01/18/23 22:23:36.473
I0118 22:23:36.490935      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6000, replica count: 3
I0118 22:23:39.542455      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:23:39.551: INFO: Creating new exec pod
Jan 18 22:23:39.564: INFO: Waiting up to 5m0s for pod "execpod-affinityq4jhs" in namespace "services-6000" to be "running"
Jan 18 22:23:39.567: INFO: Pod "execpod-affinityq4jhs": Phase="Pending", Reason="", readiness=false. Elapsed: 3.514953ms
Jan 18 22:23:41.578: INFO: Pod "execpod-affinityq4jhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.014038466s
Jan 18 22:23:41.578: INFO: Pod "execpod-affinityq4jhs" satisfied condition "running"
Jan 18 22:23:42.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan 18 22:23:42.799: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 18 22:23:42.799: INFO: stdout: ""
Jan 18 22:23:42.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c nc -v -z -w 2 10.152.183.253 80'
Jan 18 22:23:43.020: INFO: stderr: "+ nc -v -z -w 2 10.152.183.253 80\nConnection to 10.152.183.253 80 port [tcp/http] succeeded!\n"
Jan 18 22:23:43.020: INFO: stdout: ""
Jan 18 22:23:43.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.253:80/ ; done'
Jan 18 22:23:43.289: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n"
Jan 18 22:23:43.289: INFO: stdout: "\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz"
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
Jan 18 22:23:43.289: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6000, will wait for the garbage collector to delete the pods 01/18/23 22:23:43.318
Jan 18 22:23:43.386: INFO: Deleting ReplicationController affinity-clusterip took: 11.893835ms
Jan 18 22:23:43.786: INFO: Terminating ReplicationController affinity-clusterip pods took: 400.879477ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6000" for this suite. 01/18/23 22:23:45.929
------------------------------
• [SLOW TEST] [9.883 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:36.056
    Jan 18 22:23:36.056: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:23:36.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:36.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:36.433
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6000 01/18/23 22:23:36.436
    STEP: creating service affinity-clusterip in namespace services-6000 01/18/23 22:23:36.436
    STEP: creating replication controller affinity-clusterip in namespace services-6000 01/18/23 22:23:36.473
    I0118 22:23:36.490935      20 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6000, replica count: 3
    I0118 22:23:39.542455      20 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:23:39.551: INFO: Creating new exec pod
    Jan 18 22:23:39.564: INFO: Waiting up to 5m0s for pod "execpod-affinityq4jhs" in namespace "services-6000" to be "running"
    Jan 18 22:23:39.567: INFO: Pod "execpod-affinityq4jhs": Phase="Pending", Reason="", readiness=false. Elapsed: 3.514953ms
    Jan 18 22:23:41.578: INFO: Pod "execpod-affinityq4jhs": Phase="Running", Reason="", readiness=true. Elapsed: 2.014038466s
    Jan 18 22:23:41.578: INFO: Pod "execpod-affinityq4jhs" satisfied condition "running"
    Jan 18 22:23:42.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan 18 22:23:42.799: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 18 22:23:42.799: INFO: stdout: ""
    Jan 18 22:23:42.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c nc -v -z -w 2 10.152.183.253 80'
    Jan 18 22:23:43.020: INFO: stderr: "+ nc -v -z -w 2 10.152.183.253 80\nConnection to 10.152.183.253 80 port [tcp/http] succeeded!\n"
    Jan 18 22:23:43.020: INFO: stdout: ""
    Jan 18 22:23:43.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-6000 exec execpod-affinityq4jhs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.152.183.253:80/ ; done'
    Jan 18 22:23:43.289: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.152.183.253:80/\n"
    Jan 18 22:23:43.289: INFO: stdout: "\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz\naffinity-clusterip-n26fz"
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Received response from host: affinity-clusterip-n26fz
    Jan 18 22:23:43.289: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6000, will wait for the garbage collector to delete the pods 01/18/23 22:23:43.318
    Jan 18 22:23:43.386: INFO: Deleting ReplicationController affinity-clusterip took: 11.893835ms
    Jan 18 22:23:43.786: INFO: Terminating ReplicationController affinity-clusterip pods took: 400.879477ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6000" for this suite. 01/18/23 22:23:45.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:45.941
Jan 18 22:23:45.941: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:23:45.942
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:46.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:46.874
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan 18 22:23:46.900: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 22:23:46.911
Jan 18 22:23:46.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:46.918: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 22:23:46.918
Jan 18 22:23:46.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:46.943: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:47.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:47.951: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:48.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:23:48.949: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 22:23:48.952
Jan 18 22:23:48.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:23:48.977: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 18 22:23:49.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:49.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 22:23:49.982
Jan 18 22:23:49.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:49.996: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:51.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:51.008: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:52.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:52.001: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:53.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:53.002: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
Jan 18 22:23:54.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 22:23:54.004: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:23:54.014
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6345, will wait for the garbage collector to delete the pods 01/18/23 22:23:54.014
Jan 18 22:23:54.090: INFO: Deleting DaemonSet.extensions daemon-set took: 21.956287ms
Jan 18 22:23:54.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.742034ms
Jan 18 22:23:56.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:23:56.596: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:23:56.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31365"},"items":null}

Jan 18 22:23:56.606: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31365"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:56.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6345" for this suite. 01/18/23 22:23:56.638
------------------------------
• [SLOW TEST] [10.710 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:45.941
    Jan 18 22:23:45.941: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:23:45.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:46.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:46.874
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan 18 22:23:46.900: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 22:23:46.911
    Jan 18 22:23:46.918: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:46.918: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 22:23:46.918
    Jan 18 22:23:46.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:46.943: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:47.951: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:47.951: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:48.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:23:48.949: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 22:23:48.952
    Jan 18 22:23:48.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:23:48.977: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 18 22:23:49.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:49.982: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 22:23:49.982
    Jan 18 22:23:49.996: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:49.996: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:51.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:51.008: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:52.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:52.001: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:53.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:53.002: INFO: Node test-vm-1 is running 0 daemon pod, expected 1
    Jan 18 22:23:54.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 22:23:54.004: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:23:54.014
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6345, will wait for the garbage collector to delete the pods 01/18/23 22:23:54.014
    Jan 18 22:23:54.090: INFO: Deleting DaemonSet.extensions daemon-set took: 21.956287ms
    Jan 18 22:23:54.391: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.742034ms
    Jan 18 22:23:56.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:23:56.596: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:23:56.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31365"},"items":null}

    Jan 18 22:23:56.606: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31365"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:56.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6345" for this suite. 01/18/23 22:23:56.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:56.652
Jan 18 22:23:56.652: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:23:56.653
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:56.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:56.874
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:23:56.877
Jan 18 22:23:56.893: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-930" to be "running and ready"
Jan 18 22:23:56.898: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750261ms
Jan 18 22:23:56.898: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:23:58.903: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964542s
Jan 18 22:23:58.903: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 18 22:23:58.903: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/18/23 22:23:58.907
STEP: Then the orphan pod is adopted 01/18/23 22:23:58.917
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:23:59.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-930" for this suite. 01/18/23 22:23:59.929
------------------------------
• [3.290 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:56.652
    Jan 18 22:23:56.652: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:23:56.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:23:56.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:23:56.874
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 22:23:56.877
    Jan 18 22:23:56.893: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-930" to be "running and ready"
    Jan 18 22:23:56.898: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750261ms
    Jan 18 22:23:56.898: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:23:58.903: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009964542s
    Jan 18 22:23:58.903: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 18 22:23:58.903: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/18/23 22:23:58.907
    STEP: Then the orphan pod is adopted 01/18/23 22:23:58.917
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:23:59.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-930" for this suite. 01/18/23 22:23:59.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:23:59.943
Jan 18 22:23:59.943: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context 01/18/23 22:23:59.944
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:00.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:00.332
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:24:00.334
Jan 18 22:24:00.353: INFO: Waiting up to 5m0s for pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3" in namespace "security-context-7556" to be "Succeeded or Failed"
Jan 18 22:24:00.357: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.697847ms
Jan 18 22:24:02.362: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009518236s
Jan 18 22:24:04.363: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009788754s
STEP: Saw pod success 01/18/23 22:24:04.363
Jan 18 22:24:04.363: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3" satisfied condition "Succeeded or Failed"
Jan 18 22:24:04.366: INFO: Trying to get logs from node test-vm-1 pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 container test-container: <nil>
STEP: delete the pod 01/18/23 22:24:04.373
Jan 18 22:24:04.396: INFO: Waiting for pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 to disappear
Jan 18 22:24:04.401: INFO: Pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:04.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-7556" for this suite. 01/18/23 22:24:04.405
------------------------------
• [4.474 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:23:59.943
    Jan 18 22:23:59.943: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context 01/18/23 22:23:59.944
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:00.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:00.332
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:24:00.334
    Jan 18 22:24:00.353: INFO: Waiting up to 5m0s for pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3" in namespace "security-context-7556" to be "Succeeded or Failed"
    Jan 18 22:24:00.357: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.697847ms
    Jan 18 22:24:02.362: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009518236s
    Jan 18 22:24:04.363: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009788754s
    STEP: Saw pod success 01/18/23 22:24:04.363
    Jan 18 22:24:04.363: INFO: Pod "security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3" satisfied condition "Succeeded or Failed"
    Jan 18 22:24:04.366: INFO: Trying to get logs from node test-vm-1 pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:24:04.373
    Jan 18 22:24:04.396: INFO: Waiting for pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 to disappear
    Jan 18 22:24:04.401: INFO: Pod security-context-6e884378-6123-40e5-b28b-4a8f18c74ef3 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:04.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-7556" for this suite. 01/18/23 22:24:04.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:04.421
Jan 18 22:24:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename deployment 01/18/23 22:24:04.422
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:04.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:04.85
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 18 22:24:04.852: INFO: Creating simple deployment test-new-deployment
Jan 18 22:24:04.876: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jan 18 22:24:06.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/18/23 22:24:08.894
STEP: updating a scale subresource 01/18/23 22:24:08.898
STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:24:08.908
STEP: Patch a scale subresource 01/18/23 22:24:08.913
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 22:24:08.931: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9275  d2bc7e1b-1391-45fe-8073-96a7f70165ec 31489 3 2023-01-18 22:24:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064394b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 22:24:08.934: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9275  f7d326c7-3964-4b96-b313-a27acc35e4b1 31483 1 2023-01-18 22:24:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d2bc7e1b-1391-45fe-8073-96a7f70165ec 0xc0040a5527 0xc0040a5528}] [] [{kubelite Update apps/v1 2023-01-18 22:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2bc7e1b-1391-45fe-8073-96a7f70165ec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a55c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 22:24:08.939: INFO: Pod "test-new-deployment-7f5969cbc7-mtmrb" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-mtmrb test-new-deployment-7f5969cbc7- deployment-9275  6116bd7c-dc0c-4fef-98af-80d0a322b35b 31482 0 2023-01-18 22:24:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c0c08a9c370374ffb4e47397b282432081e5efd346cb291788704c32bede826 cni.projectcalico.org/podIP:10.1.192.19/32 cni.projectcalico.org/podIPs:10.1.192.19/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7d326c7-3964-4b96-b313-a27acc35e4b1 0xc0040a59d7 0xc0040a59d8}] [] [{kubelite Update v1 2023-01-18 22:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7d326c7-3964-4b96-b313-a27acc35e4b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 22:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blm58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blm58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.19,StartTime:2023-01-18 22:24:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:24:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6fd05a25b4a0144a3807f8636091d97d595a439b18d271a0fe4c71860db5bb93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan 18 22:24:08.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9275" for this suite. 01/18/23 22:24:08.944
------------------------------
• [4.545 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:04.421
    Jan 18 22:24:04.421: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename deployment 01/18/23 22:24:04.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:04.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:04.85
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 18 22:24:04.852: INFO: Creating simple deployment test-new-deployment
    Jan 18 22:24:04.876: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    Jan 18 22:24:06.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 24, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/18/23 22:24:08.894
    STEP: updating a scale subresource 01/18/23 22:24:08.898
    STEP: verifying the deployment Spec.Replicas was modified 01/18/23 22:24:08.908
    STEP: Patch a scale subresource 01/18/23 22:24:08.913
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 22:24:08.931: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9275  d2bc7e1b-1391-45fe-8073-96a7f70165ec 31489 3 2023-01-18 22:24:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 22:24:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064394b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-18 22:24:07 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 22:24:08.934: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-9275  f7d326c7-3964-4b96-b313-a27acc35e4b1 31483 1 2023-01-18 22:24:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment d2bc7e1b-1391-45fe-8073-96a7f70165ec 0xc0040a5527 0xc0040a5528}] [] [{kubelite Update apps/v1 2023-01-18 22:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2bc7e1b-1391-45fe-8073-96a7f70165ec\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kubelite Update apps/v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040a55c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 22:24:08.939: INFO: Pod "test-new-deployment-7f5969cbc7-mtmrb" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-mtmrb test-new-deployment-7f5969cbc7- deployment-9275  6116bd7c-dc0c-4fef-98af-80d0a322b35b 31482 0 2023-01-18 22:24:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7c0c08a9c370374ffb4e47397b282432081e5efd346cb291788704c32bede826 cni.projectcalico.org/podIP:10.1.192.19/32 cni.projectcalico.org/podIPs:10.1.192.19/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 f7d326c7-3964-4b96-b313-a27acc35e4b1 0xc0040a59d7 0xc0040a59d8}] [] [{kubelite Update v1 2023-01-18 22:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7d326c7-3964-4b96-b313-a27acc35e4b1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {Go-http-client Update v1 2023-01-18 22:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelite Update v1 2023-01-18 22:24:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-blm58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-blm58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:test-vm-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 22:24:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.0.4,PodIP:10.1.192.19,StartTime:2023-01-18 22:24:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 22:24:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6fd05a25b4a0144a3807f8636091d97d595a439b18d271a0fe4c71860db5bb93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.1.192.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:24:08.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9275" for this suite. 01/18/23 22:24:08.944
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:24:08.967
Jan 18 22:24:08.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:24:08.968
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:09.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:09.053
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan 18 22:24:09.091: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 22:25:09.110: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/18/23 22:25:09.115
Jan 18 22:25:09.144: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 22:25:09.163: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 22:25:09.187: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 22:25:09.220: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 22:25:09.22
Jan 18 22:25:09.220: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8939" to be "running"
Jan 18 22:25:09.224: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78695ms
Jan 18 22:25:11.229: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543428s
Jan 18 22:25:13.230: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009926262s
Jan 18 22:25:13.230: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 22:25:13.230: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
Jan 18 22:25:13.233: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.280643ms
Jan 18 22:25:13.233: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:25:13.233: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
Jan 18 22:25:13.237: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.550146ms
Jan 18 22:25:13.237: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 22:25:13.237: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
Jan 18 22:25:13.241: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.81235ms
Jan 18 22:25:13.241: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 22:25:13.241
Jan 18 22:25:13.268: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 18 22:25:13.273: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041966ms
Jan 18 22:25:15.278: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010088748s
Jan 18 22:25:17.278: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010508442s
Jan 18 22:25:17.278: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:17.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8939" for this suite. 01/18/23 22:25:17.394
------------------------------
• [SLOW TEST] [68.441 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:24:08.967
    Jan 18 22:24:08.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 22:24:08.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:24:09.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:24:09.053
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan 18 22:24:09.091: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 22:25:09.110: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/18/23 22:25:09.115
    Jan 18 22:25:09.144: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 22:25:09.163: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 22:25:09.187: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 22:25:09.220: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 22:25:09.22
    Jan 18 22:25:09.220: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8939" to be "running"
    Jan 18 22:25:09.224: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78695ms
    Jan 18 22:25:11.229: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008543428s
    Jan 18 22:25:13.230: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.009926262s
    Jan 18 22:25:13.230: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 22:25:13.230: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
    Jan 18 22:25:13.233: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.280643ms
    Jan 18 22:25:13.233: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:25:13.233: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
    Jan 18 22:25:13.237: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.550146ms
    Jan 18 22:25:13.237: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 22:25:13.237: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8939" to be "running"
    Jan 18 22:25:13.241: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.81235ms
    Jan 18 22:25:13.241: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 22:25:13.241
    Jan 18 22:25:13.268: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 18 22:25:13.273: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.041966ms
    Jan 18 22:25:15.278: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010088748s
    Jan 18 22:25:17.278: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010508442s
    Jan 18 22:25:17.278: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:17.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8939" for this suite. 01/18/23 22:25:17.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:17.41
Jan 18 22:25:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 22:25:17.412
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:17.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:17.874
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/18/23 22:25:17.877
STEP: setting up watch 01/18/23 22:25:17.877
STEP: submitting the pod to kubernetes 01/18/23 22:25:17.983
STEP: verifying the pod is in kubernetes 01/18/23 22:25:18
STEP: verifying pod creation was observed 01/18/23 22:25:18.004
Jan 18 22:25:18.004: INFO: Waiting up to 5m0s for pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46" in namespace "pods-9450" to be "running"
Jan 18 22:25:18.008: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061753ms
Jan 18 22:25:20.013: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46": Phase="Running", Reason="", readiness=true. Elapsed: 2.009451s
Jan 18 22:25:20.013: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 22:25:20.017
STEP: verifying pod deletion was observed 01/18/23 22:25:20.036
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:22.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9450" for this suite. 01/18/23 22:25:22.799
------------------------------
• [SLOW TEST] [5.404 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:17.41
    Jan 18 22:25:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 22:25:17.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:17.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:17.874
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/18/23 22:25:17.877
    STEP: setting up watch 01/18/23 22:25:17.877
    STEP: submitting the pod to kubernetes 01/18/23 22:25:17.983
    STEP: verifying the pod is in kubernetes 01/18/23 22:25:18
    STEP: verifying pod creation was observed 01/18/23 22:25:18.004
    Jan 18 22:25:18.004: INFO: Waiting up to 5m0s for pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46" in namespace "pods-9450" to be "running"
    Jan 18 22:25:18.008: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061753ms
    Jan 18 22:25:20.013: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46": Phase="Running", Reason="", readiness=true. Elapsed: 2.009451s
    Jan 18 22:25:20.013: INFO: Pod "pod-submit-remove-70f56c5a-84e4-49f8-9140-b78ef0ae5c46" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 22:25:20.017
    STEP: verifying pod deletion was observed 01/18/23 22:25:20.036
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:22.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9450" for this suite. 01/18/23 22:25:22.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:22.817
Jan 18 22:25:22.817: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:25:22.819
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:23.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:23.075
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:25:23.101
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:25:23.118
Jan 18 22:25:23.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:25:23.134: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 22:25:24.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:25:24.150: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 22:25:25.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:25:25.144: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/18/23 22:25:25.148
STEP: DeleteCollection of the DaemonSets 01/18/23 22:25:25.155
STEP: Verify that ReplicaSets have been deleted 01/18/23 22:25:25.169
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan 18 22:25:25.189: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31846"},"items":null}

Jan 18 22:25:25.195: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31846"},"items":[{"metadata":{"name":"daemon-set-v67ms","generateName":"daemon-set-","namespace":"daemonsets-9110","uid":"ca69c5e5-646c-402a-afe7-c34624b838ab","resourceVersion":"31838","creationTimestamp":"2023-01-18T22:25:23Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"afbde6a8f11cb6995230d214a5716b55de4d9430b0698c6495f1725c75570dfb","cni.projectcalico.org/podIP":"10.1.192.22/32","cni.projectcalico.org/podIPs":"10.1.192.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cae8c09a-aa96-4260-98e3-9a1add032014","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae8c09a-aa96-4260-98e3-9a1add032014\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b7mww","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b7mww","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"test-vm-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["test-vm-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"}],"hostIP":"10.0.0.4","podIP":"10.1.192.22","podIPs":[{"ip":"10.1.192.22"}],"startTime":"2023-01-18T22:25:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T22:25:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ce0daaac274c0c5e1b0a8dfb21ddf36f96b7cc45919a806166e79d160b26d4ed","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-svh9b","generateName":"daemon-set-","namespace":"daemonsets-9110","uid":"51038b57-bdef-400b-ae8f-e301ed978785","resourceVersion":"31839","creationTimestamp":"2023-01-18T22:25:23Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f9c9cf2e32f937335f2d87936b9ec226e0028fe37726d3c27b5eab40e2bca307","cni.projectcalico.org/podIP":"10.1.132.22/32","cni.projectcalico.org/podIPs":"10.1.132.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cae8c09a-aa96-4260-98e3-9a1add032014","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae8c09a-aa96-4260-98e3-9a1add032014\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r92dr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r92dr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"test-vm-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["test-vm-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"}],"hostIP":"10.0.0.5","podIP":"10.1.132.22","podIPs":[{"ip":"10.1.132.22"}],"startTime":"2023-01-18T22:25:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T22:25:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f4332b2ef6ac6874ca8962a9a385b0ad7073fe4e03852beaa8f947a82860ab74","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9110" for this suite. 01/18/23 22:25:25.211
------------------------------
• [2.408 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:22.817
    Jan 18 22:25:22.817: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:25:22.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:23.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:23.075
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 22:25:23.101
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 22:25:23.118
    Jan 18 22:25:23.134: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:25:23.134: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 22:25:24.150: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:25:24.150: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 22:25:25.144: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:25:25.144: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/18/23 22:25:25.148
    STEP: DeleteCollection of the DaemonSets 01/18/23 22:25:25.155
    STEP: Verify that ReplicaSets have been deleted 01/18/23 22:25:25.169
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan 18 22:25:25.189: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31846"},"items":null}

    Jan 18 22:25:25.195: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31846"},"items":[{"metadata":{"name":"daemon-set-v67ms","generateName":"daemon-set-","namespace":"daemonsets-9110","uid":"ca69c5e5-646c-402a-afe7-c34624b838ab","resourceVersion":"31838","creationTimestamp":"2023-01-18T22:25:23Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"afbde6a8f11cb6995230d214a5716b55de4d9430b0698c6495f1725c75570dfb","cni.projectcalico.org/podIP":"10.1.192.22/32","cni.projectcalico.org/podIPs":"10.1.192.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cae8c09a-aa96-4260-98e3-9a1add032014","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae8c09a-aa96-4260-98e3-9a1add032014\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.192.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-b7mww","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-b7mww","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"test-vm-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["test-vm-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"}],"hostIP":"10.0.0.4","podIP":"10.1.192.22","podIPs":[{"ip":"10.1.192.22"}],"startTime":"2023-01-18T22:25:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T22:25:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://ce0daaac274c0c5e1b0a8dfb21ddf36f96b7cc45919a806166e79d160b26d4ed","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-svh9b","generateName":"daemon-set-","namespace":"daemonsets-9110","uid":"51038b57-bdef-400b-ae8f-e301ed978785","resourceVersion":"31839","creationTimestamp":"2023-01-18T22:25:23Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f9c9cf2e32f937335f2d87936b9ec226e0028fe37726d3c27b5eab40e2bca307","cni.projectcalico.org/podIP":"10.1.132.22/32","cni.projectcalico.org/podIPs":"10.1.132.22/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cae8c09a-aa96-4260-98e3-9a1add032014","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cae8c09a-aa96-4260-98e3-9a1add032014\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"Go-http-client","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelite","operation":"Update","apiVersion":"v1","time":"2023-01-18T22:25:24Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.1.132.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r92dr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r92dr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"test-vm-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["test-vm-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:24Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T22:25:23Z"}],"hostIP":"10.0.0.5","podIP":"10.1.132.22","podIPs":[{"ip":"10.1.132.22"}],"startTime":"2023-01-18T22:25:23Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T22:25:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f4332b2ef6ac6874ca8962a9a385b0ad7073fe4e03852beaa8f947a82860ab74","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9110" for this suite. 01/18/23 22:25:25.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:25.227
Jan 18 22:25:25.227: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 22:25:25.228
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:25.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:25.449
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 in namespace container-probe-1463 01/18/23 22:25:25.452
Jan 18 22:25:25.474: INFO: Waiting up to 5m0s for pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9" in namespace "container-probe-1463" to be "not pending"
Jan 18 22:25:25.485: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.796141ms
Jan 18 22:25:27.490: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016339945s
Jan 18 22:25:27.490: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9" satisfied condition "not pending"
Jan 18 22:25:27.490: INFO: Started pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 in namespace container-probe-1463
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:25:27.49
Jan 18 22:25:27.494: INFO: Initial restart count of pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 is 0
Jan 18 22:25:47.556: INFO: Restart count of pod container-probe-1463/liveness-38d09676-591e-4d70-905a-d8b60a969ce9 is now 1 (20.061113856s elapsed)
STEP: deleting the pod 01/18/23 22:25:47.556
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:47.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1463" for this suite. 01/18/23 22:25:47.596
------------------------------
• [SLOW TEST] [22.391 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:25.227
    Jan 18 22:25:25.227: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:25:25.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:25.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:25.449
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 in namespace container-probe-1463 01/18/23 22:25:25.452
    Jan 18 22:25:25.474: INFO: Waiting up to 5m0s for pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9" in namespace "container-probe-1463" to be "not pending"
    Jan 18 22:25:25.485: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.796141ms
    Jan 18 22:25:27.490: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016339945s
    Jan 18 22:25:27.490: INFO: Pod "liveness-38d09676-591e-4d70-905a-d8b60a969ce9" satisfied condition "not pending"
    Jan 18 22:25:27.490: INFO: Started pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 in namespace container-probe-1463
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:25:27.49
    Jan 18 22:25:27.494: INFO: Initial restart count of pod liveness-38d09676-591e-4d70-905a-d8b60a969ce9 is 0
    Jan 18 22:25:47.556: INFO: Restart count of pod container-probe-1463/liveness-38d09676-591e-4d70-905a-d8b60a969ce9 is now 1 (20.061113856s elapsed)
    STEP: deleting the pod 01/18/23 22:25:47.556
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:47.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1463" for this suite. 01/18/23 22:25:47.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:47.621
Jan 18 22:25:47.621: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 22:25:47.623
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:47.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:47.875
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:25:47.878
Jan 18 22:25:47.895: INFO: Waiting up to 5m0s for pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9" in namespace "emptydir-8414" to be "Succeeded or Failed"
Jan 18 22:25:47.900: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66046ms
Jan 18 22:25:49.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Running", Reason="", readiness=false. Elapsed: 2.011089436s
Jan 18 22:25:51.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010562111s
STEP: Saw pod success 01/18/23 22:25:51.906
Jan 18 22:25:51.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9" satisfied condition "Succeeded or Failed"
Jan 18 22:25:51.910: INFO: Trying to get logs from node test-vm-1 pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 container test-container: <nil>
STEP: delete the pod 01/18/23 22:25:51.931
Jan 18 22:25:51.954: INFO: Waiting for pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 to disappear
Jan 18 22:25:51.959: INFO: Pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8414" for this suite. 01/18/23 22:25:51.966
------------------------------
• [4.358 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:47.621
    Jan 18 22:25:47.621: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:25:47.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:47.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:47.875
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 22:25:47.878
    Jan 18 22:25:47.895: INFO: Waiting up to 5m0s for pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9" in namespace "emptydir-8414" to be "Succeeded or Failed"
    Jan 18 22:25:47.900: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66046ms
    Jan 18 22:25:49.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Running", Reason="", readiness=false. Elapsed: 2.011089436s
    Jan 18 22:25:51.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010562111s
    STEP: Saw pod success 01/18/23 22:25:51.906
    Jan 18 22:25:51.906: INFO: Pod "pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9" satisfied condition "Succeeded or Failed"
    Jan 18 22:25:51.910: INFO: Trying to get logs from node test-vm-1 pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:25:51.931
    Jan 18 22:25:51.954: INFO: Waiting for pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 to disappear
    Jan 18 22:25:51.959: INFO: Pod pod-afe5d5bb-b457-44e5-8629-a6d1e97ed1b9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8414" for this suite. 01/18/23 22:25:51.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:51.984
Jan 18 22:25:51.984: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:25:51.986
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:52.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:52.427
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/18/23 22:25:52.43
Jan 18 22:25:52.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-7915 create -f -'
Jan 18 22:25:53.293: INFO: stderr: ""
Jan 18 22:25:53.293: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 22:25:53.293
Jan 18 22:25:54.297: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:54.297: INFO: Found 0 / 1
Jan 18 22:25:55.299: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:55.299: INFO: Found 1 / 1
Jan 18 22:25:55.299: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/18/23 22:25:55.299
Jan 18 22:25:55.302: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:55.302: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 22:25:55.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-7915 patch pod agnhost-primary-8djts -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 22:25:55.397: INFO: stderr: ""
Jan 18 22:25:55.397: INFO: stdout: "pod/agnhost-primary-8djts patched\n"
STEP: checking annotations 01/18/23 22:25:55.398
Jan 18 22:25:55.402: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 22:25:55.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:25:55.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7915" for this suite. 01/18/23 22:25:55.407
------------------------------
• [3.442 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:51.984
    Jan 18 22:25:51.984: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:25:51.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:52.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:52.427
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/18/23 22:25:52.43
    Jan 18 22:25:52.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-7915 create -f -'
    Jan 18 22:25:53.293: INFO: stderr: ""
    Jan 18 22:25:53.293: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 22:25:53.293
    Jan 18 22:25:54.297: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:54.297: INFO: Found 0 / 1
    Jan 18 22:25:55.299: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:55.299: INFO: Found 1 / 1
    Jan 18 22:25:55.299: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/18/23 22:25:55.299
    Jan 18 22:25:55.302: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:55.302: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 22:25:55.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-7915 patch pod agnhost-primary-8djts -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 18 22:25:55.397: INFO: stderr: ""
    Jan 18 22:25:55.397: INFO: stdout: "pod/agnhost-primary-8djts patched\n"
    STEP: checking annotations 01/18/23 22:25:55.398
    Jan 18 22:25:55.402: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 22:25:55.402: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:25:55.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7915" for this suite. 01/18/23 22:25:55.407
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:25:55.427
Jan 18 22:25:55.427: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:25:55.428
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:55.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:55.879
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf 01/18/23 22:25:55.893
Jan 18 22:25:55.923: INFO: Pod name my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Found 0 pods out of 1
Jan 18 22:26:00.929: INFO: Pod name my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Found 1 pods out of 1
Jan 18 22:26:00.929: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf" are running
Jan 18 22:26:00.929: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" in namespace "replication-controller-3" to be "running"
Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc": Phase="Running", Reason="", readiness=true. Elapsed: 4.588058ms
Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" satisfied condition "running"
Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:56 +0000 UTC Reason: Message:}])
Jan 18 22:26:00.934: INFO: Trying to dial the pod
Jan 18 22:26:05.948: INFO: Controller my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Got expected result from replica 1 [my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc]: "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3" for this suite. 01/18/23 22:26:05.953
------------------------------
• [SLOW TEST] [10.539 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:25:55.427
    Jan 18 22:25:55.427: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:25:55.428
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:25:55.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:25:55.879
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf 01/18/23 22:25:55.893
    Jan 18 22:25:55.923: INFO: Pod name my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Found 0 pods out of 1
    Jan 18 22:26:00.929: INFO: Pod name my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Found 1 pods out of 1
    Jan 18 22:26:00.929: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf" are running
    Jan 18 22:26:00.929: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" in namespace "replication-controller-3" to be "running"
    Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc": Phase="Running", Reason="", readiness=true. Elapsed: 4.588058ms
    Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" satisfied condition "running"
    Jan 18 22:26:00.933: INFO: Pod "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 22:25:56 +0000 UTC Reason: Message:}])
    Jan 18 22:26:00.934: INFO: Trying to dial the pod
    Jan 18 22:26:05.948: INFO: Controller my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf: Got expected result from replica 1 [my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc]: "my-hostname-basic-9d8b5506-26b9-4dd8-8aa4-605ad43698bf-kl5zc", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:05.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3" for this suite. 01/18/23 22:26:05.953
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:05.967
Jan 18 22:26:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:26:05.969
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:06.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:06.395
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:26:06.435
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:26:07.221
STEP: Deploying the webhook pod 01/18/23 22:26:07.239
STEP: Wait for the deployment to be ready 01/18/23 22:26:07.259
Jan 18 22:26:07.272: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:26:09.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:26:11.29
STEP: Verifying the service has paired with the endpoint 01/18/23 22:26:11.313
Jan 18 22:26:12.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 22:26:12.322
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:12.322
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 22:26:12.346
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 22:26:13.362
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:13.362
STEP: Having no error when timeout is longer than webhook latency 01/18/23 22:26:14.417
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:14.417
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 22:26:19.485
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:19.485
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:24.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4692" for this suite. 01/18/23 22:26:24.629
STEP: Destroying namespace "webhook-4692-markers" for this suite. 01/18/23 22:26:24.641
------------------------------
• [SLOW TEST] [18.684 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:05.967
    Jan 18 22:26:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:26:05.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:06.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:06.395
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:26:06.435
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:26:07.221
    STEP: Deploying the webhook pod 01/18/23 22:26:07.239
    STEP: Wait for the deployment to be ready 01/18/23 22:26:07.259
    Jan 18 22:26:07.272: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:26:09.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 26, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:26:11.29
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:26:11.313
    Jan 18 22:26:12.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 22:26:12.322
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:12.322
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 22:26:12.346
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 22:26:13.362
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:13.362
    STEP: Having no error when timeout is longer than webhook latency 01/18/23 22:26:14.417
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:14.417
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 22:26:19.485
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 22:26:19.485
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:24.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4692" for this suite. 01/18/23 22:26:24.629
    STEP: Destroying namespace "webhook-4692-markers" for this suite. 01/18/23 22:26:24.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:24.652
Jan 18 22:26:24.652: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 22:26:24.653
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:24.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:24.876
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:26:24.879
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 22:26:24.879
STEP: creating a pod to probe DNS 01/18/23 22:26:24.879
STEP: submitting the pod to kubernetes 01/18/23 22:26:24.879
Jan 18 22:26:24.916: INFO: Waiting up to 15m0s for pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3" in namespace "dns-6103" to be "running"
Jan 18 22:26:24.921: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892956ms
Jan 18 22:26:26.927: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010557438s
Jan 18 22:26:28.926: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.01016855s
Jan 18 22:26:28.926: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:26:28.926
STEP: looking for the results for each expected name from probers 01/18/23 22:26:28.93
Jan 18 22:26:28.946: INFO: DNS probes using dns-6103/dns-test-b77ed048-71b7-4965-a662-939e0e1671d3 succeeded

STEP: deleting the pod 01/18/23 22:26:28.946
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:28.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6103" for this suite. 01/18/23 22:26:29.039
------------------------------
• [4.467 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:24.652
    Jan 18 22:26:24.652: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 22:26:24.653
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:24.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:24.876
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:26:24.879
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 22:26:24.879
    STEP: creating a pod to probe DNS 01/18/23 22:26:24.879
    STEP: submitting the pod to kubernetes 01/18/23 22:26:24.879
    Jan 18 22:26:24.916: INFO: Waiting up to 15m0s for pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3" in namespace "dns-6103" to be "running"
    Jan 18 22:26:24.921: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.892956ms
    Jan 18 22:26:26.927: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010557438s
    Jan 18 22:26:28.926: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3": Phase="Running", Reason="", readiness=true. Elapsed: 4.01016855s
    Jan 18 22:26:28.926: INFO: Pod "dns-test-b77ed048-71b7-4965-a662-939e0e1671d3" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:26:28.926
    STEP: looking for the results for each expected name from probers 01/18/23 22:26:28.93
    Jan 18 22:26:28.946: INFO: DNS probes using dns-6103/dns-test-b77ed048-71b7-4965-a662-939e0e1671d3 succeeded

    STEP: deleting the pod 01/18/23 22:26:28.946
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:28.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6103" for this suite. 01/18/23 22:26:29.039
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:29.12
Jan 18 22:26:29.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:26:29.122
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:29.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:29.875
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-11695b85-3e93-43fb-b32d-a2ec3c2ca1c3 01/18/23 22:26:29.877
STEP: Creating a pod to test consume secrets 01/18/23 22:26:29.891
Jan 18 22:26:29.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87" in namespace "projected-4916" to be "Succeeded or Failed"
Jan 18 22:26:29.916: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63349ms
Jan 18 22:26:31.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010756127s
Jan 18 22:26:33.924: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01175748s
Jan 18 22:26:35.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010896709s
STEP: Saw pod success 01/18/23 22:26:35.923
Jan 18 22:26:35.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87" satisfied condition "Succeeded or Failed"
Jan 18 22:26:35.927: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:26:35.934
Jan 18 22:26:35.960: INFO: Waiting for pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 to disappear
Jan 18 22:26:35.969: INFO: Pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:35.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4916" for this suite. 01/18/23 22:26:35.974
------------------------------
• [SLOW TEST] [6.865 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:29.12
    Jan 18 22:26:29.121: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:26:29.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:29.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:29.875
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-11695b85-3e93-43fb-b32d-a2ec3c2ca1c3 01/18/23 22:26:29.877
    STEP: Creating a pod to test consume secrets 01/18/23 22:26:29.891
    Jan 18 22:26:29.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87" in namespace "projected-4916" to be "Succeeded or Failed"
    Jan 18 22:26:29.916: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.63349ms
    Jan 18 22:26:31.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010756127s
    Jan 18 22:26:33.924: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01175748s
    Jan 18 22:26:35.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010896709s
    STEP: Saw pod success 01/18/23 22:26:35.923
    Jan 18 22:26:35.923: INFO: Pod "pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87" satisfied condition "Succeeded or Failed"
    Jan 18 22:26:35.927: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:26:35.934
    Jan 18 22:26:35.960: INFO: Waiting for pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 to disappear
    Jan 18 22:26:35.969: INFO: Pod pod-projected-secrets-42275123-1a06-4837-b33b-597d6adbdb87 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:35.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4916" for this suite. 01/18/23 22:26:35.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:35.986
Jan 18 22:26:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename endpointslice 01/18/23 22:26:35.988
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:36.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:36.874
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan 18 22:26:36.888: INFO: Endpoints addresses: [10.0.0.4 10.0.0.5] , ports: [16443]
Jan 18 22:26:36.888: INFO: EndpointSlices addresses: [10.0.0.4 10.0.0.5] , ports: [16443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-635" for this suite. 01/18/23 22:26:36.892
------------------------------
• [0.919 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:35.986
    Jan 18 22:26:35.986: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename endpointslice 01/18/23 22:26:35.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:36.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:36.874
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan 18 22:26:36.888: INFO: Endpoints addresses: [10.0.0.4 10.0.0.5] , ports: [16443]
    Jan 18 22:26:36.888: INFO: EndpointSlices addresses: [10.0.0.4 10.0.0.5] , ports: [16443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:36.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-635" for this suite. 01/18/23 22:26:36.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:36.906
Jan 18 22:26:36.906: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:36.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:37.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:37.829
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:37.832
Jan 18 22:26:37.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24" in namespace "downward-api-6485" to be "Succeeded or Failed"
Jan 18 22:26:37.864: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756315ms
Jan 18 22:26:39.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011064417s
Jan 18 22:26:41.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010980866s
STEP: Saw pod success 01/18/23 22:26:41.869
Jan 18 22:26:41.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24" satisfied condition "Succeeded or Failed"
Jan 18 22:26:41.873: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 container client-container: <nil>
STEP: delete the pod 01/18/23 22:26:41.881
Jan 18 22:26:41.940: INFO: Waiting for pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 to disappear
Jan 18 22:26:41.949: INFO: Pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6485" for this suite. 01/18/23 22:26:41.954
------------------------------
• [SLOW TEST] [5.064 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:36.906
    Jan 18 22:26:36.906: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:36.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:37.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:37.829
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:37.832
    Jan 18 22:26:37.858: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24" in namespace "downward-api-6485" to be "Succeeded or Failed"
    Jan 18 22:26:37.864: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756315ms
    Jan 18 22:26:39.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011064417s
    Jan 18 22:26:41.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010980866s
    STEP: Saw pod success 01/18/23 22:26:41.869
    Jan 18 22:26:41.869: INFO: Pod "downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24" satisfied condition "Succeeded or Failed"
    Jan 18 22:26:41.873: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:26:41.881
    Jan 18 22:26:41.940: INFO: Waiting for pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 to disappear
    Jan 18 22:26:41.949: INFO: Pod downwardapi-volume-ec7951b3-2ec4-4b10-91e3-79be5ee07a24 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6485" for this suite. 01/18/23 22:26:41.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:41.971
Jan 18 22:26:41.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 22:26:41.972
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:42.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:42.874
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-ada65aa9-356d-4aea-99ef-0602d02c7e1d 01/18/23 22:26:42.877
STEP: Creating a pod to test consume secrets 01/18/23 22:26:42.891
Jan 18 22:26:42.916: INFO: Waiting up to 5m0s for pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559" in namespace "secrets-5425" to be "Succeeded or Failed"
Jan 18 22:26:42.921: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064463ms
Jan 18 22:26:44.927: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010671899s
Jan 18 22:26:46.926: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831101s
STEP: Saw pod success 01/18/23 22:26:46.926
Jan 18 22:26:46.926: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559" satisfied condition "Succeeded or Failed"
Jan 18 22:26:46.929: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:26:46.937
Jan 18 22:26:46.964: INFO: Waiting for pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 to disappear
Jan 18 22:26:46.970: INFO: Pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:46.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5425" for this suite. 01/18/23 22:26:46.975
------------------------------
• [SLOW TEST] [5.015 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:41.971
    Jan 18 22:26:41.971: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 22:26:41.972
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:42.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:42.874
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-ada65aa9-356d-4aea-99ef-0602d02c7e1d 01/18/23 22:26:42.877
    STEP: Creating a pod to test consume secrets 01/18/23 22:26:42.891
    Jan 18 22:26:42.916: INFO: Waiting up to 5m0s for pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559" in namespace "secrets-5425" to be "Succeeded or Failed"
    Jan 18 22:26:42.921: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Pending", Reason="", readiness=false. Elapsed: 5.064463ms
    Jan 18 22:26:44.927: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010671899s
    Jan 18 22:26:46.926: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831101s
    STEP: Saw pod success 01/18/23 22:26:46.926
    Jan 18 22:26:46.926: INFO: Pod "pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559" satisfied condition "Succeeded or Failed"
    Jan 18 22:26:46.929: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:26:46.937
    Jan 18 22:26:46.964: INFO: Waiting for pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 to disappear
    Jan 18 22:26:46.970: INFO: Pod pod-secrets-720ea7d9-5ff2-40a2-bdf0-213f858e6559 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:46.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5425" for this suite. 01/18/23 22:26:46.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:46.987
Jan 18 22:26:46.987: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sysctl 01/18/23 22:26:46.988
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:47.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:47.252
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 22:26:47.257
STEP: Watching for error events or started pod 01/18/23 22:26:47.283
STEP: Waiting for pod completion 01/18/23 22:26:49.29
Jan 18 22:26:49.290: INFO: Waiting up to 3m0s for pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf" in namespace "sysctl-6392" to be "completed"
Jan 18 22:26:49.294: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935647ms
Jan 18 22:26:51.300: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00930189s
Jan 18 22:26:51.300: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/18/23 22:26:51.309
STEP: Getting logs from the pod 01/18/23 22:26:51.309
STEP: Checking that the sysctl is actually updated 01/18/23 22:26:51.317
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:51.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-6392" for this suite. 01/18/23 22:26:51.322
------------------------------
• [4.350 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:46.987
    Jan 18 22:26:46.987: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sysctl 01/18/23 22:26:46.988
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:47.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:47.252
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 22:26:47.257
    STEP: Watching for error events or started pod 01/18/23 22:26:47.283
    STEP: Waiting for pod completion 01/18/23 22:26:49.29
    Jan 18 22:26:49.290: INFO: Waiting up to 3m0s for pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf" in namespace "sysctl-6392" to be "completed"
    Jan 18 22:26:49.294: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935647ms
    Jan 18 22:26:51.300: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00930189s
    Jan 18 22:26:51.300: INFO: Pod "sysctl-bdf6f8e6-2633-4d01-a606-99d3f181aabf" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/18/23 22:26:51.309
    STEP: Getting logs from the pod 01/18/23 22:26:51.309
    STEP: Checking that the sysctl is actually updated 01/18/23 22:26:51.317
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:51.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-6392" for this suite. 01/18/23 22:26:51.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:51.338
Jan 18 22:26:51.338: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:51.339
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:51.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:51.874
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:51.876
Jan 18 22:26:51.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f" in namespace "downward-api-2932" to be "Succeeded or Failed"
Jan 18 22:26:51.897: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769045ms
Jan 18 22:26:53.902: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008756719s
Jan 18 22:26:55.904: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010497478s
STEP: Saw pod success 01/18/23 22:26:55.904
Jan 18 22:26:55.904: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f" satisfied condition "Succeeded or Failed"
Jan 18 22:26:55.908: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f container client-container: <nil>
STEP: delete the pod 01/18/23 22:26:55.916
Jan 18 22:26:55.950: INFO: Waiting for pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f to disappear
Jan 18 22:26:55.957: INFO: Pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:26:55.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2932" for this suite. 01/18/23 22:26:55.963
------------------------------
• [4.645 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:51.338
    Jan 18 22:26:51.338: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:26:51.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:51.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:51.874
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:26:51.876
    Jan 18 22:26:51.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f" in namespace "downward-api-2932" to be "Succeeded or Failed"
    Jan 18 22:26:51.897: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769045ms
    Jan 18 22:26:53.902: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008756719s
    Jan 18 22:26:55.904: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010497478s
    STEP: Saw pod success 01/18/23 22:26:55.904
    Jan 18 22:26:55.904: INFO: Pod "downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f" satisfied condition "Succeeded or Failed"
    Jan 18 22:26:55.908: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f container client-container: <nil>
    STEP: delete the pod 01/18/23 22:26:55.916
    Jan 18 22:26:55.950: INFO: Waiting for pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f to disappear
    Jan 18 22:26:55.957: INFO: Pod downwardapi-volume-a3cf0c41-1ba1-48ff-b81f-5a8cef3f281f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:26:55.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2932" for this suite. 01/18/23 22:26:55.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:26:55.986
Jan 18 22:26:55.987: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:55.987
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:56.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:56.754
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/18/23 22:26:56.757
Jan 18 22:26:56.791: INFO: Waiting up to 5m0s for pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de" in namespace "var-expansion-1988" to be "Succeeded or Failed"
Jan 18 22:26:56.798: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.737575ms
Jan 18 22:26:58.805: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013874177s
Jan 18 22:27:00.804: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01297621s
STEP: Saw pod success 01/18/23 22:27:00.804
Jan 18 22:27:00.804: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de" satisfied condition "Succeeded or Failed"
Jan 18 22:27:00.809: INFO: Trying to get logs from node test-vm-1 pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de container dapi-container: <nil>
STEP: delete the pod 01/18/23 22:27:00.816
Jan 18 22:27:00.845: INFO: Waiting for pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de to disappear
Jan 18 22:27:00.851: INFO: Pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:00.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1988" for this suite. 01/18/23 22:27:00.856
------------------------------
• [4.883 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:26:55.986
    Jan 18 22:26:55.987: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename var-expansion 01/18/23 22:26:55.987
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:26:56.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:26:56.754
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/18/23 22:26:56.757
    Jan 18 22:26:56.791: INFO: Waiting up to 5m0s for pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de" in namespace "var-expansion-1988" to be "Succeeded or Failed"
    Jan 18 22:26:56.798: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.737575ms
    Jan 18 22:26:58.805: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013874177s
    Jan 18 22:27:00.804: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01297621s
    STEP: Saw pod success 01/18/23 22:27:00.804
    Jan 18 22:27:00.804: INFO: Pod "var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:00.809: INFO: Trying to get logs from node test-vm-1 pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de container dapi-container: <nil>
    STEP: delete the pod 01/18/23 22:27:00.816
    Jan 18 22:27:00.845: INFO: Waiting for pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de to disappear
    Jan 18 22:27:00.851: INFO: Pod var-expansion-201c4668-e3d7-4c58-be81-6d789efef8de no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:00.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1988" for this suite. 01/18/23 22:27:00.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:00.871
Jan 18 22:27:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename subpath 01/18/23 22:27:00.872
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:00.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:00.975
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 22:27:00.978
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-p4zn 01/18/23 22:27:01.008
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:27:01.008
Jan 18 22:27:01.030: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p4zn" in namespace "subpath-6348" to be "Succeeded or Failed"
Jan 18 22:27:01.035: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521851ms
Jan 18 22:27:03.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011041719s
Jan 18 22:27:05.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 4.010395507s
Jan 18 22:27:07.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 6.010004798s
Jan 18 22:27:09.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 8.010929239s
Jan 18 22:27:11.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 10.010221635s
Jan 18 22:27:13.042: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 12.011535562s
Jan 18 22:27:15.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 14.009762441s
Jan 18 22:27:17.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 16.011062278s
Jan 18 22:27:19.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 18.009643535s
Jan 18 22:27:21.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 20.011150333s
Jan 18 22:27:23.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 22.0102891s
Jan 18 22:27:25.039: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=false. Elapsed: 24.009413867s
Jan 18 22:27:27.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010862073s
STEP: Saw pod success 01/18/23 22:27:27.041
Jan 18 22:27:27.041: INFO: Pod "pod-subpath-test-configmap-p4zn" satisfied condition "Succeeded or Failed"
Jan 18 22:27:27.045: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-configmap-p4zn container test-container-subpath-configmap-p4zn: <nil>
STEP: delete the pod 01/18/23 22:27:27.053
Jan 18 22:27:27.104: INFO: Waiting for pod pod-subpath-test-configmap-p4zn to disappear
Jan 18 22:27:27.111: INFO: Pod pod-subpath-test-configmap-p4zn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-p4zn 01/18/23 22:27:27.111
Jan 18 22:27:27.112: INFO: Deleting pod "pod-subpath-test-configmap-p4zn" in namespace "subpath-6348"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan 18 22:27:27.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6348" for this suite. 01/18/23 22:27:27.121
------------------------------
• [SLOW TEST] [26.265 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:00.871
    Jan 18 22:27:00.871: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename subpath 01/18/23 22:27:00.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:00.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:00.975
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 22:27:00.978
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-p4zn 01/18/23 22:27:01.008
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 22:27:01.008
    Jan 18 22:27:01.030: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-p4zn" in namespace "subpath-6348" to be "Succeeded or Failed"
    Jan 18 22:27:01.035: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Pending", Reason="", readiness=false. Elapsed: 4.521851ms
    Jan 18 22:27:03.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011041719s
    Jan 18 22:27:05.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 4.010395507s
    Jan 18 22:27:07.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 6.010004798s
    Jan 18 22:27:09.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 8.010929239s
    Jan 18 22:27:11.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 10.010221635s
    Jan 18 22:27:13.042: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 12.011535562s
    Jan 18 22:27:15.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 14.009762441s
    Jan 18 22:27:17.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 16.011062278s
    Jan 18 22:27:19.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 18.009643535s
    Jan 18 22:27:21.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 20.011150333s
    Jan 18 22:27:23.040: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=true. Elapsed: 22.0102891s
    Jan 18 22:27:25.039: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Running", Reason="", readiness=false. Elapsed: 24.009413867s
    Jan 18 22:27:27.041: INFO: Pod "pod-subpath-test-configmap-p4zn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010862073s
    STEP: Saw pod success 01/18/23 22:27:27.041
    Jan 18 22:27:27.041: INFO: Pod "pod-subpath-test-configmap-p4zn" satisfied condition "Succeeded or Failed"
    Jan 18 22:27:27.045: INFO: Trying to get logs from node test-vm-1 pod pod-subpath-test-configmap-p4zn container test-container-subpath-configmap-p4zn: <nil>
    STEP: delete the pod 01/18/23 22:27:27.053
    Jan 18 22:27:27.104: INFO: Waiting for pod pod-subpath-test-configmap-p4zn to disappear
    Jan 18 22:27:27.111: INFO: Pod pod-subpath-test-configmap-p4zn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-p4zn 01/18/23 22:27:27.111
    Jan 18 22:27:27.112: INFO: Deleting pod "pod-subpath-test-configmap-p4zn" in namespace "subpath-6348"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:27:27.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6348" for this suite. 01/18/23 22:27:27.121
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:27:27.136
Jan 18 22:27:27.136: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 22:27:27.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:27.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:27.682
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 in namespace container-probe-9353 01/18/23 22:27:27.686
Jan 18 22:27:27.715: INFO: Waiting up to 5m0s for pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10" in namespace "container-probe-9353" to be "not pending"
Jan 18 22:27:27.721: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076966ms
Jan 18 22:27:29.727: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10": Phase="Running", Reason="", readiness=true. Elapsed: 2.011843286s
Jan 18 22:27:29.727: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10" satisfied condition "not pending"
Jan 18 22:27:29.727: INFO: Started pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 in namespace container-probe-9353
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:27:29.727
Jan 18 22:27:29.731: INFO: Initial restart count of pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 is 0
STEP: deleting the pod 01/18/23 22:31:30.424
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:30.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9353" for this suite. 01/18/23 22:31:30.48
------------------------------
• [SLOW TEST] [243.369 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:27:27.136
    Jan 18 22:27:27.136: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:27:27.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:27:27.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:27:27.682
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 in namespace container-probe-9353 01/18/23 22:27:27.686
    Jan 18 22:27:27.715: INFO: Waiting up to 5m0s for pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10" in namespace "container-probe-9353" to be "not pending"
    Jan 18 22:27:27.721: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076966ms
    Jan 18 22:27:29.727: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10": Phase="Running", Reason="", readiness=true. Elapsed: 2.011843286s
    Jan 18 22:27:29.727: INFO: Pod "busybox-00e993a7-7584-4c49-a847-aa68faf0ef10" satisfied condition "not pending"
    Jan 18 22:27:29.727: INFO: Started pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 in namespace container-probe-9353
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:27:29.727
    Jan 18 22:27:29.731: INFO: Initial restart count of pod busybox-00e993a7-7584-4c49-a847-aa68faf0ef10 is 0
    STEP: deleting the pod 01/18/23 22:31:30.424
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:30.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9353" for this suite. 01/18/23 22:31:30.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:30.507
Jan 18 22:31:30.507: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:31:30.508
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:30.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:30.874
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/18/23 22:31:30.877
Jan 18 22:31:30.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: mark a version not serverd 01/18/23 22:31:35.04
STEP: check the unserved version gets removed 01/18/23 22:31:35.063
STEP: check the other version is not changed 01/18/23 22:31:36.988
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:40.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2171" for this suite. 01/18/23 22:31:40.405
------------------------------
• [SLOW TEST] [9.910 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:30.507
    Jan 18 22:31:30.507: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 22:31:30.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:30.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:30.874
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/18/23 22:31:30.877
    Jan 18 22:31:30.878: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: mark a version not serverd 01/18/23 22:31:35.04
    STEP: check the unserved version gets removed 01/18/23 22:31:35.063
    STEP: check the other version is not changed 01/18/23 22:31:36.988
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:40.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2171" for this suite. 01/18/23 22:31:40.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:40.418
Jan 18 22:31:40.418: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context-test 01/18/23 22:31:40.419
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:40.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:40.456
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan 18 22:31:40.475: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b" in namespace "security-context-test-6819" to be "Succeeded or Failed"
Jan 18 22:31:40.479: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879844ms
Jan 18 22:31:42.486: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010505989s
Jan 18 22:31:44.486: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010389587s
Jan 18 22:31:46.484: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008701659s
Jan 18 22:31:46.484: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:46.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6819" for this suite. 01/18/23 22:31:46.513
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:40.418
    Jan 18 22:31:40.418: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context-test 01/18/23 22:31:40.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:40.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:40.456
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan 18 22:31:40.475: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b" in namespace "security-context-test-6819" to be "Succeeded or Failed"
    Jan 18 22:31:40.479: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879844ms
    Jan 18 22:31:42.486: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010505989s
    Jan 18 22:31:44.486: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010389587s
    Jan 18 22:31:46.484: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008701659s
    Jan 18 22:31:46.484: INFO: Pod "alpine-nnp-false-705db4ef-3438-430e-a1a3-398d95ad9d1b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:46.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6819" for this suite. 01/18/23 22:31:46.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:46.53
Jan 18 22:31:46.530: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename dns 01/18/23 22:31:46.531
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:46.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:46.574
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/18/23 22:31:46.576
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +notcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_tcp@PTR;sleep 1; done
 01/18/23 22:31:46.621
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +notcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_tcp@PTR;sleep 1; done
 01/18/23 22:31:46.621
STEP: creating a pod to probe DNS 01/18/23 22:31:46.621
STEP: submitting the pod to kubernetes 01/18/23 22:31:46.621
Jan 18 22:31:46.649: INFO: Waiting up to 15m0s for pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b" in namespace "dns-4420" to be "running"
Jan 18 22:31:46.655: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162704ms
Jan 18 22:31:48.662: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012952884s
Jan 18 22:31:50.661: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.012278682s
Jan 18 22:31:50.661: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 22:31:50.661
STEP: looking for the results for each expected name from probers 01/18/23 22:31:50.675
Jan 18 22:31:50.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.686: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.690: INFO: Unable to read wheezy_udp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.712: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.718: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.744: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.761: INFO: Unable to read jessie_udp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.772: INFO: Unable to read jessie_udp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.784: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
Jan 18 22:31:50.818: INFO: Lookups using dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4420 wheezy_tcp@dns-test-service.dns-4420 wheezy_udp@dns-test-service.dns-4420.svc wheezy_tcp@dns-test-service.dns-4420.svc wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4420 jessie_tcp@dns-test-service.dns-4420 jessie_udp@dns-test-service.dns-4420.svc jessie_tcp@dns-test-service.dns-4420.svc jessie_udp@_http._tcp.dns-test-service.dns-4420.svc jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc]

Jan 18 22:31:55.942: INFO: DNS probes using dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b succeeded

STEP: deleting the pod 01/18/23 22:31:55.943
STEP: deleting the test service 01/18/23 22:31:56.125
STEP: deleting the test headless service 01/18/23 22:31:56.211
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan 18 22:31:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4420" for this suite. 01/18/23 22:31:56.25
------------------------------
• [SLOW TEST] [9.747 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:46.53
    Jan 18 22:31:46.530: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename dns 01/18/23 22:31:46.531
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:46.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:46.574
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/18/23 22:31:46.576
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +notcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_tcp@PTR;sleep 1; done
     01/18/23 22:31:46.621
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4420;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4420;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4420.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4420.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4420.svc;check="$$(dig +notcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_udp@PTR;check="$$(dig +tcp +noall +answer +search 184.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.184_tcp@PTR;sleep 1; done
     01/18/23 22:31:46.621
    STEP: creating a pod to probe DNS 01/18/23 22:31:46.621
    STEP: submitting the pod to kubernetes 01/18/23 22:31:46.621
    Jan 18 22:31:46.649: INFO: Waiting up to 15m0s for pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b" in namespace "dns-4420" to be "running"
    Jan 18 22:31:46.655: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162704ms
    Jan 18 22:31:48.662: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012952884s
    Jan 18 22:31:50.661: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b": Phase="Running", Reason="", readiness=true. Elapsed: 4.012278682s
    Jan 18 22:31:50.661: INFO: Pod "dns-test-d0929293-5550-45a9-82dc-110b69126c5b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 22:31:50.661
    STEP: looking for the results for each expected name from probers 01/18/23 22:31:50.675
    Jan 18 22:31:50.681: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.686: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.690: INFO: Unable to read wheezy_udp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.695: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.699: INFO: Unable to read wheezy_udp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.707: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.712: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.718: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.744: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.761: INFO: Unable to read jessie_udp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.765: INFO: Unable to read jessie_tcp@dns-test-service.dns-4420 from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.772: INFO: Unable to read jessie_udp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.778: INFO: Unable to read jessie_tcp@dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.784: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.796: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc from pod dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b: the server could not find the requested resource (get pods dns-test-d0929293-5550-45a9-82dc-110b69126c5b)
    Jan 18 22:31:50.818: INFO: Lookups using dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4420 wheezy_tcp@dns-test-service.dns-4420 wheezy_udp@dns-test-service.dns-4420.svc wheezy_tcp@dns-test-service.dns-4420.svc wheezy_udp@_http._tcp.dns-test-service.dns-4420.svc wheezy_tcp@_http._tcp.dns-test-service.dns-4420.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4420 jessie_tcp@dns-test-service.dns-4420 jessie_udp@dns-test-service.dns-4420.svc jessie_tcp@dns-test-service.dns-4420.svc jessie_udp@_http._tcp.dns-test-service.dns-4420.svc jessie_tcp@_http._tcp.dns-test-service.dns-4420.svc]

    Jan 18 22:31:55.942: INFO: DNS probes using dns-4420/dns-test-d0929293-5550-45a9-82dc-110b69126c5b succeeded

    STEP: deleting the pod 01/18/23 22:31:55.943
    STEP: deleting the test service 01/18/23 22:31:56.125
    STEP: deleting the test headless service 01/18/23 22:31:56.211
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:31:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4420" for this suite. 01/18/23 22:31:56.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:31:56.279
Jan 18 22:31:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename sched-pred 01/18/23 22:31:56.281
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:56.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:56.319
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan 18 22:31:56.322: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 22:31:56.330: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 22:31:56.334: INFO: 
Logging pods the apiserver thinks is on node test-vm-2 before test
Jan 18 22:31:56.343: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
Jan 18 22:31:56.343: INFO: 	Container coredns ready: true, restart count 0
Jan 18 22:31:56.343: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:31:56.343: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:31:56.343: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 22:31:56.343: INFO: 	Container e2e ready: true, restart count 0
Jan 18 22:31:56.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:31:56.343: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 22:31:56.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:31:56.343: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:31:56.343: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
Jan 18 22:31:56.343: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 22:31:56.343: INFO: 
Logging pods the apiserver thinks is on node test-vm-1 before test
Jan 18 22:31:56.350: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
Jan 18 22:31:56.350: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 22:31:56.350: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
Jan 18 22:31:56.350: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 22:31:56.350: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 22:31:56.350: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
Jan 18 22:31:56.350: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:31:56.35
Jan 18 22:31:56.366: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2780" to be "running"
Jan 18 22:31:56.370: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701654ms
Jan 18 22:31:58.376: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009530025s
Jan 18 22:32:00.378: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011131346s
Jan 18 22:32:00.378: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:32:00.382
STEP: Trying to apply a random label on the found node. 01/18/23 22:32:00.409
STEP: verifying the node has the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 42 01/18/23 22:32:00.423
STEP: Trying to relaunch the pod, now with labels. 01/18/23 22:32:00.428
Jan 18 22:32:00.439: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2780" to be "not pending"
Jan 18 22:32:00.443: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.274059ms
Jan 18 22:32:02.448: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009618631s
Jan 18 22:32:04.447: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.008796318s
Jan 18 22:32:04.447: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 off the node test-vm-1 01/18/23 22:32:04.451
STEP: verifying the node doesn't have the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 01/18/23 22:32:04.469
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:04.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2780" for this suite. 01/18/23 22:32:04.478
------------------------------
• [SLOW TEST] [8.214 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:31:56.279
    Jan 18 22:31:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename sched-pred 01/18/23 22:31:56.281
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:31:56.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:31:56.319
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan 18 22:31:56.322: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 22:31:56.330: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 22:31:56.334: INFO: 
    Logging pods the apiserver thinks is on node test-vm-2 before test
    Jan 18 22:31:56.343: INFO: coredns-6f5f9b5d74-j2j8v from kube-system started at 2023-01-18 20:58:30 +0000 UTC (1 container statuses recorded)
    Jan 18 22:31:56.343: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: calico-node-kv49n from kube-system started at 2023-01-18 20:58:22 +0000 UTC (1 container statuses recorded)
    Jan 18 22:31:56.343: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: sonobuoy-e2e-job-0fc7392783254edf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 22:31:56.343: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-t6469 from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 22:31:56.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: calico-kube-controllers-5b6fd6b6d-mmvrs from kube-system started at 2023-01-18 21:17:22 +0000 UTC (1 container statuses recorded)
    Jan 18 22:31:56.343: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 22:31:56.343: INFO: 
    Logging pods the apiserver thinks is on node test-vm-1 before test
    Jan 18 22:31:56.350: INFO: calico-node-j9rg6 from kube-system started at 2023-01-18 20:57:28 +0000 UTC (1 container statuses recorded)
    Jan 18 22:31:56.350: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 22:31:56.350: INFO: sonobuoy-systemd-logs-daemon-set-5e50bc9062424429-tdsxf from sonobuoy started at 2023-01-18 21:00:28 +0000 UTC (2 container statuses recorded)
    Jan 18 22:31:56.350: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 22:31:56.350: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 22:31:56.350: INFO: sonobuoy from sonobuoy started at 2023-01-18 21:00:17 +0000 UTC (1 container statuses recorded)
    Jan 18 22:31:56.350: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 22:31:56.35
    Jan 18 22:31:56.366: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2780" to be "running"
    Jan 18 22:31:56.370: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701654ms
    Jan 18 22:31:58.376: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009530025s
    Jan 18 22:32:00.378: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.011131346s
    Jan 18 22:32:00.378: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 22:32:00.382
    STEP: Trying to apply a random label on the found node. 01/18/23 22:32:00.409
    STEP: verifying the node has the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 42 01/18/23 22:32:00.423
    STEP: Trying to relaunch the pod, now with labels. 01/18/23 22:32:00.428
    Jan 18 22:32:00.439: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2780" to be "not pending"
    Jan 18 22:32:00.443: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.274059ms
    Jan 18 22:32:02.448: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009618631s
    Jan 18 22:32:04.447: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.008796318s
    Jan 18 22:32:04.447: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 off the node test-vm-1 01/18/23 22:32:04.451
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-662168de-2e41-45dc-b156-4ae10c879f45 01/18/23 22:32:04.469
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:04.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2780" for this suite. 01/18/23 22:32:04.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:04.497
Jan 18 22:32:04.498: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 22:32:04.499
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:04.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:04.596
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/18/23 22:32:04.598
STEP: submitting the pod to kubernetes 01/18/23 22:32:04.598
Jan 18 22:32:04.616: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" in namespace "pods-9096" to be "running and ready"
Jan 18 22:32:04.620: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208657ms
Jan 18 22:32:04.620: INFO: The phase of Pod pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:32:06.626: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010405701s
Jan 18 22:32:06.626: INFO: The phase of Pod pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d is Running (Ready = true)
Jan 18 22:32:06.626: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 22:32:06.631
STEP: updating the pod 01/18/23 22:32:06.636
Jan 18 22:32:07.156: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d"
Jan 18 22:32:07.156: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" in namespace "pods-9096" to be "terminated with reason DeadlineExceeded"
Jan 18 22:32:07.160: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 3.885043ms
Jan 18 22:32:09.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009055655s
Jan 18 22:32:11.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=false. Elapsed: 4.00898431s
Jan 18 22:32:13.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009440261s
Jan 18 22:32:13.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:13.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9096" for this suite. 01/18/23 22:32:13.171
------------------------------
• [SLOW TEST] [8.687 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:04.497
    Jan 18 22:32:04.498: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 22:32:04.499
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:04.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:04.596
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/18/23 22:32:04.598
    STEP: submitting the pod to kubernetes 01/18/23 22:32:04.598
    Jan 18 22:32:04.616: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" in namespace "pods-9096" to be "running and ready"
    Jan 18 22:32:04.620: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208657ms
    Jan 18 22:32:04.620: INFO: The phase of Pod pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:32:06.626: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010405701s
    Jan 18 22:32:06.626: INFO: The phase of Pod pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d is Running (Ready = true)
    Jan 18 22:32:06.626: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 22:32:06.631
    STEP: updating the pod 01/18/23 22:32:06.636
    Jan 18 22:32:07.156: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d"
    Jan 18 22:32:07.156: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" in namespace "pods-9096" to be "terminated with reason DeadlineExceeded"
    Jan 18 22:32:07.160: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 3.885043ms
    Jan 18 22:32:09.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009055655s
    Jan 18 22:32:11.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Running", Reason="", readiness=false. Elapsed: 4.00898431s
    Jan 18 22:32:13.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.009440261s
    Jan 18 22:32:13.166: INFO: Pod "pod-update-activedeadlineseconds-ef8ccd24-c30e-45fd-803b-cbb28a0f0e1d" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:13.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9096" for this suite. 01/18/23 22:32:13.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:13.189
Jan 18 22:32:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename daemonsets 01/18/23 22:32:13.19
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:13.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:13.229
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan 18 22:32:13.258: INFO: Create a RollingUpdate DaemonSet
Jan 18 22:32:13.269: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 22:32:13.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:32:13.279: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 22:32:14.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:32:14.292: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
Jan 18 22:32:15.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 22:32:15.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 18 22:32:15.291: INFO: Update the DaemonSet to trigger a rollout
Jan 18 22:32:15.331: INFO: Updating DaemonSet daemon-set
Jan 18 22:32:18.352: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 22:32:18.372: INFO: Updating DaemonSet daemon-set
Jan 18 22:32:18.372: INFO: Make sure DaemonSet rollback is complete
Jan 18 22:32:18.378: INFO: Wrong image for pod: daemon-set-cqlp5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan 18 22:32:18.378: INFO: Pod daemon-set-cqlp5 is not available
Jan 18 22:32:21.391: INFO: Pod daemon-set-pjj4r is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:32:21.404
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-612, will wait for the garbage collector to delete the pods 01/18/23 22:32:21.404
Jan 18 22:32:21.472: INFO: Deleting DaemonSet.extensions daemon-set took: 14.52048ms
Jan 18 22:32:21.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732245ms
Jan 18 22:32:24.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 22:32:24.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 22:32:24.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33441"},"items":null}

Jan 18 22:32:24.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33441"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:24.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-612" for this suite. 01/18/23 22:32:24.812
------------------------------
• [SLOW TEST] [11.638 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:13.189
    Jan 18 22:32:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename daemonsets 01/18/23 22:32:13.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:13.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:13.229
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan 18 22:32:13.258: INFO: Create a RollingUpdate DaemonSet
    Jan 18 22:32:13.269: INFO: Check that daemon pods launch on every node of the cluster
    Jan 18 22:32:13.279: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:32:13.279: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 22:32:14.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:32:14.292: INFO: Node test-vm-2 is running 0 daemon pod, expected 1
    Jan 18 22:32:15.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 22:32:15.291: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 18 22:32:15.291: INFO: Update the DaemonSet to trigger a rollout
    Jan 18 22:32:15.331: INFO: Updating DaemonSet daemon-set
    Jan 18 22:32:18.352: INFO: Roll back the DaemonSet before rollout is complete
    Jan 18 22:32:18.372: INFO: Updating DaemonSet daemon-set
    Jan 18 22:32:18.372: INFO: Make sure DaemonSet rollback is complete
    Jan 18 22:32:18.378: INFO: Wrong image for pod: daemon-set-cqlp5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan 18 22:32:18.378: INFO: Pod daemon-set-cqlp5 is not available
    Jan 18 22:32:21.391: INFO: Pod daemon-set-pjj4r is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 22:32:21.404
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-612, will wait for the garbage collector to delete the pods 01/18/23 22:32:21.404
    Jan 18 22:32:21.472: INFO: Deleting DaemonSet.extensions daemon-set took: 14.52048ms
    Jan 18 22:32:21.573: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.732245ms
    Jan 18 22:32:24.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 22:32:24.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 22:32:24.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33441"},"items":null}

    Jan 18 22:32:24.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33441"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:24.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-612" for this suite. 01/18/23 22:32:24.812
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:24.83
Jan 18 22:32:24.830: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:32:24.831
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:24.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:24.866
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-75068a27-4733-4848-bbff-69d376a1dad0 01/18/23 22:32:24.869
STEP: Creating a pod to test consume secrets 01/18/23 22:32:24.909
Jan 18 22:32:24.933: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d" in namespace "projected-9348" to be "Succeeded or Failed"
Jan 18 22:32:24.938: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310853ms
Jan 18 22:32:26.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010177157s
Jan 18 22:32:28.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.010850696s
Jan 18 22:32:30.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010433343s
STEP: Saw pod success 01/18/23 22:32:30.944
Jan 18 22:32:30.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d" satisfied condition "Succeeded or Failed"
Jan 18 22:32:30.949: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:32:30.957
Jan 18 22:32:31.009: INFO: Waiting for pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d to disappear
Jan 18 22:32:31.019: INFO: Pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:31.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9348" for this suite. 01/18/23 22:32:31.024
------------------------------
• [SLOW TEST] [6.226 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:24.83
    Jan 18 22:32:24.830: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:32:24.831
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:24.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:24.866
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-75068a27-4733-4848-bbff-69d376a1dad0 01/18/23 22:32:24.869
    STEP: Creating a pod to test consume secrets 01/18/23 22:32:24.909
    Jan 18 22:32:24.933: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d" in namespace "projected-9348" to be "Succeeded or Failed"
    Jan 18 22:32:24.938: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310853ms
    Jan 18 22:32:26.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010177157s
    Jan 18 22:32:28.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Running", Reason="", readiness=false. Elapsed: 4.010850696s
    Jan 18 22:32:30.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010433343s
    STEP: Saw pod success 01/18/23 22:32:30.944
    Jan 18 22:32:30.944: INFO: Pod "pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d" satisfied condition "Succeeded or Failed"
    Jan 18 22:32:30.949: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:32:30.957
    Jan 18 22:32:31.009: INFO: Waiting for pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d to disappear
    Jan 18 22:32:31.019: INFO: Pod pod-projected-secrets-605eca5e-f076-44b0-af70-c632fa609e8d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:31.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9348" for this suite. 01/18/23 22:32:31.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:31.059
Jan 18 22:32:31.059: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 22:32:31.061
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:31.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:31.228
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 22:32:31.231
Jan 18 22:32:31.267: INFO: Waiting up to 5m0s for pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5" in namespace "emptydir-2628" to be "Succeeded or Failed"
Jan 18 22:32:31.271: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83465ms
Jan 18 22:32:33.276: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653833s
Jan 18 22:32:35.276: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009447961s
STEP: Saw pod success 01/18/23 22:32:35.276
Jan 18 22:32:35.277: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5" satisfied condition "Succeeded or Failed"
Jan 18 22:32:35.280: INFO: Trying to get logs from node test-vm-1 pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 container test-container: <nil>
STEP: delete the pod 01/18/23 22:32:35.288
Jan 18 22:32:35.351: INFO: Waiting for pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 to disappear
Jan 18 22:32:35.358: INFO: Pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2628" for this suite. 01/18/23 22:32:35.363
------------------------------
• [4.374 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:31.059
    Jan 18 22:32:31.059: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:32:31.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:31.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:31.228
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 22:32:31.231
    Jan 18 22:32:31.267: INFO: Waiting up to 5m0s for pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5" in namespace "emptydir-2628" to be "Succeeded or Failed"
    Jan 18 22:32:31.271: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83465ms
    Jan 18 22:32:33.276: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008653833s
    Jan 18 22:32:35.276: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009447961s
    STEP: Saw pod success 01/18/23 22:32:35.276
    Jan 18 22:32:35.277: INFO: Pod "pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5" satisfied condition "Succeeded or Failed"
    Jan 18 22:32:35.280: INFO: Trying to get logs from node test-vm-1 pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:32:35.288
    Jan 18 22:32:35.351: INFO: Waiting for pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 to disappear
    Jan 18 22:32:35.358: INFO: Pod pod-a9e8a459-264a-485f-a2a8-3dbe1fa844e5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:35.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2628" for this suite. 01/18/23 22:32:35.363
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:35.434
Jan 18 22:32:35.434: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename security-context 01/18/23 22:32:35.436
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:35.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:35.483
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:32:35.486
Jan 18 22:32:35.530: INFO: Waiting up to 5m0s for pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54" in namespace "security-context-2695" to be "Succeeded or Failed"
Jan 18 22:32:35.535: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426859ms
Jan 18 22:32:37.540: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009129739s
Jan 18 22:32:39.540: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009833187s
Jan 18 22:32:41.541: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010669537s
STEP: Saw pod success 01/18/23 22:32:41.541
Jan 18 22:32:41.541: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54" satisfied condition "Succeeded or Failed"
Jan 18 22:32:41.545: INFO: Trying to get logs from node test-vm-1 pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 container test-container: <nil>
STEP: delete the pod 01/18/23 22:32:41.553
Jan 18 22:32:41.590: INFO: Waiting for pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 to disappear
Jan 18 22:32:41.596: INFO: Pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:41.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2695" for this suite. 01/18/23 22:32:41.601
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:35.434
    Jan 18 22:32:35.434: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename security-context 01/18/23 22:32:35.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:35.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:35.483
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 22:32:35.486
    Jan 18 22:32:35.530: INFO: Waiting up to 5m0s for pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54" in namespace "security-context-2695" to be "Succeeded or Failed"
    Jan 18 22:32:35.535: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.426859ms
    Jan 18 22:32:37.540: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009129739s
    Jan 18 22:32:39.540: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009833187s
    Jan 18 22:32:41.541: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010669537s
    STEP: Saw pod success 01/18/23 22:32:41.541
    Jan 18 22:32:41.541: INFO: Pod "security-context-45469459-a77d-4f55-bf29-e0559400dc54" satisfied condition "Succeeded or Failed"
    Jan 18 22:32:41.545: INFO: Trying to get logs from node test-vm-1 pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:32:41.553
    Jan 18 22:32:41.590: INFO: Waiting for pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 to disappear
    Jan 18 22:32:41.596: INFO: Pod security-context-45469459-a77d-4f55-bf29-e0559400dc54 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:41.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2695" for this suite. 01/18/23 22:32:41.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:41.618
Jan 18 22:32:41.618: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename watch 01/18/23 22:32:41.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:41.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:41.695
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/18/23 22:32:41.729
STEP: modifying the configmap once 01/18/23 22:32:41.74
STEP: modifying the configmap a second time 01/18/23 22:32:41.8
STEP: deleting the configmap 01/18/23 22:32:41.878
STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 22:32:41.891
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 22:32:41.892
Jan 18 22:32:41.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-109  2cf53451-b263-4277-8249-84156efe57c4 33590 0 2023-01-18 22:32:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:32:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 22:32:41.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-109  2cf53451-b263-4277-8249-84156efe57c4 33591 0 2023-01-18 22:32:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:32:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-109" for this suite. 01/18/23 22:32:41.897
------------------------------
• [0.317 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:41.618
    Jan 18 22:32:41.618: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename watch 01/18/23 22:32:41.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:41.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:41.695
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/18/23 22:32:41.729
    STEP: modifying the configmap once 01/18/23 22:32:41.74
    STEP: modifying the configmap a second time 01/18/23 22:32:41.8
    STEP: deleting the configmap 01/18/23 22:32:41.878
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 22:32:41.891
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 22:32:41.892
    Jan 18 22:32:41.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-109  2cf53451-b263-4277-8249-84156efe57c4 33590 0 2023-01-18 22:32:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:32:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 22:32:41.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-109  2cf53451-b263-4277-8249-84156efe57c4 33591 0 2023-01-18 22:32:41 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 22:32:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:41.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-109" for this suite. 01/18/23 22:32:41.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:41.938
Jan 18 22:32:41.938: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:32:41.939
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.069
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:32:42.072
Jan 18 22:32:42.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-6095 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan 18 22:32:42.234: INFO: stderr: ""
Jan 18 22:32:42.234: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:32:42.234
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan 18 22:32:42.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-6095 delete pods e2e-test-httpd-pod'
Jan 18 22:32:42.429: INFO: stderr: ""
Jan 18 22:32:42.429: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:42.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6095" for this suite. 01/18/23 22:32:42.436
------------------------------
• [0.530 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:41.938
    Jan 18 22:32:41.938: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:32:41.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.069
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/18/23 22:32:42.072
    Jan 18 22:32:42.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-6095 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan 18 22:32:42.234: INFO: stderr: ""
    Jan 18 22:32:42.234: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 22:32:42.234
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan 18 22:32:42.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-6095 delete pods e2e-test-httpd-pod'
    Jan 18 22:32:42.429: INFO: stderr: ""
    Jan 18 22:32:42.429: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:42.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6095" for this suite. 01/18/23 22:32:42.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:42.469
Jan 18 22:32:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 22:32:42.47
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.514
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/18/23 22:32:42.517
STEP: getting /apis/storage.k8s.io 01/18/23 22:32:42.519
STEP: getting /apis/storage.k8s.io/v1 01/18/23 22:32:42.519
STEP: creating 01/18/23 22:32:42.52
STEP: watching 01/18/23 22:32:42.583
Jan 18 22:32:42.583: INFO: starting watch
STEP: getting 01/18/23 22:32:42.595
STEP: listing in namespace 01/18/23 22:32:42.598
STEP: listing across namespaces 01/18/23 22:32:42.602
STEP: patching 01/18/23 22:32:42.605
STEP: updating 01/18/23 22:32:42.65
Jan 18 22:32:42.672: INFO: waiting for watch events with expected annotations in namespace
Jan 18 22:32:42.672: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/18/23 22:32:42.672
STEP: deleting a collection 01/18/23 22:32:42.697
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-302" for this suite. 01/18/23 22:32:42.756
------------------------------
• [0.328 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:42.469
    Jan 18 22:32:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 22:32:42.47
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.514
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/18/23 22:32:42.517
    STEP: getting /apis/storage.k8s.io 01/18/23 22:32:42.519
    STEP: getting /apis/storage.k8s.io/v1 01/18/23 22:32:42.519
    STEP: creating 01/18/23 22:32:42.52
    STEP: watching 01/18/23 22:32:42.583
    Jan 18 22:32:42.583: INFO: starting watch
    STEP: getting 01/18/23 22:32:42.595
    STEP: listing in namespace 01/18/23 22:32:42.598
    STEP: listing across namespaces 01/18/23 22:32:42.602
    STEP: patching 01/18/23 22:32:42.605
    STEP: updating 01/18/23 22:32:42.65
    Jan 18 22:32:42.672: INFO: waiting for watch events with expected annotations in namespace
    Jan 18 22:32:42.672: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/18/23 22:32:42.672
    STEP: deleting a collection 01/18/23 22:32:42.697
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:42.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-302" for this suite. 01/18/23 22:32:42.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:42.797
Jan 18 22:32:42.798: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replicaset 01/18/23 22:32:42.799
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.852
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/18/23 22:32:42.856
STEP: Verify that the required pods have come up 01/18/23 22:32:42.885
Jan 18 22:32:42.892: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 22:32:47.900: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/18/23 22:32:47.9
Jan 18 22:32:47.904: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/18/23 22:32:47.904
STEP: DeleteCollection of the ReplicaSets 01/18/23 22:32:47.909
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 22:32:47.93
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:47.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9896" for this suite. 01/18/23 22:32:47.942
------------------------------
• [SLOW TEST] [5.195 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:42.797
    Jan 18 22:32:42.798: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replicaset 01/18/23 22:32:42.799
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:42.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:42.852
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/18/23 22:32:42.856
    STEP: Verify that the required pods have come up 01/18/23 22:32:42.885
    Jan 18 22:32:42.892: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 18 22:32:47.900: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/18/23 22:32:47.9
    Jan 18 22:32:47.904: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/18/23 22:32:47.904
    STEP: DeleteCollection of the ReplicaSets 01/18/23 22:32:47.909
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 22:32:47.93
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:47.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9896" for this suite. 01/18/23 22:32:47.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:47.995
Jan 18 22:32:47.995: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:32:47.996
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:48.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:48.077
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3177 01/18/23 22:32:48.081
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:32:48.115
STEP: creating service externalsvc in namespace services-3177 01/18/23 22:32:48.116
STEP: creating replication controller externalsvc in namespace services-3177 01/18/23 22:32:48.155
I0118 22:32:48.169968      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3177, replica count: 2
I0118 22:32:51.221830      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/18/23 22:32:51.226
Jan 18 22:32:51.260: INFO: Creating new exec pod
Jan 18 22:32:51.272: INFO: Waiting up to 5m0s for pod "execpod88gcc" in namespace "services-3177" to be "running"
Jan 18 22:32:51.276: INFO: Pod "execpod88gcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79915ms
Jan 18 22:32:53.281: INFO: Pod "execpod88gcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008596948s
Jan 18 22:32:53.281: INFO: Pod "execpod88gcc" satisfied condition "running"
Jan 18 22:32:53.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-3177 exec execpod88gcc -- /bin/sh -x -c nslookup nodeport-service.services-3177.svc.cluster.local'
Jan 18 22:32:53.543: INFO: stderr: "+ nslookup nodeport-service.services-3177.svc.cluster.local\n"
Jan 18 22:32:53.543: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nnodeport-service.services-3177.svc.cluster.local\tcanonical name = externalsvc.services-3177.svc.cluster.local.\nName:\texternalsvc.services-3177.svc.cluster.local\nAddress: 10.152.183.81\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3177, will wait for the garbage collector to delete the pods 01/18/23 22:32:53.543
Jan 18 22:32:53.613: INFO: Deleting ReplicationController externalsvc took: 14.28284ms
Jan 18 22:32:53.713: INFO: Terminating ReplicationController externalsvc pods took: 100.404093ms
Jan 18 22:32:56.564: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:32:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3177" for this suite. 01/18/23 22:32:56.6
------------------------------
• [SLOW TEST] [8.622 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:47.995
    Jan 18 22:32:47.995: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:32:47.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:48.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:48.077
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3177 01/18/23 22:32:48.081
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 22:32:48.115
    STEP: creating service externalsvc in namespace services-3177 01/18/23 22:32:48.116
    STEP: creating replication controller externalsvc in namespace services-3177 01/18/23 22:32:48.155
    I0118 22:32:48.169968      20 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3177, replica count: 2
    I0118 22:32:51.221830      20 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/18/23 22:32:51.226
    Jan 18 22:32:51.260: INFO: Creating new exec pod
    Jan 18 22:32:51.272: INFO: Waiting up to 5m0s for pod "execpod88gcc" in namespace "services-3177" to be "running"
    Jan 18 22:32:51.276: INFO: Pod "execpod88gcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79915ms
    Jan 18 22:32:53.281: INFO: Pod "execpod88gcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008596948s
    Jan 18 22:32:53.281: INFO: Pod "execpod88gcc" satisfied condition "running"
    Jan 18 22:32:53.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-3177 exec execpod88gcc -- /bin/sh -x -c nslookup nodeport-service.services-3177.svc.cluster.local'
    Jan 18 22:32:53.543: INFO: stderr: "+ nslookup nodeport-service.services-3177.svc.cluster.local\n"
    Jan 18 22:32:53.543: INFO: stdout: "Server:\t\t10.152.183.10\nAddress:\t10.152.183.10#53\n\nnodeport-service.services-3177.svc.cluster.local\tcanonical name = externalsvc.services-3177.svc.cluster.local.\nName:\texternalsvc.services-3177.svc.cluster.local\nAddress: 10.152.183.81\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3177, will wait for the garbage collector to delete the pods 01/18/23 22:32:53.543
    Jan 18 22:32:53.613: INFO: Deleting ReplicationController externalsvc took: 14.28284ms
    Jan 18 22:32:53.713: INFO: Terminating ReplicationController externalsvc pods took: 100.404093ms
    Jan 18 22:32:56.564: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:32:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3177" for this suite. 01/18/23 22:32:56.6
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:32:56.618
Jan 18 22:32:56.618: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename kubectl 01/18/23 22:32:56.619
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:56.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:56.657
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/18/23 22:32:56.659
Jan 18 22:32:56.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 create -f -'
Jan 18 22:32:57.376: INFO: stderr: ""
Jan 18 22:32:57.377: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:32:57.377
Jan 18 22:32:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:32:57.463: INFO: stderr: ""
Jan 18 22:32:57.463: INFO: stdout: ""
STEP: Replicas for name=update-demo: expected=2 actual=0 01/18/23 22:32:57.463
Jan 18 22:33:02.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:33:02.571: INFO: stderr: ""
Jan 18 22:33:02.571: INFO: stdout: "update-demo-nautilus-5tbfd update-demo-nautilus-scdrl "
Jan 18 22:33:02.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-5tbfd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:02.676: INFO: stderr: ""
Jan 18 22:33:02.676: INFO: stdout: "true"
Jan 18 22:33:02.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-5tbfd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:02.768: INFO: stderr: ""
Jan 18 22:33:02.768: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:02.768: INFO: validating pod update-demo-nautilus-5tbfd
Jan 18 22:33:02.776: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:02.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:02.776: INFO: update-demo-nautilus-5tbfd is verified up and running
Jan 18 22:33:02.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:02.865: INFO: stderr: ""
Jan 18 22:33:02.865: INFO: stdout: "true"
Jan 18 22:33:02.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:02.980: INFO: stderr: ""
Jan 18 22:33:02.980: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:02.980: INFO: validating pod update-demo-nautilus-scdrl
Jan 18 22:33:02.997: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:02.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:02.997: INFO: update-demo-nautilus-scdrl is verified up and running
STEP: scaling down the replication controller 01/18/23 22:33:02.997
Jan 18 22:33:02.999: INFO: scanned /root for discovery docs: <nil>
Jan 18 22:33:02.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 22:33:04.153: INFO: stderr: ""
Jan 18 22:33:04.153: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:33:04.153
Jan 18 22:33:04.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:33:04.258: INFO: stderr: ""
Jan 18 22:33:04.258: INFO: stdout: "update-demo-nautilus-scdrl "
Jan 18 22:33:04.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:04.353: INFO: stderr: ""
Jan 18 22:33:04.353: INFO: stdout: "true"
Jan 18 22:33:04.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:04.438: INFO: stderr: ""
Jan 18 22:33:04.438: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:04.438: INFO: validating pod update-demo-nautilus-scdrl
Jan 18 22:33:04.445: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:04.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:04.445: INFO: update-demo-nautilus-scdrl is verified up and running
STEP: scaling up the replication controller 01/18/23 22:33:04.445
Jan 18 22:33:04.447: INFO: scanned /root for discovery docs: <nil>
Jan 18 22:33:04.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 22:33:05.562: INFO: stderr: ""
Jan 18 22:33:05.562: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:33:05.562
Jan 18 22:33:05.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:33:05.669: INFO: stderr: ""
Jan 18 22:33:05.669: INFO: stdout: "update-demo-nautilus-scdrl update-demo-nautilus-49xxc "
Jan 18 22:33:05.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:05.757: INFO: stderr: ""
Jan 18 22:33:05.757: INFO: stdout: "true"
Jan 18 22:33:05.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:05.835: INFO: stderr: ""
Jan 18 22:33:05.835: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:05.835: INFO: validating pod update-demo-nautilus-scdrl
Jan 18 22:33:05.840: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:05.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:05.840: INFO: update-demo-nautilus-scdrl is verified up and running
Jan 18 22:33:05.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:05.919: INFO: stderr: ""
Jan 18 22:33:05.919: INFO: stdout: ""
Jan 18 22:33:05.919: INFO: update-demo-nautilus-49xxc is created but not running
Jan 18 22:33:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 22:33:11.055: INFO: stderr: ""
Jan 18 22:33:11.055: INFO: stdout: "update-demo-nautilus-scdrl update-demo-nautilus-49xxc "
Jan 18 22:33:11.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:11.168: INFO: stderr: ""
Jan 18 22:33:11.168: INFO: stdout: "true"
Jan 18 22:33:11.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:11.263: INFO: stderr: ""
Jan 18 22:33:11.263: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:11.263: INFO: validating pod update-demo-nautilus-scdrl
Jan 18 22:33:11.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:11.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:11.268: INFO: update-demo-nautilus-scdrl is verified up and running
Jan 18 22:33:11.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 22:33:11.352: INFO: stderr: ""
Jan 18 22:33:11.352: INFO: stdout: "true"
Jan 18 22:33:11.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 22:33:11.431: INFO: stderr: ""
Jan 18 22:33:11.431: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan 18 22:33:11.431: INFO: validating pod update-demo-nautilus-49xxc
Jan 18 22:33:11.437: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 22:33:11.437: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 22:33:11.437: INFO: update-demo-nautilus-49xxc is verified up and running
STEP: using delete to clean up resources 01/18/23 22:33:11.437
Jan 18 22:33:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 delete --grace-period=0 --force -f -'
Jan 18 22:33:11.533: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 22:33:11.533: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 22:33:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get rc,svc -l name=update-demo --no-headers'
Jan 18 22:33:11.665: INFO: stderr: "No resources found in kubectl-4010 namespace.\n"
Jan 18 22:33:11.665: INFO: stdout: ""
Jan 18 22:33:11.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 22:33:11.789: INFO: stderr: ""
Jan 18 22:33:11.789: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:11.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4010" for this suite. 01/18/23 22:33:11.801
------------------------------
• [SLOW TEST] [15.200 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:32:56.618
    Jan 18 22:32:56.618: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename kubectl 01/18/23 22:32:56.619
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:32:56.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:32:56.657
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/18/23 22:32:56.659
    Jan 18 22:32:56.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 create -f -'
    Jan 18 22:32:57.376: INFO: stderr: ""
    Jan 18 22:32:57.377: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:32:57.377
    Jan 18 22:32:57.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:32:57.463: INFO: stderr: ""
    Jan 18 22:32:57.463: INFO: stdout: ""
    STEP: Replicas for name=update-demo: expected=2 actual=0 01/18/23 22:32:57.463
    Jan 18 22:33:02.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:33:02.571: INFO: stderr: ""
    Jan 18 22:33:02.571: INFO: stdout: "update-demo-nautilus-5tbfd update-demo-nautilus-scdrl "
    Jan 18 22:33:02.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-5tbfd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:02.676: INFO: stderr: ""
    Jan 18 22:33:02.676: INFO: stdout: "true"
    Jan 18 22:33:02.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-5tbfd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:02.768: INFO: stderr: ""
    Jan 18 22:33:02.768: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:02.768: INFO: validating pod update-demo-nautilus-5tbfd
    Jan 18 22:33:02.776: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:02.776: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:02.776: INFO: update-demo-nautilus-5tbfd is verified up and running
    Jan 18 22:33:02.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:02.865: INFO: stderr: ""
    Jan 18 22:33:02.865: INFO: stdout: "true"
    Jan 18 22:33:02.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:02.980: INFO: stderr: ""
    Jan 18 22:33:02.980: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:02.980: INFO: validating pod update-demo-nautilus-scdrl
    Jan 18 22:33:02.997: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:02.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:02.997: INFO: update-demo-nautilus-scdrl is verified up and running
    STEP: scaling down the replication controller 01/18/23 22:33:02.997
    Jan 18 22:33:02.999: INFO: scanned /root for discovery docs: <nil>
    Jan 18 22:33:02.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 18 22:33:04.153: INFO: stderr: ""
    Jan 18 22:33:04.153: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:33:04.153
    Jan 18 22:33:04.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:33:04.258: INFO: stderr: ""
    Jan 18 22:33:04.258: INFO: stdout: "update-demo-nautilus-scdrl "
    Jan 18 22:33:04.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:04.353: INFO: stderr: ""
    Jan 18 22:33:04.353: INFO: stdout: "true"
    Jan 18 22:33:04.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:04.438: INFO: stderr: ""
    Jan 18 22:33:04.438: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:04.438: INFO: validating pod update-demo-nautilus-scdrl
    Jan 18 22:33:04.445: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:04.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:04.445: INFO: update-demo-nautilus-scdrl is verified up and running
    STEP: scaling up the replication controller 01/18/23 22:33:04.445
    Jan 18 22:33:04.447: INFO: scanned /root for discovery docs: <nil>
    Jan 18 22:33:04.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 18 22:33:05.562: INFO: stderr: ""
    Jan 18 22:33:05.562: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 22:33:05.562
    Jan 18 22:33:05.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:33:05.669: INFO: stderr: ""
    Jan 18 22:33:05.669: INFO: stdout: "update-demo-nautilus-scdrl update-demo-nautilus-49xxc "
    Jan 18 22:33:05.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:05.757: INFO: stderr: ""
    Jan 18 22:33:05.757: INFO: stdout: "true"
    Jan 18 22:33:05.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:05.835: INFO: stderr: ""
    Jan 18 22:33:05.835: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:05.835: INFO: validating pod update-demo-nautilus-scdrl
    Jan 18 22:33:05.840: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:05.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:05.840: INFO: update-demo-nautilus-scdrl is verified up and running
    Jan 18 22:33:05.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:05.919: INFO: stderr: ""
    Jan 18 22:33:05.919: INFO: stdout: ""
    Jan 18 22:33:05.919: INFO: update-demo-nautilus-49xxc is created but not running
    Jan 18 22:33:10.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 22:33:11.055: INFO: stderr: ""
    Jan 18 22:33:11.055: INFO: stdout: "update-demo-nautilus-scdrl update-demo-nautilus-49xxc "
    Jan 18 22:33:11.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:11.168: INFO: stderr: ""
    Jan 18 22:33:11.168: INFO: stdout: "true"
    Jan 18 22:33:11.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-scdrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:11.263: INFO: stderr: ""
    Jan 18 22:33:11.263: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:11.263: INFO: validating pod update-demo-nautilus-scdrl
    Jan 18 22:33:11.268: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:11.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:11.268: INFO: update-demo-nautilus-scdrl is verified up and running
    Jan 18 22:33:11.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 22:33:11.352: INFO: stderr: ""
    Jan 18 22:33:11.352: INFO: stdout: "true"
    Jan 18 22:33:11.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods update-demo-nautilus-49xxc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 22:33:11.431: INFO: stderr: ""
    Jan 18 22:33:11.431: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan 18 22:33:11.431: INFO: validating pod update-demo-nautilus-49xxc
    Jan 18 22:33:11.437: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 22:33:11.437: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 22:33:11.437: INFO: update-demo-nautilus-49xxc is verified up and running
    STEP: using delete to clean up resources 01/18/23 22:33:11.437
    Jan 18 22:33:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 delete --grace-period=0 --force -f -'
    Jan 18 22:33:11.533: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 22:33:11.533: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 22:33:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get rc,svc -l name=update-demo --no-headers'
    Jan 18 22:33:11.665: INFO: stderr: "No resources found in kubectl-4010 namespace.\n"
    Jan 18 22:33:11.665: INFO: stdout: ""
    Jan 18 22:33:11.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=kubectl-4010 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 22:33:11.789: INFO: stderr: ""
    Jan 18 22:33:11.789: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:11.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4010" for this suite. 01/18/23 22:33:11.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:11.819
Jan 18 22:33:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:33:11.821
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:11.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:11.995
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-g5dzl" 01/18/23 22:33:12.006
Jan 18 22:33:12.018: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
Jan 18 22:33:13.023: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
Jan 18 22:33:13.029: INFO: Found 1 replicas for "e2e-rc-g5dzl" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-g5dzl" 01/18/23 22:33:13.029
STEP: Updating a scale subresource 01/18/23 22:33:13.032
STEP: Verifying replicas where modified for replication controller "e2e-rc-g5dzl" 01/18/23 22:33:13.043
Jan 18 22:33:13.043: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
Jan 18 22:33:14.050: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
Jan 18 22:33:14.055: INFO: Found 2 replicas for "e2e-rc-g5dzl" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:14.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3352" for this suite. 01/18/23 22:33:14.062
------------------------------
• [2.266 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:11.819
    Jan 18 22:33:11.819: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:33:11.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:11.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:11.995
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-g5dzl" 01/18/23 22:33:12.006
    Jan 18 22:33:12.018: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
    Jan 18 22:33:13.023: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
    Jan 18 22:33:13.029: INFO: Found 1 replicas for "e2e-rc-g5dzl" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-g5dzl" 01/18/23 22:33:13.029
    STEP: Updating a scale subresource 01/18/23 22:33:13.032
    STEP: Verifying replicas where modified for replication controller "e2e-rc-g5dzl" 01/18/23 22:33:13.043
    Jan 18 22:33:13.043: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
    Jan 18 22:33:14.050: INFO: Get Replication Controller "e2e-rc-g5dzl" to confirm replicas
    Jan 18 22:33:14.055: INFO: Found 2 replicas for "e2e-rc-g5dzl" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:14.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3352" for this suite. 01/18/23 22:33:14.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:14.09
Jan 18 22:33:14.090: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename projected 01/18/23 22:33:14.092
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:14.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:14.128
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-6f2515a8-9490-4248-8860-d6bea48275a9 01/18/23 22:33:14.133
STEP: Creating a pod to test consume secrets 01/18/23 22:33:14.145
Jan 18 22:33:14.162: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41" in namespace "projected-6941" to be "Succeeded or Failed"
Jan 18 22:33:14.165: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718949ms
Jan 18 22:33:16.171: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009642117s
Jan 18 22:33:18.171: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009505577s
STEP: Saw pod success 01/18/23 22:33:18.171
Jan 18 22:33:18.172: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41" satisfied condition "Succeeded or Failed"
Jan 18 22:33:18.175: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:33:18.183
Jan 18 22:33:18.210: INFO: Waiting for pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 to disappear
Jan 18 22:33:18.216: INFO: Pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6941" for this suite. 01/18/23 22:33:18.221
------------------------------
• [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:14.09
    Jan 18 22:33:14.090: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename projected 01/18/23 22:33:14.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:14.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:14.128
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-6f2515a8-9490-4248-8860-d6bea48275a9 01/18/23 22:33:14.133
    STEP: Creating a pod to test consume secrets 01/18/23 22:33:14.145
    Jan 18 22:33:14.162: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41" in namespace "projected-6941" to be "Succeeded or Failed"
    Jan 18 22:33:14.165: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718949ms
    Jan 18 22:33:16.171: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009642117s
    Jan 18 22:33:18.171: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009505577s
    STEP: Saw pod success 01/18/23 22:33:18.171
    Jan 18 22:33:18.172: INFO: Pod "pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41" satisfied condition "Succeeded or Failed"
    Jan 18 22:33:18.175: INFO: Trying to get logs from node test-vm-1 pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:33:18.183
    Jan 18 22:33:18.210: INFO: Waiting for pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 to disappear
    Jan 18 22:33:18.216: INFO: Pod pod-projected-secrets-c4eff25b-3f4e-42c6-99e8-e86133e34b41 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:18.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6941" for this suite. 01/18/23 22:33:18.221
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:18.239
Jan 18 22:33:18.239: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:33:18.241
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:18.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:18.283
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-9088 01/18/23 22:33:18.285
STEP: creating a selector 01/18/23 22:33:18.285
STEP: Creating the service pods in kubernetes 01/18/23 22:33:18.285
Jan 18 22:33:18.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 22:33:18.324: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9088" to be "running and ready"
Jan 18 22:33:18.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024152ms
Jan 18 22:33:18.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:33:20.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010477829s
Jan 18 22:33:20.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:33:22.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009950115s
Jan 18 22:33:22.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:33:24.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009947708s
Jan 18 22:33:24.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:33:26.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009381364s
Jan 18 22:33:26.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:33:28.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010274666s
Jan 18 22:33:28.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 22:33:30.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010199558s
Jan 18 22:33:30.334: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 22:33:30.334: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 22:33:30.338: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9088" to be "running and ready"
Jan 18 22:33:30.341: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.424238ms
Jan 18 22:33:30.341: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 18 22:33:32.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.00889515s
Jan 18 22:33:32.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 18 22:33:34.348: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010346692s
Jan 18 22:33:34.348: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 18 22:33:36.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.008877417s
Jan 18 22:33:36.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 18 22:33:38.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009051963s
Jan 18 22:33:38.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 18 22:33:40.346: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.008351497s
Jan 18 22:33:40.346: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 22:33:40.346: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 22:33:40.349
Jan 18 22:33:40.372: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9088" to be "running"
Jan 18 22:33:40.375: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561745ms
Jan 18 22:33:42.381: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009348393s
Jan 18 22:33:42.381: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 22:33:42.385: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9088" to be "running"
Jan 18 22:33:42.388: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.41503ms
Jan 18 22:33:42.389: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 22:33:42.392: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 22:33:42.392: INFO: Going to poll 10.1.132.9 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:33:42.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.132.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9088 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:33:42.396: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:33:42.397: INFO: ExecWithOptions: Clientset creation
Jan 18 22:33:42.397: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9088/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.132.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:33:42.503: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 22:33:42.503: INFO: Going to poll 10.1.192.41 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 22:33:42.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.192.41:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9088 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 22:33:42.509: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
Jan 18 22:33:42.510: INFO: ExecWithOptions: Clientset creation
Jan 18 22:33:42.510: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9088/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.192.41%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 22:33:42.631: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:42.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9088" for this suite. 01/18/23 22:33:42.638
------------------------------
• [SLOW TEST] [24.419 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:18.239
    Jan 18 22:33:18.239: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 22:33:18.241
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:18.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:18.283
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-9088 01/18/23 22:33:18.285
    STEP: creating a selector 01/18/23 22:33:18.285
    STEP: Creating the service pods in kubernetes 01/18/23 22:33:18.285
    Jan 18 22:33:18.286: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 22:33:18.324: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9088" to be "running and ready"
    Jan 18 22:33:18.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024152ms
    Jan 18 22:33:18.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:33:20.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010477829s
    Jan 18 22:33:20.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:33:22.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.009950115s
    Jan 18 22:33:22.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:33:24.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009947708s
    Jan 18 22:33:24.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:33:26.333: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009381364s
    Jan 18 22:33:26.333: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:33:28.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010274666s
    Jan 18 22:33:28.334: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 22:33:30.334: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010199558s
    Jan 18 22:33:30.334: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 22:33:30.334: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 22:33:30.338: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9088" to be "running and ready"
    Jan 18 22:33:30.341: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 3.424238ms
    Jan 18 22:33:30.341: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 18 22:33:32.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.00889515s
    Jan 18 22:33:32.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 18 22:33:34.348: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.010346692s
    Jan 18 22:33:34.348: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 18 22:33:36.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.008877417s
    Jan 18 22:33:36.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 18 22:33:38.347: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.009051963s
    Jan 18 22:33:38.347: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 18 22:33:40.346: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.008351497s
    Jan 18 22:33:40.346: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 22:33:40.346: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 22:33:40.349
    Jan 18 22:33:40.372: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9088" to be "running"
    Jan 18 22:33:40.375: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561745ms
    Jan 18 22:33:42.381: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009348393s
    Jan 18 22:33:42.381: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 22:33:42.385: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9088" to be "running"
    Jan 18 22:33:42.388: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.41503ms
    Jan 18 22:33:42.389: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 22:33:42.392: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 22:33:42.392: INFO: Going to poll 10.1.132.9 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:33:42.396: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.132.9:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9088 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:33:42.396: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:33:42.397: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:33:42.397: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9088/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.132.9%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:33:42.503: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 22:33:42.503: INFO: Going to poll 10.1.192.41 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 22:33:42.509: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.192.41:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9088 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 22:33:42.509: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    Jan 18 22:33:42.510: INFO: ExecWithOptions: Clientset creation
    Jan 18 22:33:42.510: INFO: ExecWithOptions: execute(POST https://10.152.183.1:443/api/v1/namespaces/pod-network-test-9088/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.1.192.41%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 22:33:42.631: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:42.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9088" for this suite. 01/18/23 22:33:42.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:42.659
Jan 18 22:33:42.659: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename services 01/18/23 22:33:42.662
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:42.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:42.702
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9662 01/18/23 22:33:42.705
STEP: changing the ExternalName service to type=NodePort 01/18/23 22:33:42.715
STEP: creating replication controller externalname-service in namespace services-9662 01/18/23 22:33:42.78
I0118 22:33:42.802564      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9662, replica count: 2
I0118 22:33:45.853996      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 22:33:45.854: INFO: Creating new exec pod
Jan 18 22:33:45.866: INFO: Waiting up to 5m0s for pod "execpodfg2dm" in namespace "services-9662" to be "running"
Jan 18 22:33:45.872: INFO: Pod "execpodfg2dm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365657ms
Jan 18 22:33:47.879: INFO: Pod "execpodfg2dm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01332963s
Jan 18 22:33:47.879: INFO: Pod "execpodfg2dm" satisfied condition "running"
Jan 18 22:33:48.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan 18 22:33:49.081: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 22:33:49.081: INFO: stdout: ""
Jan 18 22:33:49.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.152.183.222 80'
Jan 18 22:33:49.311: INFO: stderr: "+ nc -v -z -w 2 10.152.183.222 80\nConnection to 10.152.183.222 80 port [tcp/http] succeeded!\n"
Jan 18 22:33:49.311: INFO: stdout: ""
Jan 18 22:33:49.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 30649'
Jan 18 22:33:49.500: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 30649\nConnection to 10.0.0.5 30649 port [tcp/*] succeeded!\n"
Jan 18 22:33:49.500: INFO: stdout: ""
Jan 18 22:33:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30649'
Jan 18 22:33:49.669: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30649\nConnection to 10.0.0.4 30649 port [tcp/*] succeeded!\n"
Jan 18 22:33:49.669: INFO: stdout: ""
Jan 18 22:33:49.669: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:49.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9662" for this suite. 01/18/23 22:33:49.764
------------------------------
• [SLOW TEST] [7.118 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:42.659
    Jan 18 22:33:42.659: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename services 01/18/23 22:33:42.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:42.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:42.702
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9662 01/18/23 22:33:42.705
    STEP: changing the ExternalName service to type=NodePort 01/18/23 22:33:42.715
    STEP: creating replication controller externalname-service in namespace services-9662 01/18/23 22:33:42.78
    I0118 22:33:42.802564      20 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9662, replica count: 2
    I0118 22:33:45.853996      20 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 22:33:45.854: INFO: Creating new exec pod
    Jan 18 22:33:45.866: INFO: Waiting up to 5m0s for pod "execpodfg2dm" in namespace "services-9662" to be "running"
    Jan 18 22:33:45.872: INFO: Pod "execpodfg2dm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.365657ms
    Jan 18 22:33:47.879: INFO: Pod "execpodfg2dm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01332963s
    Jan 18 22:33:47.879: INFO: Pod "execpodfg2dm" satisfied condition "running"
    Jan 18 22:33:48.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan 18 22:33:49.081: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 22:33:49.081: INFO: stdout: ""
    Jan 18 22:33:49.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.152.183.222 80'
    Jan 18 22:33:49.311: INFO: stderr: "+ nc -v -z -w 2 10.152.183.222 80\nConnection to 10.152.183.222 80 port [tcp/http] succeeded!\n"
    Jan 18 22:33:49.311: INFO: stdout: ""
    Jan 18 22:33:49.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.0.0.5 30649'
    Jan 18 22:33:49.500: INFO: stderr: "+ nc -v -z -w 2 10.0.0.5 30649\nConnection to 10.0.0.5 30649 port [tcp/*] succeeded!\n"
    Jan 18 22:33:49.500: INFO: stdout: ""
    Jan 18 22:33:49.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-337963596 --namespace=services-9662 exec execpodfg2dm -- /bin/sh -x -c nc -v -z -w 2 10.0.0.4 30649'
    Jan 18 22:33:49.669: INFO: stderr: "+ nc -v -z -w 2 10.0.0.4 30649\nConnection to 10.0.0.4 30649 port [tcp/*] succeeded!\n"
    Jan 18 22:33:49.669: INFO: stdout: ""
    Jan 18 22:33:49.669: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:49.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9662" for this suite. 01/18/23 22:33:49.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:49.778
Jan 18 22:33:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename job 01/18/23 22:33:49.78
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:49.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:49.822
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/18/23 22:33:49.825
STEP: Ensuring job reaches completions 01/18/23 22:33:49.842
STEP: Ensuring pods with index for job exist 01/18/23 22:33:59.849
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan 18 22:33:59.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5345" for this suite. 01/18/23 22:33:59.863
------------------------------
• [SLOW TEST] [10.103 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:49.778
    Jan 18 22:33:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename job 01/18/23 22:33:49.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:49.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:49.822
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/18/23 22:33:49.825
    STEP: Ensuring job reaches completions 01/18/23 22:33:49.842
    STEP: Ensuring pods with index for job exist 01/18/23 22:33:59.849
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:33:59.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5345" for this suite. 01/18/23 22:33:59.863
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:33:59.882
Jan 18 22:33:59.882: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename container-probe 01/18/23 22:33:59.884
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:59.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:59.918
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 in namespace container-probe-7283 01/18/23 22:33:59.922
Jan 18 22:33:59.940: INFO: Waiting up to 5m0s for pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891" in namespace "container-probe-7283" to be "not pending"
Jan 18 22:33:59.944: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292751ms
Jan 18 22:34:01.949: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008399295s
Jan 18 22:34:03.951: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Running", Reason="", readiness=true. Elapsed: 4.010448714s
Jan 18 22:34:03.951: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891" satisfied condition "not pending"
Jan 18 22:34:03.951: INFO: Started pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 in namespace container-probe-7283
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:34:03.951
Jan 18 22:34:03.955: INFO: Initial restart count of pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 is 0
STEP: deleting the pod 01/18/23 22:38:04.641
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7283" for this suite. 01/18/23 22:38:04.691
------------------------------
• [SLOW TEST] [244.824 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:33:59.882
    Jan 18 22:33:59.882: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename container-probe 01/18/23 22:33:59.884
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:33:59.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:33:59.918
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 in namespace container-probe-7283 01/18/23 22:33:59.922
    Jan 18 22:33:59.940: INFO: Waiting up to 5m0s for pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891" in namespace "container-probe-7283" to be "not pending"
    Jan 18 22:33:59.944: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292751ms
    Jan 18 22:34:01.949: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008399295s
    Jan 18 22:34:03.951: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891": Phase="Running", Reason="", readiness=true. Elapsed: 4.010448714s
    Jan 18 22:34:03.951: INFO: Pod "test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891" satisfied condition "not pending"
    Jan 18 22:34:03.951: INFO: Started pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 in namespace container-probe-7283
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 22:34:03.951
    Jan 18 22:34:03.955: INFO: Initial restart count of pod test-webserver-7b27a96f-7ae6-4be3-ae96-f7393df2c891 is 0
    STEP: deleting the pod 01/18/23 22:38:04.641
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7283" for this suite. 01/18/23 22:38:04.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:04.707
Jan 18 22:38:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 22:38:04.708
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:04.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:04.794
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:38:04.796
Jan 18 22:38:04.819: INFO: Waiting up to 5m0s for pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6" in namespace "emptydir-8772" to be "Succeeded or Failed"
Jan 18 22:38:04.823: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725338ms
Jan 18 22:38:06.829: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010351371s
Jan 18 22:38:08.828: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009373745s
STEP: Saw pod success 01/18/23 22:38:08.828
Jan 18 22:38:08.828: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6" satisfied condition "Succeeded or Failed"
Jan 18 22:38:08.832: INFO: Trying to get logs from node test-vm-1 pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 container test-container: <nil>
STEP: delete the pod 01/18/23 22:38:08.852
Jan 18 22:38:08.900: INFO: Waiting for pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 to disappear
Jan 18 22:38:08.906: INFO: Pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:08.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8772" for this suite. 01/18/23 22:38:08.91
------------------------------
• [4.224 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:04.707
    Jan 18 22:38:04.707: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:38:04.708
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:04.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:04.794
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 22:38:04.796
    Jan 18 22:38:04.819: INFO: Waiting up to 5m0s for pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6" in namespace "emptydir-8772" to be "Succeeded or Failed"
    Jan 18 22:38:04.823: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725338ms
    Jan 18 22:38:06.829: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010351371s
    Jan 18 22:38:08.828: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009373745s
    STEP: Saw pod success 01/18/23 22:38:08.828
    Jan 18 22:38:08.828: INFO: Pod "pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:08.832: INFO: Trying to get logs from node test-vm-1 pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 container test-container: <nil>
    STEP: delete the pod 01/18/23 22:38:08.852
    Jan 18 22:38:08.900: INFO: Waiting for pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 to disappear
    Jan 18 22:38:08.906: INFO: Pod pod-bb2589eb-8d28-43f9-a49a-7fdbef75a6f6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:08.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8772" for this suite. 01/18/23 22:38:08.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:08.937
Jan 18 22:38:08.938: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename emptydir 01/18/23 22:38:08.939
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:08.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:08.996
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:38:08.998
Jan 18 22:38:09.018: INFO: Waiting up to 5m0s for pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d" in namespace "emptydir-5157" to be "Succeeded or Failed"
Jan 18 22:38:09.023: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658629ms
Jan 18 22:38:11.028: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010270244s
Jan 18 22:38:13.029: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010889786s
STEP: Saw pod success 01/18/23 22:38:13.029
Jan 18 22:38:13.029: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d" satisfied condition "Succeeded or Failed"
Jan 18 22:38:13.033: INFO: Trying to get logs from node test-vm-1 pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d container test-container: <nil>
STEP: delete the pod 01/18/23 22:38:13.043
Jan 18 22:38:13.083: INFO: Waiting for pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d to disappear
Jan 18 22:38:13.092: INFO: Pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:13.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5157" for this suite. 01/18/23 22:38:13.096
------------------------------
• [4.170 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:08.937
    Jan 18 22:38:08.938: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename emptydir 01/18/23 22:38:08.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:08.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:08.996
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 22:38:08.998
    Jan 18 22:38:09.018: INFO: Waiting up to 5m0s for pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d" in namespace "emptydir-5157" to be "Succeeded or Failed"
    Jan 18 22:38:09.023: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658629ms
    Jan 18 22:38:11.028: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010270244s
    Jan 18 22:38:13.029: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010889786s
    STEP: Saw pod success 01/18/23 22:38:13.029
    Jan 18 22:38:13.029: INFO: Pod "pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:13.033: INFO: Trying to get logs from node test-vm-1 pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d container test-container: <nil>
    STEP: delete the pod 01/18/23 22:38:13.043
    Jan 18 22:38:13.083: INFO: Waiting for pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d to disappear
    Jan 18 22:38:13.092: INFO: Pod pod-e882b08c-9ae7-4816-8d3f-d2e1f4e6b43d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:13.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5157" for this suite. 01/18/23 22:38:13.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:13.11
Jan 18 22:38:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 22:38:13.112
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:13.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:13.149
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-312c2408-7a61-486d-863f-836258069d6a 01/18/23 22:38:13.152
STEP: Creating a pod to test consume secrets 01/18/23 22:38:13.162
Jan 18 22:38:13.181: INFO: Waiting up to 5m0s for pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34" in namespace "secrets-9018" to be "Succeeded or Failed"
Jan 18 22:38:13.185: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999042ms
Jan 18 22:38:15.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009666396s
Jan 18 22:38:17.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009728316s
STEP: Saw pod success 01/18/23 22:38:17.191
Jan 18 22:38:17.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34" satisfied condition "Succeeded or Failed"
Jan 18 22:38:17.198: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 container secret-env-test: <nil>
STEP: delete the pod 01/18/23 22:38:17.205
Jan 18 22:38:17.242: INFO: Waiting for pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 to disappear
Jan 18 22:38:17.248: INFO: Pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:17.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9018" for this suite. 01/18/23 22:38:17.253
------------------------------
• [4.155 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:13.11
    Jan 18 22:38:13.111: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 22:38:13.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:13.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:13.149
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-312c2408-7a61-486d-863f-836258069d6a 01/18/23 22:38:13.152
    STEP: Creating a pod to test consume secrets 01/18/23 22:38:13.162
    Jan 18 22:38:13.181: INFO: Waiting up to 5m0s for pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34" in namespace "secrets-9018" to be "Succeeded or Failed"
    Jan 18 22:38:13.185: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Pending", Reason="", readiness=false. Elapsed: 3.999042ms
    Jan 18 22:38:15.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009666396s
    Jan 18 22:38:17.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009728316s
    STEP: Saw pod success 01/18/23 22:38:17.191
    Jan 18 22:38:17.191: INFO: Pod "pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:17.198: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 container secret-env-test: <nil>
    STEP: delete the pod 01/18/23 22:38:17.205
    Jan 18 22:38:17.242: INFO: Waiting for pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 to disappear
    Jan 18 22:38:17.248: INFO: Pod pod-secrets-7dcaab19-be74-4e40-b59f-2154a1eefa34 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:17.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9018" for this suite. 01/18/23 22:38:17.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:17.267
Jan 18 22:38:17.267: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename secrets 01/18/23 22:38:17.268
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:17.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:17.334
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-c14c8c81-0a80-4329-8ddd-7078178a3573 01/18/23 22:38:17.403
STEP: Creating a pod to test consume secrets 01/18/23 22:38:17.424
Jan 18 22:38:17.442: INFO: Waiting up to 5m0s for pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22" in namespace "secrets-3733" to be "Succeeded or Failed"
Jan 18 22:38:17.445: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312133ms
Jan 18 22:38:19.450: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159399s
Jan 18 22:38:21.453: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011052247s
STEP: Saw pod success 01/18/23 22:38:21.453
Jan 18 22:38:21.453: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22" satisfied condition "Succeeded or Failed"
Jan 18 22:38:21.457: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 22:38:21.464
Jan 18 22:38:21.507: INFO: Waiting for pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 to disappear
Jan 18 22:38:21.512: INFO: Pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:21.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3733" for this suite. 01/18/23 22:38:21.517
STEP: Destroying namespace "secret-namespace-8746" for this suite. 01/18/23 22:38:21.537
------------------------------
• [4.288 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:17.267
    Jan 18 22:38:17.267: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename secrets 01/18/23 22:38:17.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:17.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:17.334
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-c14c8c81-0a80-4329-8ddd-7078178a3573 01/18/23 22:38:17.403
    STEP: Creating a pod to test consume secrets 01/18/23 22:38:17.424
    Jan 18 22:38:17.442: INFO: Waiting up to 5m0s for pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22" in namespace "secrets-3733" to be "Succeeded or Failed"
    Jan 18 22:38:17.445: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Pending", Reason="", readiness=false. Elapsed: 3.312133ms
    Jan 18 22:38:19.450: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159399s
    Jan 18 22:38:21.453: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011052247s
    STEP: Saw pod success 01/18/23 22:38:21.453
    Jan 18 22:38:21.453: INFO: Pod "pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:21.457: INFO: Trying to get logs from node test-vm-1 pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 22:38:21.464
    Jan 18 22:38:21.507: INFO: Waiting for pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 to disappear
    Jan 18 22:38:21.512: INFO: Pod pod-secrets-b68528fb-abec-42a7-8c85-cd978ef36d22 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:21.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3733" for this suite. 01/18/23 22:38:21.517
    STEP: Destroying namespace "secret-namespace-8746" for this suite. 01/18/23 22:38:21.537
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:21.556
Jan 18 22:38:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename replication-controller 01/18/23 22:38:21.557
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:21.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:21.611
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan 18 22:38:21.613: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 22:38:22.631
STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 22:38:22.649
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 22:38:23.658
Jan 18 22:38:23.685: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/18/23 22:38:23.685
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4188" for this suite. 01/18/23 22:38:24.699
------------------------------
• [3.157 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:21.556
    Jan 18 22:38:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename replication-controller 01/18/23 22:38:21.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:21.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:21.611
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan 18 22:38:21.613: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 22:38:22.631
    STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 22:38:22.649
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 22:38:23.658
    Jan 18 22:38:23.685: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/18/23 22:38:23.685
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:24.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4188" for this suite. 01/18/23 22:38:24.699
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:24.714
Jan 18 22:38:24.714: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename conformance-tests 01/18/23 22:38:24.715
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:24.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:24.75
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/18/23 22:38:24.758
Jan 18 22:38:24.758: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:24.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4311" for this suite. 01/18/23 22:38:24.767
------------------------------
• [0.068 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:24.714
    Jan 18 22:38:24.714: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename conformance-tests 01/18/23 22:38:24.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:24.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:24.75
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/18/23 22:38:24.758
    Jan 18 22:38:24.758: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:24.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4311" for this suite. 01/18/23 22:38:24.767
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:24.782
Jan 18 22:38:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename downward-api 01/18/23 22:38:24.785
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:24.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:24.827
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/18/23 22:38:24.829
Jan 18 22:38:24.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5" in namespace "downward-api-7709" to be "Succeeded or Failed"
Jan 18 22:38:24.849: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608748ms
Jan 18 22:38:26.854: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008451627s
Jan 18 22:38:28.855: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009485656s
STEP: Saw pod success 01/18/23 22:38:28.855
Jan 18 22:38:28.855: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5" satisfied condition "Succeeded or Failed"
Jan 18 22:38:28.859: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 container client-container: <nil>
STEP: delete the pod 01/18/23 22:38:28.867
Jan 18 22:38:28.901: INFO: Waiting for pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 to disappear
Jan 18 22:38:28.911: INFO: Pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:28.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7709" for this suite. 01/18/23 22:38:28.916
------------------------------
• [4.151 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:24.782
    Jan 18 22:38:24.782: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename downward-api 01/18/23 22:38:24.785
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:24.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:24.827
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/18/23 22:38:24.829
    Jan 18 22:38:24.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5" in namespace "downward-api-7709" to be "Succeeded or Failed"
    Jan 18 22:38:24.849: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608748ms
    Jan 18 22:38:26.854: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008451627s
    Jan 18 22:38:28.855: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009485656s
    STEP: Saw pod success 01/18/23 22:38:28.855
    Jan 18 22:38:28.855: INFO: Pod "downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5" satisfied condition "Succeeded or Failed"
    Jan 18 22:38:28.859: INFO: Trying to get logs from node test-vm-1 pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 container client-container: <nil>
    STEP: delete the pod 01/18/23 22:38:28.867
    Jan 18 22:38:28.901: INFO: Waiting for pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 to disappear
    Jan 18 22:38:28.911: INFO: Pod downwardapi-volume-2ffc9f58-8368-4de4-9c69-67153a8526f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:28.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7709" for this suite. 01/18/23 22:38:28.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:28.936
Jan 18 22:38:28.936: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename pods 01/18/23 22:38:28.937
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:28.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:28.972
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/18/23 22:38:28.976
Jan 18 22:38:28.996: INFO: Waiting up to 5m0s for pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f" in namespace "pods-8196" to be "running and ready"
Jan 18 22:38:29.000: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682348ms
Jan 18 22:38:29.000: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:38:31.005: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00861732s
Jan 18 22:38:31.005: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 22:38:33.007: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Running", Reason="", readiness=true. Elapsed: 4.011028339s
Jan 18 22:38:33.007: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Running (Ready = true)
Jan 18 22:38:33.007: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f" satisfied condition "running and ready"
Jan 18 22:38:33.027: INFO: Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f has hostIP: 10.0.0.4
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan 18 22:38:33.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8196" for this suite. 01/18/23 22:38:33.034
------------------------------
• [4.115 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:28.936
    Jan 18 22:38:28.936: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename pods 01/18/23 22:38:28.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:28.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:28.972
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/18/23 22:38:28.976
    Jan 18 22:38:28.996: INFO: Waiting up to 5m0s for pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f" in namespace "pods-8196" to be "running and ready"
    Jan 18 22:38:29.000: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682348ms
    Jan 18 22:38:29.000: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:38:31.005: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00861732s
    Jan 18 22:38:31.005: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 22:38:33.007: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f": Phase="Running", Reason="", readiness=true. Elapsed: 4.011028339s
    Jan 18 22:38:33.007: INFO: The phase of Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f is Running (Ready = true)
    Jan 18 22:38:33.007: INFO: Pod "pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f" satisfied condition "running and ready"
    Jan 18 22:38:33.027: INFO: Pod pod-hostip-b6aad05b-027b-46d0-80c2-d0620d18ca1f has hostIP: 10.0.0.4
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:38:33.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8196" for this suite. 01/18/23 22:38:33.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:38:33.063
Jan 18 22:38:33.063: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename gc 01/18/23 22:38:33.064
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:33.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:33.097
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/18/23 22:38:33.106
STEP: create the rc2 01/18/23 22:38:33.12
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 22:38:38.174
STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 22:38:45.173
STEP: wait for the rc to be deleted 01/18/23 22:38:45.248
Jan 18 22:38:50.290: INFO: 68 pods remaining
Jan 18 22:38:50.290: INFO: 68 pods has nil DeletionTimestamp
Jan 18 22:38:50.290: INFO: 
STEP: Gathering metrics 01/18/23 22:38:55.298
W0118 22:38:55.309222      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 22:38:55.309: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 22:38:55.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mq4g" in namespace "gc-4110"
Jan 18 22:38:55.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-qj2rr" in namespace "gc-4110"
Jan 18 22:38:55.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzxh2" in namespace "gc-4110"
Jan 18 22:38:55.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-w764s" in namespace "gc-4110"
Jan 18 22:38:55.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-87g5b" in namespace "gc-4110"
Jan 18 22:38:55.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qbx4" in namespace "gc-4110"
Jan 18 22:38:56.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-wslp8" in namespace "gc-4110"
Jan 18 22:38:56.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wlfr" in namespace "gc-4110"
Jan 18 22:38:56.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-rdr29" in namespace "gc-4110"
Jan 18 22:38:56.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-nwzsg" in namespace "gc-4110"
Jan 18 22:38:56.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jnkc" in namespace "gc-4110"
Jan 18 22:38:57.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt9lr" in namespace "gc-4110"
Jan 18 22:38:57.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-t4r44" in namespace "gc-4110"
Jan 18 22:38:57.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-wxfbq" in namespace "gc-4110"
Jan 18 22:38:57.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnd9" in namespace "gc-4110"
Jan 18 22:38:57.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mfvc" in namespace "gc-4110"
Jan 18 22:38:57.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-bctms" in namespace "gc-4110"
Jan 18 22:38:57.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-66856" in namespace "gc-4110"
Jan 18 22:38:57.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw5kt" in namespace "gc-4110"
Jan 18 22:38:58.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqsd" in namespace "gc-4110"
Jan 18 22:38:58.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgkmx" in namespace "gc-4110"
Jan 18 22:38:58.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-nb8hk" in namespace "gc-4110"
Jan 18 22:38:58.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwhv8" in namespace "gc-4110"
Jan 18 22:38:58.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc8rm" in namespace "gc-4110"
Jan 18 22:38:58.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-sgsg8" in namespace "gc-4110"
Jan 18 22:38:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmxxz" in namespace "gc-4110"
Jan 18 22:38:59.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-stm9z" in namespace "gc-4110"
Jan 18 22:38:59.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn9d2" in namespace "gc-4110"
Jan 18 22:38:59.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc4xc" in namespace "gc-4110"
Jan 18 22:38:59.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-sm8gj" in namespace "gc-4110"
Jan 18 22:39:00.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-4852l" in namespace "gc-4110"
Jan 18 22:39:00.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-w8wwz" in namespace "gc-4110"
Jan 18 22:39:00.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-ld544" in namespace "gc-4110"
Jan 18 22:39:00.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-s79ww" in namespace "gc-4110"
Jan 18 22:39:00.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-vhwkq" in namespace "gc-4110"
Jan 18 22:39:00.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xxjw" in namespace "gc-4110"
Jan 18 22:39:01.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-29wzq" in namespace "gc-4110"
Jan 18 22:39:01.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4drt" in namespace "gc-4110"
Jan 18 22:39:01.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjcbw" in namespace "gc-4110"
Jan 18 22:39:01.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-m7d5q" in namespace "gc-4110"
Jan 18 22:39:02.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-25z26" in namespace "gc-4110"
Jan 18 22:39:02.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5rm4" in namespace "gc-4110"
Jan 18 22:39:02.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-vn2sw" in namespace "gc-4110"
Jan 18 22:39:02.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-shk2j" in namespace "gc-4110"
Jan 18 22:39:02.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vhsq" in namespace "gc-4110"
Jan 18 22:39:03.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-ztc9w" in namespace "gc-4110"
Jan 18 22:39:03.262: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5jmp" in namespace "gc-4110"
Jan 18 22:39:03.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g56w" in namespace "gc-4110"
Jan 18 22:39:03.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq2ts" in namespace "gc-4110"
Jan 18 22:39:03.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7fmr" in namespace "gc-4110"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan 18 22:39:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4110" for this suite. 01/18/23 22:39:03.873
------------------------------
• [SLOW TEST] [30.863 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:38:33.063
    Jan 18 22:38:33.063: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename gc 01/18/23 22:38:33.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:38:33.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:38:33.097
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/18/23 22:38:33.106
    STEP: create the rc2 01/18/23 22:38:33.12
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 22:38:38.174
    STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 22:38:45.173
    STEP: wait for the rc to be deleted 01/18/23 22:38:45.248
    Jan 18 22:38:50.290: INFO: 68 pods remaining
    Jan 18 22:38:50.290: INFO: 68 pods has nil DeletionTimestamp
    Jan 18 22:38:50.290: INFO: 
    STEP: Gathering metrics 01/18/23 22:38:55.298
    W0118 22:38:55.309222      20 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 22:38:55.309: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 22:38:55.309: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mq4g" in namespace "gc-4110"
    Jan 18 22:38:55.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-qj2rr" in namespace "gc-4110"
    Jan 18 22:38:55.515: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzxh2" in namespace "gc-4110"
    Jan 18 22:38:55.667: INFO: Deleting pod "simpletest-rc-to-be-deleted-w764s" in namespace "gc-4110"
    Jan 18 22:38:55.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-87g5b" in namespace "gc-4110"
    Jan 18 22:38:55.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qbx4" in namespace "gc-4110"
    Jan 18 22:38:56.111: INFO: Deleting pod "simpletest-rc-to-be-deleted-wslp8" in namespace "gc-4110"
    Jan 18 22:38:56.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wlfr" in namespace "gc-4110"
    Jan 18 22:38:56.580: INFO: Deleting pod "simpletest-rc-to-be-deleted-rdr29" in namespace "gc-4110"
    Jan 18 22:38:56.828: INFO: Deleting pod "simpletest-rc-to-be-deleted-nwzsg" in namespace "gc-4110"
    Jan 18 22:38:56.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jnkc" in namespace "gc-4110"
    Jan 18 22:38:57.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-bt9lr" in namespace "gc-4110"
    Jan 18 22:38:57.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-t4r44" in namespace "gc-4110"
    Jan 18 22:38:57.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-wxfbq" in namespace "gc-4110"
    Jan 18 22:38:57.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tnd9" in namespace "gc-4110"
    Jan 18 22:38:57.524: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mfvc" in namespace "gc-4110"
    Jan 18 22:38:57.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-bctms" in namespace "gc-4110"
    Jan 18 22:38:57.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-66856" in namespace "gc-4110"
    Jan 18 22:38:57.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-fw5kt" in namespace "gc-4110"
    Jan 18 22:38:58.038: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqsd" in namespace "gc-4110"
    Jan 18 22:38:58.204: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgkmx" in namespace "gc-4110"
    Jan 18 22:38:58.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-nb8hk" in namespace "gc-4110"
    Jan 18 22:38:58.474: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwhv8" in namespace "gc-4110"
    Jan 18 22:38:58.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-bc8rm" in namespace "gc-4110"
    Jan 18 22:38:58.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-sgsg8" in namespace "gc-4110"
    Jan 18 22:38:59.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmxxz" in namespace "gc-4110"
    Jan 18 22:38:59.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-stm9z" in namespace "gc-4110"
    Jan 18 22:38:59.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-hn9d2" in namespace "gc-4110"
    Jan 18 22:38:59.731: INFO: Deleting pod "simpletest-rc-to-be-deleted-gc4xc" in namespace "gc-4110"
    Jan 18 22:38:59.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-sm8gj" in namespace "gc-4110"
    Jan 18 22:39:00.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-4852l" in namespace "gc-4110"
    Jan 18 22:39:00.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-w8wwz" in namespace "gc-4110"
    Jan 18 22:39:00.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-ld544" in namespace "gc-4110"
    Jan 18 22:39:00.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-s79ww" in namespace "gc-4110"
    Jan 18 22:39:00.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-vhwkq" in namespace "gc-4110"
    Jan 18 22:39:00.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xxjw" in namespace "gc-4110"
    Jan 18 22:39:01.149: INFO: Deleting pod "simpletest-rc-to-be-deleted-29wzq" in namespace "gc-4110"
    Jan 18 22:39:01.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4drt" in namespace "gc-4110"
    Jan 18 22:39:01.501: INFO: Deleting pod "simpletest-rc-to-be-deleted-jjcbw" in namespace "gc-4110"
    Jan 18 22:39:01.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-m7d5q" in namespace "gc-4110"
    Jan 18 22:39:02.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-25z26" in namespace "gc-4110"
    Jan 18 22:39:02.265: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5rm4" in namespace "gc-4110"
    Jan 18 22:39:02.421: INFO: Deleting pod "simpletest-rc-to-be-deleted-vn2sw" in namespace "gc-4110"
    Jan 18 22:39:02.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-shk2j" in namespace "gc-4110"
    Jan 18 22:39:02.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vhsq" in namespace "gc-4110"
    Jan 18 22:39:03.193: INFO: Deleting pod "simpletest-rc-to-be-deleted-ztc9w" in namespace "gc-4110"
    Jan 18 22:39:03.262: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5jmp" in namespace "gc-4110"
    Jan 18 22:39:03.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g56w" in namespace "gc-4110"
    Jan 18 22:39:03.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq2ts" in namespace "gc-4110"
    Jan 18 22:39:03.637: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7fmr" in namespace "gc-4110"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:39:03.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4110" for this suite. 01/18/23 22:39:03.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:39:03.928
Jan 18 22:39:03.928: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename webhook 01/18/23 22:39:03.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:39:04.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:39:04.155
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/18/23 22:39:04.33
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:39:05.305
STEP: Deploying the webhook pod 01/18/23 22:39:05.376
STEP: Wait for the deployment to be ready 01/18/23 22:39:05.412
Jan 18 22:39:05.444: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jan 18 22:39:07.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:39:09.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:39:11.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:39:13.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 22:39:15.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 22:39:17.478
STEP: Verifying the service has paired with the endpoint 01/18/23 22:39:17.534
Jan 18 22:39:18.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/18/23 22:39:18.539
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:39:18.541
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:39:18.541
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:39:18.541
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:39:18.542
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:39:18.542
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:39:18.543
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan 18 22:39:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9284" for this suite. 01/18/23 22:39:18.717
STEP: Destroying namespace "webhook-9284-markers" for this suite. 01/18/23 22:39:18.759
------------------------------
• [SLOW TEST] [14.854 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:39:03.928
    Jan 18 22:39:03.928: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename webhook 01/18/23 22:39:03.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:39:04.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:39:04.155
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/18/23 22:39:04.33
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 22:39:05.305
    STEP: Deploying the webhook pod 01/18/23 22:39:05.376
    STEP: Wait for the deployment to be ready 01/18/23 22:39:05.412
    Jan 18 22:39:05.444: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Jan 18 22:39:07.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:39:09.480: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:39:11.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:39:13.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 22:39:15.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 22, 39, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 22:39:17.478
    STEP: Verifying the service has paired with the endpoint 01/18/23 22:39:17.534
    Jan 18 22:39:18.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/18/23 22:39:18.539
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 22:39:18.541
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 22:39:18.541
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:39:18.541
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 22:39:18.542
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:39:18.542
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 22:39:18.543
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:39:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9284" for this suite. 01/18/23 22:39:18.717
    STEP: Destroying namespace "webhook-9284-markers" for this suite. 01/18/23 22:39:18.759
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/18/23 22:39:18.782
Jan 18 22:39:18.782: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
STEP: Building a namespace api object, basename statefulset 01/18/23 22:39:18.784
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:39:18.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:39:18.844
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9360 01/18/23 22:39:18.847
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/18/23 22:39:18.862
STEP: Creating pod with conflicting port in namespace statefulset-9360 01/18/23 22:39:18.87
STEP: Waiting until pod test-pod will start running in namespace statefulset-9360 01/18/23 22:39:19.029
Jan 18 22:39:19.029: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9360" to be "running"
Jan 18 22:39:19.036: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.23158ms
Jan 18 22:39:21.043: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013800052s
Jan 18 22:39:21.043: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-9360 01/18/23 22:39:21.043
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9360 01/18/23 22:39:21.058
Jan 18 22:39:21.091: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 22:39:21.140: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 22:39:21.172: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 22:39:21.184: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9360
STEP: Removing pod with conflicting port in namespace statefulset-9360 01/18/23 22:39:21.184
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9360 and will be in running state 01/18/23 22:39:21.263
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan 18 22:39:23.281: INFO: Deleting all statefulset in ns statefulset-9360
Jan 18 22:39:23.287: INFO: Scaling statefulset ss to 0
Jan 18 22:39:33.324: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 22:39:33.328: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan 18 22:39:33.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9360" for this suite. 01/18/23 22:39:33.365
------------------------------
• [SLOW TEST] [14.597 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/18/23 22:39:18.782
    Jan 18 22:39:18.782: INFO: >>> kubeConfig: /tmp/kubeconfig-337963596
    STEP: Building a namespace api object, basename statefulset 01/18/23 22:39:18.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 22:39:18.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 22:39:18.844
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9360 01/18/23 22:39:18.847
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/18/23 22:39:18.862
    STEP: Creating pod with conflicting port in namespace statefulset-9360 01/18/23 22:39:18.87
    STEP: Waiting until pod test-pod will start running in namespace statefulset-9360 01/18/23 22:39:19.029
    Jan 18 22:39:19.029: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-9360" to be "running"
    Jan 18 22:39:19.036: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.23158ms
    Jan 18 22:39:21.043: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013800052s
    Jan 18 22:39:21.043: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-9360 01/18/23 22:39:21.043
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9360 01/18/23 22:39:21.058
    Jan 18 22:39:21.091: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 18 22:39:21.140: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 22:39:21.172: INFO: Observed stateful pod in namespace: statefulset-9360, name: ss-0, uid: 310301b4-e1bb-4fb8-b985-ff5b8f8f7480, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 22:39:21.184: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9360
    STEP: Removing pod with conflicting port in namespace statefulset-9360 01/18/23 22:39:21.184
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9360 and will be in running state 01/18/23 22:39:21.263
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan 18 22:39:23.281: INFO: Deleting all statefulset in ns statefulset-9360
    Jan 18 22:39:23.287: INFO: Scaling statefulset ss to 0
    Jan 18 22:39:33.324: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 22:39:33.328: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan 18 22:39:33.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9360" for this suite. 01/18/23 22:39:33.365
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan 18 22:39:33.381: INFO: Running AfterSuite actions on node 1
Jan 18 22:39:33.381: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan 18 22:39:33.381: INFO: Running AfterSuite actions on node 1
    Jan 18 22:39:33.381: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.110 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5937.701 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h38m58.142433753s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

