I0830 06:17:05.423625      21 e2e.go:126] Starting e2e run "8bbbd299-392d-471f-98f9-905027cf33d5" on Ginkgo node 1
Aug 30 06:17:05.446: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693376225 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 30 06:17:05.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
E0830 06:17:05.631812      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 30 06:17:05.632: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 30 06:17:05.657: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 30 06:17:05.735: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 30 06:17:05.735: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Aug 30 06:17:05.735: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 30 06:17:05.748: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Aug 30 06:17:05.748: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Aug 30 06:17:05.748: INFO: e2e test version: v1.26.6
Aug 30 06:17:05.750: INFO: kube-apiserver version: v1.26.6+73ac561
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 30 06:17:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:17:05.766: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.137 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 30 06:17:05.630: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    E0830 06:17:05.631812      21 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Aug 30 06:17:05.632: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 30 06:17:05.657: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 30 06:17:05.735: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 30 06:17:05.735: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Aug 30 06:17:05.735: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 30 06:17:05.748: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Aug 30 06:17:05.748: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Aug 30 06:17:05.748: INFO: e2e test version: v1.26.6
    Aug 30 06:17:05.750: INFO: kube-apiserver version: v1.26.6+73ac561
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 30 06:17:05.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:17:05.766: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:05.791
Aug 30 06:17:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:17:05.792
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:05.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:05.874
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-93db08cc-212b-49a6-a072-32b607adb558 08/30/23 06:17:05.905
STEP: Creating a pod to test consume secrets 08/30/23 06:17:05.919
Aug 30 06:17:05.952: INFO: Waiting up to 5m0s for pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f" in namespace "secrets-852" to be "Succeeded or Failed"
Aug 30 06:17:05.969: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.966492ms
Aug 30 06:17:07.982: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030214038s
Aug 30 06:17:10.010: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058153866s
Aug 30 06:17:11.976: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024590983s
STEP: Saw pod success 08/30/23 06:17:11.976
Aug 30 06:17:11.977: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f" satisfied condition "Succeeded or Failed"
Aug 30 06:17:11.983: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:17:12.012
Aug 30 06:17:12.029: INFO: Waiting for pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f to disappear
Aug 30 06:17:12.036: INFO: Pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-852" for this suite. 08/30/23 06:17:12.049
------------------------------
• [SLOW TEST] [6.275 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:05.791
    Aug 30 06:17:05.791: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:17:05.792
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:05.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:05.874
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-93db08cc-212b-49a6-a072-32b607adb558 08/30/23 06:17:05.905
    STEP: Creating a pod to test consume secrets 08/30/23 06:17:05.919
    Aug 30 06:17:05.952: INFO: Waiting up to 5m0s for pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f" in namespace "secrets-852" to be "Succeeded or Failed"
    Aug 30 06:17:05.969: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.966492ms
    Aug 30 06:17:07.982: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030214038s
    Aug 30 06:17:10.010: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058153866s
    Aug 30 06:17:11.976: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024590983s
    STEP: Saw pod success 08/30/23 06:17:11.976
    Aug 30 06:17:11.977: INFO: Pod "pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f" satisfied condition "Succeeded or Failed"
    Aug 30 06:17:11.983: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:17:12.012
    Aug 30 06:17:12.029: INFO: Waiting for pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f to disappear
    Aug 30 06:17:12.036: INFO: Pod pod-secrets-080eaa6b-4421-481f-842a-0f2c5df83d0f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-852" for this suite. 08/30/23 06:17:12.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:12.069
Aug 30 06:17:12.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption 08/30/23 06:17:12.071
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:12.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:12.128
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/30/23 06:17:12.142
STEP: Updating PodDisruptionBudget status 08/30/23 06:17:14.158
STEP: Waiting for all pods to be running 08/30/23 06:17:14.174
Aug 30 06:17:14.184: INFO: running pods: 0 < 1
Aug 30 06:17:16.194: INFO: running pods: 0 < 1
STEP: locating a running pod 08/30/23 06:17:18.204
STEP: Waiting for the pdb to be processed 08/30/23 06:17:18.234
STEP: Patching PodDisruptionBudget status 08/30/23 06:17:18.25
STEP: Waiting for the pdb to be processed 08/30/23 06:17:18.268
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:18.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7018" for this suite. 08/30/23 06:17:18.284
------------------------------
• [SLOW TEST] [6.269 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:12.069
    Aug 30 06:17:12.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption 08/30/23 06:17:12.071
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:12.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:12.128
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:12.142
    STEP: Updating PodDisruptionBudget status 08/30/23 06:17:14.158
    STEP: Waiting for all pods to be running 08/30/23 06:17:14.174
    Aug 30 06:17:14.184: INFO: running pods: 0 < 1
    Aug 30 06:17:16.194: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/30/23 06:17:18.204
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:18.234
    STEP: Patching PodDisruptionBudget status 08/30/23 06:17:18.25
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:18.268
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:18.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7018" for this suite. 08/30/23 06:17:18.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:18.339
Aug 30 06:17:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 06:17:18.341
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:18.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:18.448
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 30 06:17:18.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:19.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1773" for this suite. 08/30/23 06:17:19.589
------------------------------
• [1.331 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:18.339
    Aug 30 06:17:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 06:17:18.341
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:18.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:18.448
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 30 06:17:18.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:19.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1773" for this suite. 08/30/23 06:17:19.589
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:19.671
Aug 30 06:17:19.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption 08/30/23 06:17:19.672
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:19.766
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:19.784
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:19.79
Aug 30 06:17:19.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption-2 08/30/23 06:17:19.796
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:19.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:19.888
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/30/23 06:17:19.916
STEP: Waiting for the pdb to be processed 08/30/23 06:17:21.957
STEP: Waiting for the pdb to be processed 08/30/23 06:17:24.012
STEP: listing a collection of PDBs across all namespaces 08/30/23 06:17:24.06
STEP: listing a collection of PDBs in namespace disruption-9922 08/30/23 06:17:24.082
STEP: deleting a collection of PDBs 08/30/23 06:17:24.089
STEP: Waiting for the PDB collection to be deleted 08/30/23 06:17:24.122
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:24.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1306" for this suite. 08/30/23 06:17:24.148
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9922" for this suite. 08/30/23 06:17:24.174
------------------------------
• [4.525 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:19.671
    Aug 30 06:17:19.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption 08/30/23 06:17:19.672
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:19.766
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:19.784
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:19.79
    Aug 30 06:17:19.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption-2 08/30/23 06:17:19.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:19.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:19.888
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:19.916
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:21.957
    STEP: Waiting for the pdb to be processed 08/30/23 06:17:24.012
    STEP: listing a collection of PDBs across all namespaces 08/30/23 06:17:24.06
    STEP: listing a collection of PDBs in namespace disruption-9922 08/30/23 06:17:24.082
    STEP: deleting a collection of PDBs 08/30/23 06:17:24.089
    STEP: Waiting for the PDB collection to be deleted 08/30/23 06:17:24.122
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:24.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1306" for this suite. 08/30/23 06:17:24.148
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9922" for this suite. 08/30/23 06:17:24.174
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:24.197
Aug 30 06:17:24.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 06:17:24.198
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:24.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:24.281
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/30/23 06:17:24.287
Aug 30 06:17:24.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 create -f -'
Aug 30 06:17:25.765: INFO: stderr: ""
Aug 30 06:17:25.765: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:25.765
Aug 30 06:17:25.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:25.888: INFO: stderr: ""
Aug 30 06:17:25.888: INFO: stdout: "update-demo-nautilus-7kqtp "
STEP: Replicas for name=update-demo: expected=2 actual=1 08/30/23 06:17:25.888
Aug 30 06:17:30.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:31.001: INFO: stderr: ""
Aug 30 06:17:31.001: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
Aug 30 06:17:31.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:31.105: INFO: stderr: ""
Aug 30 06:17:31.105: INFO: stdout: ""
Aug 30 06:17:31.105: INFO: update-demo-nautilus-7kqtp is created but not running
Aug 30 06:17:36.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:36.211: INFO: stderr: ""
Aug 30 06:17:36.211: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
Aug 30 06:17:36.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:36.309: INFO: stderr: ""
Aug 30 06:17:36.309: INFO: stdout: "true"
Aug 30 06:17:36.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 06:17:36.437: INFO: stderr: ""
Aug 30 06:17:36.437: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 06:17:36.437: INFO: validating pod update-demo-nautilus-7kqtp
Aug 30 06:17:36.453: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 06:17:36.453: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 06:17:36.453: INFO: update-demo-nautilus-7kqtp is verified up and running
Aug 30 06:17:36.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:36.558: INFO: stderr: ""
Aug 30 06:17:36.558: INFO: stdout: "true"
Aug 30 06:17:36.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 06:17:36.647: INFO: stderr: ""
Aug 30 06:17:36.647: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 06:17:36.647: INFO: validating pod update-demo-nautilus-rlnns
Aug 30 06:17:36.662: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 06:17:36.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 06:17:36.662: INFO: update-demo-nautilus-rlnns is verified up and running
STEP: scaling down the replication controller 08/30/23 06:17:36.662
Aug 30 06:17:36.666: INFO: scanned /root for discovery docs: <nil>
Aug 30 06:17:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 30 06:17:37.787: INFO: stderr: ""
Aug 30 06:17:37.787: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:37.787
Aug 30 06:17:37.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:37.896: INFO: stderr: ""
Aug 30 06:17:37.896: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/30/23 06:17:37.896
Aug 30 06:17:42.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:43.091: INFO: stderr: ""
Aug 30 06:17:43.091: INFO: stdout: "update-demo-nautilus-rlnns "
Aug 30 06:17:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:43.521: INFO: stderr: ""
Aug 30 06:17:43.521: INFO: stdout: "true"
Aug 30 06:17:43.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 06:17:43.860: INFO: stderr: ""
Aug 30 06:17:43.861: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 06:17:43.861: INFO: validating pod update-demo-nautilus-rlnns
Aug 30 06:17:44.001: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 06:17:44.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 06:17:44.001: INFO: update-demo-nautilus-rlnns is verified up and running
STEP: scaling up the replication controller 08/30/23 06:17:44.001
Aug 30 06:17:44.009: INFO: scanned /root for discovery docs: <nil>
Aug 30 06:17:44.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 30 06:17:45.456: INFO: stderr: ""
Aug 30 06:17:45.456: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:45.456
Aug 30 06:17:45.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 06:17:45.843: INFO: stderr: ""
Aug 30 06:17:45.843: INFO: stdout: "update-demo-nautilus-rlnns update-demo-nautilus-wj97n "
Aug 30 06:17:45.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:46.279: INFO: stderr: ""
Aug 30 06:17:46.279: INFO: stdout: "true"
Aug 30 06:17:46.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 06:17:46.646: INFO: stderr: ""
Aug 30 06:17:46.646: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 06:17:46.646: INFO: validating pod update-demo-nautilus-rlnns
Aug 30 06:17:46.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 06:17:46.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 06:17:46.671: INFO: update-demo-nautilus-rlnns is verified up and running
Aug 30 06:17:46.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-wj97n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 06:17:46.904: INFO: stderr: ""
Aug 30 06:17:46.904: INFO: stdout: "true"
Aug 30 06:17:46.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-wj97n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 06:17:47.114: INFO: stderr: ""
Aug 30 06:17:47.114: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 06:17:47.114: INFO: validating pod update-demo-nautilus-wj97n
Aug 30 06:17:47.144: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 06:17:47.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 06:17:47.144: INFO: update-demo-nautilus-wj97n is verified up and running
STEP: using delete to clean up resources 08/30/23 06:17:47.145
Aug 30 06:17:47.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 delete --grace-period=0 --force -f -'
Aug 30 06:17:47.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 06:17:47.364: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 06:17:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get rc,svc -l name=update-demo --no-headers'
Aug 30 06:17:47.611: INFO: stderr: "No resources found in kubectl-7607 namespace.\n"
Aug 30 06:17:47.611: INFO: stdout: ""
Aug 30 06:17:47.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 06:17:47.947: INFO: stderr: ""
Aug 30 06:17:47.948: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:47.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7607" for this suite. 08/30/23 06:17:47.973
------------------------------
• [SLOW TEST] [23.816 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:24.197
    Aug 30 06:17:24.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 06:17:24.198
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:24.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:24.281
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/30/23 06:17:24.287
    Aug 30 06:17:24.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 create -f -'
    Aug 30 06:17:25.765: INFO: stderr: ""
    Aug 30 06:17:25.765: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:25.765
    Aug 30 06:17:25.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:25.888: INFO: stderr: ""
    Aug 30 06:17:25.888: INFO: stdout: "update-demo-nautilus-7kqtp "
    STEP: Replicas for name=update-demo: expected=2 actual=1 08/30/23 06:17:25.888
    Aug 30 06:17:30.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:31.001: INFO: stderr: ""
    Aug 30 06:17:31.001: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
    Aug 30 06:17:31.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:31.105: INFO: stderr: ""
    Aug 30 06:17:31.105: INFO: stdout: ""
    Aug 30 06:17:31.105: INFO: update-demo-nautilus-7kqtp is created but not running
    Aug 30 06:17:36.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:36.211: INFO: stderr: ""
    Aug 30 06:17:36.211: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
    Aug 30 06:17:36.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:36.309: INFO: stderr: ""
    Aug 30 06:17:36.309: INFO: stdout: "true"
    Aug 30 06:17:36.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-7kqtp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 06:17:36.437: INFO: stderr: ""
    Aug 30 06:17:36.437: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 06:17:36.437: INFO: validating pod update-demo-nautilus-7kqtp
    Aug 30 06:17:36.453: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 06:17:36.453: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 06:17:36.453: INFO: update-demo-nautilus-7kqtp is verified up and running
    Aug 30 06:17:36.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:36.558: INFO: stderr: ""
    Aug 30 06:17:36.558: INFO: stdout: "true"
    Aug 30 06:17:36.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 06:17:36.647: INFO: stderr: ""
    Aug 30 06:17:36.647: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 06:17:36.647: INFO: validating pod update-demo-nautilus-rlnns
    Aug 30 06:17:36.662: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 06:17:36.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 06:17:36.662: INFO: update-demo-nautilus-rlnns is verified up and running
    STEP: scaling down the replication controller 08/30/23 06:17:36.662
    Aug 30 06:17:36.666: INFO: scanned /root for discovery docs: <nil>
    Aug 30 06:17:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 30 06:17:37.787: INFO: stderr: ""
    Aug 30 06:17:37.787: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:37.787
    Aug 30 06:17:37.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:37.896: INFO: stderr: ""
    Aug 30 06:17:37.896: INFO: stdout: "update-demo-nautilus-7kqtp update-demo-nautilus-rlnns "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/30/23 06:17:37.896
    Aug 30 06:17:42.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:43.091: INFO: stderr: ""
    Aug 30 06:17:43.091: INFO: stdout: "update-demo-nautilus-rlnns "
    Aug 30 06:17:43.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:43.521: INFO: stderr: ""
    Aug 30 06:17:43.521: INFO: stdout: "true"
    Aug 30 06:17:43.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 06:17:43.860: INFO: stderr: ""
    Aug 30 06:17:43.861: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 06:17:43.861: INFO: validating pod update-demo-nautilus-rlnns
    Aug 30 06:17:44.001: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 06:17:44.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 06:17:44.001: INFO: update-demo-nautilus-rlnns is verified up and running
    STEP: scaling up the replication controller 08/30/23 06:17:44.001
    Aug 30 06:17:44.009: INFO: scanned /root for discovery docs: <nil>
    Aug 30 06:17:44.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 30 06:17:45.456: INFO: stderr: ""
    Aug 30 06:17:45.456: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 06:17:45.456
    Aug 30 06:17:45.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 06:17:45.843: INFO: stderr: ""
    Aug 30 06:17:45.843: INFO: stdout: "update-demo-nautilus-rlnns update-demo-nautilus-wj97n "
    Aug 30 06:17:45.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:46.279: INFO: stderr: ""
    Aug 30 06:17:46.279: INFO: stdout: "true"
    Aug 30 06:17:46.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-rlnns -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 06:17:46.646: INFO: stderr: ""
    Aug 30 06:17:46.646: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 06:17:46.646: INFO: validating pod update-demo-nautilus-rlnns
    Aug 30 06:17:46.671: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 06:17:46.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 06:17:46.671: INFO: update-demo-nautilus-rlnns is verified up and running
    Aug 30 06:17:46.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-wj97n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 06:17:46.904: INFO: stderr: ""
    Aug 30 06:17:46.904: INFO: stdout: "true"
    Aug 30 06:17:46.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods update-demo-nautilus-wj97n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 06:17:47.114: INFO: stderr: ""
    Aug 30 06:17:47.114: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 06:17:47.114: INFO: validating pod update-demo-nautilus-wj97n
    Aug 30 06:17:47.144: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 06:17:47.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 06:17:47.144: INFO: update-demo-nautilus-wj97n is verified up and running
    STEP: using delete to clean up resources 08/30/23 06:17:47.145
    Aug 30 06:17:47.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 delete --grace-period=0 --force -f -'
    Aug 30 06:17:47.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 06:17:47.364: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 30 06:17:47.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get rc,svc -l name=update-demo --no-headers'
    Aug 30 06:17:47.611: INFO: stderr: "No resources found in kubectl-7607 namespace.\n"
    Aug 30 06:17:47.611: INFO: stdout: ""
    Aug 30 06:17:47.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7607 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 30 06:17:47.947: INFO: stderr: ""
    Aug 30 06:17:47.948: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:47.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7607" for this suite. 08/30/23 06:17:47.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:48.014
Aug 30 06:17:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:17:48.015
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:48.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:48.118
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-141f35be-33d6-46d2-b917-279d1591cabc 08/30/23 06:17:48.144
STEP: Creating a pod to test consume secrets 08/30/23 06:17:48.158
Aug 30 06:17:48.188: INFO: Waiting up to 5m0s for pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50" in namespace "secrets-6306" to be "Succeeded or Failed"
Aug 30 06:17:48.214: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 25.534137ms
Aug 30 06:17:50.222: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033448727s
Aug 30 06:17:52.225: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03706373s
Aug 30 06:17:54.229: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040713778s
STEP: Saw pod success 08/30/23 06:17:54.229
Aug 30 06:17:54.229: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50" satisfied condition "Succeeded or Failed"
Aug 30 06:17:54.236: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:17:54.255
Aug 30 06:17:54.279: INFO: Waiting for pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 to disappear
Aug 30 06:17:54.287: INFO: Pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:54.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6306" for this suite. 08/30/23 06:17:54.298
------------------------------
• [SLOW TEST] [6.301 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:48.014
    Aug 30 06:17:48.014: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:17:48.015
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:48.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:48.118
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-141f35be-33d6-46d2-b917-279d1591cabc 08/30/23 06:17:48.144
    STEP: Creating a pod to test consume secrets 08/30/23 06:17:48.158
    Aug 30 06:17:48.188: INFO: Waiting up to 5m0s for pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50" in namespace "secrets-6306" to be "Succeeded or Failed"
    Aug 30 06:17:48.214: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 25.534137ms
    Aug 30 06:17:50.222: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033448727s
    Aug 30 06:17:52.225: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03706373s
    Aug 30 06:17:54.229: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040713778s
    STEP: Saw pod success 08/30/23 06:17:54.229
    Aug 30 06:17:54.229: INFO: Pod "pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50" satisfied condition "Succeeded or Failed"
    Aug 30 06:17:54.236: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:17:54.255
    Aug 30 06:17:54.279: INFO: Waiting for pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 to disappear
    Aug 30 06:17:54.287: INFO: Pod pod-secrets-9adc72c9-b46e-484b-a096-11fb6c86aa50 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:54.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6306" for this suite. 08/30/23 06:17:54.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:54.317
Aug 30 06:17:54.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:17:54.318
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:54.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:54.37
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-958db332-6642-46b8-8e78-a278bd88615b 08/30/23 06:17:54.454
STEP: Creating a pod to test consume secrets 08/30/23 06:17:54.469
Aug 30 06:17:54.506: INFO: Waiting up to 5m0s for pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7" in namespace "secrets-4809" to be "Succeeded or Failed"
Aug 30 06:17:54.516: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.597618ms
Aug 30 06:17:56.525: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018909252s
Aug 30 06:17:58.539: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032966625s
STEP: Saw pod success 08/30/23 06:17:58.539
Aug 30 06:17:58.539: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7" satisfied condition "Succeeded or Failed"
Aug 30 06:17:58.548: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:17:58.573
Aug 30 06:17:58.611: INFO: Waiting for pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 to disappear
Aug 30 06:17:58.618: INFO: Pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:17:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4809" for this suite. 08/30/23 06:17:58.639
STEP: Destroying namespace "secret-namespace-2432" for this suite. 08/30/23 06:17:58.662
------------------------------
• [4.384 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:54.317
    Aug 30 06:17:54.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:17:54.318
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:54.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:54.37
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-958db332-6642-46b8-8e78-a278bd88615b 08/30/23 06:17:54.454
    STEP: Creating a pod to test consume secrets 08/30/23 06:17:54.469
    Aug 30 06:17:54.506: INFO: Waiting up to 5m0s for pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7" in namespace "secrets-4809" to be "Succeeded or Failed"
    Aug 30 06:17:54.516: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.597618ms
    Aug 30 06:17:56.525: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018909252s
    Aug 30 06:17:58.539: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032966625s
    STEP: Saw pod success 08/30/23 06:17:58.539
    Aug 30 06:17:58.539: INFO: Pod "pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7" satisfied condition "Succeeded or Failed"
    Aug 30 06:17:58.548: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:17:58.573
    Aug 30 06:17:58.611: INFO: Waiting for pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 to disappear
    Aug 30 06:17:58.618: INFO: Pod pod-secrets-6dab6335-de71-43ee-9c20-4b7a2269a2c7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:17:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4809" for this suite. 08/30/23 06:17:58.639
    STEP: Destroying namespace "secret-namespace-2432" for this suite. 08/30/23 06:17:58.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:17:58.731
Aug 30 06:17:58.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:17:58.733
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:58.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:58.808
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:17:58.836
Aug 30 06:17:58.862: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2506" to be "running and ready"
Aug 30 06:17:58.875: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.989327ms
Aug 30 06:17:58.875: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:00.894: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032178294s
Aug 30 06:18:00.894: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:02.904: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042450203s
Aug 30 06:18:02.904: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:04.927: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064804104s
Aug 30 06:18:04.963: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:06.884: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.021908683s
Aug 30 06:18:06.884: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 30 06:18:06.884: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/30/23 06:18:06.891
Aug 30 06:18:06.902: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2506" to be "running and ready"
Aug 30 06:18:06.914: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91968ms
Aug 30 06:18:06.914: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:08.923: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021052299s
Aug 30 06:18:08.923: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:18:10.921: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.019273838s
Aug 30 06:18:10.921: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 30 06:18:10.921: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/30/23 06:18:10.927
STEP: delete the pod with lifecycle hook 08/30/23 06:18:10.98
Aug 30 06:18:11.000: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 06:18:11.009: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 06:18:13.012: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 06:18:13.023: INFO: Pod pod-with-poststart-http-hook still exists
Aug 30 06:18:15.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 30 06:18:15.020: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 30 06:18:15.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2506" for this suite. 08/30/23 06:18:15.03
------------------------------
• [SLOW TEST] [16.372 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:17:58.731
    Aug 30 06:17:58.732: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:17:58.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:17:58.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:17:58.808
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:17:58.836
    Aug 30 06:17:58.862: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2506" to be "running and ready"
    Aug 30 06:17:58.875: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.989327ms
    Aug 30 06:17:58.875: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:00.894: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032178294s
    Aug 30 06:18:00.894: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:02.904: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042450203s
    Aug 30 06:18:02.904: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:04.927: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064804104s
    Aug 30 06:18:04.963: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:06.884: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 8.021908683s
    Aug 30 06:18:06.884: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 30 06:18:06.884: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/30/23 06:18:06.891
    Aug 30 06:18:06.902: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-2506" to be "running and ready"
    Aug 30 06:18:06.914: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91968ms
    Aug 30 06:18:06.914: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:08.923: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021052299s
    Aug 30 06:18:08.923: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:18:10.921: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.019273838s
    Aug 30 06:18:10.921: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 30 06:18:10.921: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/30/23 06:18:10.927
    STEP: delete the pod with lifecycle hook 08/30/23 06:18:10.98
    Aug 30 06:18:11.000: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 30 06:18:11.009: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 30 06:18:13.012: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 30 06:18:13.023: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 30 06:18:15.011: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 30 06:18:15.020: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:18:15.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2506" for this suite. 08/30/23 06:18:15.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:18:15.104
Aug 30 06:18:15.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:18:15.106
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:15.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:15.195
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 30 06:18:15.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/30/23 06:18:21.98
Aug 30 06:18:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
Aug 30 06:18:24.087: INFO: stderr: ""
Aug 30 06:18:24.087: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 06:18:24.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 delete e2e-test-crd-publish-openapi-6669-crds test-foo'
Aug 30 06:18:24.201: INFO: stderr: ""
Aug 30 06:18:24.201: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 30 06:18:24.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
Aug 30 06:18:26.153: INFO: stderr: ""
Aug 30 06:18:26.153: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 30 06:18:26.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 delete e2e-test-crd-publish-openapi-6669-crds test-foo'
Aug 30 06:18:26.266: INFO: stderr: ""
Aug 30 06:18:26.266: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/30/23 06:18:26.266
Aug 30 06:18:26.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
Aug 30 06:18:27.690: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/30/23 06:18:27.69
Aug 30 06:18:27.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
Aug 30 06:18:28.214: INFO: rc: 1
Aug 30 06:18:28.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
Aug 30 06:18:28.799: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/30/23 06:18:28.799
Aug 30 06:18:28.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
Aug 30 06:18:29.287: INFO: rc: 1
Aug 30 06:18:29.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
Aug 30 06:18:29.955: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/30/23 06:18:29.955
Aug 30 06:18:29.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds'
Aug 30 06:18:30.497: INFO: stderr: ""
Aug 30 06:18:30.497: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/30/23 06:18:30.497
Aug 30 06:18:30.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.metadata'
Aug 30 06:18:31.034: INFO: stderr: ""
Aug 30 06:18:31.034: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 30 06:18:31.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec'
Aug 30 06:18:31.644: INFO: stderr: ""
Aug 30 06:18:31.644: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 30 06:18:31.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec.bars'
Aug 30 06:18:32.223: INFO: stderr: ""
Aug 30 06:18:32.223: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/30/23 06:18:32.223
Aug 30 06:18:32.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec.bars2'
Aug 30 06:18:32.733: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:18:38.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8958" for this suite. 08/30/23 06:18:38.948
------------------------------
• [SLOW TEST] [23.898 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:18:15.104
    Aug 30 06:18:15.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:18:15.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:15.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:15.195
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 30 06:18:15.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/30/23 06:18:21.98
    Aug 30 06:18:21.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
    Aug 30 06:18:24.087: INFO: stderr: ""
    Aug 30 06:18:24.087: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 30 06:18:24.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 delete e2e-test-crd-publish-openapi-6669-crds test-foo'
    Aug 30 06:18:24.201: INFO: stderr: ""
    Aug 30 06:18:24.201: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 30 06:18:24.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
    Aug 30 06:18:26.153: INFO: stderr: ""
    Aug 30 06:18:26.153: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 30 06:18:26.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 delete e2e-test-crd-publish-openapi-6669-crds test-foo'
    Aug 30 06:18:26.266: INFO: stderr: ""
    Aug 30 06:18:26.266: INFO: stdout: "e2e-test-crd-publish-openapi-6669-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/30/23 06:18:26.266
    Aug 30 06:18:26.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
    Aug 30 06:18:27.690: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/30/23 06:18:27.69
    Aug 30 06:18:27.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
    Aug 30 06:18:28.214: INFO: rc: 1
    Aug 30 06:18:28.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
    Aug 30 06:18:28.799: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/30/23 06:18:28.799
    Aug 30 06:18:28.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 create -f -'
    Aug 30 06:18:29.287: INFO: rc: 1
    Aug 30 06:18:29.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 --namespace=crd-publish-openapi-8958 apply -f -'
    Aug 30 06:18:29.955: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/30/23 06:18:29.955
    Aug 30 06:18:29.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds'
    Aug 30 06:18:30.497: INFO: stderr: ""
    Aug 30 06:18:30.497: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/30/23 06:18:30.497
    Aug 30 06:18:30.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.metadata'
    Aug 30 06:18:31.034: INFO: stderr: ""
    Aug 30 06:18:31.034: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 30 06:18:31.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec'
    Aug 30 06:18:31.644: INFO: stderr: ""
    Aug 30 06:18:31.644: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 30 06:18:31.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec.bars'
    Aug 30 06:18:32.223: INFO: stderr: ""
    Aug 30 06:18:32.223: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6669-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/30/23 06:18:32.223
    Aug 30 06:18:32.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8958 explain e2e-test-crd-publish-openapi-6669-crds.spec.bars2'
    Aug 30 06:18:32.733: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:18:38.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8958" for this suite. 08/30/23 06:18:38.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:18:39.005
Aug 30 06:18:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 06:18:39.006
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:39.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:39.113
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/30/23 06:18:39.12
Aug 30 06:18:39.170: INFO: Waiting up to 5m0s for pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788" in namespace "var-expansion-6263" to be "Succeeded or Failed"
Aug 30 06:18:39.182: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 11.395368ms
Aug 30 06:18:41.190: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020062878s
Aug 30 06:18:43.190: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019776037s
Aug 30 06:18:45.191: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02034663s
STEP: Saw pod success 08/30/23 06:18:45.191
Aug 30 06:18:45.191: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788" satisfied condition "Succeeded or Failed"
Aug 30 06:18:45.198: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 container dapi-container: <nil>
STEP: delete the pod 08/30/23 06:18:45.214
Aug 30 06:18:45.241: INFO: Waiting for pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 to disappear
Aug 30 06:18:45.248: INFO: Pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 06:18:45.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6263" for this suite. 08/30/23 06:18:45.257
------------------------------
• [SLOW TEST] [6.274 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:18:39.005
    Aug 30 06:18:39.005: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 06:18:39.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:39.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:39.113
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/30/23 06:18:39.12
    Aug 30 06:18:39.170: INFO: Waiting up to 5m0s for pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788" in namespace "var-expansion-6263" to be "Succeeded or Failed"
    Aug 30 06:18:39.182: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 11.395368ms
    Aug 30 06:18:41.190: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020062878s
    Aug 30 06:18:43.190: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019776037s
    Aug 30 06:18:45.191: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02034663s
    STEP: Saw pod success 08/30/23 06:18:45.191
    Aug 30 06:18:45.191: INFO: Pod "var-expansion-53e86f53-a330-4975-9ce8-709b74964788" satisfied condition "Succeeded or Failed"
    Aug 30 06:18:45.198: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 06:18:45.214
    Aug 30 06:18:45.241: INFO: Waiting for pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 to disappear
    Aug 30 06:18:45.248: INFO: Pod var-expansion-53e86f53-a330-4975-9ce8-709b74964788 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:18:45.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6263" for this suite. 08/30/23 06:18:45.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:18:45.28
Aug 30 06:18:45.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 06:18:45.281
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:45.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:45.386
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 30 06:18:45.393: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/30/23 06:18:45.496
STEP: Checking rc "condition-test" has the desired failure condition set 08/30/23 06:18:45.508
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/30/23 06:18:46.568
Aug 30 06:18:46.590: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/30/23 06:18:46.604
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:18:46.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6032" for this suite. 08/30/23 06:18:46.664
------------------------------
• [1.422 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:18:45.28
    Aug 30 06:18:45.281: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 06:18:45.281
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:45.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:45.386
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 30 06:18:45.393: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/30/23 06:18:45.496
    STEP: Checking rc "condition-test" has the desired failure condition set 08/30/23 06:18:45.508
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/30/23 06:18:46.568
    Aug 30 06:18:46.590: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/30/23 06:18:46.604
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:18:46.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6032" for this suite. 08/30/23 06:18:46.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:18:46.703
Aug 30 06:18:46.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 06:18:46.704
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:46.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:46.77
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/30/23 06:18:46.775
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_tcp@PTR;sleep 1; done
 08/30/23 06:18:46.881
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_tcp@PTR;sleep 1; done
 08/30/23 06:18:46.881
STEP: creating a pod to probe DNS 08/30/23 06:18:46.881
STEP: submitting the pod to kubernetes 08/30/23 06:18:46.882
Aug 30 06:18:46.965: INFO: Waiting up to 15m0s for pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca" in namespace "dns-5798" to be "running"
Aug 30 06:18:47.031: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 66.272382ms
Aug 30 06:18:49.041: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076282222s
Aug 30 06:18:51.040: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074991531s
Aug 30 06:18:53.040: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074912072s
Aug 30 06:18:55.044: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Running", Reason="", readiness=true. Elapsed: 8.078668269s
Aug 30 06:18:55.044: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:18:55.044
STEP: looking for the results for each expected name from probers 08/30/23 06:18:55.052
Aug 30 06:18:55.266: INFO: DNS probes using dns-5798/dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca succeeded

STEP: deleting the pod 08/30/23 06:18:55.266
STEP: deleting the test service 08/30/23 06:18:55.292
STEP: deleting the test headless service 08/30/23 06:18:55.375
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 06:18:55.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5798" for this suite. 08/30/23 06:18:55.45
------------------------------
• [SLOW TEST] [8.770 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:18:46.703
    Aug 30 06:18:46.703: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 06:18:46.704
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:46.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:46.77
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/30/23 06:18:46.775
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_tcp@PTR;sleep 1; done
     08/30/23 06:18:46.881
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5798.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5798.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5798.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_udp@PTR;check="$$(dig +tcp +noall +answer +search 124.246.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.246.124_tcp@PTR;sleep 1; done
     08/30/23 06:18:46.881
    STEP: creating a pod to probe DNS 08/30/23 06:18:46.881
    STEP: submitting the pod to kubernetes 08/30/23 06:18:46.882
    Aug 30 06:18:46.965: INFO: Waiting up to 15m0s for pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca" in namespace "dns-5798" to be "running"
    Aug 30 06:18:47.031: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 66.272382ms
    Aug 30 06:18:49.041: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076282222s
    Aug 30 06:18:51.040: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074991531s
    Aug 30 06:18:53.040: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074912072s
    Aug 30 06:18:55.044: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca": Phase="Running", Reason="", readiness=true. Elapsed: 8.078668269s
    Aug 30 06:18:55.044: INFO: Pod "dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:18:55.044
    STEP: looking for the results for each expected name from probers 08/30/23 06:18:55.052
    Aug 30 06:18:55.266: INFO: DNS probes using dns-5798/dns-test-98ecf6a2-a22f-44f6-bc09-cd9a7020c1ca succeeded

    STEP: deleting the pod 08/30/23 06:18:55.266
    STEP: deleting the test service 08/30/23 06:18:55.292
    STEP: deleting the test headless service 08/30/23 06:18:55.375
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:18:55.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5798" for this suite. 08/30/23 06:18:55.45
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:18:55.474
Aug 30 06:18:55.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 06:18:55.475
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:55.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:55.537
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1142 08/30/23 06:18:55.542
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-1142 08/30/23 06:18:55.557
W0830 06:18:55.588848      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1142 08/30/23 06:18:55.588
Aug 30 06:18:55.612: INFO: Found 0 stateful pods, waiting for 1
Aug 30 06:19:05.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/30/23 06:19:05.621
Aug 30 06:19:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:19:05.926: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:19:05.926: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:19:05.926: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:19:05.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 06:19:15.942: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:19:15.942: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:19:16.009: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 30 06:19:16.009: INFO: ss-0  10.135.139.190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
Aug 30 06:19:16.009: INFO: 
Aug 30 06:19:16.009: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 30 06:19:17.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.963810776s
Aug 30 06:19:18.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952696558s
Aug 30 06:19:19.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942831157s
Aug 30 06:19:20.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.933700737s
Aug 30 06:19:21.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.924541073s
Aug 30 06:19:22.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916650981s
Aug 30 06:19:23.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907468575s
Aug 30 06:19:24.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898012768s
Aug 30 06:19:25.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 882.692469ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1142 08/30/23 06:19:26.1
Aug 30 06:19:26.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:19:26.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 06:19:26.379: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:19:26.379: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:19:26.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:19:26.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 06:19:26.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:19:26.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:19:26.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:19:26.897: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 30 06:19:26.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:19:26.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:19:26.908: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 30 06:19:36.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:19:36.919: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:19:36.919: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/30/23 06:19:36.919
Aug 30 06:19:36.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:19:37.179: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:19:37.179: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:19:37.179: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:19:37.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:19:37.430: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:19:37.430: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:19:37.430: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:19:37.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:19:37.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:19:37.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:19:37.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:19:37.655: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:19:37.665: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 30 06:19:47.710: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:19:47.710: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:19:47.710: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:19:47.765: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 30 06:19:47.765: INFO: ss-0  10.135.139.190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
Aug 30 06:19:47.765: INFO: ss-1  10.135.139.185  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
Aug 30 06:19:47.765: INFO: ss-2  10.135.139.183  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
Aug 30 06:19:47.765: INFO: 
Aug 30 06:19:47.765: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 06:19:48.773: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 30 06:19:48.773: INFO: ss-0  10.135.139.190  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
Aug 30 06:19:48.773: INFO: ss-1  10.135.139.185  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
Aug 30 06:19:48.773: INFO: ss-2  10.135.139.183  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
Aug 30 06:19:48.773: INFO: 
Aug 30 06:19:48.773: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 30 06:19:49.790: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Aug 30 06:19:49.790: INFO: ss-2  10.135.139.183  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
Aug 30 06:19:49.790: INFO: 
Aug 30 06:19:49.790: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 30 06:19:50.803: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964363892s
Aug 30 06:19:51.811: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95150562s
Aug 30 06:19:52.821: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.943689028s
Aug 30 06:19:53.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.932558892s
Aug 30 06:19:54.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.924545054s
Aug 30 06:19:55.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.914919565s
Aug 30 06:19:56.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 905.6816ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1142 08/30/23 06:19:57.858
Aug 30 06:19:57.865: INFO: Scaling statefulset ss to 0
Aug 30 06:19:57.893: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 06:19:57.902: INFO: Deleting all statefulset in ns statefulset-1142
Aug 30 06:19:57.911: INFO: Scaling statefulset ss to 0
Aug 30 06:19:57.937: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:19:57.946: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:19:57.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1142" for this suite. 08/30/23 06:19:57.996
------------------------------
• [SLOW TEST] [62.554 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:18:55.474
    Aug 30 06:18:55.474: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 06:18:55.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:18:55.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:18:55.537
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1142 08/30/23 06:18:55.542
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-1142 08/30/23 06:18:55.557
    W0830 06:18:55.588848      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1142 08/30/23 06:18:55.588
    Aug 30 06:18:55.612: INFO: Found 0 stateful pods, waiting for 1
    Aug 30 06:19:05.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/30/23 06:19:05.621
    Aug 30 06:19:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:19:05.926: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:19:05.926: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:19:05.926: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:19:05.934: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 30 06:19:15.942: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:19:15.942: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:19:16.009: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 30 06:19:16.009: INFO: ss-0  10.135.139.190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
    Aug 30 06:19:16.009: INFO: 
    Aug 30 06:19:16.009: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 30 06:19:17.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.963810776s
    Aug 30 06:19:18.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.952696558s
    Aug 30 06:19:19.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.942831157s
    Aug 30 06:19:20.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.933700737s
    Aug 30 06:19:21.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.924541073s
    Aug 30 06:19:22.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916650981s
    Aug 30 06:19:23.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.907468575s
    Aug 30 06:19:24.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898012768s
    Aug 30 06:19:25.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 882.692469ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1142 08/30/23 06:19:26.1
    Aug 30 06:19:26.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:19:26.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 06:19:26.379: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:19:26.379: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:19:26.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:19:26.617: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 30 06:19:26.617: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:19:26.617: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:19:26.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:19:26.897: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 30 06:19:26.897: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:19:26.897: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:19:26.908: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 30 06:19:36.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:19:36.919: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:19:36.919: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/30/23 06:19:36.919
    Aug 30 06:19:36.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:19:37.179: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:19:37.179: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:19:37.179: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:19:37.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:19:37.430: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:19:37.430: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:19:37.430: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:19:37.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-1142 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:19:37.655: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:19:37.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:19:37.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:19:37.655: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:19:37.665: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 30 06:19:47.710: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:19:47.710: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:19:47.710: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:19:47.765: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 30 06:19:47.765: INFO: ss-0  10.135.139.190  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
    Aug 30 06:19:47.765: INFO: ss-1  10.135.139.185  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
    Aug 30 06:19:47.765: INFO: ss-2  10.135.139.183  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
    Aug 30 06:19:47.765: INFO: 
    Aug 30 06:19:47.765: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 30 06:19:48.773: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 30 06:19:48.773: INFO: ss-0  10.135.139.190  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:18:55 +0000 UTC  }]
    Aug 30 06:19:48.773: INFO: ss-1  10.135.139.185  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
    Aug 30 06:19:48.773: INFO: ss-2  10.135.139.183  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
    Aug 30 06:19:48.773: INFO: 
    Aug 30 06:19:48.773: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 30 06:19:49.790: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
    Aug 30 06:19:49.790: INFO: ss-2  10.135.139.183  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 06:19:16 +0000 UTC  }]
    Aug 30 06:19:49.790: INFO: 
    Aug 30 06:19:49.790: INFO: StatefulSet ss has not reached scale 0, at 1
    Aug 30 06:19:50.803: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.964363892s
    Aug 30 06:19:51.811: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.95150562s
    Aug 30 06:19:52.821: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.943689028s
    Aug 30 06:19:53.830: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.932558892s
    Aug 30 06:19:54.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.924545054s
    Aug 30 06:19:55.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.914919565s
    Aug 30 06:19:56.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 905.6816ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1142 08/30/23 06:19:57.858
    Aug 30 06:19:57.865: INFO: Scaling statefulset ss to 0
    Aug 30 06:19:57.893: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 06:19:57.902: INFO: Deleting all statefulset in ns statefulset-1142
    Aug 30 06:19:57.911: INFO: Scaling statefulset ss to 0
    Aug 30 06:19:57.937: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:19:57.946: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:19:57.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1142" for this suite. 08/30/23 06:19:57.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:19:58.03
Aug 30 06:19:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 06:19:58.031
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:19:58.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:19:58.082
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:19:58.087
W0830 06:19:58.114085      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:19:58.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954" in namespace "downward-api-2419" to be "Succeeded or Failed"
Aug 30 06:19:58.125: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 11.266387ms
Aug 30 06:20:00.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019198302s
Aug 30 06:20:02.149: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035325168s
Aug 30 06:20:04.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019289254s
STEP: Saw pod success 08/30/23 06:20:04.133
Aug 30 06:20:04.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954" satisfied condition "Succeeded or Failed"
Aug 30 06:20:04.141: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 container client-container: <nil>
STEP: delete the pod 08/30/23 06:20:04.158
Aug 30 06:20:04.176: INFO: Waiting for pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 to disappear
Aug 30 06:20:04.182: INFO: Pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2419" for this suite. 08/30/23 06:20:04.191
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:19:58.03
    Aug 30 06:19:58.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 06:19:58.031
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:19:58.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:19:58.082
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:19:58.087
    W0830 06:19:58.114085      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:19:58.114: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954" in namespace "downward-api-2419" to be "Succeeded or Failed"
    Aug 30 06:19:58.125: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 11.266387ms
    Aug 30 06:20:00.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019198302s
    Aug 30 06:20:02.149: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035325168s
    Aug 30 06:20:04.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019289254s
    STEP: Saw pod success 08/30/23 06:20:04.133
    Aug 30 06:20:04.133: INFO: Pod "downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954" satisfied condition "Succeeded or Failed"
    Aug 30 06:20:04.141: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:20:04.158
    Aug 30 06:20:04.176: INFO: Waiting for pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 to disappear
    Aug 30 06:20:04.182: INFO: Pod downwardapi-volume-df79043f-eb1e-4ea8-8305-15176f651954 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2419" for this suite. 08/30/23 06:20:04.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:04.212
Aug 30 06:20:04.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:20:04.213
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:04.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:04.277
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-a5bb3e2e-4179-446f-9f2f-9b20a70ec998 08/30/23 06:20:04.284
STEP: Creating a pod to test consume secrets 08/30/23 06:20:04.295
W0830 06:20:04.316447      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:20:04.316: INFO: Waiting up to 5m0s for pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe" in namespace "secrets-2840" to be "Succeeded or Failed"
Aug 30 06:20:04.324: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.16792ms
Aug 30 06:20:06.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017213245s
Aug 30 06:20:08.334: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017827371s
Aug 30 06:20:10.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016457653s
STEP: Saw pod success 08/30/23 06:20:10.333
Aug 30 06:20:10.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe" satisfied condition "Succeeded or Failed"
Aug 30 06:20:10.340: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:20:10.362
Aug 30 06:20:10.390: INFO: Waiting for pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe to disappear
Aug 30 06:20:10.400: INFO: Pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:10.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2840" for this suite. 08/30/23 06:20:10.41
------------------------------
• [SLOW TEST] [6.216 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:04.212
    Aug 30 06:20:04.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:20:04.213
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:04.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:04.277
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-a5bb3e2e-4179-446f-9f2f-9b20a70ec998 08/30/23 06:20:04.284
    STEP: Creating a pod to test consume secrets 08/30/23 06:20:04.295
    W0830 06:20:04.316447      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:20:04.316: INFO: Waiting up to 5m0s for pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe" in namespace "secrets-2840" to be "Succeeded or Failed"
    Aug 30 06:20:04.324: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.16792ms
    Aug 30 06:20:06.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017213245s
    Aug 30 06:20:08.334: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017827371s
    Aug 30 06:20:10.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016457653s
    STEP: Saw pod success 08/30/23 06:20:10.333
    Aug 30 06:20:10.333: INFO: Pod "pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe" satisfied condition "Succeeded or Failed"
    Aug 30 06:20:10.340: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:20:10.362
    Aug 30 06:20:10.390: INFO: Waiting for pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe to disappear
    Aug 30 06:20:10.400: INFO: Pod pod-secrets-db052e46-3022-4b6f-82b8-e8e032eda2fe no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:10.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2840" for this suite. 08/30/23 06:20:10.41
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:10.429
Aug 30 06:20:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:20:10.43
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:10.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:10.484
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/30/23 06:20:10.489
STEP: Ensuring ResourceQuota status is calculated 08/30/23 06:20:10.506
STEP: Creating a ResourceQuota with not terminating scope 08/30/23 06:20:12.516
STEP: Ensuring ResourceQuota status is calculated 08/30/23 06:20:12.535
STEP: Creating a long running pod 08/30/23 06:20:14.546
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/30/23 06:20:14.574
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/30/23 06:20:16.584
STEP: Deleting the pod 08/30/23 06:20:18.594
STEP: Ensuring resource quota status released the pod usage 08/30/23 06:20:18.613
STEP: Creating a terminating pod 08/30/23 06:20:20.623
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/30/23 06:20:20.646
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/30/23 06:20:22.657
STEP: Deleting the pod 08/30/23 06:20:24.667
STEP: Ensuring resource quota status released the pod usage 08/30/23 06:20:24.685
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:26.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9820" for this suite. 08/30/23 06:20:26.708
------------------------------
• [SLOW TEST] [16.296 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:10.429
    Aug 30 06:20:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:20:10.43
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:10.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:10.484
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/30/23 06:20:10.489
    STEP: Ensuring ResourceQuota status is calculated 08/30/23 06:20:10.506
    STEP: Creating a ResourceQuota with not terminating scope 08/30/23 06:20:12.516
    STEP: Ensuring ResourceQuota status is calculated 08/30/23 06:20:12.535
    STEP: Creating a long running pod 08/30/23 06:20:14.546
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/30/23 06:20:14.574
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/30/23 06:20:16.584
    STEP: Deleting the pod 08/30/23 06:20:18.594
    STEP: Ensuring resource quota status released the pod usage 08/30/23 06:20:18.613
    STEP: Creating a terminating pod 08/30/23 06:20:20.623
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/30/23 06:20:20.646
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/30/23 06:20:22.657
    STEP: Deleting the pod 08/30/23 06:20:24.667
    STEP: Ensuring resource quota status released the pod usage 08/30/23 06:20:24.685
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:26.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9820" for this suite. 08/30/23 06:20:26.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:26.727
Aug 30 06:20:26.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:20:26.728
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:26.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:26.777
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/30/23 06:20:26.783
STEP: Creating a ResourceQuota 08/30/23 06:20:31.795
STEP: Ensuring resource quota status is calculated 08/30/23 06:20:31.828
STEP: Creating a Service 08/30/23 06:20:33.862
STEP: Creating a NodePort Service 08/30/23 06:20:34.048
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/30/23 06:20:34.214
STEP: Ensuring resource quota status captures service creation 08/30/23 06:20:34.362
STEP: Deleting Services 08/30/23 06:20:36.371
STEP: Ensuring resource quota status released usage 08/30/23 06:20:36.501
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:38.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-490" for this suite. 08/30/23 06:20:38.52
------------------------------
• [SLOW TEST] [11.810 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:26.727
    Aug 30 06:20:26.727: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:20:26.728
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:26.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:26.777
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/30/23 06:20:26.783
    STEP: Creating a ResourceQuota 08/30/23 06:20:31.795
    STEP: Ensuring resource quota status is calculated 08/30/23 06:20:31.828
    STEP: Creating a Service 08/30/23 06:20:33.862
    STEP: Creating a NodePort Service 08/30/23 06:20:34.048
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/30/23 06:20:34.214
    STEP: Ensuring resource quota status captures service creation 08/30/23 06:20:34.362
    STEP: Deleting Services 08/30/23 06:20:36.371
    STEP: Ensuring resource quota status released usage 08/30/23 06:20:36.501
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:38.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-490" for this suite. 08/30/23 06:20:38.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:38.543
Aug 30 06:20:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 06:20:38.544
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:38.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:38.601
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/30/23 06:20:38.606
W0830 06:20:38.627353      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 08/30/23 06:20:38.627
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:50.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2339" for this suite. 08/30/23 06:20:50.657
------------------------------
• [SLOW TEST] [12.150 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:38.543
    Aug 30 06:20:38.543: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 06:20:38.544
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:38.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:38.601
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/30/23 06:20:38.606
    W0830 06:20:38.627353      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 08/30/23 06:20:38.627
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:50.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2339" for this suite. 08/30/23 06:20:50.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:50.694
Aug 30 06:20:50.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-webhook 08/30/23 06:20:50.695
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:50.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:50.751
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/30/23 06:20:50.781
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/30/23 06:20:51.378
STEP: Deploying the custom resource conversion webhook pod 08/30/23 06:20:51.47
STEP: Wait for the deployment to be ready 08/30/23 06:20:51.524
Aug 30 06:20:51.579: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:20:53.629
STEP: Verifying the service has paired with the endpoint 08/30/23 06:20:53.678
Aug 30 06:20:54.679: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 30 06:20:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Creating a v1 custom resource 08/30/23 06:20:57.366
STEP: v2 custom resource should be converted 08/30/23 06:20:57.377
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-9605" for this suite. 08/30/23 06:20:58.188
------------------------------
• [SLOW TEST] [7.595 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:50.694
    Aug 30 06:20:50.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-webhook 08/30/23 06:20:50.695
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:50.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:50.751
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/30/23 06:20:50.781
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/30/23 06:20:51.378
    STEP: Deploying the custom resource conversion webhook pod 08/30/23 06:20:51.47
    STEP: Wait for the deployment to be ready 08/30/23 06:20:51.524
    Aug 30 06:20:51.579: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:20:53.629
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:20:53.678
    Aug 30 06:20:54.679: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 30 06:20:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Creating a v1 custom resource 08/30/23 06:20:57.366
    STEP: v2 custom resource should be converted 08/30/23 06:20:57.377
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:57.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-9605" for this suite. 08/30/23 06:20:58.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:58.291
Aug 30 06:20:58.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubelet-test 08/30/23 06:20:58.292
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:58.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:58.496
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:20:58.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-1202" for this suite. 08/30/23 06:20:58.743
------------------------------
• [0.566 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:58.291
    Aug 30 06:20:58.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubelet-test 08/30/23 06:20:58.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:58.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:58.496
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:20:58.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-1202" for this suite. 08/30/23 06:20:58.743
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:20:58.86
Aug 30 06:20:58.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-pred 08/30/23 06:20:58.861
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:58.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:58.952
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 30 06:20:58.957: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 06:20:59.049: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 06:20:59.085: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.183 before test
Aug 30 06:20:59.122: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 06:20:59.123: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:20:59.123: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:20:59.123: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 06:20:59.123: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.123: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:20:59.124: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:20:59.124: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:20:59.124: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Aug 30 06:20:59.124: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Aug 30 06:20:59.124: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 06:20:59.124: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:20:59.124: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 06:20:59.124: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container vpn ready: true, restart count 1
Aug 30 06:20:59.124: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 06:20:59.124: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:20:59.124: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.124: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 06:20:59.124: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 06:20:59.125: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 06:20:59.125: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 06:20:59.125: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 06:20:59.125: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Aug 30 06:20:59.125: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 06:20:59.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.125: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:20:59.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.125: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:20:59.125: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 06:20:59.125: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.125: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:20:59.125: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:20:59.126: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.126: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 06:20:59.126: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.126: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 06:20:59.126: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 06:20:59.126: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 06:20:59.126: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.126: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:20:59.126: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:20:59.126: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.126: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:20:59.126: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.127: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.127: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:20:59.127: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.127: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 06:20:59.127: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.127: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:20:59.127: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.127: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 06:20:59.127: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.127: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 06:20:59.127: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 06:20:59.128: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:20:59.128: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container metrics ready: true, restart count 2
Aug 30 06:20:59.128: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 06:20:59.128: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 06:20:59.128: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:20:59.128: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:20:59.129: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.185 before test
Aug 30 06:20:59.206: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:20:59.206: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:20:59.206: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:20:59.206: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:20:59.206: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:20:59.206: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:20:59.206: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:20:59.206: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:20:59.206: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container console ready: true, restart count 0
Aug 30 06:20:59.206: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:20:59.206: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:20:59.206: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container registry ready: true, restart count 0
Aug 30 06:20:59.206: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:20:59.206: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:20:59.206: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container router ready: true, restart count 0
Aug 30 06:20:59.206: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container migrator ready: true, restart count 0
Aug 30 06:20:59.206: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:20:59.206: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:20:59.206: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:20:59.206: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:20:59.206: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:20:59.206: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 06:20:59.206: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:20:59.206: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:20:59.206: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 06:20:59.206: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.206: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:20:59.206: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:20:59.207: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 06:20:59.207: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:20:59.207: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:20:59.207: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:20:59.207: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:20:59.207: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:20:59.207: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:20:59.207: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:20:59.207: INFO: collect-profiles-28222905-lg5th from openshift-operator-lifecycle-manager started at 2023-08-30 05:45:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:20:59.207: INFO: collect-profiles-28222920-565wn from openshift-operator-lifecycle-manager started at 2023-08-30 06:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:20:59.207: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:20:59.207: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:20:59.207: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container service-ca-controller ready: true, restart count 0
Aug 30 06:20:59.207: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container e2e ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:20:59.207: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:20:59.207: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:20:59.207: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.207: INFO: 	Container tigera-operator ready: true, restart count 6
Aug 30 06:20:59.207: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.190 before test
Aug 30 06:20:59.268: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:20:59.269: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:20:59.269: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-qprs9 from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:20:59.269: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:20:59.269: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:20:59.269: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:20:59.269: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:20:59.269: INFO: bin-false702e339b-f1bf-4db6-a0e7-7e696327b354 from kubelet-test-1202 started at 2023-08-30 06:20:58 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.269: INFO: 	Container bin-false702e339b-f1bf-4db6-a0e7-7e696327b354 ready: false, restart count 0
Aug 30 06:20:59.269: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:20:59.270: INFO: csi-snapshot-controller-b5685b8b7-ngwpq from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:20:59.270: INFO: csi-snapshot-webhook-645cd76dd7-zhxgz from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:20:59.270: INFO: console-76ccb968f7-vbr6g from openshift-console started at 2023-08-30 04:06:47 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container console ready: true, restart count 0
Aug 30 06:20:59.270: INFO: downloads-55ff47758f-h27k2 from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:20:59.270: INFO: dns-default-7pjrk from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:20:59.270: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.270: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:20:59.270: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:20:59.270: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 06:20:59.270: INFO: ingress-canary-49qtb from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.270: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:20:59.270: INFO: router-default-9c97f6b97-nf95w from openshift-ingress started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container router ready: true, restart count 0
Aug 30 06:20:59.271: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 04:04:44 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:20:59.271: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:20:59.271: INFO: prometheus-adapter-5d4fdc4794-dmn57 from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:20:59.271: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 04:04:58 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.271: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:20:59.271: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:20:59.272: INFO: prometheus-operator-admission-webhook-748bd6896b-whl7j from openshift-monitoring started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.272: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:20:59.272: INFO: telemeter-client-cc9864968-5b52m from openshift-monitoring started at 2023-08-30 04:04:45 +0000 UTC (3 container statuses recorded)
Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container reload ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 06:20:59.272: INFO: thanos-querier-7bd6db4456-j4sgj from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:20:59.272: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.272: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:20:59.272: INFO: multus-admission-controller-68f648d7b7-bgrhp from openshift-multus started at 2023-08-30 04:02:48 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.272: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:20:59.272: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:20:59.273: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:20:59.273: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:20:59.273: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:20:59.273: INFO: network-operator-68ffb666f9-dn5j6 from openshift-network-operator started at 2023-08-30 03:53:54 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container network-operator ready: true, restart count 1
Aug 30 06:20:59.273: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 06:20:59.273: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:20:59.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:20:59.273: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/30/23 06:20:59.273
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.178014eafb96ebc3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/30/23 06:20:59.381
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:21:00.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-8274" for this suite. 08/30/23 06:21:00.383
------------------------------
• [1.546 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:20:58.86
    Aug 30 06:20:58.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-pred 08/30/23 06:20:58.861
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:20:58.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:20:58.952
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 30 06:20:58.957: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 30 06:20:59.049: INFO: Waiting for terminating namespaces to be deleted...
    Aug 30 06:20:59.085: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.183 before test
    Aug 30 06:20:59.122: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 30 06:20:59.123: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:20:59.123: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Aug 30 06:20:59.123: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:20:59.123: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Aug 30 06:20:59.123: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.123: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container vpn ready: true, restart count 1
    Aug 30 06:20:59.124: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.124: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Aug 30 06:20:59.124: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Aug 30 06:20:59.125: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container console-operator ready: true, restart count 1
    Aug 30 06:20:59.125: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Aug 30 06:20:59.125: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container dns-operator ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.125: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:20:59.125: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container ingress-operator ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container insights-operator ready: true, restart count 1
    Aug 30 06:20:59.126: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Aug 30 06:20:59.126: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container marketplace-operator ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.126: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:20:59.126: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.127: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.127: INFO: 	Container check-endpoints ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.127: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.127: INFO: 	Container catalog-operator ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.127: INFO: 	Container olm-operator ready: true, restart count 0
    Aug 30 06:20:59.127: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container package-server-manager ready: true, restart count 0
    Aug 30 06:20:59.128: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:20:59.128: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container metrics ready: true, restart count 2
    Aug 30 06:20:59.128: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container push-gateway ready: true, restart count 0
    Aug 30 06:20:59.128: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container service-ca-operator ready: true, restart count 1
    Aug 30 06:20:59.128: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.128: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:20:59.128: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:20:59.129: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.185 before test
    Aug 30 06:20:59.206: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container registry ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container migrator ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:20:59.206: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.206: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:20:59.206: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: collect-profiles-28222905-lg5th from openshift-operator-lifecycle-manager started at 2023-08-30 05:45:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:20:59.207: INFO: collect-profiles-28222920-565wn from openshift-operator-lifecycle-manager started at 2023-08-30 06:00:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:20:59.207: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:20:59.207: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container service-ca-controller ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container e2e ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:20:59.207: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.207: INFO: 	Container tigera-operator ready: true, restart count 6
    Aug 30 06:20:59.207: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.190 before test
    Aug 30 06:20:59.268: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-qprs9 from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:20:59.269: INFO: bin-false702e339b-f1bf-4db6-a0e7-7e696327b354 from kubelet-test-1202 started at 2023-08-30 06:20:58 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.269: INFO: 	Container bin-false702e339b-f1bf-4db6-a0e7-7e696327b354 ready: false, restart count 0
    Aug 30 06:20:59.269: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: csi-snapshot-controller-b5685b8b7-ngwpq from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: csi-snapshot-webhook-645cd76dd7-zhxgz from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: console-76ccb968f7-vbr6g from openshift-console started at 2023-08-30 04:06:47 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: downloads-55ff47758f-h27k2 from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: dns-default-7pjrk from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container pvc-permissions ready: false, restart count 0
    Aug 30 06:20:59.270: INFO: ingress-canary-49qtb from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.270: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:20:59.270: INFO: router-default-9c97f6b97-nf95w from openshift-ingress started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 04:04:44 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:20:59.271: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: prometheus-adapter-5d4fdc4794-dmn57 from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 04:04:58 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.271: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:20:59.271: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: prometheus-operator-admission-webhook-748bd6896b-whl7j from openshift-monitoring started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.272: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: telemeter-client-cc9864968-5b52m from openshift-monitoring started at 2023-08-30 04:04:45 +0000 UTC (3 container statuses recorded)
    Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container reload ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container telemeter-client ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: thanos-querier-7bd6db4456-j4sgj from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
    Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.272: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: multus-admission-controller-68f648d7b7-bgrhp from openshift-multus started at 2023-08-30 04:02:48 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.272: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:20:59.272: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: network-operator-68ffb666f9-dn5j6 from openshift-network-operator started at 2023-08-30 03:53:54 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container network-operator ready: true, restart count 1
    Aug 30 06:20:59.273: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:20:59.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:20:59.273: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/30/23 06:20:59.273
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.178014eafb96ebc3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 08/30/23 06:20:59.381
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:21:00.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-8274" for this suite. 08/30/23 06:21:00.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:21:00.408
Aug 30 06:21:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:21:00.409
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:21:00.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:21:00.57
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/30/23 06:21:00.575
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:21:00.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9095" for this suite. 08/30/23 06:21:00.663
------------------------------
• [0.290 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:21:00.408
    Aug 30 06:21:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:21:00.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:21:00.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:21:00.57
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/30/23 06:21:00.575
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:21:00.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9095" for this suite. 08/30/23 06:21:00.663
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:21:00.699
Aug 30 06:21:00.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 06:21:00.7
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:21:00.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:21:00.775
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8984 08/30/23 06:21:00.781
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/30/23 06:21:00.804
Aug 30 06:21:00.856: INFO: Found 0 stateful pods, waiting for 3
Aug 30 06:21:10.865: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:21:10.865: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:21:10.865: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/30/23 06:21:10.89
Aug 30 06:21:10.927: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/30/23 06:21:10.927
STEP: Not applying an update when the partition is greater than the number of replicas 08/30/23 06:21:20.978
STEP: Performing a canary update 08/30/23 06:21:20.979
Aug 30 06:21:21.030: INFO: Updating stateful set ss2
Aug 30 06:21:21.048: INFO: Waiting for Pod statefulset-8984/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/30/23 06:21:31.112
Aug 30 06:21:31.431: INFO: Found 2 stateful pods, waiting for 3
Aug 30 06:21:41.440: INFO: Found 2 stateful pods, waiting for 3
Aug 30 06:21:51.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:21:51.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:21:51.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 30 06:22:01.443: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:22:01.443: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:22:01.443: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 30 06:22:11.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:22:11.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:22:11.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/30/23 06:22:11.458
Aug 30 06:22:11.497: INFO: Updating stateful set ss2
Aug 30 06:22:11.516: INFO: Waiting for Pod statefulset-8984/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 30 06:22:21.577: INFO: Updating stateful set ss2
Aug 30 06:22:21.592: INFO: Waiting for StatefulSet statefulset-8984/ss2 to complete update
Aug 30 06:22:21.592: INFO: Waiting for Pod statefulset-8984/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 06:22:31.620: INFO: Deleting all statefulset in ns statefulset-8984
Aug 30 06:22:31.631: INFO: Scaling statefulset ss2 to 0
Aug 30 06:22:41.677: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:22:41.691: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:22:41.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8984" for this suite. 08/30/23 06:22:41.799
------------------------------
• [SLOW TEST] [101.127 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:21:00.699
    Aug 30 06:21:00.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 06:21:00.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:21:00.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:21:00.775
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8984 08/30/23 06:21:00.781
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/30/23 06:21:00.804
    Aug 30 06:21:00.856: INFO: Found 0 stateful pods, waiting for 3
    Aug 30 06:21:10.865: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:21:10.865: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:21:10.865: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/30/23 06:21:10.89
    Aug 30 06:21:10.927: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/30/23 06:21:10.927
    STEP: Not applying an update when the partition is greater than the number of replicas 08/30/23 06:21:20.978
    STEP: Performing a canary update 08/30/23 06:21:20.979
    Aug 30 06:21:21.030: INFO: Updating stateful set ss2
    Aug 30 06:21:21.048: INFO: Waiting for Pod statefulset-8984/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/30/23 06:21:31.112
    Aug 30 06:21:31.431: INFO: Found 2 stateful pods, waiting for 3
    Aug 30 06:21:41.440: INFO: Found 2 stateful pods, waiting for 3
    Aug 30 06:21:51.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:21:51.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:21:51.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
    Aug 30 06:22:01.443: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:22:01.443: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:22:01.443: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
    Aug 30 06:22:11.441: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:22:11.441: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:22:11.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/30/23 06:22:11.458
    Aug 30 06:22:11.497: INFO: Updating stateful set ss2
    Aug 30 06:22:11.516: INFO: Waiting for Pod statefulset-8984/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 30 06:22:21.577: INFO: Updating stateful set ss2
    Aug 30 06:22:21.592: INFO: Waiting for StatefulSet statefulset-8984/ss2 to complete update
    Aug 30 06:22:21.592: INFO: Waiting for Pod statefulset-8984/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 06:22:31.620: INFO: Deleting all statefulset in ns statefulset-8984
    Aug 30 06:22:31.631: INFO: Scaling statefulset ss2 to 0
    Aug 30 06:22:41.677: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:22:41.691: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:22:41.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8984" for this suite. 08/30/23 06:22:41.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:22:41.827
Aug 30 06:22:41.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename proxy 08/30/23 06:22:41.827
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:41.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:41.885
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 30 06:22:41.890: INFO: Creating pod...
W0830 06:22:41.968811      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:22:41.968: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8479" to be "running"
Aug 30 06:22:41.981: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777482ms
Aug 30 06:22:44.015: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.046187795s
Aug 30 06:22:44.015: INFO: Pod "agnhost" satisfied condition "running"
Aug 30 06:22:44.015: INFO: Creating service...
Aug 30 06:22:44.056: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=DELETE
Aug 30 06:22:44.174: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 06:22:44.174: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=OPTIONS
Aug 30 06:22:44.256: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 06:22:44.256: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=PATCH
Aug 30 06:22:44.276: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 06:22:44.276: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=POST
Aug 30 06:22:44.343: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 06:22:44.354: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=PUT
Aug 30 06:22:44.441: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 06:22:44.441: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 30 06:22:44.522: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 06:22:44.522: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 30 06:22:44.593: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 06:22:44.593: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 30 06:22:44.617: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 06:22:44.617: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=POST
Aug 30 06:22:44.641: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 06:22:44.642: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=PUT
Aug 30 06:22:44.657: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 06:22:44.657: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=GET
Aug 30 06:22:44.664: INFO: http.Client request:GET StatusCode:301
Aug 30 06:22:44.664: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=GET
Aug 30 06:22:44.687: INFO: http.Client request:GET StatusCode:301
Aug 30 06:22:44.687: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=HEAD
Aug 30 06:22:44.694: INFO: http.Client request:HEAD StatusCode:301
Aug 30 06:22:44.694: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 30 06:22:44.707: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 30 06:22:44.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8479" for this suite. 08/30/23 06:22:44.72
------------------------------
• [2.912 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:22:41.827
    Aug 30 06:22:41.827: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename proxy 08/30/23 06:22:41.827
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:41.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:41.885
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 30 06:22:41.890: INFO: Creating pod...
    W0830 06:22:41.968811      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:22:41.968: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8479" to be "running"
    Aug 30 06:22:41.981: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 12.777482ms
    Aug 30 06:22:44.015: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.046187795s
    Aug 30 06:22:44.015: INFO: Pod "agnhost" satisfied condition "running"
    Aug 30 06:22:44.015: INFO: Creating service...
    Aug 30 06:22:44.056: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=DELETE
    Aug 30 06:22:44.174: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 30 06:22:44.174: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=OPTIONS
    Aug 30 06:22:44.256: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 30 06:22:44.256: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=PATCH
    Aug 30 06:22:44.276: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 30 06:22:44.276: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=POST
    Aug 30 06:22:44.343: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 30 06:22:44.354: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=PUT
    Aug 30 06:22:44.441: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 30 06:22:44.441: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 30 06:22:44.522: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 30 06:22:44.522: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 30 06:22:44.593: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 30 06:22:44.593: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 30 06:22:44.617: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 30 06:22:44.617: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=POST
    Aug 30 06:22:44.641: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 30 06:22:44.642: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 30 06:22:44.657: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 30 06:22:44.657: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=GET
    Aug 30 06:22:44.664: INFO: http.Client request:GET StatusCode:301
    Aug 30 06:22:44.664: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=GET
    Aug 30 06:22:44.687: INFO: http.Client request:GET StatusCode:301
    Aug 30 06:22:44.687: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/pods/agnhost/proxy?method=HEAD
    Aug 30 06:22:44.694: INFO: http.Client request:HEAD StatusCode:301
    Aug 30 06:22:44.694: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-8479/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 30 06:22:44.707: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:22:44.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8479" for this suite. 08/30/23 06:22:44.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:22:44.747
Aug 30 06:22:44.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-webhook 08/30/23 06:22:44.748
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:44.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:44.841
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/30/23 06:22:44.847
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/30/23 06:22:45.21
STEP: Deploying the custom resource conversion webhook pod 08/30/23 06:22:45.245
STEP: Wait for the deployment to be ready 08/30/23 06:22:45.301
Aug 30 06:22:45.318: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:22:47.385
STEP: Verifying the service has paired with the endpoint 08/30/23 06:22:47.405
Aug 30 06:22:48.408: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 30 06:22:48.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Creating a v1 custom resource 08/30/23 06:22:51.144
STEP: Create a v2 custom resource 08/30/23 06:22:51.188
STEP: List CRs in v1 08/30/23 06:22:51.266
STEP: List CRs in v2 08/30/23 06:22:51.278
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:22:51.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-6145" for this suite. 08/30/23 06:22:51.942
------------------------------
• [SLOW TEST] [7.220 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:22:44.747
    Aug 30 06:22:44.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-webhook 08/30/23 06:22:44.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:44.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:44.841
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/30/23 06:22:44.847
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/30/23 06:22:45.21
    STEP: Deploying the custom resource conversion webhook pod 08/30/23 06:22:45.245
    STEP: Wait for the deployment to be ready 08/30/23 06:22:45.301
    Aug 30 06:22:45.318: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:22:47.385
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:22:47.405
    Aug 30 06:22:48.408: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 30 06:22:48.431: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Creating a v1 custom resource 08/30/23 06:22:51.144
    STEP: Create a v2 custom resource 08/30/23 06:22:51.188
    STEP: List CRs in v1 08/30/23 06:22:51.266
    STEP: List CRs in v2 08/30/23 06:22:51.278
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:22:51.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-6145" for this suite. 08/30/23 06:22:51.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:22:51.969
Aug 30 06:22:51.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 06:22:51.97
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:52.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:52.025
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/30/23 06:22:52.053
W0830 06:22:52.069408      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up. 08/30/23 06:22:52.069
Aug 30 06:22:52.077: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 06:22:57.101: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 06:22:57.101
STEP: Getting /status 08/30/23 06:22:57.101
Aug 30 06:22:57.114: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/30/23 06:22:57.114
Aug 30 06:22:57.159: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/30/23 06:22:57.159
Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: ADDED
Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.163: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.163: INFO: Found replicaset test-rs in namespace replicaset-3657 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 06:22:57.163: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/30/23 06:22:57.163
Aug 30 06:22:57.163: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 06:22:57.193: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/30/23 06:22:57.194
Aug 30 06:22:57.199: INFO: Observed &ReplicaSet event: ADDED
Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.200: INFO: Observed replicaset test-rs in namespace replicaset-3657 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 30 06:22:57.200: INFO: Found replicaset test-rs in namespace replicaset-3657 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 30 06:22:57.200: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:22:57.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3657" for this suite. 08/30/23 06:22:57.237
------------------------------
• [SLOW TEST] [5.295 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:22:51.969
    Aug 30 06:22:51.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 06:22:51.97
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:52.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:52.025
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/30/23 06:22:52.053
    W0830 06:22:52.069408      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up. 08/30/23 06:22:52.069
    Aug 30 06:22:52.077: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 30 06:22:57.101: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 06:22:57.101
    STEP: Getting /status 08/30/23 06:22:57.101
    Aug 30 06:22:57.114: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/30/23 06:22:57.114
    Aug 30 06:22:57.159: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/30/23 06:22:57.159
    Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: ADDED
    Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.162: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.163: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.163: INFO: Found replicaset test-rs in namespace replicaset-3657 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 30 06:22:57.163: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/30/23 06:22:57.163
    Aug 30 06:22:57.163: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 30 06:22:57.193: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/30/23 06:22:57.194
    Aug 30 06:22:57.199: INFO: Observed &ReplicaSet event: ADDED
    Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.200: INFO: Observed replicaset test-rs in namespace replicaset-3657 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 30 06:22:57.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 30 06:22:57.200: INFO: Found replicaset test-rs in namespace replicaset-3657 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 30 06:22:57.200: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:22:57.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3657" for this suite. 08/30/23 06:22:57.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:22:57.264
Aug 30 06:22:57.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 06:22:57.266
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:57.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:57.324
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/30/23 06:22:57.35
Aug 30 06:22:57.368: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4054  11ea06cc-e26a-4d79-b69a-19c4be40de4f 67291 0 2023-08-30 06:22:57 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-08-30 06:22:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7gdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7gdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:22:57.368: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4054" to be "running and ready"
Aug 30 06:22:57.426: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 57.825343ms
Aug 30 06:22:57.466: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:22:59.488: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.119054788s
Aug 30 06:22:59.488: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 30 06:22:59.488: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/30/23 06:22:59.488
Aug 30 06:22:59.488: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4054 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:22:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:22:59.489: INFO: ExecWithOptions: Clientset creation
Aug 30 06:22:59.489: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-4054/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/30/23 06:22:59.793
Aug 30 06:22:59.793: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4054 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:22:59.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:22:59.807: INFO: ExecWithOptions: Clientset creation
Aug 30 06:22:59.808: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-4054/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:23:00.045: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 06:23:00.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4054" for this suite. 08/30/23 06:23:00.157
------------------------------
• [2.927 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:22:57.264
    Aug 30 06:22:57.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 06:22:57.266
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:22:57.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:22:57.324
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/30/23 06:22:57.35
    Aug 30 06:22:57.368: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4054  11ea06cc-e26a-4d79-b69a-19c4be40de4f 67291 0 2023-08-30 06:22:57 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-08-30 06:22:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7gdn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7gdn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:22:57.368: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4054" to be "running and ready"
    Aug 30 06:22:57.426: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 57.825343ms
    Aug 30 06:22:57.466: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:22:59.488: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.119054788s
    Aug 30 06:22:59.488: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 30 06:22:59.488: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/30/23 06:22:59.488
    Aug 30 06:22:59.488: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4054 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:22:59.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:22:59.489: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:22:59.489: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-4054/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/30/23 06:22:59.793
    Aug 30 06:22:59.793: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4054 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:22:59.793: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:22:59.807: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:22:59.808: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-4054/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:23:00.045: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:23:00.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4054" for this suite. 08/30/23 06:23:00.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:23:00.2
Aug 30 06:23:00.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename containers 08/30/23 06:23:00.201
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:00.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:00.282
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/30/23 06:23:00.288
Aug 30 06:23:00.305: INFO: Waiting up to 5m0s for pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f" in namespace "containers-1125" to be "Succeeded or Failed"
Aug 30 06:23:00.314: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812392ms
Aug 30 06:23:02.357: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05192323s
Aug 30 06:23:04.350: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044826693s
STEP: Saw pod success 08/30/23 06:23:04.35
Aug 30 06:23:04.350: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f" satisfied condition "Succeeded or Failed"
Aug 30 06:23:04.390: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:23:04.481
Aug 30 06:23:04.533: INFO: Waiting for pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f to disappear
Aug 30 06:23:04.542: INFO: Pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 30 06:23:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1125" for this suite. 08/30/23 06:23:04.715
------------------------------
• [4.537 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:23:00.2
    Aug 30 06:23:00.200: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename containers 08/30/23 06:23:00.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:00.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:00.282
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/30/23 06:23:00.288
    Aug 30 06:23:00.305: INFO: Waiting up to 5m0s for pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f" in namespace "containers-1125" to be "Succeeded or Failed"
    Aug 30 06:23:00.314: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.812392ms
    Aug 30 06:23:02.357: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05192323s
    Aug 30 06:23:04.350: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044826693s
    STEP: Saw pod success 08/30/23 06:23:04.35
    Aug 30 06:23:04.350: INFO: Pod "client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f" satisfied condition "Succeeded or Failed"
    Aug 30 06:23:04.390: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:23:04.481
    Aug 30 06:23:04.533: INFO: Waiting for pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f to disappear
    Aug 30 06:23:04.542: INFO: Pod client-containers-20c4d0ad-1af3-447f-b259-0dcd187d9c2f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:23:04.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1125" for this suite. 08/30/23 06:23:04.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:23:04.739
Aug 30 06:23:04.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-runtime 08/30/23 06:23:04.74
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:04.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:04.849
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/30/23 06:23:04.898
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/30/23 06:23:23.11
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/30/23 06:23:23.119
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/30/23 06:23:23.134
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/30/23 06:23:23.134
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/30/23 06:23:23.235
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/30/23 06:23:26.267
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/30/23 06:23:28.291
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/30/23 06:23:28.304
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/30/23 06:23:28.305
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/30/23 06:23:28.344
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/30/23 06:23:29.36
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/30/23 06:23:32.394
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/30/23 06:23:32.408
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/30/23 06:23:32.408
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 30 06:23:32.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3347" for this suite. 08/30/23 06:23:32.475
------------------------------
• [SLOW TEST] [27.755 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:23:04.739
    Aug 30 06:23:04.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-runtime 08/30/23 06:23:04.74
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:04.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:04.849
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/30/23 06:23:04.898
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/30/23 06:23:23.11
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/30/23 06:23:23.119
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/30/23 06:23:23.134
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/30/23 06:23:23.134
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/30/23 06:23:23.235
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/30/23 06:23:26.267
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/30/23 06:23:28.291
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/30/23 06:23:28.304
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/30/23 06:23:28.305
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/30/23 06:23:28.344
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/30/23 06:23:29.36
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/30/23 06:23:32.394
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/30/23 06:23:32.408
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/30/23 06:23:32.408
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:23:32.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3347" for this suite. 08/30/23 06:23:32.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:23:32.496
Aug 30 06:23:32.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:23:32.497
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:32.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:32.543
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-9824 08/30/23 06:23:32.548
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/30/23 06:23:32.643
STEP: creating service externalsvc in namespace services-9824 08/30/23 06:23:32.643
STEP: creating replication controller externalsvc in namespace services-9824 08/30/23 06:23:32.672
I0830 06:23:32.682050      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9824, replica count: 2
I0830 06:23:35.736907      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/30/23 06:23:35.744
Aug 30 06:23:35.786: INFO: Creating new exec pod
Aug 30 06:23:35.809: INFO: Waiting up to 5m0s for pod "execpodslgbc" in namespace "services-9824" to be "running"
Aug 30 06:23:35.817: INFO: Pod "execpodslgbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.944706ms
Aug 30 06:23:37.826: INFO: Pod "execpodslgbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016577008s
Aug 30 06:23:37.826: INFO: Pod "execpodslgbc" satisfied condition "running"
Aug 30 06:23:37.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9824 exec execpodslgbc -- /bin/sh -x -c nslookup nodeport-service.services-9824.svc.cluster.local'
Aug 30 06:23:38.105: INFO: stderr: "+ nslookup nodeport-service.services-9824.svc.cluster.local\n"
Aug 30 06:23:38.105: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-9824.svc.cluster.local\tcanonical name = externalsvc.services-9824.svc.cluster.local.\nName:\texternalsvc.services-9824.svc.cluster.local\nAddress: 172.21.100.21\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9824, will wait for the garbage collector to delete the pods 08/30/23 06:23:38.105
Aug 30 06:23:38.178: INFO: Deleting ReplicationController externalsvc took: 15.043978ms
Aug 30 06:23:38.379: INFO: Terminating ReplicationController externalsvc pods took: 201.226245ms
Aug 30 06:23:41.011: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:23:41.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9824" for this suite. 08/30/23 06:23:41.042
------------------------------
• [SLOW TEST] [8.564 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:23:32.496
    Aug 30 06:23:32.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:23:32.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:32.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:32.543
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-9824 08/30/23 06:23:32.548
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/30/23 06:23:32.643
    STEP: creating service externalsvc in namespace services-9824 08/30/23 06:23:32.643
    STEP: creating replication controller externalsvc in namespace services-9824 08/30/23 06:23:32.672
    I0830 06:23:32.682050      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9824, replica count: 2
    I0830 06:23:35.736907      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/30/23 06:23:35.744
    Aug 30 06:23:35.786: INFO: Creating new exec pod
    Aug 30 06:23:35.809: INFO: Waiting up to 5m0s for pod "execpodslgbc" in namespace "services-9824" to be "running"
    Aug 30 06:23:35.817: INFO: Pod "execpodslgbc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.944706ms
    Aug 30 06:23:37.826: INFO: Pod "execpodslgbc": Phase="Running", Reason="", readiness=true. Elapsed: 2.016577008s
    Aug 30 06:23:37.826: INFO: Pod "execpodslgbc" satisfied condition "running"
    Aug 30 06:23:37.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9824 exec execpodslgbc -- /bin/sh -x -c nslookup nodeport-service.services-9824.svc.cluster.local'
    Aug 30 06:23:38.105: INFO: stderr: "+ nslookup nodeport-service.services-9824.svc.cluster.local\n"
    Aug 30 06:23:38.105: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-9824.svc.cluster.local\tcanonical name = externalsvc.services-9824.svc.cluster.local.\nName:\texternalsvc.services-9824.svc.cluster.local\nAddress: 172.21.100.21\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9824, will wait for the garbage collector to delete the pods 08/30/23 06:23:38.105
    Aug 30 06:23:38.178: INFO: Deleting ReplicationController externalsvc took: 15.043978ms
    Aug 30 06:23:38.379: INFO: Terminating ReplicationController externalsvc pods took: 201.226245ms
    Aug 30 06:23:41.011: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:23:41.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9824" for this suite. 08/30/23 06:23:41.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:23:41.061
Aug 30 06:23:41.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 06:23:41.062
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:41.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:41.216
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2324 08/30/23 06:23:41.226
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/30/23 06:23:41.237
STEP: Creating stateful set ss in namespace statefulset-2324 08/30/23 06:23:41.244
W0830 06:23:41.270212      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2324 08/30/23 06:23:41.27
Aug 30 06:23:41.277: INFO: Found 0 stateful pods, waiting for 1
Aug 30 06:23:51.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/30/23 06:23:51.286
Aug 30 06:23:51.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:23:51.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:23:51.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:23:51.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:23:51.550: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 30 06:24:01.560: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:24:01.560: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:24:01.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998155s
Aug 30 06:24:02.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98740305s
Aug 30 06:24:03.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979579193s
Aug 30 06:24:04.631: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970820528s
Aug 30 06:24:05.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.96169936s
Aug 30 06:24:06.684: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.923037588s
Aug 30 06:24:07.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.907651936s
Aug 30 06:24:08.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.900185307s
Aug 30 06:24:09.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.885716399s
Aug 30 06:24:10.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 877.225593ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2324 08/30/23 06:24:11.724
Aug 30 06:24:11.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:24:11.954: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 06:24:11.954: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:24:11.954: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:24:11.963: INFO: Found 1 stateful pods, waiting for 3
Aug 30 06:24:21.986: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:24:21.986: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 06:24:21.986: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/30/23 06:24:21.986
STEP: Scale down will halt with unhealthy stateful pod 08/30/23 06:24:21.986
Aug 30 06:24:22.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:24:22.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:24:22.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:24:22.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:24:22.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:24:22.978: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:24:22.978: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:24:22.978: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:24:22.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 06:24:23.223: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 06:24:23.223: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 06:24:23.223: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 06:24:23.223: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:24:23.236: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 30 06:24:33.257: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:24:33.257: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:24:33.257: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 30 06:24:33.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997828s
Aug 30 06:24:34.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990412708s
Aug 30 06:24:35.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981482554s
Aug 30 06:24:36.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972043765s
Aug 30 06:24:37.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962191161s
Aug 30 06:24:38.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952425095s
Aug 30 06:24:39.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942516832s
Aug 30 06:24:40.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.932444744s
Aug 30 06:24:41.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916467519s
Aug 30 06:24:42.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 902.840873ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2324 08/30/23 06:24:43.385
Aug 30 06:24:43.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:24:43.675: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 06:24:43.675: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:24:43.675: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:24:43.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:24:43.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 06:24:43.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:24:43.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:24:43.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 06:24:44.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 06:24:44.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 06:24:44.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 06:24:44.203: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/30/23 06:24:54.256
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 06:24:54.256: INFO: Deleting all statefulset in ns statefulset-2324
Aug 30 06:24:54.290: INFO: Scaling statefulset ss to 0
Aug 30 06:24:54.361: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 06:24:54.433: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:24:54.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2324" for this suite. 08/30/23 06:24:54.577
------------------------------
• [SLOW TEST] [73.566 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:23:41.061
    Aug 30 06:23:41.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 06:23:41.062
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:23:41.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:23:41.216
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2324 08/30/23 06:23:41.226
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/30/23 06:23:41.237
    STEP: Creating stateful set ss in namespace statefulset-2324 08/30/23 06:23:41.244
    W0830 06:23:41.270212      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2324 08/30/23 06:23:41.27
    Aug 30 06:23:41.277: INFO: Found 0 stateful pods, waiting for 1
    Aug 30 06:23:51.286: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/30/23 06:23:51.286
    Aug 30 06:23:51.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:23:51.542: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:23:51.542: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:23:51.542: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:23:51.550: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 30 06:24:01.560: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:24:01.560: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:24:01.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998155s
    Aug 30 06:24:02.613: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98740305s
    Aug 30 06:24:03.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.979579193s
    Aug 30 06:24:04.631: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.970820528s
    Aug 30 06:24:05.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.96169936s
    Aug 30 06:24:06.684: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.923037588s
    Aug 30 06:24:07.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.907651936s
    Aug 30 06:24:08.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.900185307s
    Aug 30 06:24:09.715: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.885716399s
    Aug 30 06:24:10.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 877.225593ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2324 08/30/23 06:24:11.724
    Aug 30 06:24:11.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:24:11.954: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 06:24:11.954: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:24:11.954: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:24:11.963: INFO: Found 1 stateful pods, waiting for 3
    Aug 30 06:24:21.986: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:24:21.986: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 06:24:21.986: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/30/23 06:24:21.986
    STEP: Scale down will halt with unhealthy stateful pod 08/30/23 06:24:21.986
    Aug 30 06:24:22.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:24:22.548: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:24:22.548: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:24:22.548: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:24:22.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:24:22.978: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:24:22.978: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:24:22.978: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:24:22.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 06:24:23.223: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 06:24:23.223: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 06:24:23.223: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 06:24:23.223: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:24:23.236: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
    Aug 30 06:24:33.257: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:24:33.257: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:24:33.257: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 30 06:24:33.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997828s
    Aug 30 06:24:34.297: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990412708s
    Aug 30 06:24:35.306: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981482554s
    Aug 30 06:24:36.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972043765s
    Aug 30 06:24:37.325: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962191161s
    Aug 30 06:24:38.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952425095s
    Aug 30 06:24:39.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942516832s
    Aug 30 06:24:40.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.932444744s
    Aug 30 06:24:41.376: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916467519s
    Aug 30 06:24:42.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 902.840873ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2324 08/30/23 06:24:43.385
    Aug 30 06:24:43.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:24:43.675: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 06:24:43.675: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:24:43.675: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:24:43.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:24:43.956: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 06:24:43.956: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:24:43.956: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:24:43.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-2324 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 06:24:44.203: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 06:24:44.203: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 06:24:44.203: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 06:24:44.203: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/30/23 06:24:54.256
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 06:24:54.256: INFO: Deleting all statefulset in ns statefulset-2324
    Aug 30 06:24:54.290: INFO: Scaling statefulset ss to 0
    Aug 30 06:24:54.361: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 06:24:54.433: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:24:54.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2324" for this suite. 08/30/23 06:24:54.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:24:54.631
Aug 30 06:24:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 06:24:54.632
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:24:54.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:24:54.767
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 30 06:24:54.792: INFO: Creating deployment "webserver-deployment"
W0830 06:24:55.814645      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:24:55.814: INFO: Waiting for observed generation 1
Aug 30 06:24:57.847: INFO: Waiting for all required pods to come up
Aug 30 06:24:57.858: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/30/23 06:24:57.858
Aug 30 06:24:57.858: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rljgx" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5lrsn" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6qdnx" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-72mmd" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-79p6p" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7chsn" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m8g29" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mbt6t" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q6rgz" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qv72j" in namespace "deployment-4098" to be "running"
Aug 30 06:24:57.865: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.733687ms
Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667662ms
Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070921ms
Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335327ms
Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051306ms
Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.747011ms
Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.578744ms
Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.291452ms
Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j": Phase="Pending", Reason="", readiness=false. Elapsed: 9.802481ms
Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398878ms
Aug 30 06:24:59.874: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.014999052s
Aug 30 06:24:59.874: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn" satisfied condition "running"
Aug 30 06:24:59.878: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018957826s
Aug 30 06:24:59.878: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx" satisfied condition "running"
Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29": Phase="Running", Reason="", readiness=true. Elapsed: 2.021677028s
Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29" satisfied condition "running"
Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t": Phase="Running", Reason="", readiness=true. Elapsed: 2.021951876s
Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t" satisfied condition "running"
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j": Phase="Running", Reason="", readiness=true. Elapsed: 2.022777529s
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j" satisfied condition "running"
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023576435s
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd" satisfied condition "running"
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.024035365s
Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx" satisfied condition "running"
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.023587458s
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn" satisfied condition "running"
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.023600894s
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz" satisfied condition "running"
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p": Phase="Running", Reason="", readiness=true. Elapsed: 2.024115716s
Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p" satisfied condition "running"
Aug 30 06:24:59.883: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 30 06:24:59.903: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 30 06:24:59.973: INFO: Updating deployment webserver-deployment
Aug 30 06:24:59.973: INFO: Waiting for observed generation 2
Aug 30 06:25:01.995: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 30 06:25:02.019: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 30 06:25:02.026: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 06:25:02.061: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 30 06:25:02.061: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 30 06:25:02.069: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 30 06:25:02.084: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 30 06:25:02.084: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 30 06:25:02.128: INFO: Updating deployment webserver-deployment
Aug 30 06:25:02.128: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 30 06:25:02.163: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 30 06:25:02.196: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 06:25:02.245: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4098  508ff1ae-657e-4177-a0b5-beaba114812f 68881 3 2023-08-30 06:24:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032261d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-30 06:25:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-30 06:25:02 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 30 06:25:02.285: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4098  d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 68875 3 2023-08-30 06:24:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 508ff1ae-657e-4177-a0b5-beaba114812f 0xc001960a37 0xc001960a38}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508ff1ae-657e-4177-a0b5-beaba114812f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001960ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 06:25:02.285: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 30 06:25:02.285: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4098  979a1bfe-b88e-4f8d-a84b-ceaa0e883966 68873 3 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 508ff1ae-657e-4177-a0b5-beaba114812f 0xc001960947 0xc001960948}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508ff1ae-657e-4177-a0b5-beaba114812f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019609d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 30 06:25:02.329: INFO: Pod "webserver-deployment-7f5969cbc7-2mdcq" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2mdcq webserver-deployment-7f5969cbc7- deployment-4098  831183f2-44d1-4e22-8375-f912c3f96563 68901 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001960fe7 0xc001960fe8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l94r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l94r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-54knc" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-54knc webserver-deployment-7f5969cbc7- deployment-4098  0ab1fbcf-bcbd-480d-80fa-f4ec581e8b0b 68890 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961197 0xc001961198}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9k5vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9k5vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5lrsn webserver-deployment-7f5969cbc7- deployment-4098  206e90b7-c747-4050-ac86-0406cf169c8e 68699 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bab8ce583925563a8049f11ee27e5a93daef9f45cd6b79da64b322229149143a cni.projectcalico.org/podIP:172.30.224.1/32 cni.projectcalico.org/podIPs:172.30.224.1/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.1"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961367 0xc001961368}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsb8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsb8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.1,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e156dbc5e865a5ab55d4b91a388bc25cc51dd10778c3791361fc5a54459a6718,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-5nh57" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5nh57 webserver-deployment-7f5969cbc7- deployment-4098  b46c9521-fde0-4bf0-8d29-6c89a16d1dbd 68878 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc0019615d7 0xc0019615d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qs94q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qs94q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6qdnx webserver-deployment-7f5969cbc7- deployment-4098  844b7a5c-4122-421c-bfac-30c07075b3bd 68710 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:37e47d93877cb7c87a66c4e95d454d9d156491cc7a3df31bf617f9f26c7ed354 cni.projectcalico.org/podIP:172.30.86.191/32 cni.projectcalico.org/podIPs:172.30.86.191/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.86.191"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc0019617a7 0xc0019617a8}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hn8pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hn8pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.191,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://087bb315c9874241b2cec12465004b3aa84818ca8f2e6c9409d1c7efd00be5f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-72mmd webserver-deployment-7f5969cbc7- deployment-4098  c9ca5462-2d35-4931-942d-b29707110453 68718 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:39a1424954e707cb91f02f32a279d119f062d0518a96d727d2fdd3ccf7e15dc7 cni.projectcalico.org/podIP:172.30.58.105/32 cni.projectcalico.org/podIPs:172.30.58.105/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.105"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961a37 0xc001961a38}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txvqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txvqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.105,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://86375e83ce2dad63ef64f6ff9869d970252add87908864d5af273b6f77c6d090,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-77zpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-77zpw webserver-deployment-7f5969cbc7- deployment-4098  9c1f9a77-1bd5-4aac-8d35-c9a4d6dce794 68887 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961ca7 0xc001961ca8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxgs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxgs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-79p6p webserver-deployment-7f5969cbc7- deployment-4098  526cb666-44b9-4efb-ab79-501ea5b6f8b6 68703 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8caaf4f5e8f7b770a7cd4a60f6e95d0a44f2a696c4e879be6fc4e939d21f82ab cni.projectcalico.org/podIP:172.30.224.63/32 cni.projectcalico.org/podIPs:172.30.224.63/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.63"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961e77 0xc001961e78}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ffqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ffqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.63,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4f0688f32671385d311fccc776c8cb8439de3768566746b4578a4dd9e7e89016,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7chsn webserver-deployment-7f5969cbc7- deployment-4098  a390ce63-df7a-4e95-8230-e7d127b877c9 68697 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bbbe2f29c22f75755fcd194f61bdfb7c0e3c0003577bced64b3c51d71b337640 cni.projectcalico.org/podIP:172.30.224.60/32 cni.projectcalico.org/podIPs:172.30.224.60/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.60"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc107 0xc00abbc108}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt5xx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt5xx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.60,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://16767a43fa6ae37e60723bc32b9004fb88651fe9388fdbae80a5f03d13123fb3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-d7r74" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7r74 webserver-deployment-7f5969cbc7- deployment-4098  225e0691-c651-490d-a61e-843eab1fe5d7 68900 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc377 0xc00abbc378}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps5xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps5xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-jfj4f" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jfj4f webserver-deployment-7f5969cbc7- deployment-4098  cffb40b2-ac53-425a-8eb1-07d93d966667 68907 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc537 0xc00abbc538}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d57ll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d57ll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbt6t webserver-deployment-7f5969cbc7- deployment-4098  0490984f-a4b2-43fd-9919-bf371409f114 68707 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9b82be32bb10ab2365d421259a5f258b0c962422fa3726842003c563866b73e6 cni.projectcalico.org/podIP:172.30.86.147/32 cni.projectcalico.org/podIPs:172.30.86.147/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.86.147"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc707 0xc00abbc708}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhtgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhtgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.147,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://82e717de3655d2dd97de879fd5850ea59ae067af603a9d6e08fbd0caaa6d9840,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qv72j webserver-deployment-7f5969cbc7- deployment-4098  8bb12d74-1cf2-4f03-9916-d0b8a2364614 68713 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1de00fefe53af51f351e1bf9beedeb276b51d7d9259f8ec232e2f99325950626 cni.projectcalico.org/podIP:172.30.58.108/32 cni.projectcalico.org/podIPs:172.30.58.108/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.108"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc997 0xc00abbc998}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zn26w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zn26w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.108,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7c03af78454175ed4df2799d7d03dd751d39a20b98fc295b751c771b918fefcb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rljgx webserver-deployment-7f5969cbc7- deployment-4098  e975ec8f-a55d-4078-979b-6679388b7e56 68704 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a4a760d08a8927c608477e046830f5b0a193b017db6259b4914a8bfd34cce9d cni.projectcalico.org/podIP:172.30.86.190/32 cni.projectcalico.org/podIPs:172.30.86.190/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.86.190"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbcc27 0xc00abbcc28}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvchb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvchb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.190,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f33d8391c9f8045092301122b8cf23f70150869e35811bcf3a121e0c59877d6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-whqwj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whqwj webserver-deployment-7f5969cbc7- deployment-4098  1637e025-5c1b-4faf-8f8f-548ce29bf17b 68895 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbce97 0xc00abbce98}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qfqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qfqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-d9f79cb5-4skc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4skc4 webserver-deployment-d9f79cb5- deployment-4098  1566dea5-785d-4ff1-85be-f62d1b4ebea1 68855 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a7a32862e4422a90ae9ffb10613f0365154c0a81c3d723062705bcc9edf1f5f3 cni.projectcalico.org/podIP:172.30.86.139/32 cni.projectcalico.org/podIPs:172.30.86.139/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.86.139"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd037 0xc00abbd038}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf8nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf8nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-d9f79cb5-6vznb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6vznb webserver-deployment-d9f79cb5- deployment-4098  b928ee53-df2d-46c4-a97a-4e036b964f77 68832 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a1988ca378d3ac0ec60295d4b14d99f61432ee63fe8ac6ea29a69f7c08716754 cni.projectcalico.org/podIP:172.30.86.131/32 cni.projectcalico.org/podIPs:172.30.86.131/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.86.131"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd2d7 0xc00abbd2d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4gp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4gp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-8792s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8792s webserver-deployment-d9f79cb5- deployment-4098  393c713b-43fc-4f80-9d8c-ef3fc8b9a3e5 68906 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd547 0xc00abbd548}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mv6jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mv6jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-8vzbv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8vzbv webserver-deployment-d9f79cb5- deployment-4098  75ecc67c-68f0-4860-964c-f21d4de635a0 68896 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd6d7 0xc00abbd6d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnwfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnwfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-gls6s" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gls6s webserver-deployment-d9f79cb5- deployment-4098  0c907d92-85c9-40e8-b239-ced8d8838ed1 68897 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd897 0xc00abbd898}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfmfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfmfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-k5zgg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k5zgg webserver-deployment-d9f79cb5- deployment-4098  0f217ffa-b643-46ce-bd96-1d04065c68a5 68886 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbda57 0xc00abbda58}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bc6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bc6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-lmt5f" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lmt5f webserver-deployment-d9f79cb5- deployment-4098  dd14e32c-efb5-4501-a4aa-8086d8e346d6 68818 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5985c8b7a5d6d9104f84992bc50ded869d69b90fa9b487ed17ca3f75efc5172c cni.projectcalico.org/podIP:172.30.58.111/32 cni.projectcalico.org/podIPs:172.30.58.111/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.111"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbdc37 0xc00abbdc38}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4d7nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4d7nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-swfbf" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-swfbf webserver-deployment-d9f79cb5- deployment-4098  27f1f1be-b999-47ba-9285-4baf893c22e1 68819 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:50988e58c4398324e0d1b16c6bec94f1c83f9d114f634618e1020712a70383dd cni.projectcalico.org/podIP:172.30.224.16/32 cni.projectcalico.org/podIPs:172.30.224.16/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.224.16"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbdec7 0xc00abbdec8}] [] [{calico Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mj9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mj9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-tt8qt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tt8qt webserver-deployment-d9f79cb5- deployment-4098  c4ca1b52-8f64-476f-95ac-5d73f51f4cb5 68902 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c6127 0xc00a9c6128}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8mzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8mzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.334: INFO: Pod "webserver-deployment-d9f79cb5-v26fg" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v26fg webserver-deployment-d9f79cb5- deployment-4098  202cc3e2-c959-4d71-98cf-8957859d0e92 68840 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f2282604d69ea21cf376a5b2a478714805d163c871cad841746869917c21c16e cni.projectcalico.org/podIP:172.30.58.112/32 cni.projectcalico.org/podIPs:172.30.58.112/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.112"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c62f7 0xc00a9c62f8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs6jp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs6jp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.335: INFO: Pod "webserver-deployment-d9f79cb5-v5ljl" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v5ljl webserver-deployment-d9f79cb5- deployment-4098  75d41695-974a-464a-84ae-792be4c0d16d 68905 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c6567 0xc00a9c6568}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqf6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqf6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 06:25:02.335: INFO: Pod "webserver-deployment-d9f79cb5-w5fxj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w5fxj webserver-deployment-d9f79cb5- deployment-4098  fab7c605-10e5-411a-9ec4-d4f507962c3f 68904 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c66f7 0xc00a9c66f8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v7htg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v7htg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:02.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4098" for this suite. 08/30/23 06:25:02.401
------------------------------
• [SLOW TEST] [7.838 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:24:54.631
    Aug 30 06:24:54.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 06:24:54.632
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:24:54.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:24:54.767
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 30 06:24:54.792: INFO: Creating deployment "webserver-deployment"
    W0830 06:24:55.814645      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:24:55.814: INFO: Waiting for observed generation 1
    Aug 30 06:24:57.847: INFO: Waiting for all required pods to come up
    Aug 30 06:24:57.858: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/30/23 06:24:57.858
    Aug 30 06:24:57.858: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rljgx" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5lrsn" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6qdnx" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-72mmd" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-79p6p" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7chsn" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-m8g29" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-mbt6t" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q6rgz" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.859: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-qv72j" in namespace "deployment-4098" to be "running"
    Aug 30 06:24:57.865: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.733687ms
    Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.667662ms
    Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070921ms
    Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn": Phase="Pending", Reason="", readiness=false. Elapsed: 9.335327ms
    Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz": Phase="Pending", Reason="", readiness=false. Elapsed: 9.051306ms
    Aug 30 06:24:57.868: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx": Phase="Pending", Reason="", readiness=false. Elapsed: 9.747011ms
    Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.578744ms
    Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.291452ms
    Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j": Phase="Pending", Reason="", readiness=false. Elapsed: 9.802481ms
    Aug 30 06:24:57.869: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.398878ms
    Aug 30 06:24:59.874: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.014999052s
    Aug 30 06:24:59.874: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn" satisfied condition "running"
    Aug 30 06:24:59.878: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018957826s
    Aug 30 06:24:59.878: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx" satisfied condition "running"
    Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29": Phase="Running", Reason="", readiness=true. Elapsed: 2.021677028s
    Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-m8g29" satisfied condition "running"
    Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t": Phase="Running", Reason="", readiness=true. Elapsed: 2.021951876s
    Aug 30 06:24:59.881: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t" satisfied condition "running"
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j": Phase="Running", Reason="", readiness=true. Elapsed: 2.022777529s
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j" satisfied condition "running"
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023576435s
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd" satisfied condition "running"
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx": Phase="Running", Reason="", readiness=true. Elapsed: 2.024035365s
    Aug 30 06:24:59.882: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx" satisfied condition "running"
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn": Phase="Running", Reason="", readiness=true. Elapsed: 2.023587458s
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn" satisfied condition "running"
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.023600894s
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-q6rgz" satisfied condition "running"
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p": Phase="Running", Reason="", readiness=true. Elapsed: 2.024115716s
    Aug 30 06:24:59.883: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p" satisfied condition "running"
    Aug 30 06:24:59.883: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 30 06:24:59.903: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 30 06:24:59.973: INFO: Updating deployment webserver-deployment
    Aug 30 06:24:59.973: INFO: Waiting for observed generation 2
    Aug 30 06:25:01.995: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 30 06:25:02.019: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 30 06:25:02.026: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 30 06:25:02.061: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 30 06:25:02.061: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 30 06:25:02.069: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 30 06:25:02.084: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 30 06:25:02.084: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 30 06:25:02.128: INFO: Updating deployment webserver-deployment
    Aug 30 06:25:02.128: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 30 06:25:02.163: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 30 06:25:02.196: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 06:25:02.245: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4098  508ff1ae-657e-4177-a0b5-beaba114812f 68881 3 2023-08-30 06:24:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032261d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-30 06:25:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-30 06:25:02 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 30 06:25:02.285: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4098  d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 68875 3 2023-08-30 06:24:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 508ff1ae-657e-4177-a0b5-beaba114812f 0xc001960a37 0xc001960a38}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508ff1ae-657e-4177-a0b5-beaba114812f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001960ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 06:25:02.285: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 30 06:25:02.285: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4098  979a1bfe-b88e-4f8d-a84b-ceaa0e883966 68873 3 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 508ff1ae-657e-4177-a0b5-beaba114812f 0xc001960947 0xc001960948}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"508ff1ae-657e-4177-a0b5-beaba114812f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0019609d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 06:25:02.329: INFO: Pod "webserver-deployment-7f5969cbc7-2mdcq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2mdcq webserver-deployment-7f5969cbc7- deployment-4098  831183f2-44d1-4e22-8375-f912c3f96563 68901 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001960fe7 0xc001960fe8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l94r6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l94r6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-54knc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-54knc webserver-deployment-7f5969cbc7- deployment-4098  0ab1fbcf-bcbd-480d-80fa-f4ec581e8b0b 68890 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961197 0xc001961198}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9k5vw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9k5vw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-5lrsn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5lrsn webserver-deployment-7f5969cbc7- deployment-4098  206e90b7-c747-4050-ac86-0406cf169c8e 68699 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bab8ce583925563a8049f11ee27e5a93daef9f45cd6b79da64b322229149143a cni.projectcalico.org/podIP:172.30.224.1/32 cni.projectcalico.org/podIPs:172.30.224.1/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.224.1"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961367 0xc001961368}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsb8f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsb8f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.1,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://e156dbc5e865a5ab55d4b91a388bc25cc51dd10778c3791361fc5a54459a6718,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-5nh57" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5nh57 webserver-deployment-7f5969cbc7- deployment-4098  b46c9521-fde0-4bf0-8d29-6c89a16d1dbd 68878 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc0019615d7 0xc0019615d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qs94q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qs94q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.330: INFO: Pod "webserver-deployment-7f5969cbc7-6qdnx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6qdnx webserver-deployment-7f5969cbc7- deployment-4098  844b7a5c-4122-421c-bfac-30c07075b3bd 68710 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:37e47d93877cb7c87a66c4e95d454d9d156491cc7a3df31bf617f9f26c7ed354 cni.projectcalico.org/podIP:172.30.86.191/32 cni.projectcalico.org/podIPs:172.30.86.191/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.86.191"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc0019617a7 0xc0019617a8}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hn8pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hn8pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.191,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://087bb315c9874241b2cec12465004b3aa84818ca8f2e6c9409d1c7efd00be5f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.191,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-72mmd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-72mmd webserver-deployment-7f5969cbc7- deployment-4098  c9ca5462-2d35-4931-942d-b29707110453 68718 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:39a1424954e707cb91f02f32a279d119f062d0518a96d727d2fdd3ccf7e15dc7 cni.projectcalico.org/podIP:172.30.58.105/32 cni.projectcalico.org/podIPs:172.30.58.105/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.105"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961a37 0xc001961a38}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-txvqh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-txvqh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.105,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://86375e83ce2dad63ef64f6ff9869d970252add87908864d5af273b6f77c6d090,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-77zpw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-77zpw webserver-deployment-7f5969cbc7- deployment-4098  9c1f9a77-1bd5-4aac-8d35-c9a4d6dce794 68887 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961ca7 0xc001961ca8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pxgs5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pxgs5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-79p6p" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-79p6p webserver-deployment-7f5969cbc7- deployment-4098  526cb666-44b9-4efb-ab79-501ea5b6f8b6 68703 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:8caaf4f5e8f7b770a7cd4a60f6e95d0a44f2a696c4e879be6fc4e939d21f82ab cni.projectcalico.org/podIP:172.30.224.63/32 cni.projectcalico.org/podIPs:172.30.224.63/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.224.63"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc001961e77 0xc001961e78}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2ffqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2ffqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.63,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://4f0688f32671385d311fccc776c8cb8439de3768566746b4578a4dd9e7e89016,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.331: INFO: Pod "webserver-deployment-7f5969cbc7-7chsn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7chsn webserver-deployment-7f5969cbc7- deployment-4098  a390ce63-df7a-4e95-8230-e7d127b877c9 68697 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:bbbe2f29c22f75755fcd194f61bdfb7c0e3c0003577bced64b3c51d71b337640 cni.projectcalico.org/podIP:172.30.224.60/32 cni.projectcalico.org/podIPs:172.30.224.60/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.224.60"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc107 0xc00abbc108}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rt5xx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rt5xx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:172.30.224.60,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://16767a43fa6ae37e60723bc32b9004fb88651fe9388fdbae80a5f03d13123fb3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.224.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-d7r74" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-d7r74 webserver-deployment-7f5969cbc7- deployment-4098  225e0691-c651-490d-a61e-843eab1fe5d7 68900 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc377 0xc00abbc378}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ps5xn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ps5xn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-jfj4f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jfj4f webserver-deployment-7f5969cbc7- deployment-4098  cffb40b2-ac53-425a-8eb1-07d93d966667 68907 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc537 0xc00abbc538}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d57ll,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d57ll,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-mbt6t" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-mbt6t webserver-deployment-7f5969cbc7- deployment-4098  0490984f-a4b2-43fd-9919-bf371409f114 68707 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9b82be32bb10ab2365d421259a5f258b0c962422fa3726842003c563866b73e6 cni.projectcalico.org/podIP:172.30.86.147/32 cni.projectcalico.org/podIPs:172.30.86.147/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.86.147"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc707 0xc00abbc708}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rhtgx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rhtgx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.147,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://82e717de3655d2dd97de879fd5850ea59ae067af603a9d6e08fbd0caaa6d9840,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.147,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-qv72j" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qv72j webserver-deployment-7f5969cbc7- deployment-4098  8bb12d74-1cf2-4f03-9916-d0b8a2364614 68713 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:1de00fefe53af51f351e1bf9beedeb276b51d7d9259f8ec232e2f99325950626 cni.projectcalico.org/podIP:172.30.58.108/32 cni.projectcalico.org/podIPs:172.30.58.108/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.108"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbc997 0xc00abbc998}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zn26w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zn26w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.108,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7c03af78454175ed4df2799d7d03dd751d39a20b98fc295b751c771b918fefcb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.108,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-rljgx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rljgx webserver-deployment-7f5969cbc7- deployment-4098  e975ec8f-a55d-4078-979b-6679388b7e56 68704 0 2023-08-30 06:24:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:9a4a760d08a8927c608477e046830f5b0a193b017db6259b4914a8bfd34cce9d cni.projectcalico.org/podIP:172.30.86.190/32 cni.projectcalico.org/podIPs:172.30.86.190/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.86.190"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbcc27 0xc00abbcc28}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:24:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:24:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvchb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvchb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:24:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:172.30.86.190,StartTime:2023-08-30 06:24:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:24:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f33d8391c9f8045092301122b8cf23f70150869e35811bcf3a121e0c59877d6f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.86.190,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-7f5969cbc7-whqwj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-whqwj webserver-deployment-7f5969cbc7- deployment-4098  1637e025-5c1b-4faf-8f8f-548ce29bf17b 68895 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 979a1bfe-b88e-4f8d-a84b-ceaa0e883966 0xc00abbce97 0xc00abbce98}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"979a1bfe-b88e-4f8d-a84b-ceaa0e883966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qfqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qfqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-d9f79cb5-4skc4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4skc4 webserver-deployment-d9f79cb5- deployment-4098  1566dea5-785d-4ff1-85be-f62d1b4ebea1 68855 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a7a32862e4422a90ae9ffb10613f0365154c0a81c3d723062705bcc9edf1f5f3 cni.projectcalico.org/podIP:172.30.86.139/32 cni.projectcalico.org/podIPs:172.30.86.139/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.86.139"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd037 0xc00abbd038}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bf8nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bf8nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.332: INFO: Pod "webserver-deployment-d9f79cb5-6vznb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6vznb webserver-deployment-d9f79cb5- deployment-4098  b928ee53-df2d-46c4-a97a-4e036b964f77 68832 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:a1988ca378d3ac0ec60295d4b14d99f61432ee63fe8ac6ea29a69f7c08716754 cni.projectcalico.org/podIP:172.30.86.131/32 cni.projectcalico.org/podIPs:172.30.86.131/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.86.131"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd2d7 0xc00abbd2d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4gp2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4gp2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.183,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.183,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-8792s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8792s webserver-deployment-d9f79cb5- deployment-4098  393c713b-43fc-4f80-9d8c-ef3fc8b9a3e5 68906 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd547 0xc00abbd548}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mv6jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mv6jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-8vzbv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8vzbv webserver-deployment-d9f79cb5- deployment-4098  75ecc67c-68f0-4860-964c-f21d4de635a0 68896 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd6d7 0xc00abbd6d8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnwfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnwfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-gls6s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gls6s webserver-deployment-d9f79cb5- deployment-4098  0c907d92-85c9-40e8-b239-ced8d8838ed1 68897 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbd897 0xc00abbd898}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfmfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfmfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-k5zgg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-k5zgg webserver-deployment-d9f79cb5- deployment-4098  0f217ffa-b643-46ce-bd96-1d04065c68a5 68886 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbda57 0xc00abbda58}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bc6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bc6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-lmt5f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-lmt5f webserver-deployment-d9f79cb5- deployment-4098  dd14e32c-efb5-4501-a4aa-8086d8e346d6 68818 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5985c8b7a5d6d9104f84992bc50ded869d69b90fa9b487ed17ca3f75efc5172c cni.projectcalico.org/podIP:172.30.58.111/32 cni.projectcalico.org/podIPs:172.30.58.111/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.111"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbdc37 0xc00abbdc38}] [] [{kube-controller-manager Update v1 2023-08-30 06:24:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4d7nq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4d7nq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-swfbf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-swfbf webserver-deployment-d9f79cb5- deployment-4098  27f1f1be-b999-47ba-9285-4baf893c22e1 68819 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:50988e58c4398324e0d1b16c6bec94f1c83f9d114f634618e1020712a70383dd cni.projectcalico.org/podIP:172.30.224.16/32 cni.projectcalico.org/podIPs:172.30.224.16/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.224.16"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00abbdec7 0xc00abbdec8}] [] [{calico Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {multus Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mj9l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mj9l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.333: INFO: Pod "webserver-deployment-d9f79cb5-tt8qt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-tt8qt webserver-deployment-d9f79cb5- deployment-4098  c4ca1b52-8f64-476f-95ac-5d73f51f4cb5 68902 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c6127 0xc00a9c6128}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8mzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8mzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.334: INFO: Pod "webserver-deployment-d9f79cb5-v26fg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v26fg webserver-deployment-d9f79cb5- deployment-4098  202cc3e2-c959-4d71-98cf-8957859d0e92 68840 0 2023-08-30 06:25:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f2282604d69ea21cf376a5b2a478714805d163c871cad841746869917c21c16e cni.projectcalico.org/podIP:172.30.58.112/32 cni.projectcalico.org/podIPs:172.30.58.112/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.112"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c62f7 0xc00a9c62f8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 06:25:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 06:25:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs6jp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs6jp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:25:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 06:25:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.335: INFO: Pod "webserver-deployment-d9f79cb5-v5ljl" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-v5ljl webserver-deployment-d9f79cb5- deployment-4098  75d41695-974a-464a-84ae-792be4c0d16d 68905 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c6567 0xc00a9c6568}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pqf6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pqf6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 06:25:02.335: INFO: Pod "webserver-deployment-d9f79cb5-w5fxj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-w5fxj webserver-deployment-d9f79cb5- deployment-4098  fab7c605-10e5-411a-9ec4-d4f507962c3f 68904 0 2023-08-30 06:25:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a 0xc00a9c66f7 0xc00a9c66f8}] [] [{kube-controller-manager Update v1 2023-08-30 06:25:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d9bd1b6c-1f4d-4a7f-9b3e-503d4703674a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v7htg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v7htg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-9q5mp,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:02.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4098" for this suite. 08/30/23 06:25:02.401
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:02.471
Aug 30 06:25:02.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename tables 08/30/23 06:25:02.474
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:02.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:02.647
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:02.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-2980" for this suite. 08/30/23 06:25:02.714
------------------------------
• [0.282 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:02.471
    Aug 30 06:25:02.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename tables 08/30/23 06:25:02.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:02.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:02.647
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:02.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-2980" for this suite. 08/30/23 06:25:02.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:02.756
Aug 30 06:25:02.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:25:02.758
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:02.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:02.923
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-6fe17a9a-dfbb-4ba9-8d6b-04223b6c51a9 08/30/23 06:25:02.965
STEP: Creating a pod to test consume secrets 08/30/23 06:25:03.007
W0830 06:25:03.040179      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:25:03.040: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c" in namespace "projected-61" to be "Succeeded or Failed"
Aug 30 06:25:03.050: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.414288ms
Aug 30 06:25:05.060: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020216885s
Aug 30 06:25:07.065: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025179042s
Aug 30 06:25:09.058: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018082962s
STEP: Saw pod success 08/30/23 06:25:09.058
Aug 30 06:25:09.058: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c" satisfied condition "Succeeded or Failed"
Aug 30 06:25:09.093: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:25:09.227
Aug 30 06:25:09.259: INFO: Waiting for pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c to disappear
Aug 30 06:25:09.267: INFO: Pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:09.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-61" for this suite. 08/30/23 06:25:09.28
------------------------------
• [SLOW TEST] [6.554 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:02.756
    Aug 30 06:25:02.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:25:02.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:02.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:02.923
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-6fe17a9a-dfbb-4ba9-8d6b-04223b6c51a9 08/30/23 06:25:02.965
    STEP: Creating a pod to test consume secrets 08/30/23 06:25:03.007
    W0830 06:25:03.040179      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:25:03.040: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c" in namespace "projected-61" to be "Succeeded or Failed"
    Aug 30 06:25:03.050: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.414288ms
    Aug 30 06:25:05.060: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020216885s
    Aug 30 06:25:07.065: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025179042s
    Aug 30 06:25:09.058: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018082962s
    STEP: Saw pod success 08/30/23 06:25:09.058
    Aug 30 06:25:09.058: INFO: Pod "pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c" satisfied condition "Succeeded or Failed"
    Aug 30 06:25:09.093: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:25:09.227
    Aug 30 06:25:09.259: INFO: Waiting for pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c to disappear
    Aug 30 06:25:09.267: INFO: Pod pod-projected-secrets-ab5e20a9-5da1-448e-b3a4-a7835a85886c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:09.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-61" for this suite. 08/30/23 06:25:09.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:09.313
Aug 30 06:25:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename watch 08/30/23 06:25:09.315
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:09.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:09.411
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/30/23 06:25:09.419
STEP: starting a background goroutine to produce watch events 08/30/23 06:25:09.465
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/30/23 06:25:09.465
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:12.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5922" for this suite. 08/30/23 06:25:12.184
------------------------------
• [2.929 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:09.313
    Aug 30 06:25:09.313: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename watch 08/30/23 06:25:09.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:09.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:09.411
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/30/23 06:25:09.419
    STEP: starting a background goroutine to produce watch events 08/30/23 06:25:09.465
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/30/23 06:25:09.465
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:12.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5922" for this suite. 08/30/23 06:25:12.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:12.243
Aug 30 06:25:12.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 06:25:12.246
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:12.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:12.343
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/30/23 06:25:12.348
Aug 30 06:25:12.348: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1305 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/30/23 06:25:12.419
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:12.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1305" for this suite. 08/30/23 06:25:12.446
------------------------------
• [0.229 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:12.243
    Aug 30 06:25:12.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 06:25:12.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:12.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:12.343
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/30/23 06:25:12.348
    Aug 30 06:25:12.348: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1305 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/30/23 06:25:12.419
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:12.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1305" for this suite. 08/30/23 06:25:12.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:12.476
Aug 30 06:25:12.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 06:25:12.477
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:12.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:12.572
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/30/23 06:25:12.578
W0830 06:25:12.595982      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:25:12.596: INFO: Waiting up to 5m0s for pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e" in namespace "downward-api-8543" to be "running and ready"
Aug 30 06:25:12.609: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.268641ms
Aug 30 06:25:12.609: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:14.620: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023889004s
Aug 30 06:25:14.620: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:16.618: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.021826118s
Aug 30 06:25:16.618: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Running (Ready = true)
Aug 30 06:25:16.618: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e" satisfied condition "running and ready"
Aug 30 06:25:17.161: INFO: Successfully updated pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:19.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8543" for this suite. 08/30/23 06:25:19.231
------------------------------
• [SLOW TEST] [6.777 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:12.476
    Aug 30 06:25:12.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 06:25:12.477
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:12.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:12.572
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/30/23 06:25:12.578
    W0830 06:25:12.595982      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:25:12.596: INFO: Waiting up to 5m0s for pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e" in namespace "downward-api-8543" to be "running and ready"
    Aug 30 06:25:12.609: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.268641ms
    Aug 30 06:25:12.609: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:14.620: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023889004s
    Aug 30 06:25:14.620: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:16.618: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e": Phase="Running", Reason="", readiness=true. Elapsed: 4.021826118s
    Aug 30 06:25:16.618: INFO: The phase of Pod annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e is Running (Ready = true)
    Aug 30 06:25:16.618: INFO: Pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e" satisfied condition "running and ready"
    Aug 30 06:25:17.161: INFO: Successfully updated pod "annotationupdate65716cf0-1dc9-487c-83e4-9c7e7ad3db5e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:19.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8543" for this suite. 08/30/23 06:25:19.231
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:19.254
Aug 30 06:25:19.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 06:25:19.255
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:19.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:19.321
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/30/23 06:25:19.326
STEP: submitting the pod to kubernetes 08/30/23 06:25:19.326
Aug 30 06:25:19.365: INFO: Waiting up to 5m0s for pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" in namespace "pods-3762" to be "running and ready"
Aug 30 06:25:19.378: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.58631ms
Aug 30 06:25:19.378: INFO: The phase of Pod pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:21.387: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021860888s
Aug 30 06:25:21.388: INFO: The phase of Pod pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e is Running (Ready = true)
Aug 30 06:25:21.388: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/30/23 06:25:21.394
STEP: updating the pod 08/30/23 06:25:21.403
Aug 30 06:25:21.926: INFO: Successfully updated pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e"
Aug 30 06:25:21.926: INFO: Waiting up to 5m0s for pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" in namespace "pods-3762" to be "running"
Aug 30 06:25:21.934: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Running", Reason="", readiness=true. Elapsed: 7.788052ms
Aug 30 06:25:21.934: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/30/23 06:25:21.934
Aug 30 06:25:21.941: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:21.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3762" for this suite. 08/30/23 06:25:21.951
------------------------------
• [2.718 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:19.254
    Aug 30 06:25:19.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 06:25:19.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:19.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:19.321
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/30/23 06:25:19.326
    STEP: submitting the pod to kubernetes 08/30/23 06:25:19.326
    Aug 30 06:25:19.365: INFO: Waiting up to 5m0s for pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" in namespace "pods-3762" to be "running and ready"
    Aug 30 06:25:19.378: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.58631ms
    Aug 30 06:25:19.378: INFO: The phase of Pod pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:21.387: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Running", Reason="", readiness=true. Elapsed: 2.021860888s
    Aug 30 06:25:21.388: INFO: The phase of Pod pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e is Running (Ready = true)
    Aug 30 06:25:21.388: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/30/23 06:25:21.394
    STEP: updating the pod 08/30/23 06:25:21.403
    Aug 30 06:25:21.926: INFO: Successfully updated pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e"
    Aug 30 06:25:21.926: INFO: Waiting up to 5m0s for pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" in namespace "pods-3762" to be "running"
    Aug 30 06:25:21.934: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e": Phase="Running", Reason="", readiness=true. Elapsed: 7.788052ms
    Aug 30 06:25:21.934: INFO: Pod "pod-update-583d4537-dbb0-47bf-982b-6ef87873b59e" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/30/23 06:25:21.934
    Aug 30 06:25:21.941: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:21.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3762" for this suite. 08/30/23 06:25:21.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:21.975
Aug 30 06:25:21.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:25:21.977
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:22.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:22.033
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-204e5eee-6927-495d-9269-0a8e57b140b1 08/30/23 06:25:22.04
STEP: Creating a pod to test consume configMaps 08/30/23 06:25:22.055
Aug 30 06:25:22.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f" in namespace "projected-5661" to be "Succeeded or Failed"
Aug 30 06:25:22.083: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.906319ms
Aug 30 06:25:24.091: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017082049s
Aug 30 06:25:26.100: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026135274s
STEP: Saw pod success 08/30/23 06:25:26.1
Aug 30 06:25:26.100: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f" satisfied condition "Succeeded or Failed"
Aug 30 06:25:26.144: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:25:26.282
Aug 30 06:25:26.462: INFO: Waiting for pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f to disappear
Aug 30 06:25:26.469: INFO: Pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:26.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5661" for this suite. 08/30/23 06:25:26.479
------------------------------
• [4.559 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:21.975
    Aug 30 06:25:21.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:25:21.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:22.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:22.033
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-204e5eee-6927-495d-9269-0a8e57b140b1 08/30/23 06:25:22.04
    STEP: Creating a pod to test consume configMaps 08/30/23 06:25:22.055
    Aug 30 06:25:22.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f" in namespace "projected-5661" to be "Succeeded or Failed"
    Aug 30 06:25:22.083: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.906319ms
    Aug 30 06:25:24.091: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017082049s
    Aug 30 06:25:26.100: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026135274s
    STEP: Saw pod success 08/30/23 06:25:26.1
    Aug 30 06:25:26.100: INFO: Pod "pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f" satisfied condition "Succeeded or Failed"
    Aug 30 06:25:26.144: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:25:26.282
    Aug 30 06:25:26.462: INFO: Waiting for pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f to disappear
    Aug 30 06:25:26.469: INFO: Pod pod-projected-configmaps-16a44781-ee97-4a99-b134-6144d94c3a9f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:26.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5661" for this suite. 08/30/23 06:25:26.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:26.539
Aug 30 06:25:26.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:25:26.54
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:26.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:26.637
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/30/23 06:25:26.642
W0830 06:25:26.661874      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:25:26.662: INFO: Waiting up to 5m0s for pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c" in namespace "projected-3536" to be "running and ready"
Aug 30 06:25:26.680: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.121188ms
Aug 30 06:25:26.680: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:28.688: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026539784s
Aug 30 06:25:28.688: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:30.717: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Running", Reason="", readiness=true. Elapsed: 4.055281865s
Aug 30 06:25:30.717: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Running (Ready = true)
Aug 30 06:25:30.717: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c" satisfied condition "running and ready"
Aug 30 06:25:31.357: INFO: Successfully updated pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:33.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3536" for this suite. 08/30/23 06:25:33.52
------------------------------
• [SLOW TEST] [7.010 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:26.539
    Aug 30 06:25:26.540: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:25:26.54
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:26.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:26.637
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/30/23 06:25:26.642
    W0830 06:25:26.661874      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:25:26.662: INFO: Waiting up to 5m0s for pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c" in namespace "projected-3536" to be "running and ready"
    Aug 30 06:25:26.680: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.121188ms
    Aug 30 06:25:26.680: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:28.688: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026539784s
    Aug 30 06:25:28.688: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:30.717: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c": Phase="Running", Reason="", readiness=true. Elapsed: 4.055281865s
    Aug 30 06:25:30.717: INFO: The phase of Pod labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c is Running (Ready = true)
    Aug 30 06:25:30.717: INFO: Pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c" satisfied condition "running and ready"
    Aug 30 06:25:31.357: INFO: Successfully updated pod "labelsupdate704f7e31-8208-4048-8d3e-247c05c3fc2c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:33.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3536" for this suite. 08/30/23 06:25:33.52
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:33.55
Aug 30 06:25:33.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:25:33.551
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:33.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:33.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:25:33.761
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:25:34.354
STEP: Deploying the webhook pod 08/30/23 06:25:34.378
STEP: Wait for the deployment to be ready 08/30/23 06:25:34.403
Aug 30 06:25:34.420: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:25:36.457
STEP: Verifying the service has paired with the endpoint 08/30/23 06:25:36.485
Aug 30 06:25:37.485: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/30/23 06:25:37.492
STEP: create a pod that should be updated by the webhook 08/30/23 06:25:37.523
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:37.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4475" for this suite. 08/30/23 06:25:37.793
STEP: Destroying namespace "webhook-4475-markers" for this suite. 08/30/23 06:25:37.814
------------------------------
• [4.284 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:33.55
    Aug 30 06:25:33.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:25:33.551
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:33.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:33.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:25:33.761
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:25:34.354
    STEP: Deploying the webhook pod 08/30/23 06:25:34.378
    STEP: Wait for the deployment to be ready 08/30/23 06:25:34.403
    Aug 30 06:25:34.420: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:25:36.457
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:25:36.485
    Aug 30 06:25:37.485: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/30/23 06:25:37.492
    STEP: create a pod that should be updated by the webhook 08/30/23 06:25:37.523
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:37.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4475" for this suite. 08/30/23 06:25:37.793
    STEP: Destroying namespace "webhook-4475-markers" for this suite. 08/30/23 06:25:37.814
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:37.835
Aug 30 06:25:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 06:25:37.836
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:37.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:37.935
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/30/23 06:25:37.943
STEP: submitting the pod to kubernetes 08/30/23 06:25:37.943
STEP: verifying QOS class is set on the pod 08/30/23 06:25:37.962
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:37.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3881" for this suite. 08/30/23 06:25:38
------------------------------
• [0.211 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:37.835
    Aug 30 06:25:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 06:25:37.836
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:37.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:37.935
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/30/23 06:25:37.943
    STEP: submitting the pod to kubernetes 08/30/23 06:25:37.943
    STEP: verifying QOS class is set on the pod 08/30/23 06:25:37.962
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:37.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3881" for this suite. 08/30/23 06:25:38
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:38.05
Aug 30 06:25:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:25:38.051
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:38.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:38.099
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-6694 08/30/23 06:25:38.104
STEP: creating service affinity-nodeport-transition in namespace services-6694 08/30/23 06:25:38.104
STEP: creating replication controller affinity-nodeport-transition in namespace services-6694 08/30/23 06:25:38.16
I0830 06:25:38.173776      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6694, replica count: 3
I0830 06:25:41.224921      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 06:25:44.225601      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 06:25:44.250: INFO: Creating new exec pod
Aug 30 06:25:44.269: INFO: Waiting up to 5m0s for pod "execpod-affinityxshj2" in namespace "services-6694" to be "running"
Aug 30 06:25:44.306: INFO: Pod "execpod-affinityxshj2": Phase="Pending", Reason="", readiness=false. Elapsed: 37.421857ms
Aug 30 06:25:46.315: INFO: Pod "execpod-affinityxshj2": Phase="Running", Reason="", readiness=true. Elapsed: 2.046081781s
Aug 30 06:25:46.315: INFO: Pod "execpod-affinityxshj2" satisfied condition "running"
Aug 30 06:25:47.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 30 06:25:47.608: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 30 06:25:47.608: INFO: stdout: ""
Aug 30 06:25:47.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 172.21.50.93 80'
Aug 30 06:25:47.818: INFO: stderr: "+ nc -v -z -w 2 172.21.50.93 80\nConnection to 172.21.50.93 80 port [tcp/http] succeeded!\n"
Aug 30 06:25:47.818: INFO: stdout: ""
Aug 30 06:25:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 31042'
Aug 30 06:25:48.073: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 31042\nConnection to 10.135.139.190 31042 port [tcp/*] succeeded!\n"
Aug 30 06:25:48.073: INFO: stdout: ""
Aug 30 06:25:48.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 31042'
Aug 30 06:25:48.280: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 31042\nConnection to 10.135.139.183 31042 port [tcp/*] succeeded!\n"
Aug 30 06:25:48.280: INFO: stdout: ""
Aug 30 06:25:48.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31042/ ; done'
Aug 30 06:25:48.720: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n"
Aug 30 06:25:48.720: INFO: stdout: "\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-mtf9d"
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:48.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31042/ ; done'
Aug 30 06:25:49.216: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n"
Aug 30 06:25:49.216: INFO: stdout: "\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d"
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
Aug 30 06:25:49.216: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6694, will wait for the garbage collector to delete the pods 08/30/23 06:25:49.239
Aug 30 06:25:49.323: INFO: Deleting ReplicationController affinity-nodeport-transition took: 26.171253ms
Aug 30 06:25:49.424: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.12376ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:52.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6694" for this suite. 08/30/23 06:25:52.295
------------------------------
• [SLOW TEST] [14.263 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:38.05
    Aug 30 06:25:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:25:38.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:38.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:38.099
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-6694 08/30/23 06:25:38.104
    STEP: creating service affinity-nodeport-transition in namespace services-6694 08/30/23 06:25:38.104
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6694 08/30/23 06:25:38.16
    I0830 06:25:38.173776      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6694, replica count: 3
    I0830 06:25:41.224921      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0830 06:25:44.225601      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 06:25:44.250: INFO: Creating new exec pod
    Aug 30 06:25:44.269: INFO: Waiting up to 5m0s for pod "execpod-affinityxshj2" in namespace "services-6694" to be "running"
    Aug 30 06:25:44.306: INFO: Pod "execpod-affinityxshj2": Phase="Pending", Reason="", readiness=false. Elapsed: 37.421857ms
    Aug 30 06:25:46.315: INFO: Pod "execpod-affinityxshj2": Phase="Running", Reason="", readiness=true. Elapsed: 2.046081781s
    Aug 30 06:25:46.315: INFO: Pod "execpod-affinityxshj2" satisfied condition "running"
    Aug 30 06:25:47.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 30 06:25:47.608: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 30 06:25:47.608: INFO: stdout: ""
    Aug 30 06:25:47.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 172.21.50.93 80'
    Aug 30 06:25:47.818: INFO: stderr: "+ nc -v -z -w 2 172.21.50.93 80\nConnection to 172.21.50.93 80 port [tcp/http] succeeded!\n"
    Aug 30 06:25:47.818: INFO: stdout: ""
    Aug 30 06:25:47.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 31042'
    Aug 30 06:25:48.073: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 31042\nConnection to 10.135.139.190 31042 port [tcp/*] succeeded!\n"
    Aug 30 06:25:48.073: INFO: stdout: ""
    Aug 30 06:25:48.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 31042'
    Aug 30 06:25:48.280: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 31042\nConnection to 10.135.139.183 31042 port [tcp/*] succeeded!\n"
    Aug 30 06:25:48.280: INFO: stdout: ""
    Aug 30 06:25:48.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31042/ ; done'
    Aug 30 06:25:48.720: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n"
    Aug 30 06:25:48.720: INFO: stdout: "\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-z4cdk\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-kvmdw\naffinity-nodeport-transition-mtf9d"
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-z4cdk
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-kvmdw
    Aug 30 06:25:48.720: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:48.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6694 exec execpod-affinityxshj2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31042/ ; done'
    Aug 30 06:25:49.216: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31042/\n"
    Aug 30 06:25:49.216: INFO: stdout: "\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d\naffinity-nodeport-transition-mtf9d"
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Received response from host: affinity-nodeport-transition-mtf9d
    Aug 30 06:25:49.216: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6694, will wait for the garbage collector to delete the pods 08/30/23 06:25:49.239
    Aug 30 06:25:49.323: INFO: Deleting ReplicationController affinity-nodeport-transition took: 26.171253ms
    Aug 30 06:25:49.424: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.12376ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:52.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6694" for this suite. 08/30/23 06:25:52.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:52.314
Aug 30 06:25:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 06:25:52.315
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:52.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:52.365
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 30 06:25:52.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: creating the pod 08/30/23 06:25:52.37
STEP: submitting the pod to kubernetes 08/30/23 06:25:52.37
Aug 30 06:25:52.390: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29" in namespace "pods-8993" to be "running and ready"
Aug 30 06:25:52.401: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29": Phase="Pending", Reason="", readiness=false. Elapsed: 11.068997ms
Aug 30 06:25:52.401: INFO: The phase of Pod pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:25:54.408: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29": Phase="Running", Reason="", readiness=true. Elapsed: 2.018970562s
Aug 30 06:25:54.409: INFO: The phase of Pod pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29 is Running (Ready = true)
Aug 30 06:25:54.409: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:54.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8993" for this suite. 08/30/23 06:25:54.471
------------------------------
• [2.174 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:52.314
    Aug 30 06:25:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 06:25:52.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:52.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:52.365
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 30 06:25:52.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: creating the pod 08/30/23 06:25:52.37
    STEP: submitting the pod to kubernetes 08/30/23 06:25:52.37
    Aug 30 06:25:52.390: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29" in namespace "pods-8993" to be "running and ready"
    Aug 30 06:25:52.401: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29": Phase="Pending", Reason="", readiness=false. Elapsed: 11.068997ms
    Aug 30 06:25:52.401: INFO: The phase of Pod pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:25:54.408: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29": Phase="Running", Reason="", readiness=true. Elapsed: 2.018970562s
    Aug 30 06:25:54.409: INFO: The phase of Pod pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29 is Running (Ready = true)
    Aug 30 06:25:54.409: INFO: Pod "pod-logs-websocket-d5e4b983-61e7-4361-bd09-9badb8f17e29" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:54.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8993" for this suite. 08/30/23 06:25:54.471
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:54.488
Aug 30 06:25:54.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 06:25:54.489
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:54.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:54.545
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/30/23 06:25:54.551
W0830 06:25:54.572034      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:25:54.572: INFO: Waiting up to 5m0s for pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408" in namespace "var-expansion-6006" to be "Succeeded or Failed"
Aug 30 06:25:54.580: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083616ms
Aug 30 06:25:56.587: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015502836s
Aug 30 06:25:58.593: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021576542s
STEP: Saw pod success 08/30/23 06:25:58.593
Aug 30 06:25:58.593: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408" satisfied condition "Succeeded or Failed"
Aug 30 06:25:58.601: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 container dapi-container: <nil>
STEP: delete the pod 08/30/23 06:25:58.665
Aug 30 06:25:58.691: INFO: Waiting for pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 to disappear
Aug 30 06:25:58.700: INFO: Pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 06:25:58.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6006" for this suite. 08/30/23 06:25:58.71
------------------------------
• [4.253 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:54.488
    Aug 30 06:25:54.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 06:25:54.489
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:54.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:54.545
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/30/23 06:25:54.551
    W0830 06:25:54.572034      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:25:54.572: INFO: Waiting up to 5m0s for pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408" in namespace "var-expansion-6006" to be "Succeeded or Failed"
    Aug 30 06:25:54.580: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083616ms
    Aug 30 06:25:56.587: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015502836s
    Aug 30 06:25:58.593: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021576542s
    STEP: Saw pod success 08/30/23 06:25:58.593
    Aug 30 06:25:58.593: INFO: Pod "var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408" satisfied condition "Succeeded or Failed"
    Aug 30 06:25:58.601: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 06:25:58.665
    Aug 30 06:25:58.691: INFO: Waiting for pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 to disappear
    Aug 30 06:25:58.700: INFO: Pod var-expansion-815b292f-f599-4681-9f23-03dbe1ea8408 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:25:58.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6006" for this suite. 08/30/23 06:25:58.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:25:58.742
Aug 30 06:25:58.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 06:25:58.743
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:58.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:58.818
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
W0830 06:25:58.844953      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:25:58.845: INFO: Waiting up to 2m0s for pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" in namespace "var-expansion-338" to be "container 0 failed with reason CreateContainerConfigError"
Aug 30 06:25:58.878: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214": Phase="Pending", Reason="", readiness=false. Elapsed: 33.302076ms
Aug 30 06:26:00.886: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041719607s
Aug 30 06:26:00.886: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 30 06:26:00.886: INFO: Deleting pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" in namespace "var-expansion-338"
Aug 30 06:26:00.900: INFO: Wait up to 5m0s for pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 06:26:04.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-338" for this suite. 08/30/23 06:26:04.932
------------------------------
• [SLOW TEST] [6.208 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:25:58.742
    Aug 30 06:25:58.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 06:25:58.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:25:58.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:25:58.818
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    W0830 06:25:58.844953      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:25:58.845: INFO: Waiting up to 2m0s for pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" in namespace "var-expansion-338" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 30 06:25:58.878: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214": Phase="Pending", Reason="", readiness=false. Elapsed: 33.302076ms
    Aug 30 06:26:00.886: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041719607s
    Aug 30 06:26:00.886: INFO: Pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 30 06:26:00.886: INFO: Deleting pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" in namespace "var-expansion-338"
    Aug 30 06:26:00.900: INFO: Wait up to 5m0s for pod "var-expansion-74ebbb80-1ce3-4096-b82d-c24f6e698214" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:26:04.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-338" for this suite. 08/30/23 06:26:04.932
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:26:04.952
Aug 30 06:26:04.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 06:26:04.953
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:26:05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:26:05.006
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/30/23 06:26:05.011
W0830 06:26:05.030259      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:26:05.030: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-16" to be "running and ready"
Aug 30 06:26:05.050: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 19.624308ms
Aug 30 06:26:05.050: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:26:07.059: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028800015s
Aug 30 06:26:07.059: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:26:09.058: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.027514019s
Aug 30 06:26:09.058: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 30 06:26:09.058: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/30/23 06:26:09.064
STEP: Then the orphan pod is adopted 08/30/23 06:26:09.1
STEP: When the matched label of one of its pods change 08/30/23 06:26:10.118
Aug 30 06:26:10.131: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/30/23 06:26:10.16
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:26:11.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-16" for this suite. 08/30/23 06:26:11.185
------------------------------
• [SLOW TEST] [6.254 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:26:04.952
    Aug 30 06:26:04.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 06:26:04.953
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:26:05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:26:05.006
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/30/23 06:26:05.011
    W0830 06:26:05.030259      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:26:05.030: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-16" to be "running and ready"
    Aug 30 06:26:05.050: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 19.624308ms
    Aug 30 06:26:05.050: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:26:07.059: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028800015s
    Aug 30 06:26:07.059: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:26:09.058: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.027514019s
    Aug 30 06:26:09.058: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 30 06:26:09.058: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/30/23 06:26:09.064
    STEP: Then the orphan pod is adopted 08/30/23 06:26:09.1
    STEP: When the matched label of one of its pods change 08/30/23 06:26:10.118
    Aug 30 06:26:10.131: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/30/23 06:26:10.16
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:26:11.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-16" for this suite. 08/30/23 06:26:11.185
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:26:11.206
Aug 30 06:26:11.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption 08/30/23 06:26:11.207
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:26:11.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:26:11.295
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 30 06:26:11.351: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 06:27:11.476: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/30/23 06:27:11.492
Aug 30 06:27:11.536: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 30 06:27:11.550: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 30 06:27:11.588: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 30 06:27:11.610: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 30 06:27:11.663: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 30 06:27:11.676: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/30/23 06:27:11.676
Aug 30 06:27:11.676: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:11.685: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.240262ms
Aug 30 06:27:13.696: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020360539s
Aug 30 06:27:15.696: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.020309505s
Aug 30 06:27:15.696: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 30 06:27:15.696: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:15.702: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.145897ms
Aug 30 06:27:15.702: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 06:27:15.703: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:15.708: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.930212ms
Aug 30 06:27:15.708: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 06:27:15.709: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:15.715: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.538073ms
Aug 30 06:27:15.715: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 06:27:15.715: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:15.721: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.308867ms
Aug 30 06:27:15.721: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 06:27:15.721: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
Aug 30 06:27:15.729: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.125158ms
Aug 30 06:27:15.729: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/30/23 06:27:15.729
Aug 30 06:27:15.751: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 30 06:27:15.758: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999285ms
Aug 30 06:27:17.767: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01591543s
Aug 30 06:27:19.787: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036572642s
Aug 30 06:27:21.767: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015980866s
Aug 30 06:27:23.766: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.015554532s
Aug 30 06:27:23.766: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:27:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-80" for this suite. 08/30/23 06:27:23.964
------------------------------
• [SLOW TEST] [72.775 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:26:11.206
    Aug 30 06:26:11.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption 08/30/23 06:26:11.207
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:26:11.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:26:11.295
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 30 06:26:11.351: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 06:27:11.476: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/30/23 06:27:11.492
    Aug 30 06:27:11.536: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 30 06:27:11.550: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 30 06:27:11.588: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 30 06:27:11.610: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 30 06:27:11.663: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 30 06:27:11.676: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/30/23 06:27:11.676
    Aug 30 06:27:11.676: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:11.685: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 9.240262ms
    Aug 30 06:27:13.696: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020360539s
    Aug 30 06:27:15.696: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.020309505s
    Aug 30 06:27:15.696: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 30 06:27:15.696: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:15.702: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.145897ms
    Aug 30 06:27:15.702: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 06:27:15.703: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:15.708: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.930212ms
    Aug 30 06:27:15.708: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 06:27:15.709: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:15.715: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.538073ms
    Aug 30 06:27:15.715: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 06:27:15.715: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:15.721: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.308867ms
    Aug 30 06:27:15.721: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 06:27:15.721: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-80" to be "running"
    Aug 30 06:27:15.729: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.125158ms
    Aug 30 06:27:15.729: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/30/23 06:27:15.729
    Aug 30 06:27:15.751: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 30 06:27:15.758: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.999285ms
    Aug 30 06:27:17.767: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01591543s
    Aug 30 06:27:19.787: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036572642s
    Aug 30 06:27:21.767: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015980866s
    Aug 30 06:27:23.766: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.015554532s
    Aug 30 06:27:23.766: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:27:23.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-80" for this suite. 08/30/23 06:27:23.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:27:23.984
Aug 30 06:27:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename taint-multiple-pods 08/30/23 06:27:23.985
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:27:24.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:27:24.045
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 30 06:27:24.052: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 06:28:24.159: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 30 06:28:24.175: INFO: Starting informer...
STEP: Starting pods... 08/30/23 06:28:24.175
Aug 30 06:28:24.413: INFO: Pod1 is running on 10.135.139.190. Tainting Node
Aug 30 06:28:24.631: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9708" to be "running"
Aug 30 06:28:24.638: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.852793ms
Aug 30 06:28:26.674: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.042302953s
Aug 30 06:28:26.674: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 30 06:28:26.674: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9708" to be "running"
Aug 30 06:28:26.696: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 22.539056ms
Aug 30 06:28:26.696: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 30 06:28:26.696: INFO: Pod2 is running on 10.135.139.190. Tainting Node
STEP: Trying to apply a taint on the Node 08/30/23 06:28:26.696
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 06:28:26.717
STEP: Waiting for Pod1 and Pod2 to be deleted 08/30/23 06:28:26.727
Aug 30 06:28:33.273: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 30 06:28:53.422: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 06:28:53.453
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:28:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-9708" for this suite. 08/30/23 06:28:53.477
------------------------------
• [SLOW TEST] [89.512 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:27:23.984
    Aug 30 06:27:23.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename taint-multiple-pods 08/30/23 06:27:23.985
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:27:24.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:27:24.045
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 30 06:27:24.052: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 06:28:24.159: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 30 06:28:24.175: INFO: Starting informer...
    STEP: Starting pods... 08/30/23 06:28:24.175
    Aug 30 06:28:24.413: INFO: Pod1 is running on 10.135.139.190. Tainting Node
    Aug 30 06:28:24.631: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9708" to be "running"
    Aug 30 06:28:24.638: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.852793ms
    Aug 30 06:28:26.674: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.042302953s
    Aug 30 06:28:26.674: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 30 06:28:26.674: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9708" to be "running"
    Aug 30 06:28:26.696: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 22.539056ms
    Aug 30 06:28:26.696: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 30 06:28:26.696: INFO: Pod2 is running on 10.135.139.190. Tainting Node
    STEP: Trying to apply a taint on the Node 08/30/23 06:28:26.696
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 06:28:26.717
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/30/23 06:28:26.727
    Aug 30 06:28:33.273: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 30 06:28:53.422: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 06:28:53.453
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:28:53.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-9708" for this suite. 08/30/23 06:28:53.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:28:53.499
Aug 30 06:28:53.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption 08/30/23 06:28:53.5
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:28:53.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:28:53.591
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 30 06:28:53.663: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 06:29:53.868: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:29:53.909
Aug 30 06:29:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption-path 08/30/23 06:29:53.937
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:29:54.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:29:54.056
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/30/23 06:29:54.084
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 06:29:54.085
Aug 30 06:29:54.138: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4367" to be "running"
Aug 30 06:29:54.145: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779889ms
Aug 30 06:29:56.156: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018021497s
Aug 30 06:29:56.156: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 06:29:56.163
Aug 30 06:29:56.192: INFO: found a healthy node: 10.135.139.190
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 30 06:30:04.349: INFO: pods created so far: [1 1 1]
Aug 30 06:30:04.349: INFO: length of pods created so far: 3
Aug 30 06:30:10.369: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:17.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4367" for this suite. 08/30/23 06:30:17.545
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9721" for this suite. 08/30/23 06:30:17.578
------------------------------
• [SLOW TEST] [84.106 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:28:53.499
    Aug 30 06:28:53.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption 08/30/23 06:28:53.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:28:53.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:28:53.591
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 30 06:28:53.663: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 06:29:53.868: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:29:53.909
    Aug 30 06:29:53.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption-path 08/30/23 06:29:53.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:29:54.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:29:54.056
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/30/23 06:29:54.084
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 06:29:54.085
    Aug 30 06:29:54.138: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-4367" to be "running"
    Aug 30 06:29:54.145: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.779889ms
    Aug 30 06:29:56.156: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.018021497s
    Aug 30 06:29:56.156: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 06:29:56.163
    Aug 30 06:29:56.192: INFO: found a healthy node: 10.135.139.190
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 30 06:30:04.349: INFO: pods created so far: [1 1 1]
    Aug 30 06:30:04.349: INFO: length of pods created so far: 3
    Aug 30 06:30:10.369: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:17.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:17.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4367" for this suite. 08/30/23 06:30:17.545
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9721" for this suite. 08/30/23 06:30:17.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:17.607
Aug 30 06:30:17.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:30:17.608
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:17.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:17.714
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
Aug 30 06:30:17.729: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f3218cb3-9844-4a8b-8317-d07f63b4bdf0 08/30/23 06:30:17.729
STEP: Creating secret with name s-test-opt-upd-59b00ab1-ede1-4015-9adf-bc2c24d697fe 08/30/23 06:30:17.738
STEP: Creating the pod 08/30/23 06:30:17.746
W0830 06:30:17.762822      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:30:17.762: INFO: Waiting up to 5m0s for pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6" in namespace "secrets-2165" to be "running and ready"
Aug 30 06:30:17.769: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064038ms
Aug 30 06:30:17.769: INFO: The phase of Pod pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:30:19.777: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014658932s
Aug 30 06:30:19.777: INFO: The phase of Pod pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6 is Running (Ready = true)
Aug 30 06:30:19.777: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f3218cb3-9844-4a8b-8317-d07f63b4bdf0 08/30/23 06:30:19.889
STEP: Updating secret s-test-opt-upd-59b00ab1-ede1-4015-9adf-bc2c24d697fe 08/30/23 06:30:19.909
STEP: Creating secret with name s-test-opt-create-449beb99-28ca-4fad-8054-1b662dc626c0 08/30/23 06:30:19.919
STEP: waiting to observe update in volume 08/30/23 06:30:19.93
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2165" for this suite. 08/30/23 06:30:22.252
------------------------------
• [4.676 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:17.607
    Aug 30 06:30:17.607: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:30:17.608
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:17.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:17.714
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    Aug 30 06:30:17.729: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-f3218cb3-9844-4a8b-8317-d07f63b4bdf0 08/30/23 06:30:17.729
    STEP: Creating secret with name s-test-opt-upd-59b00ab1-ede1-4015-9adf-bc2c24d697fe 08/30/23 06:30:17.738
    STEP: Creating the pod 08/30/23 06:30:17.746
    W0830 06:30:17.762822      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "dels-volume-test", "upds-volume-test", "creates-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:30:17.762: INFO: Waiting up to 5m0s for pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6" in namespace "secrets-2165" to be "running and ready"
    Aug 30 06:30:17.769: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064038ms
    Aug 30 06:30:17.769: INFO: The phase of Pod pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:30:19.777: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6": Phase="Running", Reason="", readiness=true. Elapsed: 2.014658932s
    Aug 30 06:30:19.777: INFO: The phase of Pod pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6 is Running (Ready = true)
    Aug 30 06:30:19.777: INFO: Pod "pod-secrets-44c65e94-fb84-432a-8a98-16fb065532a6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f3218cb3-9844-4a8b-8317-d07f63b4bdf0 08/30/23 06:30:19.889
    STEP: Updating secret s-test-opt-upd-59b00ab1-ede1-4015-9adf-bc2c24d697fe 08/30/23 06:30:19.909
    STEP: Creating secret with name s-test-opt-create-449beb99-28ca-4fad-8054-1b662dc626c0 08/30/23 06:30:19.919
    STEP: waiting to observe update in volume 08/30/23 06:30:19.93
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:22.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2165" for this suite. 08/30/23 06:30:22.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:22.288
Aug 30 06:30:22.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-runtime 08/30/23 06:30:22.289
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:22.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:22.371
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/30/23 06:30:22.377
STEP: wait for the container to reach Failed 08/30/23 06:30:22.427
STEP: get the container status 08/30/23 06:30:27.479
STEP: the container should be terminated 08/30/23 06:30:27.49
STEP: the termination message should be set 08/30/23 06:30:27.491
Aug 30 06:30:27.491: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/30/23 06:30:27.491
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:27.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9218" for this suite. 08/30/23 06:30:27.532
------------------------------
• [SLOW TEST] [5.264 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:22.288
    Aug 30 06:30:22.288: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-runtime 08/30/23 06:30:22.289
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:22.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:22.371
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/30/23 06:30:22.377
    STEP: wait for the container to reach Failed 08/30/23 06:30:22.427
    STEP: get the container status 08/30/23 06:30:27.479
    STEP: the container should be terminated 08/30/23 06:30:27.49
    STEP: the termination message should be set 08/30/23 06:30:27.491
    Aug 30 06:30:27.491: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/30/23 06:30:27.491
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:27.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9218" for this suite. 08/30/23 06:30:27.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:27.554
Aug 30 06:30:27.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename limitrange 08/30/23 06:30:27.555
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:27.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:27.62
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/30/23 06:30:27.628
STEP: Setting up watch 08/30/23 06:30:27.628
STEP: Submitting a LimitRange 08/30/23 06:30:27.742
STEP: Verifying LimitRange creation was observed 08/30/23 06:30:27.775
STEP: Fetching the LimitRange to ensure it has proper values 08/30/23 06:30:27.775
Aug 30 06:30:27.784: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 30 06:30:27.784: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/30/23 06:30:27.784
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/30/23 06:30:27.801
Aug 30 06:30:27.814: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 30 06:30:27.814: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/30/23 06:30:27.814
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/30/23 06:30:27.825
Aug 30 06:30:27.834: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 30 06:30:27.834: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/30/23 06:30:27.834
STEP: Failing to create a Pod with more than max resources 08/30/23 06:30:27.838
STEP: Updating a LimitRange 08/30/23 06:30:27.841
STEP: Verifying LimitRange updating is effective 08/30/23 06:30:27.852
STEP: Creating a Pod with less than former min resources 08/30/23 06:30:29.866
STEP: Failing to create a Pod with more than max resources 08/30/23 06:30:29.934
STEP: Deleting a LimitRange 08/30/23 06:30:29.938
STEP: Verifying the LimitRange was deleted 08/30/23 06:30:29.996
Aug 30 06:30:35.004: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/30/23 06:30:35.004
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:35.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7540" for this suite. 08/30/23 06:30:35.032
------------------------------
• [SLOW TEST] [7.514 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:27.554
    Aug 30 06:30:27.554: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename limitrange 08/30/23 06:30:27.555
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:27.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:27.62
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/30/23 06:30:27.628
    STEP: Setting up watch 08/30/23 06:30:27.628
    STEP: Submitting a LimitRange 08/30/23 06:30:27.742
    STEP: Verifying LimitRange creation was observed 08/30/23 06:30:27.775
    STEP: Fetching the LimitRange to ensure it has proper values 08/30/23 06:30:27.775
    Aug 30 06:30:27.784: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 30 06:30:27.784: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/30/23 06:30:27.784
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/30/23 06:30:27.801
    Aug 30 06:30:27.814: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 30 06:30:27.814: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/30/23 06:30:27.814
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/30/23 06:30:27.825
    Aug 30 06:30:27.834: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 30 06:30:27.834: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/30/23 06:30:27.834
    STEP: Failing to create a Pod with more than max resources 08/30/23 06:30:27.838
    STEP: Updating a LimitRange 08/30/23 06:30:27.841
    STEP: Verifying LimitRange updating is effective 08/30/23 06:30:27.852
    STEP: Creating a Pod with less than former min resources 08/30/23 06:30:29.866
    STEP: Failing to create a Pod with more than max resources 08/30/23 06:30:29.934
    STEP: Deleting a LimitRange 08/30/23 06:30:29.938
    STEP: Verifying the LimitRange was deleted 08/30/23 06:30:29.996
    Aug 30 06:30:35.004: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/30/23 06:30:35.004
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:35.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7540" for this suite. 08/30/23 06:30:35.032
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:35.068
Aug 30 06:30:35.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/30/23 06:30:35.07
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:35.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:35.115
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/30/23 06:30:35.121
STEP: Creating hostNetwork=false pod 08/30/23 06:30:35.121
Aug 30 06:30:36.152: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1052" to be "running and ready"
Aug 30 06:30:36.159: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.438756ms
Aug 30 06:30:36.159: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:30:38.168: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016031136s
Aug 30 06:30:38.168: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 30 06:30:38.168: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/30/23 06:30:38.175
Aug 30 06:30:38.187: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1052" to be "running and ready"
Aug 30 06:30:38.194: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644358ms
Aug 30 06:30:38.194: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:30:40.202: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382736s
Aug 30 06:30:40.202: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 30 06:30:40.202: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/30/23 06:30:40.208
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/30/23 06:30:40.208
Aug 30 06:30:40.209: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:40.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:40.209: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:40.210: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 06:30:40.361: INFO: Exec stderr: ""
Aug 30 06:30:40.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:40.361: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:40.361: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 06:30:40.606: INFO: Exec stderr: ""
Aug 30 06:30:40.606: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:40.607: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:40.607: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 06:30:40.808: INFO: Exec stderr: ""
Aug 30 06:30:40.808: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:40.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:40.809: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:40.809: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 06:30:41.001: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/30/23 06:30:41.001
Aug 30 06:30:41.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.002: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.002: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 30 06:30:41.116: INFO: Exec stderr: ""
Aug 30 06:30:41.116: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.117: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.117: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 30 06:30:41.240: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/30/23 06:30:41.24
Aug 30 06:30:41.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.241: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.241: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 06:30:41.370: INFO: Exec stderr: ""
Aug 30 06:30:41.370: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.370: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.370: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 30 06:30:41.541: INFO: Exec stderr: ""
Aug 30 06:30:41.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.542: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.542: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 06:30:41.671: INFO: Exec stderr: ""
Aug 30 06:30:41.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:30:41.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:30:41.672: INFO: ExecWithOptions: Clientset creation
Aug 30 06:30:41.672: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 30 06:30:41.779: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:41.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1052" for this suite. 08/30/23 06:30:41.797
------------------------------
• [SLOW TEST] [6.747 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:35.068
    Aug 30 06:30:35.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/30/23 06:30:35.07
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:35.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:35.115
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/30/23 06:30:35.121
    STEP: Creating hostNetwork=false pod 08/30/23 06:30:35.121
    Aug 30 06:30:36.152: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-1052" to be "running and ready"
    Aug 30 06:30:36.159: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.438756ms
    Aug 30 06:30:36.159: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:30:38.168: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016031136s
    Aug 30 06:30:38.168: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 30 06:30:38.168: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/30/23 06:30:38.175
    Aug 30 06:30:38.187: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-1052" to be "running and ready"
    Aug 30 06:30:38.194: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644358ms
    Aug 30 06:30:38.194: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:30:40.202: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014382736s
    Aug 30 06:30:40.202: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 30 06:30:40.202: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/30/23 06:30:40.208
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/30/23 06:30:40.208
    Aug 30 06:30:40.209: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:40.209: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:40.209: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:40.210: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 30 06:30:40.361: INFO: Exec stderr: ""
    Aug 30 06:30:40.361: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:40.361: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:40.361: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 30 06:30:40.606: INFO: Exec stderr: ""
    Aug 30 06:30:40.606: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:40.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:40.607: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:40.607: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 30 06:30:40.808: INFO: Exec stderr: ""
    Aug 30 06:30:40.808: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:40.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:40.809: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:40.809: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 30 06:30:41.001: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/30/23 06:30:41.001
    Aug 30 06:30:41.002: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.002: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.002: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 30 06:30:41.116: INFO: Exec stderr: ""
    Aug 30 06:30:41.116: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.117: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.117: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 30 06:30:41.240: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/30/23 06:30:41.24
    Aug 30 06:30:41.240: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.241: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.241: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 30 06:30:41.370: INFO: Exec stderr: ""
    Aug 30 06:30:41.370: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.370: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.370: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 30 06:30:41.541: INFO: Exec stderr: ""
    Aug 30 06:30:41.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.542: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.542: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 30 06:30:41.671: INFO: Exec stderr: ""
    Aug 30 06:30:41.672: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1052 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:30:41.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:30:41.672: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:30:41.672: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1052/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 30 06:30:41.779: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:41.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-1052" for this suite. 08/30/23 06:30:41.797
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:41.816
Aug 30 06:30:41.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 06:30:41.817
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:41.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:41.875
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/30/23 06:30:41.9
STEP: Patching the Job 08/30/23 06:30:41.914
STEP: Watching for Job to be patched 08/30/23 06:30:41.952
Aug 30 06:30:41.955: INFO: Event ADDED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 30 06:30:41.955: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 30 06:30:41.955: INFO: Event MODIFIED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/30/23 06:30:41.955
STEP: Watching for Job to be updated 08/30/23 06:30:42.011
Aug 30 06:30:42.014: INFO: Event MODIFIED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:42.014: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/30/23 06:30:42.014
Aug 30 06:30:42.029: INFO: Job: e2e-gj6rf as labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf]
STEP: Waiting for job to complete 08/30/23 06:30:42.029
STEP: Delete a job collection with a labelselector 08/30/23 06:30:54.079
STEP: Watching for Job to be deleted 08/30/23 06:30:54.115
Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.119: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 30 06:30:54.121: INFO: Event DELETED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/30/23 06:30:54.121
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 06:30:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5505" for this suite. 08/30/23 06:30:54.162
------------------------------
• [SLOW TEST] [12.383 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:41.816
    Aug 30 06:30:41.816: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 06:30:41.817
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:41.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:41.875
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/30/23 06:30:41.9
    STEP: Patching the Job 08/30/23 06:30:41.914
    STEP: Watching for Job to be patched 08/30/23 06:30:41.952
    Aug 30 06:30:41.955: INFO: Event ADDED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 30 06:30:41.955: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 30 06:30:41.955: INFO: Event MODIFIED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/30/23 06:30:41.955
    STEP: Watching for Job to be updated 08/30/23 06:30:42.011
    Aug 30 06:30:42.014: INFO: Event MODIFIED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:42.014: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/30/23 06:30:42.014
    Aug 30 06:30:42.029: INFO: Job: e2e-gj6rf as labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf]
    STEP: Waiting for job to complete 08/30/23 06:30:42.029
    STEP: Delete a job collection with a labelselector 08/30/23 06:30:54.079
    STEP: Watching for Job to be deleted 08/30/23 06:30:54.115
    Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.118: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.119: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event MODIFIED observed for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 30 06:30:54.121: INFO: Event DELETED found for Job e2e-gj6rf in namespace job-5505 with labels: map[e2e-gj6rf:patched e2e-job-label:e2e-gj6rf] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/30/23 06:30:54.121
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:30:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5505" for this suite. 08/30/23 06:30:54.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:30:54.201
Aug 30 06:30:54.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 06:30:54.202
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:54.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:54.266
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 08/30/23 06:30:54.379
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:30:54.408
Aug 30 06:30:54.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:30:54.438: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:30:55.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:30:55.454: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:30:56.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 06:30:56.455: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:30:57.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 06:30:57.457: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 08/30/23 06:30:57.467
Aug 30 06:30:57.477: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/30/23 06:30:57.477
Aug 30 06:30:57.500: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/30/23 06:30:57.5
Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: ADDED
Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.504: INFO: Found daemon set daemon-set in namespace daemonsets-2329 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 06:30:57.504: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/30/23 06:30:57.504
STEP: watching for the daemon set status to be patched 08/30/23 06:30:57.522
Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: ADDED
Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.526: INFO: Observed daemon set daemon-set in namespace daemonsets-2329 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
Aug 30 06:30:57.526: INFO: Found daemon set daemon-set in namespace daemonsets-2329 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 30 06:30:57.526: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 06:30:57.535
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2329, will wait for the garbage collector to delete the pods 08/30/23 06:30:57.535
Aug 30 06:30:57.612: INFO: Deleting DaemonSet.extensions daemon-set took: 16.692756ms
Aug 30 06:30:57.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.130029ms
Aug 30 06:31:00.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:31:00.028: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 06:31:00.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74577"},"items":null}

Aug 30 06:31:00.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74577"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:00.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2329" for this suite. 08/30/23 06:31:00.118
------------------------------
• [SLOW TEST] [5.937 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:30:54.201
    Aug 30 06:30:54.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 06:30:54.202
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:30:54.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:30:54.266
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 08/30/23 06:30:54.379
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:30:54.408
    Aug 30 06:30:54.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:30:54.438: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:30:55.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:30:55.454: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:30:56.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 06:30:56.455: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:30:57.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 06:30:57.457: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 08/30/23 06:30:57.467
    Aug 30 06:30:57.477: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/30/23 06:30:57.477
    Aug 30 06:30:57.500: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/30/23 06:30:57.5
    Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: ADDED
    Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.503: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.504: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.504: INFO: Found daemon set daemon-set in namespace daemonsets-2329 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 30 06:30:57.504: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/30/23 06:30:57.504
    STEP: watching for the daemon set status to be patched 08/30/23 06:30:57.522
    Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: ADDED
    Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.525: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.526: INFO: Observed daemon set daemon-set in namespace daemonsets-2329 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 30 06:30:57.526: INFO: Observed &DaemonSet event: MODIFIED
    Aug 30 06:30:57.526: INFO: Found daemon set daemon-set in namespace daemonsets-2329 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 30 06:30:57.526: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 06:30:57.535
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2329, will wait for the garbage collector to delete the pods 08/30/23 06:30:57.535
    Aug 30 06:30:57.612: INFO: Deleting DaemonSet.extensions daemon-set took: 16.692756ms
    Aug 30 06:30:57.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.130029ms
    Aug 30 06:31:00.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:31:00.028: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 06:31:00.041: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74577"},"items":null}

    Aug 30 06:31:00.048: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74577"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:00.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2329" for this suite. 08/30/23 06:31:00.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:00.141
Aug 30 06:31:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption 08/30/23 06:31:00.142
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:00.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:00.188
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/30/23 06:31:00.204
STEP: Waiting for all pods to be running 08/30/23 06:31:02.334
Aug 30 06:31:02.371: INFO: running pods: 0 < 3
Aug 30 06:31:04.380: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:06.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2267" for this suite. 08/30/23 06:31:06.4
------------------------------
• [SLOW TEST] [6.280 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:00.141
    Aug 30 06:31:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption 08/30/23 06:31:00.142
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:00.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:00.188
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/30/23 06:31:00.204
    STEP: Waiting for all pods to be running 08/30/23 06:31:02.334
    Aug 30 06:31:02.371: INFO: running pods: 0 < 3
    Aug 30 06:31:04.380: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:06.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2267" for this suite. 08/30/23 06:31:06.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:06.425
Aug 30 06:31:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 06:31:06.426
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:06.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:06.516
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:31:06.522
Aug 30 06:31:06.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522" in namespace "downward-api-5222" to be "Succeeded or Failed"
Aug 30 06:31:06.597: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Pending", Reason="", readiness=false. Elapsed: 10.990586ms
Aug 30 06:31:08.605: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019655951s
Aug 30 06:31:10.607: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021762318s
STEP: Saw pod success 08/30/23 06:31:10.607
Aug 30 06:31:10.608: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522" satisfied condition "Succeeded or Failed"
Aug 30 06:31:10.614: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 container client-container: <nil>
STEP: delete the pod 08/30/23 06:31:10.639
Aug 30 06:31:10.669: INFO: Waiting for pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 to disappear
Aug 30 06:31:10.682: INFO: Pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5222" for this suite. 08/30/23 06:31:10.721
------------------------------
• [4.317 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:06.425
    Aug 30 06:31:06.425: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 06:31:06.426
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:06.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:06.516
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:31:06.522
    Aug 30 06:31:06.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522" in namespace "downward-api-5222" to be "Succeeded or Failed"
    Aug 30 06:31:06.597: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Pending", Reason="", readiness=false. Elapsed: 10.990586ms
    Aug 30 06:31:08.605: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019655951s
    Aug 30 06:31:10.607: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021762318s
    STEP: Saw pod success 08/30/23 06:31:10.607
    Aug 30 06:31:10.608: INFO: Pod "downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522" satisfied condition "Succeeded or Failed"
    Aug 30 06:31:10.614: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:31:10.639
    Aug 30 06:31:10.669: INFO: Waiting for pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 to disappear
    Aug 30 06:31:10.682: INFO: Pod downwardapi-volume-59913730-859e-47af-a4ab-eaba3a8b3522 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:10.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5222" for this suite. 08/30/23 06:31:10.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:10.746
Aug 30 06:31:10.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename csiinlinevolumes 08/30/23 06:31:10.748
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:10.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:10.803
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/30/23 06:31:10.811
STEP: getting 08/30/23 06:31:10.877
STEP: listing 08/30/23 06:31:10.891
STEP: deleting 08/30/23 06:31:10.898
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:10.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-1879" for this suite. 08/30/23 06:31:10.958
------------------------------
• [0.244 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:10.746
    Aug 30 06:31:10.747: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename csiinlinevolumes 08/30/23 06:31:10.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:10.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:10.803
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/30/23 06:31:10.811
    STEP: getting 08/30/23 06:31:10.877
    STEP: listing 08/30/23 06:31:10.891
    STEP: deleting 08/30/23 06:31:10.898
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:10.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-1879" for this suite. 08/30/23 06:31:10.958
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:10.99
Aug 30 06:31:10.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename subpath 08/30/23 06:31:10.992
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:11.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:11.046
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/30/23 06:31:11.055
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-tzb2 08/30/23 06:31:11.091
STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:31:11.091
Aug 30 06:31:11.107: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tzb2" in namespace "subpath-8759" to be "Succeeded or Failed"
Aug 30 06:31:11.122: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.548418ms
Aug 30 06:31:13.129: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021143789s
Aug 30 06:31:15.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023410226s
Aug 30 06:31:17.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 6.023292159s
Aug 30 06:31:19.130: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 8.022696109s
Aug 30 06:31:21.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 10.023278359s
Aug 30 06:31:23.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 12.023193197s
Aug 30 06:31:25.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 14.023042183s
Aug 30 06:31:27.186: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 16.077972164s
Aug 30 06:31:29.130: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 18.022960849s
Aug 30 06:31:31.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 20.023567422s
Aug 30 06:31:33.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 22.023164448s
Aug 30 06:31:35.132: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=false. Elapsed: 24.024080838s
Aug 30 06:31:37.132: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024352158s
STEP: Saw pod success 08/30/23 06:31:37.132
Aug 30 06:31:37.132: INFO: Pod "pod-subpath-test-configmap-tzb2" satisfied condition "Succeeded or Failed"
Aug 30 06:31:37.143: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-configmap-tzb2 container test-container-subpath-configmap-tzb2: <nil>
STEP: delete the pod 08/30/23 06:31:37.16
Aug 30 06:31:37.217: INFO: Waiting for pod pod-subpath-test-configmap-tzb2 to disappear
Aug 30 06:31:37.225: INFO: Pod pod-subpath-test-configmap-tzb2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-tzb2 08/30/23 06:31:37.225
Aug 30 06:31:37.225: INFO: Deleting pod "pod-subpath-test-configmap-tzb2" in namespace "subpath-8759"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:37.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8759" for this suite. 08/30/23 06:31:37.245
------------------------------
• [SLOW TEST] [26.275 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:10.99
    Aug 30 06:31:10.991: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename subpath 08/30/23 06:31:10.992
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:11.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:11.046
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/30/23 06:31:11.055
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-tzb2 08/30/23 06:31:11.091
    STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:31:11.091
    Aug 30 06:31:11.107: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-tzb2" in namespace "subpath-8759" to be "Succeeded or Failed"
    Aug 30 06:31:11.122: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.548418ms
    Aug 30 06:31:13.129: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.021143789s
    Aug 30 06:31:15.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023410226s
    Aug 30 06:31:17.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 6.023292159s
    Aug 30 06:31:19.130: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 8.022696109s
    Aug 30 06:31:21.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 10.023278359s
    Aug 30 06:31:23.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 12.023193197s
    Aug 30 06:31:25.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 14.023042183s
    Aug 30 06:31:27.186: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 16.077972164s
    Aug 30 06:31:29.130: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 18.022960849s
    Aug 30 06:31:31.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 20.023567422s
    Aug 30 06:31:33.131: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=true. Elapsed: 22.023164448s
    Aug 30 06:31:35.132: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Running", Reason="", readiness=false. Elapsed: 24.024080838s
    Aug 30 06:31:37.132: INFO: Pod "pod-subpath-test-configmap-tzb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.024352158s
    STEP: Saw pod success 08/30/23 06:31:37.132
    Aug 30 06:31:37.132: INFO: Pod "pod-subpath-test-configmap-tzb2" satisfied condition "Succeeded or Failed"
    Aug 30 06:31:37.143: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-configmap-tzb2 container test-container-subpath-configmap-tzb2: <nil>
    STEP: delete the pod 08/30/23 06:31:37.16
    Aug 30 06:31:37.217: INFO: Waiting for pod pod-subpath-test-configmap-tzb2 to disappear
    Aug 30 06:31:37.225: INFO: Pod pod-subpath-test-configmap-tzb2 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-tzb2 08/30/23 06:31:37.225
    Aug 30 06:31:37.225: INFO: Deleting pod "pod-subpath-test-configmap-tzb2" in namespace "subpath-8759"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:37.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8759" for this suite. 08/30/23 06:31:37.245
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:37.272
Aug 30 06:31:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename controllerrevisions 08/30/23 06:31:37.273
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:37.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:37.341
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-4s6tb-daemon-set" 08/30/23 06:31:37.523
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:31:37.546
Aug 30 06:31:37.567: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
Aug 30 06:31:37.567: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:31:38.587: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
Aug 30 06:31:38.588: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:31:39.586: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 2
Aug 30 06:31:39.586: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:31:40.585: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 3
Aug 30 06:31:40.585: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-4s6tb-daemon-set
STEP: Confirm DaemonSet "e2e-4s6tb-daemon-set" successfully created with "daemonset-name=e2e-4s6tb-daemon-set" label 08/30/23 06:31:40.594
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4s6tb-daemon-set" 08/30/23 06:31:40.622
Aug 30 06:31:40.634: INFO: Located ControllerRevision: "e2e-4s6tb-daemon-set-5c7856c888"
STEP: Patching ControllerRevision "e2e-4s6tb-daemon-set-5c7856c888" 08/30/23 06:31:40.661
Aug 30 06:31:40.675: INFO: e2e-4s6tb-daemon-set-5c7856c888 has been patched
STEP: Create a new ControllerRevision 08/30/23 06:31:40.675
Aug 30 06:31:40.684: INFO: Created ControllerRevision: e2e-4s6tb-daemon-set-5499578686
STEP: Confirm that there are two ControllerRevisions 08/30/23 06:31:40.684
Aug 30 06:31:40.685: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 30 06:31:40.692: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-4s6tb-daemon-set-5c7856c888" 08/30/23 06:31:40.692
STEP: Confirm that there is only one ControllerRevision 08/30/23 06:31:40.703
Aug 30 06:31:40.703: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 30 06:31:40.709: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-4s6tb-daemon-set-5499578686" 08/30/23 06:31:40.716
Aug 30 06:31:40.730: INFO: e2e-4s6tb-daemon-set-5499578686 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/30/23 06:31:40.731
W0830 06:31:40.743195      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/30/23 06:31:40.743
Aug 30 06:31:40.743: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 30 06:31:41.750: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 30 06:31:41.758: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4s6tb-daemon-set-5499578686=updated" 08/30/23 06:31:41.758
STEP: Confirm that there is only one ControllerRevision 08/30/23 06:31:41.78
Aug 30 06:31:41.780: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 30 06:31:41.786: INFO: Found 1 ControllerRevisions
Aug 30 06:31:41.793: INFO: ControllerRevision "e2e-4s6tb-daemon-set-5859db5567" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-4s6tb-daemon-set" 08/30/23 06:31:41.802
STEP: deleting DaemonSet.extensions e2e-4s6tb-daemon-set in namespace controllerrevisions-1712, will wait for the garbage collector to delete the pods 08/30/23 06:31:41.802
Aug 30 06:31:41.886: INFO: Deleting DaemonSet.extensions e2e-4s6tb-daemon-set took: 22.719982ms
Aug 30 06:31:42.087: INFO: Terminating DaemonSet.extensions e2e-4s6tb-daemon-set pods took: 200.727517ms
Aug 30 06:31:43.894: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
Aug 30 06:31:43.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4s6tb-daemon-set
Aug 30 06:31:43.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75311"},"items":null}

Aug 30 06:31:43.908: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75312"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:43.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-1712" for this suite. 08/30/23 06:31:43.955
------------------------------
• [SLOW TEST] [6.705 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:37.272
    Aug 30 06:31:37.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename controllerrevisions 08/30/23 06:31:37.273
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:37.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:37.341
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-4s6tb-daemon-set" 08/30/23 06:31:37.523
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:31:37.546
    Aug 30 06:31:37.567: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
    Aug 30 06:31:37.567: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:31:38.587: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
    Aug 30 06:31:38.588: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:31:39.586: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 2
    Aug 30 06:31:39.586: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:31:40.585: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 3
    Aug 30 06:31:40.585: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-4s6tb-daemon-set
    STEP: Confirm DaemonSet "e2e-4s6tb-daemon-set" successfully created with "daemonset-name=e2e-4s6tb-daemon-set" label 08/30/23 06:31:40.594
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4s6tb-daemon-set" 08/30/23 06:31:40.622
    Aug 30 06:31:40.634: INFO: Located ControllerRevision: "e2e-4s6tb-daemon-set-5c7856c888"
    STEP: Patching ControllerRevision "e2e-4s6tb-daemon-set-5c7856c888" 08/30/23 06:31:40.661
    Aug 30 06:31:40.675: INFO: e2e-4s6tb-daemon-set-5c7856c888 has been patched
    STEP: Create a new ControllerRevision 08/30/23 06:31:40.675
    Aug 30 06:31:40.684: INFO: Created ControllerRevision: e2e-4s6tb-daemon-set-5499578686
    STEP: Confirm that there are two ControllerRevisions 08/30/23 06:31:40.684
    Aug 30 06:31:40.685: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 30 06:31:40.692: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-4s6tb-daemon-set-5c7856c888" 08/30/23 06:31:40.692
    STEP: Confirm that there is only one ControllerRevision 08/30/23 06:31:40.703
    Aug 30 06:31:40.703: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 30 06:31:40.709: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-4s6tb-daemon-set-5499578686" 08/30/23 06:31:40.716
    Aug 30 06:31:40.730: INFO: e2e-4s6tb-daemon-set-5499578686 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/30/23 06:31:40.731
    W0830 06:31:40.743195      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/30/23 06:31:40.743
    Aug 30 06:31:40.743: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 30 06:31:41.750: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 30 06:31:41.758: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4s6tb-daemon-set-5499578686=updated" 08/30/23 06:31:41.758
    STEP: Confirm that there is only one ControllerRevision 08/30/23 06:31:41.78
    Aug 30 06:31:41.780: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 30 06:31:41.786: INFO: Found 1 ControllerRevisions
    Aug 30 06:31:41.793: INFO: ControllerRevision "e2e-4s6tb-daemon-set-5859db5567" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-4s6tb-daemon-set" 08/30/23 06:31:41.802
    STEP: deleting DaemonSet.extensions e2e-4s6tb-daemon-set in namespace controllerrevisions-1712, will wait for the garbage collector to delete the pods 08/30/23 06:31:41.802
    Aug 30 06:31:41.886: INFO: Deleting DaemonSet.extensions e2e-4s6tb-daemon-set took: 22.719982ms
    Aug 30 06:31:42.087: INFO: Terminating DaemonSet.extensions e2e-4s6tb-daemon-set pods took: 200.727517ms
    Aug 30 06:31:43.894: INFO: Number of nodes with available pods controlled by daemonset e2e-4s6tb-daemon-set: 0
    Aug 30 06:31:43.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4s6tb-daemon-set
    Aug 30 06:31:43.902: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75311"},"items":null}

    Aug 30 06:31:43.908: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75312"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:43.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-1712" for this suite. 08/30/23 06:31:43.955
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:43.979
Aug 30 06:31:43.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename certificates 08/30/23 06:31:43.981
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:44.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:44.048
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/30/23 06:31:44.68
STEP: getting /apis/certificates.k8s.io 08/30/23 06:31:44.685
STEP: getting /apis/certificates.k8s.io/v1 08/30/23 06:31:44.688
STEP: creating 08/30/23 06:31:44.689
STEP: getting 08/30/23 06:31:44.716
STEP: listing 08/30/23 06:31:44.723
STEP: watching 08/30/23 06:31:44.731
Aug 30 06:31:44.731: INFO: starting watch
STEP: patching 08/30/23 06:31:44.733
STEP: updating 08/30/23 06:31:44.743
Aug 30 06:31:44.752: INFO: waiting for watch events with expected annotations
Aug 30 06:31:44.752: INFO: saw patched and updated annotations
STEP: getting /approval 08/30/23 06:31:44.752
STEP: patching /approval 08/30/23 06:31:44.759
STEP: updating /approval 08/30/23 06:31:44.769
STEP: getting /status 08/30/23 06:31:44.779
STEP: patching /status 08/30/23 06:31:44.785
STEP: updating /status 08/30/23 06:31:44.796
STEP: deleting 08/30/23 06:31:44.805
STEP: deleting a collection 08/30/23 06:31:44.828
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:44.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-8070" for this suite. 08/30/23 06:31:44.867
------------------------------
• [0.906 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:43.979
    Aug 30 06:31:43.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename certificates 08/30/23 06:31:43.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:44.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:44.048
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/30/23 06:31:44.68
    STEP: getting /apis/certificates.k8s.io 08/30/23 06:31:44.685
    STEP: getting /apis/certificates.k8s.io/v1 08/30/23 06:31:44.688
    STEP: creating 08/30/23 06:31:44.689
    STEP: getting 08/30/23 06:31:44.716
    STEP: listing 08/30/23 06:31:44.723
    STEP: watching 08/30/23 06:31:44.731
    Aug 30 06:31:44.731: INFO: starting watch
    STEP: patching 08/30/23 06:31:44.733
    STEP: updating 08/30/23 06:31:44.743
    Aug 30 06:31:44.752: INFO: waiting for watch events with expected annotations
    Aug 30 06:31:44.752: INFO: saw patched and updated annotations
    STEP: getting /approval 08/30/23 06:31:44.752
    STEP: patching /approval 08/30/23 06:31:44.759
    STEP: updating /approval 08/30/23 06:31:44.769
    STEP: getting /status 08/30/23 06:31:44.779
    STEP: patching /status 08/30/23 06:31:44.785
    STEP: updating /status 08/30/23 06:31:44.796
    STEP: deleting 08/30/23 06:31:44.805
    STEP: deleting a collection 08/30/23 06:31:44.828
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:44.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-8070" for this suite. 08/30/23 06:31:44.867
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:44.886
Aug 30 06:31:44.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 06:31:44.887
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:44.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:44.94
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/30/23 06:31:44.961
Aug 30 06:31:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7235 cluster-info'
Aug 30 06:31:45.074: INFO: stderr: ""
Aug 30 06:31:45.074: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7235" for this suite. 08/30/23 06:31:45.082
------------------------------
• [0.218 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:44.886
    Aug 30 06:31:44.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 06:31:44.887
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:44.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:44.94
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/30/23 06:31:44.961
    Aug 30 06:31:44.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7235 cluster-info'
    Aug 30 06:31:45.074: INFO: stderr: ""
    Aug 30 06:31:45.074: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7235" for this suite. 08/30/23 06:31:45.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:45.104
Aug 30 06:31:45.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 06:31:45.106
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:45.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:45.159
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/30/23 06:31:45.171
W0830 06:31:45.185398      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be added 08/30/23 06:31:45.185
STEP: waiting for available Replicas 08/30/23 06:31:45.185
STEP: patching ReplicationController 08/30/23 06:31:49.23
STEP: waiting for RC to be modified 08/30/23 06:31:49.243
STEP: patching ReplicationController status 08/30/23 06:31:49.244
STEP: waiting for RC to be modified 08/30/23 06:31:49.253
STEP: waiting for available Replicas 08/30/23 06:31:49.254
STEP: fetching ReplicationController status 08/30/23 06:31:49.269
STEP: patching ReplicationController scale 08/30/23 06:31:49.276
STEP: waiting for RC to be modified 08/30/23 06:31:49.288
STEP: waiting for ReplicationController's scale to be the max amount 08/30/23 06:31:49.288
STEP: fetching ReplicationController; ensuring that it's patched 08/30/23 06:31:53.654
STEP: updating ReplicationController status 08/30/23 06:31:53.662
STEP: waiting for RC to be modified 08/30/23 06:31:53.672
STEP: listing all ReplicationControllers 08/30/23 06:31:53.672
STEP: checking that ReplicationController has expected values 08/30/23 06:31:53.68
STEP: deleting ReplicationControllers by collection 08/30/23 06:31:53.68
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/30/23 06:31:53.699
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:31:53.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6965" for this suite. 08/30/23 06:31:53.809
------------------------------
• [SLOW TEST] [8.723 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:45.104
    Aug 30 06:31:45.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 06:31:45.106
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:45.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:45.159
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/30/23 06:31:45.171
    W0830 06:31:45.185398      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for RC to be added 08/30/23 06:31:45.185
    STEP: waiting for available Replicas 08/30/23 06:31:45.185
    STEP: patching ReplicationController 08/30/23 06:31:49.23
    STEP: waiting for RC to be modified 08/30/23 06:31:49.243
    STEP: patching ReplicationController status 08/30/23 06:31:49.244
    STEP: waiting for RC to be modified 08/30/23 06:31:49.253
    STEP: waiting for available Replicas 08/30/23 06:31:49.254
    STEP: fetching ReplicationController status 08/30/23 06:31:49.269
    STEP: patching ReplicationController scale 08/30/23 06:31:49.276
    STEP: waiting for RC to be modified 08/30/23 06:31:49.288
    STEP: waiting for ReplicationController's scale to be the max amount 08/30/23 06:31:49.288
    STEP: fetching ReplicationController; ensuring that it's patched 08/30/23 06:31:53.654
    STEP: updating ReplicationController status 08/30/23 06:31:53.662
    STEP: waiting for RC to be modified 08/30/23 06:31:53.672
    STEP: listing all ReplicationControllers 08/30/23 06:31:53.672
    STEP: checking that ReplicationController has expected values 08/30/23 06:31:53.68
    STEP: deleting ReplicationControllers by collection 08/30/23 06:31:53.68
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/30/23 06:31:53.699
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:31:53.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6965" for this suite. 08/30/23 06:31:53.809
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:31:53.828
Aug 30 06:31:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:31:53.829
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:53.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:53.894
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
W0830 06:31:53.936956      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:31:53.937: INFO: created pod
Aug 30 06:31:53.937: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6964" to be "Succeeded or Failed"
Aug 30 06:31:53.944: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.194314ms
Aug 30 06:31:55.954: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017115133s
Aug 30 06:31:57.980: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042944725s
STEP: Saw pod success 08/30/23 06:31:57.98
Aug 30 06:31:57.980: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 30 06:32:27.980: INFO: polling logs
Aug 30 06:32:27.998: INFO: Pod logs: 
I0830 06:31:55.081496       1 log.go:198] OK: Got token
I0830 06:31:55.081547       1 log.go:198] validating with in-cluster discovery
I0830 06:31:55.082120       1 log.go:198] OK: got issuer https://kubernetes.default.svc
I0830 06:31:55.082156       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693377714, NotBefore:1693377114, IssuedAt:1693377114, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"280a423d-8844-440d-b0bd-2d42bb43f9d9"}}}
I0830 06:31:55.097296       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0830 06:31:55.111872       1 log.go:198] OK: Validated signature on JWT
I0830 06:31:55.111954       1 log.go:198] OK: Got valid claims from token!
I0830 06:31:55.111987       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693377714, NotBefore:1693377114, IssuedAt:1693377114, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"280a423d-8844-440d-b0bd-2d42bb43f9d9"}}}

Aug 30 06:32:27.998: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:28.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6964" for this suite. 08/30/23 06:32:28.019
------------------------------
• [SLOW TEST] [34.221 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:31:53.828
    Aug 30 06:31:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:31:53.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:31:53.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:31:53.894
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    W0830 06:31:53.936956      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "oidc-discovery-validator" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "oidc-discovery-validator" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "oidc-discovery-validator" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "oidc-discovery-validator" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:31:53.937: INFO: created pod
    Aug 30 06:31:53.937: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6964" to be "Succeeded or Failed"
    Aug 30 06:31:53.944: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.194314ms
    Aug 30 06:31:55.954: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017115133s
    Aug 30 06:31:57.980: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042944725s
    STEP: Saw pod success 08/30/23 06:31:57.98
    Aug 30 06:31:57.980: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 30 06:32:27.980: INFO: polling logs
    Aug 30 06:32:27.998: INFO: Pod logs: 
    I0830 06:31:55.081496       1 log.go:198] OK: Got token
    I0830 06:31:55.081547       1 log.go:198] validating with in-cluster discovery
    I0830 06:31:55.082120       1 log.go:198] OK: got issuer https://kubernetes.default.svc
    I0830 06:31:55.082156       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693377714, NotBefore:1693377114, IssuedAt:1693377114, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"280a423d-8844-440d-b0bd-2d42bb43f9d9"}}}
    I0830 06:31:55.097296       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0830 06:31:55.111872       1 log.go:198] OK: Validated signature on JWT
    I0830 06:31:55.111954       1 log.go:198] OK: Got valid claims from token!
    I0830 06:31:55.111987       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-6964:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693377714, NotBefore:1693377114, IssuedAt:1693377114, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6964", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"280a423d-8844-440d-b0bd-2d42bb43f9d9"}}}

    Aug 30 06:32:27.998: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:28.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6964" for this suite. 08/30/23 06:32:28.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:28.049
Aug 30 06:32:28.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-pred 08/30/23 06:32:28.05
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:28.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:28.119
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 30 06:32:28.133: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 06:32:28.168: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 06:32:28.204: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.183 before test
Aug 30 06:32:28.256: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.256: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 06:32:28.256: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.256: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:32:28.257: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:32:28.257: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 06:32:28.257: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container vpn ready: true, restart count 1
Aug 30 06:32:28.257: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:32:28.257: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 06:32:28.257: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 06:32:28.257: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:32:28.257: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:32:28.257: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 06:32:28.257: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Aug 30 06:32:28.257: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container console ready: true, restart count 0
Aug 30 06:32:28.257: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:32:28.257: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.257: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.257: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:32:28.257: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:32:28.257: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.257: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container router ready: true, restart count 0
Aug 30 06:32:28.257: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 06:32:28.257: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.257: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 06:32:28.258: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 06:32:28.258: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:32:28.258: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 06:32:28.258: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:32:28.258: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:32:28.258: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:32:28.258: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:32:28.258: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:32:28.258: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:32:28.258: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:32:28.258: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:32:28.258: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:32:28.258: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 06:32:28.258: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:32:28.258: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 06:32:28.258: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 06:32:28.258: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 06:32:28.258: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:32:28.258: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container metrics ready: true, restart count 2
Aug 30 06:32:28.258: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 06:32:28.258: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 06:32:28.258: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.258: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:32:28.258: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.185 before test
Aug 30 06:32:28.296: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:32:28.296: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:32:28.296: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:32:28.296: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:32:28.296: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:32:28.296: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:32:28.296: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:32:28.296: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:32:28.296: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:32:28.296: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container console ready: true, restart count 0
Aug 30 06:32:28.296: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:32:28.296: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:32:28.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.296: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.296: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:32:28.297: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container registry ready: true, restart count 0
Aug 30 06:32:28.297: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:32:28.297: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:32:28.297: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container router ready: true, restart count 0
Aug 30 06:32:28.297: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.297: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container migrator ready: true, restart count 0
Aug 30 06:32:28.297: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:32:28.297: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:32:28.297: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:32:28.297: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:32:28.297: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:32:28.297: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:32:28.297: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:32:28.297: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 06:32:28.297: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:32:28.298: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 06:32:28.298: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:32:28.298: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:32:28.298: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 06:32:28.298: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:32:28.298: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container reload ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 06:32:28.298: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:32:28.298: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:32:28.298: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.298: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:32:28.298: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.299: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:32:28.299: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:32:28.299: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.299: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:32:28.299: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:32:28.299: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container network-operator ready: true, restart count 0
Aug 30 06:32:28.299: INFO: collect-profiles-28222920-565wn from openshift-operator-lifecycle-manager started at 2023-08-30 06:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:32:28.299: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:32:28.299: INFO: collect-profiles-28222950-256v9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:30:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:32:28.299: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:32:28.299: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container service-ca-controller ready: true, restart count 0
Aug 30 06:32:28.299: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container e2e ready: true, restart count 0
Aug 30 06:32:28.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:32:28.299: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:32:28.299: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:32:28.299: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.299: INFO: 	Container tigera-operator ready: true, restart count 6
Aug 30 06:32:28.299: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.190 before test
Aug 30 06:32:28.335: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:32:28.335: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:32:28.335: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:32:28.335: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:32:28.335: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:32:28.335: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:32:28.335: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:32:28.335: INFO: dns-default-n295h from openshift-dns started at 2023-08-30 06:28:53 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:32:28.335: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.335: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:32:28.335: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:32:28.335: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 06:32:28.335: INFO: ingress-canary-h6ntm from openshift-ingress-canary started at 2023-08-30 06:28:53 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.335: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:32:28.335: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.336: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.336: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:32:28.336: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:32:28.336: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:32:28.336: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:32:28.336: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:32:28.336: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:32:28.336: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 06:32:28.336: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:32:28.336: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:32:28.336: INFO: oidc-discovery-validator from svcaccounts-6964 started at 2023-08-30 06:31:53 +0000 UTC (1 container statuses recorded)
Aug 30 06:32:28.336: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 06:32:28.336
Aug 30 06:32:28.353: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9712" to be "running"
Aug 30 06:32:28.359: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.534561ms
Aug 30 06:32:30.368: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015287385s
Aug 30 06:32:30.368: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 06:32:30.375
STEP: Trying to apply a random label on the found node. 08/30/23 06:32:30.396
STEP: verifying the node has the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 42 08/30/23 06:32:30.415
STEP: Trying to relaunch the pod, now with labels. 08/30/23 06:32:30.423
Aug 30 06:32:30.436: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9712" to be "not pending"
Aug 30 06:32:30.443: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.365421ms
Aug 30 06:32:32.452: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015890567s
Aug 30 06:32:32.452: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 off the node 10.135.139.190 08/30/23 06:32:32.459
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 08/30/23 06:32:32.506
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:32.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9712" for this suite. 08/30/23 06:32:32.523
------------------------------
• [4.494 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:28.049
    Aug 30 06:32:28.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-pred 08/30/23 06:32:28.05
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:28.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:28.119
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 30 06:32:28.133: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 30 06:32:28.168: INFO: Waiting for terminating namespaces to be deleted...
    Aug 30 06:32:28.204: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.183 before test
    Aug 30 06:32:28.256: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.256: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 30 06:32:28.256: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.256: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container vpn ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container console-operator ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Aug 30 06:32:28.257: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container dns-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container ingress-operator ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:32:28.257: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container insights-operator ready: true, restart count 1
    Aug 30 06:32:28.257: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.257: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Aug 30 06:32:28.258: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container marketplace-operator ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:32:28.258: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container check-endpoints ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container catalog-operator ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container olm-operator ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container package-server-manager ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container metrics ready: true, restart count 2
    Aug 30 06:32:28.258: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container push-gateway ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container service-ca-operator ready: true, restart count 1
    Aug 30 06:32:28.258: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.258: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:32:28.258: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.185 before test
    Aug 30 06:32:28.296: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.296: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.296: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container registry ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container migrator ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:32:28.297: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 30 06:32:28.297: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container reload ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container telemeter-client ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.298: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:32:28.298: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container network-operator ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: collect-profiles-28222920-565wn from openshift-operator-lifecycle-manager started at 2023-08-30 06:00:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:32:28.299: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:32:28.299: INFO: collect-profiles-28222950-256v9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:30:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:32:28.299: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container service-ca-controller ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container e2e ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:32:28.299: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.299: INFO: 	Container tigera-operator ready: true, restart count 6
    Aug 30 06:32:28.299: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.190 before test
    Aug 30 06:32:28.335: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: dns-default-n295h from openshift-dns started at 2023-08-30 06:28:53 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container pvc-permissions ready: false, restart count 0
    Aug 30 06:32:28.335: INFO: ingress-canary-h6ntm from openshift-ingress-canary started at 2023-08-30 06:28:53 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.335: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:32:28.335: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:32:28.336: INFO: oidc-discovery-validator from svcaccounts-6964 started at 2023-08-30 06:31:53 +0000 UTC (1 container statuses recorded)
    Aug 30 06:32:28.336: INFO: 	Container oidc-discovery-validator ready: false, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 06:32:28.336
    Aug 30 06:32:28.353: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9712" to be "running"
    Aug 30 06:32:28.359: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.534561ms
    Aug 30 06:32:30.368: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015287385s
    Aug 30 06:32:30.368: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 06:32:30.375
    STEP: Trying to apply a random label on the found node. 08/30/23 06:32:30.396
    STEP: verifying the node has the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 42 08/30/23 06:32:30.415
    STEP: Trying to relaunch the pod, now with labels. 08/30/23 06:32:30.423
    Aug 30 06:32:30.436: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9712" to be "not pending"
    Aug 30 06:32:30.443: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.365421ms
    Aug 30 06:32:32.452: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.015890567s
    Aug 30 06:32:32.452: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 off the node 10.135.139.190 08/30/23 06:32:32.459
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-d4d9c511-1a6b-4a90-9ff4-b10933054635 08/30/23 06:32:32.506
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:32.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9712" for this suite. 08/30/23 06:32:32.523
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:32.543
Aug 30 06:32:32.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename discovery 08/30/23 06:32:32.544
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:32.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:32.63
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/30/23 06:32:32.637
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 30 06:32:32.946: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 30 06:32:32.948: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 30 06:32:32.948: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 30 06:32:32.948: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 30 06:32:32.948: INFO: Checking APIGroup: apps
Aug 30 06:32:32.950: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 30 06:32:32.950: INFO: Versions found [{apps/v1 v1}]
Aug 30 06:32:32.950: INFO: apps/v1 matches apps/v1
Aug 30 06:32:32.950: INFO: Checking APIGroup: events.k8s.io
Aug 30 06:32:32.952: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 30 06:32:32.952: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 30 06:32:32.952: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 30 06:32:32.952: INFO: Checking APIGroup: authentication.k8s.io
Aug 30 06:32:32.954: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 30 06:32:32.954: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 30 06:32:32.954: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 30 06:32:32.954: INFO: Checking APIGroup: authorization.k8s.io
Aug 30 06:32:32.956: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 30 06:32:32.957: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 30 06:32:32.957: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 30 06:32:32.957: INFO: Checking APIGroup: autoscaling
Aug 30 06:32:32.959: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 30 06:32:32.959: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 30 06:32:32.959: INFO: autoscaling/v2 matches autoscaling/v2
Aug 30 06:32:32.959: INFO: Checking APIGroup: batch
Aug 30 06:32:32.961: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 30 06:32:32.961: INFO: Versions found [{batch/v1 v1}]
Aug 30 06:32:32.961: INFO: batch/v1 matches batch/v1
Aug 30 06:32:32.961: INFO: Checking APIGroup: certificates.k8s.io
Aug 30 06:32:32.963: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 30 06:32:32.963: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 30 06:32:32.963: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 30 06:32:32.963: INFO: Checking APIGroup: networking.k8s.io
Aug 30 06:32:32.965: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 30 06:32:32.965: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 30 06:32:32.965: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 30 06:32:32.965: INFO: Checking APIGroup: policy
Aug 30 06:32:32.967: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 30 06:32:32.967: INFO: Versions found [{policy/v1 v1}]
Aug 30 06:32:32.967: INFO: policy/v1 matches policy/v1
Aug 30 06:32:32.967: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 30 06:32:32.968: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 30 06:32:32.969: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 30 06:32:32.969: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 30 06:32:32.969: INFO: Checking APIGroup: storage.k8s.io
Aug 30 06:32:32.971: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 30 06:32:32.971: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 30 06:32:32.971: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 30 06:32:32.971: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 30 06:32:32.972: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 30 06:32:32.973: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 30 06:32:32.973: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 30 06:32:32.973: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 30 06:32:32.975: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 30 06:32:32.975: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 30 06:32:32.975: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 30 06:32:32.975: INFO: Checking APIGroup: scheduling.k8s.io
Aug 30 06:32:32.977: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 30 06:32:32.977: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 30 06:32:32.977: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 30 06:32:32.977: INFO: Checking APIGroup: coordination.k8s.io
Aug 30 06:32:32.979: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 30 06:32:32.979: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 30 06:32:32.979: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 30 06:32:32.979: INFO: Checking APIGroup: node.k8s.io
Aug 30 06:32:32.981: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 30 06:32:32.981: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 30 06:32:32.981: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 30 06:32:32.981: INFO: Checking APIGroup: discovery.k8s.io
Aug 30 06:32:32.983: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 30 06:32:32.983: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 30 06:32:32.983: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 30 06:32:32.983: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 30 06:32:32.985: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 30 06:32:32.985: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 30 06:32:32.985: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 30 06:32:32.985: INFO: Checking APIGroup: apps.openshift.io
Aug 30 06:32:32.987: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Aug 30 06:32:32.987: INFO: Versions found [{apps.openshift.io/v1 v1}]
Aug 30 06:32:32.987: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Aug 30 06:32:32.987: INFO: Checking APIGroup: authorization.openshift.io
Aug 30 06:32:32.989: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Aug 30 06:32:32.989: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Aug 30 06:32:32.989: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Aug 30 06:32:32.989: INFO: Checking APIGroup: build.openshift.io
Aug 30 06:32:32.991: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Aug 30 06:32:32.991: INFO: Versions found [{build.openshift.io/v1 v1}]
Aug 30 06:32:32.991: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Aug 30 06:32:32.991: INFO: Checking APIGroup: image.openshift.io
Aug 30 06:32:32.993: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Aug 30 06:32:32.993: INFO: Versions found [{image.openshift.io/v1 v1}]
Aug 30 06:32:32.993: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Aug 30 06:32:32.993: INFO: Checking APIGroup: oauth.openshift.io
Aug 30 06:32:32.995: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Aug 30 06:32:32.995: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Aug 30 06:32:32.995: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Aug 30 06:32:32.995: INFO: Checking APIGroup: project.openshift.io
Aug 30 06:32:32.997: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Aug 30 06:32:32.997: INFO: Versions found [{project.openshift.io/v1 v1}]
Aug 30 06:32:32.997: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Aug 30 06:32:32.997: INFO: Checking APIGroup: quota.openshift.io
Aug 30 06:32:32.999: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Aug 30 06:32:32.999: INFO: Versions found [{quota.openshift.io/v1 v1}]
Aug 30 06:32:32.999: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Aug 30 06:32:32.999: INFO: Checking APIGroup: route.openshift.io
Aug 30 06:32:33.001: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Aug 30 06:32:33.001: INFO: Versions found [{route.openshift.io/v1 v1}]
Aug 30 06:32:33.001: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Aug 30 06:32:33.001: INFO: Checking APIGroup: security.openshift.io
Aug 30 06:32:33.003: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Aug 30 06:32:33.003: INFO: Versions found [{security.openshift.io/v1 v1}]
Aug 30 06:32:33.003: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Aug 30 06:32:33.003: INFO: Checking APIGroup: template.openshift.io
Aug 30 06:32:33.005: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Aug 30 06:32:33.006: INFO: Versions found [{template.openshift.io/v1 v1}]
Aug 30 06:32:33.006: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Aug 30 06:32:33.006: INFO: Checking APIGroup: user.openshift.io
Aug 30 06:32:33.008: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Aug 30 06:32:33.008: INFO: Versions found [{user.openshift.io/v1 v1}]
Aug 30 06:32:33.008: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Aug 30 06:32:33.008: INFO: Checking APIGroup: packages.operators.coreos.com
Aug 30 06:32:33.011: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Aug 30 06:32:33.011: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Aug 30 06:32:33.011: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Aug 30 06:32:33.011: INFO: Checking APIGroup: config.openshift.io
Aug 30 06:32:33.013: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Aug 30 06:32:33.013: INFO: Versions found [{config.openshift.io/v1 v1}]
Aug 30 06:32:33.013: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Aug 30 06:32:33.013: INFO: Checking APIGroup: operator.openshift.io
Aug 30 06:32:33.015: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Aug 30 06:32:33.015: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.015: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Aug 30 06:32:33.015: INFO: Checking APIGroup: apiserver.openshift.io
Aug 30 06:32:33.016: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Aug 30 06:32:33.016: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Aug 30 06:32:33.016: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Aug 30 06:32:33.016: INFO: Checking APIGroup: cloudcredential.openshift.io
Aug 30 06:32:33.018: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Aug 30 06:32:33.018: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Aug 30 06:32:33.018: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Aug 30 06:32:33.019: INFO: Checking APIGroup: console.openshift.io
Aug 30 06:32:33.020: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Aug 30 06:32:33.020: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.020: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Aug 30 06:32:33.020: INFO: Checking APIGroup: crd.projectcalico.org
Aug 30 06:32:33.022: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 30 06:32:33.022: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 30 06:32:33.022: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 30 06:32:33.022: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Aug 30 06:32:33.024: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Aug 30 06:32:33.024: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Aug 30 06:32:33.024: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Aug 30 06:32:33.024: INFO: Checking APIGroup: ingress.operator.openshift.io
Aug 30 06:32:33.027: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Aug 30 06:32:33.027: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Aug 30 06:32:33.027: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Aug 30 06:32:33.027: INFO: Checking APIGroup: k8s.cni.cncf.io
Aug 30 06:32:33.029: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Aug 30 06:32:33.029: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Aug 30 06:32:33.029: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Aug 30 06:32:33.029: INFO: Checking APIGroup: machineconfiguration.openshift.io
Aug 30 06:32:33.031: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Aug 30 06:32:33.031: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Aug 30 06:32:33.031: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Aug 30 06:32:33.031: INFO: Checking APIGroup: monitoring.coreos.com
Aug 30 06:32:33.033: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 30 06:32:33.033: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 30 06:32:33.033: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 30 06:32:33.033: INFO: Checking APIGroup: network.operator.openshift.io
Aug 30 06:32:33.035: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Aug 30 06:32:33.035: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Aug 30 06:32:33.035: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Aug 30 06:32:33.035: INFO: Checking APIGroup: operator.tigera.io
Aug 30 06:32:33.037: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Aug 30 06:32:33.037: INFO: Versions found [{operator.tigera.io/v1 v1}]
Aug 30 06:32:33.037: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Aug 30 06:32:33.037: INFO: Checking APIGroup: operators.coreos.com
Aug 30 06:32:33.039: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Aug 30 06:32:33.039: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Aug 30 06:32:33.039: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Aug 30 06:32:33.040: INFO: Checking APIGroup: performance.openshift.io
Aug 30 06:32:33.042: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Aug 30 06:32:33.042: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.042: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Aug 30 06:32:33.042: INFO: Checking APIGroup: samples.operator.openshift.io
Aug 30 06:32:33.044: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Aug 30 06:32:33.044: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Aug 30 06:32:33.044: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Aug 30 06:32:33.044: INFO: Checking APIGroup: security.internal.openshift.io
Aug 30 06:32:33.046: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Aug 30 06:32:33.046: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Aug 30 06:32:33.046: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Aug 30 06:32:33.046: INFO: Checking APIGroup: snapshot.storage.k8s.io
Aug 30 06:32:33.048: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Aug 30 06:32:33.048: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Aug 30 06:32:33.048: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Aug 30 06:32:33.048: INFO: Checking APIGroup: tuned.openshift.io
Aug 30 06:32:33.050: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Aug 30 06:32:33.050: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Aug 30 06:32:33.050: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Aug 30 06:32:33.050: INFO: Checking APIGroup: controlplane.operator.openshift.io
Aug 30 06:32:33.052: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Aug 30 06:32:33.052: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.052: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Aug 30 06:32:33.052: INFO: Checking APIGroup: ibm.com
Aug 30 06:32:33.096: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Aug 30 06:32:33.097: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Aug 30 06:32:33.097: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Aug 30 06:32:33.097: INFO: Checking APIGroup: migration.k8s.io
Aug 30 06:32:33.146: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Aug 30 06:32:33.146: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.146: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Aug 30 06:32:33.146: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Aug 30 06:32:33.196: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Aug 30 06:32:33.196: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Aug 30 06:32:33.196: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Aug 30 06:32:33.196: INFO: Checking APIGroup: helm.openshift.io
Aug 30 06:32:33.246: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Aug 30 06:32:33.246: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Aug 30 06:32:33.246: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Aug 30 06:32:33.246: INFO: Checking APIGroup: metrics.k8s.io
Aug 30 06:32:33.296: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 30 06:32:33.296: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 30 06:32:33.296: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1779" for this suite. 08/30/23 06:32:33.352
------------------------------
• [0.870 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:32.543
    Aug 30 06:32:32.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename discovery 08/30/23 06:32:32.544
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:32.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:32.63
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/30/23 06:32:32.637
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 30 06:32:32.946: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 30 06:32:32.948: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 30 06:32:32.948: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 30 06:32:32.948: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 30 06:32:32.948: INFO: Checking APIGroup: apps
    Aug 30 06:32:32.950: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 30 06:32:32.950: INFO: Versions found [{apps/v1 v1}]
    Aug 30 06:32:32.950: INFO: apps/v1 matches apps/v1
    Aug 30 06:32:32.950: INFO: Checking APIGroup: events.k8s.io
    Aug 30 06:32:32.952: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 30 06:32:32.952: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 30 06:32:32.952: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 30 06:32:32.952: INFO: Checking APIGroup: authentication.k8s.io
    Aug 30 06:32:32.954: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 30 06:32:32.954: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 30 06:32:32.954: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 30 06:32:32.954: INFO: Checking APIGroup: authorization.k8s.io
    Aug 30 06:32:32.956: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 30 06:32:32.957: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 30 06:32:32.957: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 30 06:32:32.957: INFO: Checking APIGroup: autoscaling
    Aug 30 06:32:32.959: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 30 06:32:32.959: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 30 06:32:32.959: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 30 06:32:32.959: INFO: Checking APIGroup: batch
    Aug 30 06:32:32.961: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 30 06:32:32.961: INFO: Versions found [{batch/v1 v1}]
    Aug 30 06:32:32.961: INFO: batch/v1 matches batch/v1
    Aug 30 06:32:32.961: INFO: Checking APIGroup: certificates.k8s.io
    Aug 30 06:32:32.963: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 30 06:32:32.963: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 30 06:32:32.963: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 30 06:32:32.963: INFO: Checking APIGroup: networking.k8s.io
    Aug 30 06:32:32.965: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 30 06:32:32.965: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 30 06:32:32.965: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 30 06:32:32.965: INFO: Checking APIGroup: policy
    Aug 30 06:32:32.967: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 30 06:32:32.967: INFO: Versions found [{policy/v1 v1}]
    Aug 30 06:32:32.967: INFO: policy/v1 matches policy/v1
    Aug 30 06:32:32.967: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 30 06:32:32.968: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 30 06:32:32.969: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 30 06:32:32.969: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 30 06:32:32.969: INFO: Checking APIGroup: storage.k8s.io
    Aug 30 06:32:32.971: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 30 06:32:32.971: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 30 06:32:32.971: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 30 06:32:32.971: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 30 06:32:32.972: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 30 06:32:32.973: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 30 06:32:32.973: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 30 06:32:32.973: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 30 06:32:32.975: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 30 06:32:32.975: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 30 06:32:32.975: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 30 06:32:32.975: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 30 06:32:32.977: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 30 06:32:32.977: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 30 06:32:32.977: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 30 06:32:32.977: INFO: Checking APIGroup: coordination.k8s.io
    Aug 30 06:32:32.979: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 30 06:32:32.979: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 30 06:32:32.979: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 30 06:32:32.979: INFO: Checking APIGroup: node.k8s.io
    Aug 30 06:32:32.981: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 30 06:32:32.981: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 30 06:32:32.981: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 30 06:32:32.981: INFO: Checking APIGroup: discovery.k8s.io
    Aug 30 06:32:32.983: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 30 06:32:32.983: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 30 06:32:32.983: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 30 06:32:32.983: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 30 06:32:32.985: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 30 06:32:32.985: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 30 06:32:32.985: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 30 06:32:32.985: INFO: Checking APIGroup: apps.openshift.io
    Aug 30 06:32:32.987: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Aug 30 06:32:32.987: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Aug 30 06:32:32.987: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Aug 30 06:32:32.987: INFO: Checking APIGroup: authorization.openshift.io
    Aug 30 06:32:32.989: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Aug 30 06:32:32.989: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Aug 30 06:32:32.989: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Aug 30 06:32:32.989: INFO: Checking APIGroup: build.openshift.io
    Aug 30 06:32:32.991: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Aug 30 06:32:32.991: INFO: Versions found [{build.openshift.io/v1 v1}]
    Aug 30 06:32:32.991: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Aug 30 06:32:32.991: INFO: Checking APIGroup: image.openshift.io
    Aug 30 06:32:32.993: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Aug 30 06:32:32.993: INFO: Versions found [{image.openshift.io/v1 v1}]
    Aug 30 06:32:32.993: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Aug 30 06:32:32.993: INFO: Checking APIGroup: oauth.openshift.io
    Aug 30 06:32:32.995: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Aug 30 06:32:32.995: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Aug 30 06:32:32.995: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Aug 30 06:32:32.995: INFO: Checking APIGroup: project.openshift.io
    Aug 30 06:32:32.997: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Aug 30 06:32:32.997: INFO: Versions found [{project.openshift.io/v1 v1}]
    Aug 30 06:32:32.997: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Aug 30 06:32:32.997: INFO: Checking APIGroup: quota.openshift.io
    Aug 30 06:32:32.999: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Aug 30 06:32:32.999: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Aug 30 06:32:32.999: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Aug 30 06:32:32.999: INFO: Checking APIGroup: route.openshift.io
    Aug 30 06:32:33.001: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Aug 30 06:32:33.001: INFO: Versions found [{route.openshift.io/v1 v1}]
    Aug 30 06:32:33.001: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Aug 30 06:32:33.001: INFO: Checking APIGroup: security.openshift.io
    Aug 30 06:32:33.003: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Aug 30 06:32:33.003: INFO: Versions found [{security.openshift.io/v1 v1}]
    Aug 30 06:32:33.003: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Aug 30 06:32:33.003: INFO: Checking APIGroup: template.openshift.io
    Aug 30 06:32:33.005: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Aug 30 06:32:33.006: INFO: Versions found [{template.openshift.io/v1 v1}]
    Aug 30 06:32:33.006: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Aug 30 06:32:33.006: INFO: Checking APIGroup: user.openshift.io
    Aug 30 06:32:33.008: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Aug 30 06:32:33.008: INFO: Versions found [{user.openshift.io/v1 v1}]
    Aug 30 06:32:33.008: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Aug 30 06:32:33.008: INFO: Checking APIGroup: packages.operators.coreos.com
    Aug 30 06:32:33.011: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Aug 30 06:32:33.011: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Aug 30 06:32:33.011: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Aug 30 06:32:33.011: INFO: Checking APIGroup: config.openshift.io
    Aug 30 06:32:33.013: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Aug 30 06:32:33.013: INFO: Versions found [{config.openshift.io/v1 v1}]
    Aug 30 06:32:33.013: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Aug 30 06:32:33.013: INFO: Checking APIGroup: operator.openshift.io
    Aug 30 06:32:33.015: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Aug 30 06:32:33.015: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.015: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Aug 30 06:32:33.015: INFO: Checking APIGroup: apiserver.openshift.io
    Aug 30 06:32:33.016: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Aug 30 06:32:33.016: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Aug 30 06:32:33.016: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Aug 30 06:32:33.016: INFO: Checking APIGroup: cloudcredential.openshift.io
    Aug 30 06:32:33.018: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Aug 30 06:32:33.018: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Aug 30 06:32:33.018: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Aug 30 06:32:33.019: INFO: Checking APIGroup: console.openshift.io
    Aug 30 06:32:33.020: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Aug 30 06:32:33.020: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.020: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Aug 30 06:32:33.020: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 30 06:32:33.022: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 30 06:32:33.022: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 30 06:32:33.022: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 30 06:32:33.022: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Aug 30 06:32:33.024: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Aug 30 06:32:33.024: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Aug 30 06:32:33.024: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Aug 30 06:32:33.024: INFO: Checking APIGroup: ingress.operator.openshift.io
    Aug 30 06:32:33.027: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Aug 30 06:32:33.027: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Aug 30 06:32:33.027: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Aug 30 06:32:33.027: INFO: Checking APIGroup: k8s.cni.cncf.io
    Aug 30 06:32:33.029: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Aug 30 06:32:33.029: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Aug 30 06:32:33.029: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Aug 30 06:32:33.029: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Aug 30 06:32:33.031: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Aug 30 06:32:33.031: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Aug 30 06:32:33.031: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Aug 30 06:32:33.031: INFO: Checking APIGroup: monitoring.coreos.com
    Aug 30 06:32:33.033: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Aug 30 06:32:33.033: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.033: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Aug 30 06:32:33.033: INFO: Checking APIGroup: network.operator.openshift.io
    Aug 30 06:32:33.035: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Aug 30 06:32:33.035: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Aug 30 06:32:33.035: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Aug 30 06:32:33.035: INFO: Checking APIGroup: operator.tigera.io
    Aug 30 06:32:33.037: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Aug 30 06:32:33.037: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Aug 30 06:32:33.037: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Aug 30 06:32:33.037: INFO: Checking APIGroup: operators.coreos.com
    Aug 30 06:32:33.039: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Aug 30 06:32:33.039: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.039: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Aug 30 06:32:33.040: INFO: Checking APIGroup: performance.openshift.io
    Aug 30 06:32:33.042: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Aug 30 06:32:33.042: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.042: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Aug 30 06:32:33.042: INFO: Checking APIGroup: samples.operator.openshift.io
    Aug 30 06:32:33.044: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Aug 30 06:32:33.044: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Aug 30 06:32:33.044: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Aug 30 06:32:33.044: INFO: Checking APIGroup: security.internal.openshift.io
    Aug 30 06:32:33.046: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Aug 30 06:32:33.046: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Aug 30 06:32:33.046: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Aug 30 06:32:33.046: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Aug 30 06:32:33.048: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Aug 30 06:32:33.048: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Aug 30 06:32:33.048: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Aug 30 06:32:33.048: INFO: Checking APIGroup: tuned.openshift.io
    Aug 30 06:32:33.050: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Aug 30 06:32:33.050: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Aug 30 06:32:33.050: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Aug 30 06:32:33.050: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Aug 30 06:32:33.052: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Aug 30 06:32:33.052: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.052: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Aug 30 06:32:33.052: INFO: Checking APIGroup: ibm.com
    Aug 30 06:32:33.096: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Aug 30 06:32:33.097: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.097: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Aug 30 06:32:33.097: INFO: Checking APIGroup: migration.k8s.io
    Aug 30 06:32:33.146: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Aug 30 06:32:33.146: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.146: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Aug 30 06:32:33.146: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Aug 30 06:32:33.196: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Aug 30 06:32:33.196: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Aug 30 06:32:33.196: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Aug 30 06:32:33.196: INFO: Checking APIGroup: helm.openshift.io
    Aug 30 06:32:33.246: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Aug 30 06:32:33.246: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Aug 30 06:32:33.246: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Aug 30 06:32:33.246: INFO: Checking APIGroup: metrics.k8s.io
    Aug 30 06:32:33.296: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 30 06:32:33.296: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 30 06:32:33.296: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1779" for this suite. 08/30/23 06:32:33.352
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:33.414
Aug 30 06:32:33.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 06:32:33.415
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:33.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:33.494
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-6hwtw" 08/30/23 06:32:33.499
W0830 06:32:33.511249      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:32:33.511: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
Aug 30 06:32:34.518: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
Aug 30 06:32:34.525: INFO: Found 1 replicas for "e2e-rc-6hwtw" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-6hwtw" 08/30/23 06:32:34.525
STEP: Updating a scale subresource 08/30/23 06:32:34.531
STEP: Verifying replicas where modified for replication controller "e2e-rc-6hwtw" 08/30/23 06:32:34.541
Aug 30 06:32:34.541: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
Aug 30 06:32:35.548: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
Aug 30 06:32:35.555: INFO: Found 2 replicas for "e2e-rc-6hwtw" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:35.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7831" for this suite. 08/30/23 06:32:35.564
------------------------------
• [2.167 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:33.414
    Aug 30 06:32:33.414: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 06:32:33.415
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:33.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:33.494
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-6hwtw" 08/30/23 06:32:33.499
    W0830 06:32:33.511249      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:32:33.511: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
    Aug 30 06:32:34.518: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
    Aug 30 06:32:34.525: INFO: Found 1 replicas for "e2e-rc-6hwtw" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-6hwtw" 08/30/23 06:32:34.525
    STEP: Updating a scale subresource 08/30/23 06:32:34.531
    STEP: Verifying replicas where modified for replication controller "e2e-rc-6hwtw" 08/30/23 06:32:34.541
    Aug 30 06:32:34.541: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
    Aug 30 06:32:35.548: INFO: Get Replication Controller "e2e-rc-6hwtw" to confirm replicas
    Aug 30 06:32:35.555: INFO: Found 2 replicas for "e2e-rc-6hwtw" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:35.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7831" for this suite. 08/30/23 06:32:35.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:35.583
Aug 30 06:32:35.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:32:35.584
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:35.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:35.654
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:32:35.729
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:32:35.947
STEP: Deploying the webhook pod 08/30/23 06:32:35.978
STEP: Wait for the deployment to be ready 08/30/23 06:32:36.002
Aug 30 06:32:36.024: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:32:38.058
STEP: Verifying the service has paired with the endpoint 08/30/23 06:32:38.078
Aug 30 06:32:39.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 30 06:32:39.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3384-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 06:32:39.608
STEP: Creating a custom resource that should be mutated by the webhook 08/30/23 06:32:39.639
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:42.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3710" for this suite. 08/30/23 06:32:42.456
STEP: Destroying namespace "webhook-3710-markers" for this suite. 08/30/23 06:32:42.518
------------------------------
• [SLOW TEST] [6.977 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:35.583
    Aug 30 06:32:35.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:32:35.584
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:35.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:35.654
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:32:35.729
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:32:35.947
    STEP: Deploying the webhook pod 08/30/23 06:32:35.978
    STEP: Wait for the deployment to be ready 08/30/23 06:32:36.002
    Aug 30 06:32:36.024: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:32:38.058
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:32:38.078
    Aug 30 06:32:39.079: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 30 06:32:39.088: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3384-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 06:32:39.608
    STEP: Creating a custom resource that should be mutated by the webhook 08/30/23 06:32:39.639
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:42.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3710" for this suite. 08/30/23 06:32:42.456
    STEP: Destroying namespace "webhook-3710-markers" for this suite. 08/30/23 06:32:42.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:42.563
Aug 30 06:32:42.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-runtime 08/30/23 06:32:42.566
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:42.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:42.665
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/30/23 06:32:42.672
STEP: wait for the container to reach Succeeded 08/30/23 06:32:43.708
STEP: get the container status 08/30/23 06:32:47.785
STEP: the container should be terminated 08/30/23 06:32:47.796
STEP: the termination message should be set 08/30/23 06:32:47.796
Aug 30 06:32:47.796: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/30/23 06:32:47.796
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:47.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8051" for this suite. 08/30/23 06:32:47.859
------------------------------
• [SLOW TEST] [5.322 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:42.563
    Aug 30 06:32:42.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-runtime 08/30/23 06:32:42.566
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:42.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:42.665
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/30/23 06:32:42.672
    STEP: wait for the container to reach Succeeded 08/30/23 06:32:43.708
    STEP: get the container status 08/30/23 06:32:47.785
    STEP: the container should be terminated 08/30/23 06:32:47.796
    STEP: the termination message should be set 08/30/23 06:32:47.796
    Aug 30 06:32:47.796: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/30/23 06:32:47.796
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:47.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8051" for this suite. 08/30/23 06:32:47.859
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:47.886
Aug 30 06:32:47.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:32:47.887
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:47.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:48.011
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/30/23 06:32:48.118
Aug 30 06:32:48.176: INFO: Waiting up to 5m0s for pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866" in namespace "svcaccounts-269" to be "Succeeded or Failed"
Aug 30 06:32:48.213: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 25.281664ms
Aug 30 06:32:50.233: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045110428s
Aug 30 06:32:52.220: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033018154s
Aug 30 06:32:54.252: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064540227s
STEP: Saw pod success 08/30/23 06:32:54.252
Aug 30 06:32:54.252: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866" satisfied condition "Succeeded or Failed"
Aug 30 06:32:54.270: INFO: Trying to get logs from node 10.135.139.190 pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:32:54.325
Aug 30 06:32:54.390: INFO: Waiting for pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 to disappear
Aug 30 06:32:54.397: INFO: Pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:54.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-269" for this suite. 08/30/23 06:32:54.427
------------------------------
• [SLOW TEST] [6.562 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:47.886
    Aug 30 06:32:47.886: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:32:47.887
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:47.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:48.011
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/30/23 06:32:48.118
    Aug 30 06:32:48.176: INFO: Waiting up to 5m0s for pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866" in namespace "svcaccounts-269" to be "Succeeded or Failed"
    Aug 30 06:32:48.213: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 25.281664ms
    Aug 30 06:32:50.233: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045110428s
    Aug 30 06:32:52.220: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033018154s
    Aug 30 06:32:54.252: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064540227s
    STEP: Saw pod success 08/30/23 06:32:54.252
    Aug 30 06:32:54.252: INFO: Pod "test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866" satisfied condition "Succeeded or Failed"
    Aug 30 06:32:54.270: INFO: Trying to get logs from node 10.135.139.190 pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:32:54.325
    Aug 30 06:32:54.390: INFO: Waiting for pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 to disappear
    Aug 30 06:32:54.397: INFO: Pod test-pod-b9c308f6-6eec-43df-bf54-e8055cf2e866 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:54.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-269" for this suite. 08/30/23 06:32:54.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:54.451
Aug 30 06:32:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:32:54.452
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:54.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:54.525
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-8089/configmap-test-f9a74357-0c81-45c4-a574-27dcbac3b358 08/30/23 06:32:54.531
STEP: Creating a pod to test consume configMaps 08/30/23 06:32:54.545
W0830 06:32:54.582358      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:32:54.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc" in namespace "configmap-8089" to be "Succeeded or Failed"
Aug 30 06:32:54.593: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.644011ms
Aug 30 06:32:56.601: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018673363s
Aug 30 06:32:58.622: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039605994s
STEP: Saw pod success 08/30/23 06:32:58.622
Aug 30 06:32:58.622: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc" satisfied condition "Succeeded or Failed"
Aug 30 06:32:58.635: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc container env-test: <nil>
STEP: delete the pod 08/30/23 06:32:58.675
Aug 30 06:32:58.694: INFO: Waiting for pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc to disappear
Aug 30 06:32:58.702: INFO: Pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:58.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8089" for this suite. 08/30/23 06:32:58.728
------------------------------
• [4.298 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:54.451
    Aug 30 06:32:54.451: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:32:54.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:54.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:54.525
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-8089/configmap-test-f9a74357-0c81-45c4-a574-27dcbac3b358 08/30/23 06:32:54.531
    STEP: Creating a pod to test consume configMaps 08/30/23 06:32:54.545
    W0830 06:32:54.582358      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:32:54.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc" in namespace "configmap-8089" to be "Succeeded or Failed"
    Aug 30 06:32:54.593: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.644011ms
    Aug 30 06:32:56.601: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018673363s
    Aug 30 06:32:58.622: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039605994s
    STEP: Saw pod success 08/30/23 06:32:58.622
    Aug 30 06:32:58.622: INFO: Pod "pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc" satisfied condition "Succeeded or Failed"
    Aug 30 06:32:58.635: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc container env-test: <nil>
    STEP: delete the pod 08/30/23 06:32:58.675
    Aug 30 06:32:58.694: INFO: Waiting for pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc to disappear
    Aug 30 06:32:58.702: INFO: Pod pod-configmaps-d6a01db0-15f1-4ead-b9a5-f74d63d4eadc no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:58.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8089" for this suite. 08/30/23 06:32:58.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:58.757
Aug 30 06:32:58.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 06:32:58.758
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:58.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:58.812
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/30/23 06:32:58.818
Aug 30 06:32:58.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2291 api-versions'
Aug 30 06:32:59.265: INFO: stderr: ""
Aug 30 06:32:59.265: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 06:32:59.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2291" for this suite. 08/30/23 06:32:59.292
------------------------------
• [0.554 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:58.757
    Aug 30 06:32:58.758: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 06:32:58.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:58.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:58.812
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/30/23 06:32:58.818
    Aug 30 06:32:58.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2291 api-versions'
    Aug 30 06:32:59.265: INFO: stderr: ""
    Aug 30 06:32:59.265: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:32:59.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2291" for this suite. 08/30/23 06:32:59.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:32:59.314
Aug 30 06:32:59.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context 08/30/23 06:32:59.315
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:59.377
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:59.386
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/30/23 06:32:59.403
Aug 30 06:32:59.424: INFO: Waiting up to 5m0s for pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c" in namespace "security-context-5682" to be "Succeeded or Failed"
Aug 30 06:32:59.435: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.263702ms
Aug 30 06:33:01.500: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076180215s
Aug 30 06:33:03.476: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052614449s
Aug 30 06:33:05.452: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028700681s
STEP: Saw pod success 08/30/23 06:33:05.453
Aug 30 06:33:05.453: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c" satisfied condition "Succeeded or Failed"
Aug 30 06:33:05.491: INFO: Trying to get logs from node 10.135.139.190 pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c container test-container: <nil>
STEP: delete the pod 08/30/23 06:33:05.55
Aug 30 06:33:05.577: INFO: Waiting for pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c to disappear
Aug 30 06:33:05.583: INFO: Pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:05.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5682" for this suite. 08/30/23 06:33:05.592
------------------------------
• [SLOW TEST] [6.313 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:32:59.314
    Aug 30 06:32:59.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context 08/30/23 06:32:59.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:32:59.377
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:32:59.386
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/30/23 06:32:59.403
    Aug 30 06:32:59.424: INFO: Waiting up to 5m0s for pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c" in namespace "security-context-5682" to be "Succeeded or Failed"
    Aug 30 06:32:59.435: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.263702ms
    Aug 30 06:33:01.500: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.076180215s
    Aug 30 06:33:03.476: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052614449s
    Aug 30 06:33:05.452: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028700681s
    STEP: Saw pod success 08/30/23 06:33:05.453
    Aug 30 06:33:05.453: INFO: Pod "security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:05.491: INFO: Trying to get logs from node 10.135.139.190 pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c container test-container: <nil>
    STEP: delete the pod 08/30/23 06:33:05.55
    Aug 30 06:33:05.577: INFO: Waiting for pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c to disappear
    Aug 30 06:33:05.583: INFO: Pod security-context-d360ff4e-4025-486d-be11-b41a80f2ed8c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:05.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5682" for this suite. 08/30/23 06:33:05.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:05.629
Aug 30 06:33:05.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:33:05.63
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:05.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:05.86
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-d91593d9-71c3-4ceb-8a7d-9c221a409121 08/30/23 06:33:06.01
STEP: Creating a pod to test consume configMaps 08/30/23 06:33:06.029
Aug 30 06:33:06.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2" in namespace "configmap-1213" to be "Succeeded or Failed"
Aug 30 06:33:06.061: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.066309ms
Aug 30 06:33:08.069: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018925107s
Aug 30 06:33:10.072: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022522385s
STEP: Saw pod success 08/30/23 06:33:10.072
Aug 30 06:33:10.072: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2" satisfied condition "Succeeded or Failed"
Aug 30 06:33:10.084: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:33:10.112
Aug 30 06:33:10.156: INFO: Waiting for pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 to disappear
Aug 30 06:33:10.163: INFO: Pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1213" for this suite. 08/30/23 06:33:10.187
------------------------------
• [4.596 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:05.629
    Aug 30 06:33:05.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:33:05.63
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:05.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:05.86
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-d91593d9-71c3-4ceb-8a7d-9c221a409121 08/30/23 06:33:06.01
    STEP: Creating a pod to test consume configMaps 08/30/23 06:33:06.029
    Aug 30 06:33:06.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2" in namespace "configmap-1213" to be "Succeeded or Failed"
    Aug 30 06:33:06.061: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.066309ms
    Aug 30 06:33:08.069: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018925107s
    Aug 30 06:33:10.072: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022522385s
    STEP: Saw pod success 08/30/23 06:33:10.072
    Aug 30 06:33:10.072: INFO: Pod "pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:10.084: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:33:10.112
    Aug 30 06:33:10.156: INFO: Waiting for pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 to disappear
    Aug 30 06:33:10.163: INFO: Pod pod-configmaps-a215e0a6-a498-4224-a393-fa599e5de0b2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:10.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1213" for this suite. 08/30/23 06:33:10.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:10.225
Aug 30 06:33:10.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:33:10.233
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:10.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:10.291
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-c8495f43-57e0-4dd6-ab78-9c71a0be07bf 08/30/23 06:33:10.296
STEP: Creating a pod to test consume configMaps 08/30/23 06:33:10.319
Aug 30 06:33:10.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1" in namespace "configmap-6209" to be "Succeeded or Failed"
Aug 30 06:33:10.401: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.524407ms
Aug 30 06:33:12.408: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019682552s
Aug 30 06:33:14.412: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023249377s
Aug 30 06:33:16.410: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021034189s
STEP: Saw pod success 08/30/23 06:33:16.41
Aug 30 06:33:16.410: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1" satisfied condition "Succeeded or Failed"
Aug 30 06:33:16.416: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 container configmap-volume-test: <nil>
STEP: delete the pod 08/30/23 06:33:16.432
Aug 30 06:33:16.450: INFO: Waiting for pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 to disappear
Aug 30 06:33:16.457: INFO: Pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:16.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6209" for this suite. 08/30/23 06:33:16.466
------------------------------
• [SLOW TEST] [6.257 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:10.225
    Aug 30 06:33:10.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:33:10.233
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:10.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:10.291
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-c8495f43-57e0-4dd6-ab78-9c71a0be07bf 08/30/23 06:33:10.296
    STEP: Creating a pod to test consume configMaps 08/30/23 06:33:10.319
    Aug 30 06:33:10.389: INFO: Waiting up to 5m0s for pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1" in namespace "configmap-6209" to be "Succeeded or Failed"
    Aug 30 06:33:10.401: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.524407ms
    Aug 30 06:33:12.408: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019682552s
    Aug 30 06:33:14.412: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023249377s
    Aug 30 06:33:16.410: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021034189s
    STEP: Saw pod success 08/30/23 06:33:16.41
    Aug 30 06:33:16.410: INFO: Pod "pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:16.416: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 container configmap-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:33:16.432
    Aug 30 06:33:16.450: INFO: Waiting for pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 to disappear
    Aug 30 06:33:16.457: INFO: Pod pod-configmaps-51de8db7-bfbe-4a03-a5d7-c1b0bdcbefe1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:16.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6209" for this suite. 08/30/23 06:33:16.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:16.485
Aug 30 06:33:16.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:33:16.486
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:16.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:16.551
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-71459bba-ae76-4a00-b45a-c07cda1f686d 08/30/23 06:33:16.559
STEP: Creating a pod to test consume configMaps 08/30/23 06:33:16.576
W0830 06:33:16.593004      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:33:16.593: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14" in namespace "projected-7610" to be "Succeeded or Failed"
Aug 30 06:33:16.599: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281678ms
Aug 30 06:33:18.611: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017699026s
Aug 30 06:33:20.608: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015357556s
STEP: Saw pod success 08/30/23 06:33:20.608
Aug 30 06:33:20.609: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14" satisfied condition "Succeeded or Failed"
Aug 30 06:33:20.616: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:33:20.703
Aug 30 06:33:20.728: INFO: Waiting for pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 to disappear
Aug 30 06:33:20.735: INFO: Pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7610" for this suite. 08/30/23 06:33:20.744
------------------------------
• [4.276 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:16.485
    Aug 30 06:33:16.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:33:16.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:16.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:16.551
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-71459bba-ae76-4a00-b45a-c07cda1f686d 08/30/23 06:33:16.559
    STEP: Creating a pod to test consume configMaps 08/30/23 06:33:16.576
    W0830 06:33:16.593004      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:33:16.593: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14" in namespace "projected-7610" to be "Succeeded or Failed"
    Aug 30 06:33:16.599: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281678ms
    Aug 30 06:33:18.611: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017699026s
    Aug 30 06:33:20.608: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015357556s
    STEP: Saw pod success 08/30/23 06:33:20.608
    Aug 30 06:33:20.609: INFO: Pod "pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:20.616: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:33:20.703
    Aug 30 06:33:20.728: INFO: Waiting for pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 to disappear
    Aug 30 06:33:20.735: INFO: Pod pod-projected-configmaps-14193f11-60a0-4046-b2fb-e82490fa6b14 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7610" for this suite. 08/30/23 06:33:20.744
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:20.764
Aug 30 06:33:20.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:33:20.765
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:20.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:20.899
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/30/23 06:33:20.904
Aug 30 06:33:21.927: INFO: Waiting up to 5m0s for pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077" in namespace "emptydir-1032" to be "Succeeded or Failed"
Aug 30 06:33:21.945: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Pending", Reason="", readiness=false. Elapsed: 17.496396ms
Aug 30 06:33:23.954: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026528457s
Aug 30 06:33:25.955: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027677234s
STEP: Saw pod success 08/30/23 06:33:25.955
Aug 30 06:33:25.955: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077" satisfied condition "Succeeded or Failed"
Aug 30 06:33:25.972: INFO: Trying to get logs from node 10.135.139.190 pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 container test-container: <nil>
STEP: delete the pod 08/30/23 06:33:25.997
Aug 30 06:33:26.069: INFO: Waiting for pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 to disappear
Aug 30 06:33:26.079: INFO: Pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:26.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1032" for this suite. 08/30/23 06:33:26.106
------------------------------
• [SLOW TEST] [5.386 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:20.764
    Aug 30 06:33:20.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:33:20.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:20.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:20.899
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/30/23 06:33:20.904
    Aug 30 06:33:21.927: INFO: Waiting up to 5m0s for pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077" in namespace "emptydir-1032" to be "Succeeded or Failed"
    Aug 30 06:33:21.945: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Pending", Reason="", readiness=false. Elapsed: 17.496396ms
    Aug 30 06:33:23.954: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026528457s
    Aug 30 06:33:25.955: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027677234s
    STEP: Saw pod success 08/30/23 06:33:25.955
    Aug 30 06:33:25.955: INFO: Pod "pod-76138793-97f1-4562-9f3d-6bbb24ddf077" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:25.972: INFO: Trying to get logs from node 10.135.139.190 pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 container test-container: <nil>
    STEP: delete the pod 08/30/23 06:33:25.997
    Aug 30 06:33:26.069: INFO: Waiting for pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 to disappear
    Aug 30 06:33:26.079: INFO: Pod pod-76138793-97f1-4562-9f3d-6bbb24ddf077 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:26.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1032" for this suite. 08/30/23 06:33:26.106
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:26.151
Aug 30 06:33:26.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:33:26.153
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:26.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:26.212
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/30/23 06:33:26.217
W0830 06:33:26.251989      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:33:26.252: INFO: Waiting up to 5m0s for pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb" in namespace "projected-8051" to be "running and ready"
Aug 30 06:33:26.262: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.07139ms
Aug 30 06:33:26.262: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:33:28.270: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018019984s
Aug 30 06:33:28.270: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:33:30.271: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.018891789s
Aug 30 06:33:30.271: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Running (Ready = true)
Aug 30 06:33:30.271: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb" satisfied condition "running and ready"
Aug 30 06:33:30.816: INFO: Successfully updated pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8051" for this suite. 08/30/23 06:33:32.86
------------------------------
• [SLOW TEST] [6.728 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:26.151
    Aug 30 06:33:26.152: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:33:26.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:26.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:26.212
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/30/23 06:33:26.217
    W0830 06:33:26.251989      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:33:26.252: INFO: Waiting up to 5m0s for pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb" in namespace "projected-8051" to be "running and ready"
    Aug 30 06:33:26.262: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.07139ms
    Aug 30 06:33:26.262: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:33:28.270: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018019984s
    Aug 30 06:33:28.270: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:33:30.271: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.018891789s
    Aug 30 06:33:30.271: INFO: The phase of Pod annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb is Running (Ready = true)
    Aug 30 06:33:30.271: INFO: Pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb" satisfied condition "running and ready"
    Aug 30 06:33:30.816: INFO: Successfully updated pod "annotationupdate9fe0c202-4936-48f4-8355-6303fadca3bb"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:32.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8051" for this suite. 08/30/23 06:33:32.86
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:32.88
Aug 30 06:33:32.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 06:33:32.881
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:32.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:32.932
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/30/23 06:33:32.938
W0830 06:33:32.955062      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 08/30/23 06:33:32.955
STEP: delete the deployment 08/30/23 06:33:33.482
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/30/23 06:33:33.503
STEP: Gathering metrics 08/30/23 06:33:34.05
W0830 06:33:34.064033      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 06:33:34.064: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:34.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-864" for this suite. 08/30/23 06:33:34.071
------------------------------
• [1.209 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:32.88
    Aug 30 06:33:32.881: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 06:33:32.881
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:32.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:32.932
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/30/23 06:33:32.938
    W0830 06:33:32.955062      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 08/30/23 06:33:32.955
    STEP: delete the deployment 08/30/23 06:33:33.482
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/30/23 06:33:33.503
    STEP: Gathering metrics 08/30/23 06:33:34.05
    W0830 06:33:34.064033      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 06:33:34.064: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:34.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-864" for this suite. 08/30/23 06:33:34.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:34.094
Aug 30 06:33:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename init-container 08/30/23 06:33:34.095
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:34.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:34.147
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/30/23 06:33:34.153
Aug 30 06:33:34.153: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:39.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-254" for this suite. 08/30/23 06:33:39.703
------------------------------
• [SLOW TEST] [5.629 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:34.094
    Aug 30 06:33:34.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename init-container 08/30/23 06:33:34.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:34.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:34.147
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/30/23 06:33:34.153
    Aug 30 06:33:34.153: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:39.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-254" for this suite. 08/30/23 06:33:39.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:39.723
Aug 30 06:33:39.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:33:39.726
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:39.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:39.794
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:33:39.933
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:33:40.335
STEP: Deploying the webhook pod 08/30/23 06:33:40.352
STEP: Wait for the deployment to be ready 08/30/23 06:33:40.377
Aug 30 06:33:40.433: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Aug 30 06:33:42.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 06:33:44.472
STEP: Verifying the service has paired with the endpoint 08/30/23 06:33:44.492
Aug 30 06:33:45.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/30/23 06:33:45.502
STEP: create a namespace for the webhook 08/30/23 06:33:45.547
STEP: create a configmap should be unconditionally rejected by the webhook 08/30/23 06:33:45.572
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:45.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5862" for this suite. 08/30/23 06:33:45.757
STEP: Destroying namespace "webhook-5862-markers" for this suite. 08/30/23 06:33:45.779
------------------------------
• [SLOW TEST] [6.079 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:39.723
    Aug 30 06:33:39.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:33:39.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:39.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:39.794
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:33:39.933
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:33:40.335
    STEP: Deploying the webhook pod 08/30/23 06:33:40.352
    STEP: Wait for the deployment to be ready 08/30/23 06:33:40.377
    Aug 30 06:33:40.433: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Aug 30 06:33:42.461: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 33, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 06:33:44.472
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:33:44.492
    Aug 30 06:33:45.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/30/23 06:33:45.502
    STEP: create a namespace for the webhook 08/30/23 06:33:45.547
    STEP: create a configmap should be unconditionally rejected by the webhook 08/30/23 06:33:45.572
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:45.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5862" for this suite. 08/30/23 06:33:45.757
    STEP: Destroying namespace "webhook-5862-markers" for this suite. 08/30/23 06:33:45.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:45.805
Aug 30 06:33:45.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:33:45.806
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:45.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:45.866
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:33:45.876
W0830 06:33:45.901173      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:33:45.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5" in namespace "projected-3430" to be "Succeeded or Failed"
Aug 30 06:33:45.942: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.659588ms
Aug 30 06:33:47.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049386786s
Aug 30 06:33:49.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049001081s
STEP: Saw pod success 08/30/23 06:33:49.95
Aug 30 06:33:49.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5" satisfied condition "Succeeded or Failed"
Aug 30 06:33:49.957: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 container client-container: <nil>
STEP: delete the pod 08/30/23 06:33:49.972
Aug 30 06:33:49.992: INFO: Waiting for pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 to disappear
Aug 30 06:33:49.999: INFO: Pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:49.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3430" for this suite. 08/30/23 06:33:50.008
------------------------------
• [4.224 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:45.805
    Aug 30 06:33:45.806: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:33:45.806
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:45.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:45.866
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:33:45.876
    W0830 06:33:45.901173      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:33:45.901: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5" in namespace "projected-3430" to be "Succeeded or Failed"
    Aug 30 06:33:45.942: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.659588ms
    Aug 30 06:33:47.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049386786s
    Aug 30 06:33:49.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049001081s
    STEP: Saw pod success 08/30/23 06:33:49.95
    Aug 30 06:33:49.950: INFO: Pod "downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5" satisfied condition "Succeeded or Failed"
    Aug 30 06:33:49.957: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:33:49.972
    Aug 30 06:33:49.992: INFO: Waiting for pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 to disappear
    Aug 30 06:33:49.999: INFO: Pod downwardapi-volume-1492da84-1a13-4b82-82dd-45c207d2afb5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:49.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3430" for this suite. 08/30/23 06:33:50.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:50.03
Aug 30 06:33:50.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sysctl 08/30/23 06:33:50.031
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:50.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:50.082
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/30/23 06:33:50.093
W0830 06:33:50.117003      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Watching for error events or started pod 08/30/23 06:33:50.117
STEP: Waiting for pod completion 08/30/23 06:33:52.13
Aug 30 06:33:52.130: INFO: Waiting up to 3m0s for pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58" in namespace "sysctl-1666" to be "completed"
Aug 30 06:33:52.137: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.500212ms
Aug 30 06:33:54.144: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014660076s
Aug 30 06:33:54.145: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/30/23 06:33:54.151
STEP: Getting logs from the pod 08/30/23 06:33:54.151
STEP: Checking that the sysctl is actually updated 08/30/23 06:33:54.166
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-1666" for this suite. 08/30/23 06:33:54.175
------------------------------
• [4.172 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:50.03
    Aug 30 06:33:50.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sysctl 08/30/23 06:33:50.031
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:50.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:50.082
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/30/23 06:33:50.093
    W0830 06:33:50.117003      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Watching for error events or started pod 08/30/23 06:33:50.117
    STEP: Waiting for pod completion 08/30/23 06:33:52.13
    Aug 30 06:33:52.130: INFO: Waiting up to 3m0s for pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58" in namespace "sysctl-1666" to be "completed"
    Aug 30 06:33:52.137: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.500212ms
    Aug 30 06:33:54.144: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014660076s
    Aug 30 06:33:54.145: INFO: Pod "sysctl-5b3efb55-6d04-46b8-8898-1ddca00a3c58" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/30/23 06:33:54.151
    STEP: Getting logs from the pod 08/30/23 06:33:54.151
    STEP: Checking that the sysctl is actually updated 08/30/23 06:33:54.166
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-1666" for this suite. 08/30/23 06:33:54.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:54.204
Aug 30 06:33:54.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 06:33:54.204
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:54.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:54.253
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5108.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/30/23 06:33:54.258
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5108.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/30/23 06:33:54.258
STEP: creating a pod to probe /etc/hosts 08/30/23 06:33:54.258
STEP: submitting the pod to kubernetes 08/30/23 06:33:54.258
W0830 06:33:54.275926      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:33:54.276: INFO: Waiting up to 15m0s for pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58" in namespace "dns-5108" to be "running"
Aug 30 06:33:54.290: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Pending", Reason="", readiness=false. Elapsed: 14.498626ms
Aug 30 06:33:56.299: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02300004s
Aug 30 06:33:58.298: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Running", Reason="", readiness=true. Elapsed: 4.022408542s
Aug 30 06:33:58.298: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:33:58.298
STEP: looking for the results for each expected name from probers 08/30/23 06:33:58.305
Aug 30 06:33:58.347: INFO: DNS probes using dns-5108/dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58 succeeded

STEP: deleting the pod 08/30/23 06:33:58.347
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 06:33:58.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5108" for this suite. 08/30/23 06:33:58.376
------------------------------
• [4.190 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:54.204
    Aug 30 06:33:54.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 06:33:54.204
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:54.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:54.253
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5108.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/30/23 06:33:54.258
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5108.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5108.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/30/23 06:33:54.258
    STEP: creating a pod to probe /etc/hosts 08/30/23 06:33:54.258
    STEP: submitting the pod to kubernetes 08/30/23 06:33:54.258
    W0830 06:33:54.275926      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:33:54.276: INFO: Waiting up to 15m0s for pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58" in namespace "dns-5108" to be "running"
    Aug 30 06:33:54.290: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Pending", Reason="", readiness=false. Elapsed: 14.498626ms
    Aug 30 06:33:56.299: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02300004s
    Aug 30 06:33:58.298: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58": Phase="Running", Reason="", readiness=true. Elapsed: 4.022408542s
    Aug 30 06:33:58.298: INFO: Pod "dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:33:58.298
    STEP: looking for the results for each expected name from probers 08/30/23 06:33:58.305
    Aug 30 06:33:58.347: INFO: DNS probes using dns-5108/dns-test-cb8b8ab5-f7ee-469f-a342-f883d9deed58 succeeded

    STEP: deleting the pod 08/30/23 06:33:58.347
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:33:58.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5108" for this suite. 08/30/23 06:33:58.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:33:58.394
Aug 30 06:33:58.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:33:58.395
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:58.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:58.492
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 in namespace container-probe-8854 08/30/23 06:33:58.498
W0830 06:33:58.521437      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:33:58.521: INFO: Waiting up to 5m0s for pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903" in namespace "container-probe-8854" to be "not pending"
Aug 30 06:33:58.545: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903": Phase="Pending", Reason="", readiness=false. Elapsed: 23.295843ms
Aug 30 06:34:00.554: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903": Phase="Running", Reason="", readiness=true. Elapsed: 2.033011356s
Aug 30 06:34:00.554: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903" satisfied condition "not pending"
Aug 30 06:34:00.554: INFO: Started pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 in namespace container-probe-8854
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:34:00.554
Aug 30 06:34:00.561: INFO: Initial restart count of pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 is 0
STEP: deleting the pod 08/30/23 06:38:02.149
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8854" for this suite. 08/30/23 06:38:02.23
------------------------------
• [SLOW TEST] [243.904 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:33:58.394
    Aug 30 06:33:58.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:33:58.395
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:33:58.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:33:58.492
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 in namespace container-probe-8854 08/30/23 06:33:58.498
    W0830 06:33:58.521437      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:33:58.521: INFO: Waiting up to 5m0s for pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903" in namespace "container-probe-8854" to be "not pending"
    Aug 30 06:33:58.545: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903": Phase="Pending", Reason="", readiness=false. Elapsed: 23.295843ms
    Aug 30 06:34:00.554: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903": Phase="Running", Reason="", readiness=true. Elapsed: 2.033011356s
    Aug 30 06:34:00.554: INFO: Pod "test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903" satisfied condition "not pending"
    Aug 30 06:34:00.554: INFO: Started pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 in namespace container-probe-8854
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:34:00.554
    Aug 30 06:34:00.561: INFO: Initial restart count of pod test-webserver-3ab721a1-b043-439e-b554-23ccf04bf903 is 0
    STEP: deleting the pod 08/30/23 06:38:02.149
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:02.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8854" for this suite. 08/30/23 06:38:02.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:02.302
Aug 30 06:38:02.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:38:02.304
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:02.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:02.413
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-3509 08/30/23 06:38:02.45
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[] 08/30/23 06:38:02.487
Aug 30 06:38:02.496: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 30 06:38:03.607: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3509 08/30/23 06:38:03.637
Aug 30 06:38:03.712: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3509" to be "running and ready"
Aug 30 06:38:03.780: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.311191ms
Aug 30 06:38:03.780: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:38:05.787: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.05013465s
Aug 30 06:38:05.787: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 30 06:38:05.787: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod1:[80]] 08/30/23 06:38:05.796
Aug 30 06:38:05.818: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/30/23 06:38:05.818
Aug 30 06:38:05.818: INFO: Creating new exec pod
Aug 30 06:38:05.832: INFO: Waiting up to 5m0s for pod "execpod5mchx" in namespace "services-3509" to be "running"
Aug 30 06:38:05.839: INFO: Pod "execpod5mchx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.367065ms
Aug 30 06:38:07.850: INFO: Pod "execpod5mchx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01852227s
Aug 30 06:38:07.850: INFO: Pod "execpod5mchx" satisfied condition "running"
Aug 30 06:38:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 30 06:38:09.102: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:09.102: INFO: stdout: ""
Aug 30 06:38:09.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
Aug 30 06:38:09.383: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:09.383: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-3509 08/30/23 06:38:09.383
Aug 30 06:38:09.408: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3509" to be "running and ready"
Aug 30 06:38:09.416: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941414ms
Aug 30 06:38:09.416: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:38:11.425: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016144769s
Aug 30 06:38:11.425: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:38:13.429: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.020361089s
Aug 30 06:38:13.429: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 30 06:38:13.429: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod1:[80] pod2:[80]] 08/30/23 06:38:13.443
Aug 30 06:38:13.473: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/30/23 06:38:13.473
Aug 30 06:38:14.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 30 06:38:14.705: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:14.705: INFO: stdout: ""
Aug 30 06:38:14.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
Aug 30 06:38:14.970: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:14.970: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-3509 08/30/23 06:38:14.97
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod2:[80]] 08/30/23 06:38:14.995
Aug 30 06:38:15.030: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/30/23 06:38:15.03
Aug 30 06:38:16.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 30 06:38:16.252: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:16.252: INFO: stdout: ""
Aug 30 06:38:16.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
Aug 30 06:38:16.483: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:16.483: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-3509 08/30/23 06:38:16.483
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[] 08/30/23 06:38:16.504
Aug 30 06:38:16.549: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:16.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3509" for this suite. 08/30/23 06:38:16.629
------------------------------
• [SLOW TEST] [14.348 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:02.302
    Aug 30 06:38:02.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:38:02.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:02.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:02.413
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-3509 08/30/23 06:38:02.45
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[] 08/30/23 06:38:02.487
    Aug 30 06:38:02.496: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 30 06:38:03.607: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3509 08/30/23 06:38:03.637
    Aug 30 06:38:03.712: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3509" to be "running and ready"
    Aug 30 06:38:03.780: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 42.311191ms
    Aug 30 06:38:03.780: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:38:05.787: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.05013465s
    Aug 30 06:38:05.787: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 30 06:38:05.787: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod1:[80]] 08/30/23 06:38:05.796
    Aug 30 06:38:05.818: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/30/23 06:38:05.818
    Aug 30 06:38:05.818: INFO: Creating new exec pod
    Aug 30 06:38:05.832: INFO: Waiting up to 5m0s for pod "execpod5mchx" in namespace "services-3509" to be "running"
    Aug 30 06:38:05.839: INFO: Pod "execpod5mchx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.367065ms
    Aug 30 06:38:07.850: INFO: Pod "execpod5mchx": Phase="Running", Reason="", readiness=true. Elapsed: 2.01852227s
    Aug 30 06:38:07.850: INFO: Pod "execpod5mchx" satisfied condition "running"
    Aug 30 06:38:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 30 06:38:09.102: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:09.102: INFO: stdout: ""
    Aug 30 06:38:09.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
    Aug 30 06:38:09.383: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:09.383: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-3509 08/30/23 06:38:09.383
    Aug 30 06:38:09.408: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3509" to be "running and ready"
    Aug 30 06:38:09.416: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.941414ms
    Aug 30 06:38:09.416: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:38:11.425: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016144769s
    Aug 30 06:38:11.425: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:38:13.429: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.020361089s
    Aug 30 06:38:13.429: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 30 06:38:13.429: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod1:[80] pod2:[80]] 08/30/23 06:38:13.443
    Aug 30 06:38:13.473: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/30/23 06:38:13.473
    Aug 30 06:38:14.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 30 06:38:14.705: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:14.705: INFO: stdout: ""
    Aug 30 06:38:14.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
    Aug 30 06:38:14.970: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:14.970: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-3509 08/30/23 06:38:14.97
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[pod2:[80]] 08/30/23 06:38:14.995
    Aug 30 06:38:15.030: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/30/23 06:38:15.03
    Aug 30 06:38:16.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 30 06:38:16.252: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:16.252: INFO: stdout: ""
    Aug 30 06:38:16.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3509 exec execpod5mchx -- /bin/sh -x -c nc -v -z -w 2 172.21.125.250 80'
    Aug 30 06:38:16.483: INFO: stderr: "+ nc -v -z -w 2 172.21.125.250 80\nConnection to 172.21.125.250 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:16.483: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-3509 08/30/23 06:38:16.483
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3509 to expose endpoints map[] 08/30/23 06:38:16.504
    Aug 30 06:38:16.549: INFO: successfully validated that service endpoint-test2 in namespace services-3509 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:16.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3509" for this suite. 08/30/23 06:38:16.629
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:16.651
Aug 30 06:38:16.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:38:16.653
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:16.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:16.74
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-9397 08/30/23 06:38:16.745
STEP: creating service affinity-clusterip-transition in namespace services-9397 08/30/23 06:38:16.745
STEP: creating replication controller affinity-clusterip-transition in namespace services-9397 08/30/23 06:38:16.812
I0830 06:38:16.837016      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9397, replica count: 3
I0830 06:38:19.888804      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 06:38:19.908: INFO: Creating new exec pod
Aug 30 06:38:19.926: INFO: Waiting up to 5m0s for pod "execpod-affinityt6dj9" in namespace "services-9397" to be "running"
Aug 30 06:38:19.934: INFO: Pod "execpod-affinityt6dj9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.637901ms
Aug 30 06:38:21.944: INFO: Pod "execpod-affinityt6dj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017201786s
Aug 30 06:38:21.944: INFO: Pod "execpod-affinityt6dj9" satisfied condition "running"
Aug 30 06:38:22.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 30 06:38:23.237: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:23.237: INFO: stdout: ""
Aug 30 06:38:23.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c nc -v -z -w 2 172.21.28.247 80'
Aug 30 06:38:23.526: INFO: stderr: "+ nc -v -z -w 2 172.21.28.247 80\nConnection to 172.21.28.247 80 port [tcp/http] succeeded!\n"
Aug 30 06:38:23.526: INFO: stdout: ""
Aug 30 06:38:23.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.28.247:80/ ; done'
Aug 30 06:38:23.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n"
Aug 30 06:38:23.949: INFO: stdout: "\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-cwps4"
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
Aug 30 06:38:23.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.28.247:80/ ; done'
Aug 30 06:38:24.345: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n"
Aug 30 06:38:24.345: INFO: stdout: "\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l"
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
Aug 30 06:38:24.345: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9397, will wait for the garbage collector to delete the pods 08/30/23 06:38:24.368
Aug 30 06:38:24.438: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.131857ms
Aug 30 06:38:24.539: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.845742ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:27.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9397" for this suite. 08/30/23 06:38:27.378
------------------------------
• [SLOW TEST] [10.755 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:16.651
    Aug 30 06:38:16.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:38:16.653
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:16.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:16.74
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-9397 08/30/23 06:38:16.745
    STEP: creating service affinity-clusterip-transition in namespace services-9397 08/30/23 06:38:16.745
    STEP: creating replication controller affinity-clusterip-transition in namespace services-9397 08/30/23 06:38:16.812
    I0830 06:38:16.837016      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9397, replica count: 3
    I0830 06:38:19.888804      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 06:38:19.908: INFO: Creating new exec pod
    Aug 30 06:38:19.926: INFO: Waiting up to 5m0s for pod "execpod-affinityt6dj9" in namespace "services-9397" to be "running"
    Aug 30 06:38:19.934: INFO: Pod "execpod-affinityt6dj9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.637901ms
    Aug 30 06:38:21.944: INFO: Pod "execpod-affinityt6dj9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017201786s
    Aug 30 06:38:21.944: INFO: Pod "execpod-affinityt6dj9" satisfied condition "running"
    Aug 30 06:38:22.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 30 06:38:23.237: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:23.237: INFO: stdout: ""
    Aug 30 06:38:23.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c nc -v -z -w 2 172.21.28.247 80'
    Aug 30 06:38:23.526: INFO: stderr: "+ nc -v -z -w 2 172.21.28.247 80\nConnection to 172.21.28.247 80 port [tcp/http] succeeded!\n"
    Aug 30 06:38:23.526: INFO: stdout: ""
    Aug 30 06:38:23.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.28.247:80/ ; done'
    Aug 30 06:38:23.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n"
    Aug 30 06:38:23.949: INFO: stdout: "\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-875k6\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-cwps4\naffinity-clusterip-transition-cwps4"
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-875k6
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
    Aug 30 06:38:23.949: INFO: Received response from host: affinity-clusterip-transition-cwps4
    Aug 30 06:38:23.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-9397 exec execpod-affinityt6dj9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.28.247:80/ ; done'
    Aug 30 06:38:24.345: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.28.247:80/\n"
    Aug 30 06:38:24.345: INFO: stdout: "\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l\naffinity-clusterip-transition-s7l2l"
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Received response from host: affinity-clusterip-transition-s7l2l
    Aug 30 06:38:24.345: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9397, will wait for the garbage collector to delete the pods 08/30/23 06:38:24.368
    Aug 30 06:38:24.438: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.131857ms
    Aug 30 06:38:24.539: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.845742ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:27.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9397" for this suite. 08/30/23 06:38:27.378
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:27.407
Aug 30 06:38:27.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:38:27.408
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:27.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:27.476
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 30 06:38:27.511: INFO: Got root ca configmap in namespace "svcaccounts-6520"
Aug 30 06:38:27.564: INFO: Deleted root ca configmap in namespace "svcaccounts-6520"
STEP: waiting for a new root ca configmap created 08/30/23 06:38:28.065
Aug 30 06:38:28.075: INFO: Recreated root ca configmap in namespace "svcaccounts-6520"
Aug 30 06:38:28.122: INFO: Updated root ca configmap in namespace "svcaccounts-6520"
STEP: waiting for the root ca configmap reconciled 08/30/23 06:38:28.622
Aug 30 06:38:28.632: INFO: Reconciled root ca configmap in namespace "svcaccounts-6520"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:28.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6520" for this suite. 08/30/23 06:38:28.643
------------------------------
• [1.255 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:27.407
    Aug 30 06:38:27.407: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:38:27.408
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:27.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:27.476
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 30 06:38:27.511: INFO: Got root ca configmap in namespace "svcaccounts-6520"
    Aug 30 06:38:27.564: INFO: Deleted root ca configmap in namespace "svcaccounts-6520"
    STEP: waiting for a new root ca configmap created 08/30/23 06:38:28.065
    Aug 30 06:38:28.075: INFO: Recreated root ca configmap in namespace "svcaccounts-6520"
    Aug 30 06:38:28.122: INFO: Updated root ca configmap in namespace "svcaccounts-6520"
    STEP: waiting for the root ca configmap reconciled 08/30/23 06:38:28.622
    Aug 30 06:38:28.632: INFO: Reconciled root ca configmap in namespace "svcaccounts-6520"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:28.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6520" for this suite. 08/30/23 06:38:28.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:28.664
Aug 30 06:38:28.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 06:38:28.665
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:28.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:28.727
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/30/23 06:38:28.732
STEP: patching the Namespace 08/30/23 06:38:28.815
STEP: get the Namespace and ensuring it has the label 08/30/23 06:38:28.846
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:28.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-111" for this suite. 08/30/23 06:38:28.905
STEP: Destroying namespace "nspatchtest-1a879bc7-d023-47f0-b209-f48d29be94a4-502" for this suite. 08/30/23 06:38:28.963
------------------------------
• [0.355 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:28.664
    Aug 30 06:38:28.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 06:38:28.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:28.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:28.727
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/30/23 06:38:28.732
    STEP: patching the Namespace 08/30/23 06:38:28.815
    STEP: get the Namespace and ensuring it has the label 08/30/23 06:38:28.846
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:28.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-111" for this suite. 08/30/23 06:38:28.905
    STEP: Destroying namespace "nspatchtest-1a879bc7-d023-47f0-b209-f48d29be94a4-502" for this suite. 08/30/23 06:38:28.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:29.019
Aug 30 06:38:29.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:38:29.02
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:29.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:29.131
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/30/23 06:38:29.137
Aug 30 06:38:30.225: INFO: Waiting up to 5m0s for pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7" in namespace "emptydir-106" to be "Succeeded or Failed"
Aug 30 06:38:30.233: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516326ms
Aug 30 06:38:32.248: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022493649s
Aug 30 06:38:34.243: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017241186s
STEP: Saw pod success 08/30/23 06:38:34.243
Aug 30 06:38:34.243: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7" satisfied condition "Succeeded or Failed"
Aug 30 06:38:34.250: INFO: Trying to get logs from node 10.135.139.190 pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 container test-container: <nil>
STEP: delete the pod 08/30/23 06:38:34.308
Aug 30 06:38:34.328: INFO: Waiting for pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 to disappear
Aug 30 06:38:34.336: INFO: Pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:38:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-106" for this suite. 08/30/23 06:38:34.345
------------------------------
• [SLOW TEST] [5.361 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:29.019
    Aug 30 06:38:29.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:38:29.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:29.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:29.131
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/30/23 06:38:29.137
    Aug 30 06:38:30.225: INFO: Waiting up to 5m0s for pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7" in namespace "emptydir-106" to be "Succeeded or Failed"
    Aug 30 06:38:30.233: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.516326ms
    Aug 30 06:38:32.248: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022493649s
    Aug 30 06:38:34.243: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017241186s
    STEP: Saw pod success 08/30/23 06:38:34.243
    Aug 30 06:38:34.243: INFO: Pod "pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7" satisfied condition "Succeeded or Failed"
    Aug 30 06:38:34.250: INFO: Trying to get logs from node 10.135.139.190 pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 container test-container: <nil>
    STEP: delete the pod 08/30/23 06:38:34.308
    Aug 30 06:38:34.328: INFO: Waiting for pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 to disappear
    Aug 30 06:38:34.336: INFO: Pod pod-79b84ec5-3eea-41fe-9f2f-26ffe98e1fd7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:38:34.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-106" for this suite. 08/30/23 06:38:34.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:38:34.388
Aug 30 06:38:34.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename subpath 08/30/23 06:38:34.389
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:34.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:34.47
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/30/23 06:38:34.475
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rmtr 08/30/23 06:38:34.522
STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:38:34.522
Aug 30 06:38:34.546: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rmtr" in namespace "subpath-6576" to be "Succeeded or Failed"
Aug 30 06:38:34.554: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.138353ms
Aug 30 06:38:36.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018032285s
Aug 30 06:38:38.563: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 4.016466954s
Aug 30 06:38:40.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 6.0173788s
Aug 30 06:38:42.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 8.017723623s
Aug 30 06:38:44.562: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 10.015576202s
Aug 30 06:38:46.581: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 12.03461017s
Aug 30 06:38:48.566: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 14.019969022s
Aug 30 06:38:50.563: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 16.016834582s
Aug 30 06:38:52.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 18.017455401s
Aug 30 06:38:54.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 20.017401415s
Aug 30 06:38:56.562: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 22.016080238s
Aug 30 06:38:58.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=false. Elapsed: 24.017686105s
Aug 30 06:39:00.572: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025802544s
STEP: Saw pod success 08/30/23 06:39:00.572
Aug 30 06:39:00.572: INFO: Pod "pod-subpath-test-downwardapi-rmtr" satisfied condition "Succeeded or Failed"
Aug 30 06:39:00.580: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-downwardapi-rmtr container test-container-subpath-downwardapi-rmtr: <nil>
STEP: delete the pod 08/30/23 06:39:00.671
Aug 30 06:39:00.702: INFO: Waiting for pod pod-subpath-test-downwardapi-rmtr to disappear
Aug 30 06:39:00.710: INFO: Pod pod-subpath-test-downwardapi-rmtr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rmtr 08/30/23 06:39:00.71
Aug 30 06:39:00.710: INFO: Deleting pod "pod-subpath-test-downwardapi-rmtr" in namespace "subpath-6576"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:00.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6576" for this suite. 08/30/23 06:39:00.728
------------------------------
• [SLOW TEST] [26.383 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:38:34.388
    Aug 30 06:38:34.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename subpath 08/30/23 06:38:34.389
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:38:34.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:38:34.47
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/30/23 06:38:34.475
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rmtr 08/30/23 06:38:34.522
    STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:38:34.522
    Aug 30 06:38:34.546: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rmtr" in namespace "subpath-6576" to be "Succeeded or Failed"
    Aug 30 06:38:34.554: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Pending", Reason="", readiness=false. Elapsed: 8.138353ms
    Aug 30 06:38:36.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018032285s
    Aug 30 06:38:38.563: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 4.016466954s
    Aug 30 06:38:40.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 6.0173788s
    Aug 30 06:38:42.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 8.017723623s
    Aug 30 06:38:44.562: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 10.015576202s
    Aug 30 06:38:46.581: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 12.03461017s
    Aug 30 06:38:48.566: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 14.019969022s
    Aug 30 06:38:50.563: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 16.016834582s
    Aug 30 06:38:52.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 18.017455401s
    Aug 30 06:38:54.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 20.017401415s
    Aug 30 06:38:56.562: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=true. Elapsed: 22.016080238s
    Aug 30 06:38:58.564: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Running", Reason="", readiness=false. Elapsed: 24.017686105s
    Aug 30 06:39:00.572: INFO: Pod "pod-subpath-test-downwardapi-rmtr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.025802544s
    STEP: Saw pod success 08/30/23 06:39:00.572
    Aug 30 06:39:00.572: INFO: Pod "pod-subpath-test-downwardapi-rmtr" satisfied condition "Succeeded or Failed"
    Aug 30 06:39:00.580: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-downwardapi-rmtr container test-container-subpath-downwardapi-rmtr: <nil>
    STEP: delete the pod 08/30/23 06:39:00.671
    Aug 30 06:39:00.702: INFO: Waiting for pod pod-subpath-test-downwardapi-rmtr to disappear
    Aug 30 06:39:00.710: INFO: Pod pod-subpath-test-downwardapi-rmtr no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rmtr 08/30/23 06:39:00.71
    Aug 30 06:39:00.710: INFO: Deleting pod "pod-subpath-test-downwardapi-rmtr" in namespace "subpath-6576"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:00.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6576" for this suite. 08/30/23 06:39:00.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:00.771
Aug 30 06:39:00.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:39:00.772
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:00.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:00.888
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:39:00.894
W0830 06:39:00.917005      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:39:00.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221" in namespace "projected-16" to be "Succeeded or Failed"
Aug 30 06:39:00.929: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Pending", Reason="", readiness=false. Elapsed: 12.678239ms
Aug 30 06:39:02.939: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022162455s
Aug 30 06:39:04.941: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023853631s
STEP: Saw pod success 08/30/23 06:39:04.941
Aug 30 06:39:04.941: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221" satisfied condition "Succeeded or Failed"
Aug 30 06:39:04.949: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 container client-container: <nil>
STEP: delete the pod 08/30/23 06:39:05.056
Aug 30 06:39:05.085: INFO: Waiting for pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 to disappear
Aug 30 06:39:05.098: INFO: Pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:05.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-16" for this suite. 08/30/23 06:39:05.107
------------------------------
• [4.357 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:00.771
    Aug 30 06:39:00.771: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:39:00.772
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:00.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:00.888
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:39:00.894
    W0830 06:39:00.917005      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:39:00.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221" in namespace "projected-16" to be "Succeeded or Failed"
    Aug 30 06:39:00.929: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Pending", Reason="", readiness=false. Elapsed: 12.678239ms
    Aug 30 06:39:02.939: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022162455s
    Aug 30 06:39:04.941: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023853631s
    STEP: Saw pod success 08/30/23 06:39:04.941
    Aug 30 06:39:04.941: INFO: Pod "downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221" satisfied condition "Succeeded or Failed"
    Aug 30 06:39:04.949: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:39:05.056
    Aug 30 06:39:05.085: INFO: Waiting for pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 to disappear
    Aug 30 06:39:05.098: INFO: Pod downwardapi-volume-afd314d1-898a-4223-94c7-1fc19181a221 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:05.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-16" for this suite. 08/30/23 06:39:05.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:05.129
Aug 30 06:39:05.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:39:05.131
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:05.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:05.269
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:39:05.418
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:39:05.988
STEP: Deploying the webhook pod 08/30/23 06:39:06.029
STEP: Wait for the deployment to be ready 08/30/23 06:39:06.061
Aug 30 06:39:06.081: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/30/23 06:39:08.11
STEP: Verifying the service has paired with the endpoint 08/30/23 06:39:08.136
Aug 30 06:39:09.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/30/23 06:39:09.169
STEP: create a pod 08/30/23 06:39:09.214
Aug 30 06:39:09.232: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9886" to be "running"
Aug 30 06:39:09.239: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.422417ms
Aug 30 06:39:11.249: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016462071s
Aug 30 06:39:11.249: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/30/23 06:39:11.249
Aug 30 06:39:11.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=webhook-9886 attach --namespace=webhook-9886 to-be-attached-pod -i -c=container1'
Aug 30 06:39:11.403: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9886" for this suite. 08/30/23 06:39:11.615
STEP: Destroying namespace "webhook-9886-markers" for this suite. 08/30/23 06:39:11.642
------------------------------
• [SLOW TEST] [6.547 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:05.129
    Aug 30 06:39:05.129: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:39:05.131
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:05.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:05.269
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:39:05.418
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:39:05.988
    STEP: Deploying the webhook pod 08/30/23 06:39:06.029
    STEP: Wait for the deployment to be ready 08/30/23 06:39:06.061
    Aug 30 06:39:06.081: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/30/23 06:39:08.11
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:39:08.136
    Aug 30 06:39:09.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/30/23 06:39:09.169
    STEP: create a pod 08/30/23 06:39:09.214
    Aug 30 06:39:09.232: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9886" to be "running"
    Aug 30 06:39:09.239: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.422417ms
    Aug 30 06:39:11.249: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016462071s
    Aug 30 06:39:11.249: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/30/23 06:39:11.249
    Aug 30 06:39:11.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=webhook-9886 attach --namespace=webhook-9886 to-be-attached-pod -i -c=container1'
    Aug 30 06:39:11.403: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:11.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9886" for this suite. 08/30/23 06:39:11.615
    STEP: Destroying namespace "webhook-9886-markers" for this suite. 08/30/23 06:39:11.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:11.678
Aug 30 06:39:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 06:39:11.68
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:11.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:11.753
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:39:11.758
W0830 06:39:11.813227      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:39:11.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5" in namespace "downward-api-6161" to be "Succeeded or Failed"
Aug 30 06:39:11.823: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.201029ms
Aug 30 06:39:13.837: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023635039s
Aug 30 06:39:15.832: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018831002s
Aug 30 06:39:17.833: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01997308s
STEP: Saw pod success 08/30/23 06:39:17.833
Aug 30 06:39:17.833: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5" satisfied condition "Succeeded or Failed"
Aug 30 06:39:17.842: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 container client-container: <nil>
STEP: delete the pod 08/30/23 06:39:17.859
Aug 30 06:39:17.884: INFO: Waiting for pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 to disappear
Aug 30 06:39:17.890: INFO: Pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:17.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6161" for this suite. 08/30/23 06:39:17.899
------------------------------
• [SLOW TEST] [6.240 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:11.678
    Aug 30 06:39:11.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 06:39:11.68
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:11.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:11.753
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:39:11.758
    W0830 06:39:11.813227      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:39:11.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5" in namespace "downward-api-6161" to be "Succeeded or Failed"
    Aug 30 06:39:11.823: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.201029ms
    Aug 30 06:39:13.837: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023635039s
    Aug 30 06:39:15.832: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018831002s
    Aug 30 06:39:17.833: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01997308s
    STEP: Saw pod success 08/30/23 06:39:17.833
    Aug 30 06:39:17.833: INFO: Pod "downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5" satisfied condition "Succeeded or Failed"
    Aug 30 06:39:17.842: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:39:17.859
    Aug 30 06:39:17.884: INFO: Waiting for pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 to disappear
    Aug 30 06:39:17.890: INFO: Pod downwardapi-volume-edf4243e-2a96-409e-88f6-100d1755b9f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:17.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6161" for this suite. 08/30/23 06:39:17.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:17.923
Aug 30 06:39:17.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:39:17.924
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:17.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:17.978
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:39:18.081
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:39:18.552
STEP: Deploying the webhook pod 08/30/23 06:39:18.566
STEP: Wait for the deployment to be ready 08/30/23 06:39:18.593
Aug 30 06:39:18.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 06:39:20.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 06:39:22.654
STEP: Verifying the service has paired with the endpoint 08/30/23 06:39:22.679
Aug 30 06:39:23.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/30/23 06:39:23.686
STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.729
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/30/23 06:39:23.782
STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.805
STEP: Patching a validating webhook configuration's rules to include the create operation 08/30/23 06:39:23.848
STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.867
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:23.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8618" for this suite. 08/30/23 06:39:24.049
STEP: Destroying namespace "webhook-8618-markers" for this suite. 08/30/23 06:39:24.066
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:17.923
    Aug 30 06:39:17.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:39:17.924
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:17.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:17.978
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:39:18.081
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:39:18.552
    STEP: Deploying the webhook pod 08/30/23 06:39:18.566
    STEP: Wait for the deployment to be ready 08/30/23 06:39:18.593
    Aug 30 06:39:18.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 06:39:20.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 39, 18, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 06:39:22.654
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:39:22.679
    Aug 30 06:39:23.679: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/30/23 06:39:23.686
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.729
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/30/23 06:39:23.782
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.805
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/30/23 06:39:23.848
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 06:39:23.867
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:23.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8618" for this suite. 08/30/23 06:39:24.049
    STEP: Destroying namespace "webhook-8618-markers" for this suite. 08/30/23 06:39:24.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:24.095
Aug 30 06:39:24.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:39:24.096
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:24.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:24.153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 30 06:39:24.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 06:39:30.676
Aug 30 06:39:30.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 create -f -'
Aug 30 06:39:32.512: INFO: stderr: ""
Aug 30 06:39:32.512: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 06:39:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-4017-crds test-cr'
Aug 30 06:39:32.622: INFO: stderr: ""
Aug 30 06:39:32.622: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 30 06:39:32.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 apply -f -'
Aug 30 06:39:34.316: INFO: stderr: ""
Aug 30 06:39:34.316: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 30 06:39:34.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-4017-crds test-cr'
Aug 30 06:39:34.436: INFO: stderr: ""
Aug 30 06:39:34.436: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/30/23 06:39:34.436
Aug 30 06:39:34.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 explain e2e-test-crd-publish-openapi-4017-crds'
Aug 30 06:39:38.121: INFO: stderr: ""
Aug 30 06:39:38.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4017-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:39:44.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5028" for this suite. 08/30/23 06:39:44.314
------------------------------
• [SLOW TEST] [20.241 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:24.095
    Aug 30 06:39:24.095: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:39:24.096
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:24.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:24.153
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 30 06:39:24.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 06:39:30.676
    Aug 30 06:39:30.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 create -f -'
    Aug 30 06:39:32.512: INFO: stderr: ""
    Aug 30 06:39:32.512: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 30 06:39:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-4017-crds test-cr'
    Aug 30 06:39:32.622: INFO: stderr: ""
    Aug 30 06:39:32.622: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 30 06:39:32.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 apply -f -'
    Aug 30 06:39:34.316: INFO: stderr: ""
    Aug 30 06:39:34.316: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 30 06:39:34.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 --namespace=crd-publish-openapi-5028 delete e2e-test-crd-publish-openapi-4017-crds test-cr'
    Aug 30 06:39:34.436: INFO: stderr: ""
    Aug 30 06:39:34.436: INFO: stdout: "e2e-test-crd-publish-openapi-4017-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/30/23 06:39:34.436
    Aug 30 06:39:34.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-5028 explain e2e-test-crd-publish-openapi-4017-crds'
    Aug 30 06:39:38.121: INFO: stderr: ""
    Aug 30 06:39:38.121: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4017-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:39:44.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5028" for this suite. 08/30/23 06:39:44.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:39:44.339
Aug 30 06:39:44.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pod-network-test 08/30/23 06:39:44.34
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:44.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:44.41
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5874 08/30/23 06:39:44.42
STEP: creating a selector 08/30/23 06:39:44.42
STEP: Creating the service pods in kubernetes 08/30/23 06:39:44.42
Aug 30 06:39:44.420: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
W0830 06:39:44.490013      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:39:44.554: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5874" to be "running and ready"
Aug 30 06:39:44.577: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.392943ms
Aug 30 06:39:44.577: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:39:46.588: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033311427s
Aug 30 06:39:46.588: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:39:48.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032757719s
Aug 30 06:39:48.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:39:50.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.033013423s
Aug 30 06:39:50.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:39:52.598: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043441157s
Aug 30 06:39:52.598: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:39:54.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032328095s
Aug 30 06:39:54.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:39:56.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.033638743s
Aug 30 06:39:56.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:39:58.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037734653s
Aug 30 06:39:58.592: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:40:00.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.035302353s
Aug 30 06:40:00.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:40:02.599: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044630076s
Aug 30 06:40:02.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:40:04.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.035478591s
Aug 30 06:40:04.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:40:06.598: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.043762923s
Aug 30 06:40:06.598: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 30 06:40:06.598: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 30 06:40:06.612: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5874" to be "running and ready"
Aug 30 06:40:06.652: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 39.910117ms
Aug 30 06:40:06.652: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 30 06:40:06.652: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 30 06:40:06.665: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5874" to be "running and ready"
Aug 30 06:40:06.685: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.87948ms
Aug 30 06:40:06.686: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 30 06:40:06.686: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/30/23 06:40:06.708
Aug 30 06:40:06.754: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5874" to be "running"
Aug 30 06:40:06.764: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.669603ms
Aug 30 06:40:08.821: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06762885s
Aug 30 06:40:10.776: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022758794s
Aug 30 06:40:10.776: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 30 06:40:10.788: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5874" to be "running"
Aug 30 06:40:10.806: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.410541ms
Aug 30 06:40:10.806: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 30 06:40:10.816: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 06:40:10.816: INFO: Going to poll 172.30.86.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:40:10.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.86.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:40:10.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:40:10.857: INFO: ExecWithOptions: Clientset creation
Aug 30 06:40:10.857: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.86.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:40:11.103: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 30 06:40:11.103: INFO: Going to poll 172.30.224.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:40:11.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.224.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:40:11.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:40:11.115: INFO: ExecWithOptions: Clientset creation
Aug 30 06:40:11.115: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.224.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:40:11.288: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 30 06:40:11.288: INFO: Going to poll 172.30.58.85 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:40:11.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.58.85:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:40:11.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:40:11.300: INFO: ExecWithOptions: Clientset creation
Aug 30 06:40:11.300: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.58.85%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:40:11.495: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 30 06:40:11.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5874" for this suite. 08/30/23 06:40:11.515
------------------------------
• [SLOW TEST] [27.196 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:39:44.339
    Aug 30 06:39:44.340: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pod-network-test 08/30/23 06:39:44.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:39:44.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:39:44.41
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5874 08/30/23 06:39:44.42
    STEP: creating a selector 08/30/23 06:39:44.42
    STEP: Creating the service pods in kubernetes 08/30/23 06:39:44.42
    Aug 30 06:39:44.420: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    W0830 06:39:44.490013      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:39:44.554: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5874" to be "running and ready"
    Aug 30 06:39:44.577: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.392943ms
    Aug 30 06:39:44.577: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:39:46.588: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033311427s
    Aug 30 06:39:46.588: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:39:48.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.032757719s
    Aug 30 06:39:48.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:39:50.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.033013423s
    Aug 30 06:39:50.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:39:52.598: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.043441157s
    Aug 30 06:39:52.598: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:39:54.587: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.032328095s
    Aug 30 06:39:54.587: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:39:56.588: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.033638743s
    Aug 30 06:39:56.588: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:39:58.592: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.037734653s
    Aug 30 06:39:58.592: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:40:00.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.035302353s
    Aug 30 06:40:00.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:40:02.599: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.044630076s
    Aug 30 06:40:02.600: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:40:04.590: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.035478591s
    Aug 30 06:40:04.590: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:40:06.598: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.043762923s
    Aug 30 06:40:06.598: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 30 06:40:06.598: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 30 06:40:06.612: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5874" to be "running and ready"
    Aug 30 06:40:06.652: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 39.910117ms
    Aug 30 06:40:06.652: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 30 06:40:06.652: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 30 06:40:06.665: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5874" to be "running and ready"
    Aug 30 06:40:06.685: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 19.87948ms
    Aug 30 06:40:06.686: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 30 06:40:06.686: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/30/23 06:40:06.708
    Aug 30 06:40:06.754: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5874" to be "running"
    Aug 30 06:40:06.764: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.669603ms
    Aug 30 06:40:08.821: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06762885s
    Aug 30 06:40:10.776: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022758794s
    Aug 30 06:40:10.776: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 30 06:40:10.788: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5874" to be "running"
    Aug 30 06:40:10.806: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 18.410541ms
    Aug 30 06:40:10.806: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 30 06:40:10.816: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 30 06:40:10.816: INFO: Going to poll 172.30.86.184 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:40:10.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.86.184:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:40:10.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:40:10.857: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:40:10.857: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.86.184%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:40:11.103: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 30 06:40:11.103: INFO: Going to poll 172.30.224.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:40:11.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.224.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:40:11.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:40:11.115: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:40:11.115: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.224.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:40:11.288: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 30 06:40:11.288: INFO: Going to poll 172.30.58.85 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:40:11.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.58.85:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5874 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:40:11.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:40:11.300: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:40:11.300: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5874/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.58.85%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:40:11.495: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:40:11.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5874" for this suite. 08/30/23 06:40:11.515
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:40:11.536
Aug 30 06:40:11.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:40:11.539
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:40:11.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:40:11.6
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
W0830 06:40:11.642419      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 06:41:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-290" for this suite. 08/30/23 06:41:11.67
------------------------------
• [SLOW TEST] [60.226 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:40:11.536
    Aug 30 06:40:11.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:40:11.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:40:11.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:40:11.6
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    W0830 06:40:11.642419      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:41:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-290" for this suite. 08/30/23 06:41:11.67
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:41:11.762
Aug 30 06:41:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename aggregator 08/30/23 06:41:11.763
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:11.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:11.856
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 30 06:41:11.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/30/23 06:41:11.867
Aug 30 06:41:12.259: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 30 06:41:14.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:16.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:18.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:20.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:22.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:24.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:26.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:28.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:30.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:32.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:34.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:36.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:41:38.640: INFO: Waited 202.278086ms for the sample-apiserver to be ready to handle requests.
I0830 06:41:39.812797      21 request.go:690] Waited for 1.035110937s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/cloudcredential.openshift.io/v1
STEP: Read Status for v1alpha1.wardle.example.com 08/30/23 06:41:40.057
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/30/23 06:41:40.09
STEP: List APIServices 08/30/23 06:41:40.144
Aug 30 06:41:40.205: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 30 06:41:40.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-3348" for this suite. 08/30/23 06:41:41.036
------------------------------
• [SLOW TEST] [29.322 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:41:11.762
    Aug 30 06:41:11.762: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename aggregator 08/30/23 06:41:11.763
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:11.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:11.856
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 30 06:41:11.865: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/30/23 06:41:11.867
    Aug 30 06:41:12.259: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 30 06:41:14.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:16.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:18.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:20.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:22.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:24.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:26.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:28.428: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:30.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:32.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:34.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:36.423: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:41:38.640: INFO: Waited 202.278086ms for the sample-apiserver to be ready to handle requests.
    I0830 06:41:39.812797      21 request.go:690] Waited for 1.035110937s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/cloudcredential.openshift.io/v1
    STEP: Read Status for v1alpha1.wardle.example.com 08/30/23 06:41:40.057
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/30/23 06:41:40.09
    STEP: List APIServices 08/30/23 06:41:40.144
    Aug 30 06:41:40.205: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:41:40.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-3348" for this suite. 08/30/23 06:41:41.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:41:41.085
Aug 30 06:41:41.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:41:41.086
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:41.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:41.196
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-e4aec8cf-6239-4ff8-a6b9-8ef48c5bb8a3 08/30/23 06:41:41.21
STEP: Creating a pod to test consume configMaps 08/30/23 06:41:41.225
W0830 06:41:41.300405      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:41:41.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14" in namespace "projected-3385" to be "Succeeded or Failed"
Aug 30 06:41:41.377: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 76.993876ms
Aug 30 06:41:43.421: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120886758s
Aug 30 06:41:45.389: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089084142s
Aug 30 06:41:47.390: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090340176s
STEP: Saw pod success 08/30/23 06:41:47.39
Aug 30 06:41:47.391: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14" satisfied condition "Succeeded or Failed"
Aug 30 06:41:47.418: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/30/23 06:41:47.463
Aug 30 06:41:47.493: INFO: Waiting for pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 to disappear
Aug 30 06:41:47.503: INFO: Pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:41:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3385" for this suite. 08/30/23 06:41:47.517
------------------------------
• [SLOW TEST] [6.477 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:41:41.085
    Aug 30 06:41:41.085: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:41:41.086
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:41.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:41.196
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-e4aec8cf-6239-4ff8-a6b9-8ef48c5bb8a3 08/30/23 06:41:41.21
    STEP: Creating a pod to test consume configMaps 08/30/23 06:41:41.225
    W0830 06:41:41.300405      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:41:41.300: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14" in namespace "projected-3385" to be "Succeeded or Failed"
    Aug 30 06:41:41.377: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 76.993876ms
    Aug 30 06:41:43.421: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120886758s
    Aug 30 06:41:45.389: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.089084142s
    Aug 30 06:41:47.390: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.090340176s
    STEP: Saw pod success 08/30/23 06:41:47.39
    Aug 30 06:41:47.391: INFO: Pod "pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14" satisfied condition "Succeeded or Failed"
    Aug 30 06:41:47.418: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:41:47.463
    Aug 30 06:41:47.493: INFO: Waiting for pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 to disappear
    Aug 30 06:41:47.503: INFO: Pod pod-projected-configmaps-35767d1e-be37-485c-a75a-efef282cde14 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:41:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3385" for this suite. 08/30/23 06:41:47.517
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:41:47.564
Aug 30 06:41:47.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:41:47.565
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:47.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:47.723
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:41:47.852
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:41:48.4
STEP: Deploying the webhook pod 08/30/23 06:41:48.446
STEP: Wait for the deployment to be ready 08/30/23 06:41:48.489
Aug 30 06:41:48.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:41:50.56
STEP: Verifying the service has paired with the endpoint 08/30/23 06:41:50.59
Aug 30 06:41:51.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 30 06:41:51.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/30/23 06:41:52.145
STEP: Creating a custom resource that should be denied by the webhook 08/30/23 06:41:52.22
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/30/23 06:41:54.305
STEP: Updating the custom resource with disallowed data should be denied 08/30/23 06:41:54.359
STEP: Deleting the custom resource should be denied 08/30/23 06:41:54.394
STEP: Remove the offending key and value from the custom resource data 08/30/23 06:41:54.416
STEP: Deleting the updated custom resource should be successful 08/30/23 06:41:54.454
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:41:55.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5980" for this suite. 08/30/23 06:41:55.197
STEP: Destroying namespace "webhook-5980-markers" for this suite. 08/30/23 06:41:55.227
------------------------------
• [SLOW TEST] [7.686 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:41:47.564
    Aug 30 06:41:47.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:41:47.565
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:47.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:47.723
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:41:47.852
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:41:48.4
    STEP: Deploying the webhook pod 08/30/23 06:41:48.446
    STEP: Wait for the deployment to be ready 08/30/23 06:41:48.489
    Aug 30 06:41:48.527: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:41:50.56
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:41:50.59
    Aug 30 06:41:51.590: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 30 06:41:51.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/30/23 06:41:52.145
    STEP: Creating a custom resource that should be denied by the webhook 08/30/23 06:41:52.22
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/30/23 06:41:54.305
    STEP: Updating the custom resource with disallowed data should be denied 08/30/23 06:41:54.359
    STEP: Deleting the custom resource should be denied 08/30/23 06:41:54.394
    STEP: Remove the offending key and value from the custom resource data 08/30/23 06:41:54.416
    STEP: Deleting the updated custom resource should be successful 08/30/23 06:41:54.454
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:41:55.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5980" for this suite. 08/30/23 06:41:55.197
    STEP: Destroying namespace "webhook-5980-markers" for this suite. 08/30/23 06:41:55.227
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:41:55.251
Aug 30 06:41:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename limitrange 08/30/23 06:41:55.252
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:55.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:55.382
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-wjvzw" in namespace "limitrange-5544" 08/30/23 06:41:55.393
STEP: Creating another limitRange in another namespace 08/30/23 06:41:55.453
Aug 30 06:41:55.522: INFO: Namespace "e2e-limitrange-wjvzw-1200" created
Aug 30 06:41:55.522: INFO: Creating LimitRange "e2e-limitrange-wjvzw" in namespace "e2e-limitrange-wjvzw-1200"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-wjvzw" 08/30/23 06:41:55.557
Aug 30 06:41:55.585: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-wjvzw" in "limitrange-5544" namespace 08/30/23 06:41:55.585
Aug 30 06:41:55.614: INFO: LimitRange "e2e-limitrange-wjvzw" has been patched
STEP: Delete LimitRange "e2e-limitrange-wjvzw" by Collection with labelSelector: "e2e-limitrange-wjvzw=patched" 08/30/23 06:41:55.614
STEP: Confirm that the limitRange "e2e-limitrange-wjvzw" has been deleted 08/30/23 06:41:55.769
Aug 30 06:41:55.770: INFO: Requesting list of LimitRange to confirm quantity
Aug 30 06:41:55.780: INFO: Found 0 LimitRange with label "e2e-limitrange-wjvzw=patched"
Aug 30 06:41:55.780: INFO: LimitRange "e2e-limitrange-wjvzw" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-wjvzw" 08/30/23 06:41:55.78
Aug 30 06:41:55.792: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 30 06:41:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5544" for this suite. 08/30/23 06:41:55.809
STEP: Destroying namespace "e2e-limitrange-wjvzw-1200" for this suite. 08/30/23 06:41:55.832
------------------------------
• [0.601 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:41:55.251
    Aug 30 06:41:55.251: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename limitrange 08/30/23 06:41:55.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:55.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:55.382
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-wjvzw" in namespace "limitrange-5544" 08/30/23 06:41:55.393
    STEP: Creating another limitRange in another namespace 08/30/23 06:41:55.453
    Aug 30 06:41:55.522: INFO: Namespace "e2e-limitrange-wjvzw-1200" created
    Aug 30 06:41:55.522: INFO: Creating LimitRange "e2e-limitrange-wjvzw" in namespace "e2e-limitrange-wjvzw-1200"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-wjvzw" 08/30/23 06:41:55.557
    Aug 30 06:41:55.585: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-wjvzw" in "limitrange-5544" namespace 08/30/23 06:41:55.585
    Aug 30 06:41:55.614: INFO: LimitRange "e2e-limitrange-wjvzw" has been patched
    STEP: Delete LimitRange "e2e-limitrange-wjvzw" by Collection with labelSelector: "e2e-limitrange-wjvzw=patched" 08/30/23 06:41:55.614
    STEP: Confirm that the limitRange "e2e-limitrange-wjvzw" has been deleted 08/30/23 06:41:55.769
    Aug 30 06:41:55.770: INFO: Requesting list of LimitRange to confirm quantity
    Aug 30 06:41:55.780: INFO: Found 0 LimitRange with label "e2e-limitrange-wjvzw=patched"
    Aug 30 06:41:55.780: INFO: LimitRange "e2e-limitrange-wjvzw" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-wjvzw" 08/30/23 06:41:55.78
    Aug 30 06:41:55.792: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:41:55.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5544" for this suite. 08/30/23 06:41:55.809
    STEP: Destroying namespace "e2e-limitrange-wjvzw-1200" for this suite. 08/30/23 06:41:55.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:41:55.854
Aug 30 06:41:55.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:41:55.855
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:55.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:55.925
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:41:56.095
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:41:56.428
STEP: Deploying the webhook pod 08/30/23 06:41:56.453
STEP: Wait for the deployment to be ready 08/30/23 06:41:56.498
Aug 30 06:41:56.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 06:41:58.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 06:42:00.574
STEP: Verifying the service has paired with the endpoint 08/30/23 06:42:00.612
Aug 30 06:42:01.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/30/23 06:42:01.623
STEP: create a pod that should be denied by the webhook 08/30/23 06:42:01.681
STEP: create a pod that causes the webhook to hang 08/30/23 06:42:01.719
STEP: create a configmap that should be denied by the webhook 08/30/23 06:42:11.743
STEP: create a configmap that should be admitted by the webhook 08/30/23 06:42:11.775
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/30/23 06:42:11.802
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/30/23 06:42:11.832
STEP: create a namespace that bypass the webhook 08/30/23 06:42:11.853
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/30/23 06:42:11.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:12.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-70" for this suite. 08/30/23 06:42:12.176
STEP: Destroying namespace "webhook-70-markers" for this suite. 08/30/23 06:42:12.2
------------------------------
• [SLOW TEST] [16.368 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:41:55.854
    Aug 30 06:41:55.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:41:55.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:41:55.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:41:55.925
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:41:56.095
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:41:56.428
    STEP: Deploying the webhook pod 08/30/23 06:41:56.453
    STEP: Wait for the deployment to be ready 08/30/23 06:41:56.498
    Aug 30 06:41:56.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 06:41:58.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 41, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 06:42:00.574
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:42:00.612
    Aug 30 06:42:01.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/30/23 06:42:01.623
    STEP: create a pod that should be denied by the webhook 08/30/23 06:42:01.681
    STEP: create a pod that causes the webhook to hang 08/30/23 06:42:01.719
    STEP: create a configmap that should be denied by the webhook 08/30/23 06:42:11.743
    STEP: create a configmap that should be admitted by the webhook 08/30/23 06:42:11.775
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/30/23 06:42:11.802
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/30/23 06:42:11.832
    STEP: create a namespace that bypass the webhook 08/30/23 06:42:11.853
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/30/23 06:42:11.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:12.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-70" for this suite. 08/30/23 06:42:12.176
    STEP: Destroying namespace "webhook-70-markers" for this suite. 08/30/23 06:42:12.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:12.224
Aug 30 06:42:12.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:42:12.226
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:12.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:12.29
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-9255-delete-me 08/30/23 06:42:12.311
STEP: Waiting for the RuntimeClass to disappear 08/30/23 06:42:12.351
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9255" for this suite. 08/30/23 06:42:12.453
------------------------------
• [0.252 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:12.224
    Aug 30 06:42:12.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:42:12.226
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:12.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:12.29
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-9255-delete-me 08/30/23 06:42:12.311
    STEP: Waiting for the RuntimeClass to disappear 08/30/23 06:42:12.351
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:12.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9255" for this suite. 08/30/23 06:42:12.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:12.477
Aug 30 06:42:12.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 06:42:12.478
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:12.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:12.59
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/30/23 06:42:12.598
STEP: setting up watch 08/30/23 06:42:12.598
STEP: submitting the pod to kubernetes 08/30/23 06:42:12.735
STEP: verifying the pod is in kubernetes 08/30/23 06:42:12.799
STEP: verifying pod creation was observed 08/30/23 06:42:12.84
Aug 30 06:42:12.840: INFO: Waiting up to 5m0s for pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94" in namespace "pods-5139" to be "running"
Aug 30 06:42:12.875: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94": Phase="Pending", Reason="", readiness=false. Elapsed: 35.28612ms
Aug 30 06:42:14.886: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94": Phase="Running", Reason="", readiness=true. Elapsed: 2.046066247s
Aug 30 06:42:14.886: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94" satisfied condition "running"
STEP: deleting the pod gracefully 08/30/23 06:42:14.896
STEP: verifying pod deletion was observed 08/30/23 06:42:14.953
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5139" for this suite. 08/30/23 06:42:17.947
------------------------------
• [SLOW TEST] [5.495 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:12.477
    Aug 30 06:42:12.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 06:42:12.478
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:12.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:12.59
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/30/23 06:42:12.598
    STEP: setting up watch 08/30/23 06:42:12.598
    STEP: submitting the pod to kubernetes 08/30/23 06:42:12.735
    STEP: verifying the pod is in kubernetes 08/30/23 06:42:12.799
    STEP: verifying pod creation was observed 08/30/23 06:42:12.84
    Aug 30 06:42:12.840: INFO: Waiting up to 5m0s for pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94" in namespace "pods-5139" to be "running"
    Aug 30 06:42:12.875: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94": Phase="Pending", Reason="", readiness=false. Elapsed: 35.28612ms
    Aug 30 06:42:14.886: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94": Phase="Running", Reason="", readiness=true. Elapsed: 2.046066247s
    Aug 30 06:42:14.886: INFO: Pod "pod-submit-remove-837bcb9a-1f29-4136-9cdf-2b70f6eacf94" satisfied condition "running"
    STEP: deleting the pod gracefully 08/30/23 06:42:14.896
    STEP: verifying pod deletion was observed 08/30/23 06:42:14.953
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:17.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5139" for this suite. 08/30/23 06:42:17.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:17.974
Aug 30 06:42:17.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:42:17.975
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:18.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:18.045
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 30 06:42:18.135: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3703 to be scheduled
Aug 30 06:42:18.157: INFO: 1 pods are not scheduled: [runtimeclass-3703/test-runtimeclass-runtimeclass-3703-preconfigured-handler-scp62(40e580d0-141b-4f8c-9d3d-b491021840dc)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:20.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3703" for this suite. 08/30/23 06:42:20.237
------------------------------
• [2.285 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:17.974
    Aug 30 06:42:17.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:42:17.975
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:18.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:18.045
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 30 06:42:18.135: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3703 to be scheduled
    Aug 30 06:42:18.157: INFO: 1 pods are not scheduled: [runtimeclass-3703/test-runtimeclass-runtimeclass-3703-preconfigured-handler-scp62(40e580d0-141b-4f8c-9d3d-b491021840dc)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:20.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3703" for this suite. 08/30/23 06:42:20.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:20.263
Aug 30 06:42:20.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:42:20.264
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:20.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:20.323
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/30/23 06:42:20.33
Aug 30 06:42:20.331: INFO: Creating e2e-svc-a-wgqnc
Aug 30 06:42:20.378: INFO: Creating e2e-svc-b-j4cv9
Aug 30 06:42:20.465: INFO: Creating e2e-svc-c-52zcm
STEP: deleting service collection 08/30/23 06:42:20.548
Aug 30 06:42:20.639: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:20.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6293" for this suite. 08/30/23 06:42:20.659
------------------------------
• [0.452 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:20.263
    Aug 30 06:42:20.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:42:20.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:20.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:20.323
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/30/23 06:42:20.33
    Aug 30 06:42:20.331: INFO: Creating e2e-svc-a-wgqnc
    Aug 30 06:42:20.378: INFO: Creating e2e-svc-b-j4cv9
    Aug 30 06:42:20.465: INFO: Creating e2e-svc-c-52zcm
    STEP: deleting service collection 08/30/23 06:42:20.548
    Aug 30 06:42:20.639: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:20.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6293" for this suite. 08/30/23 06:42:20.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:20.72
Aug 30 06:42:20.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:42:20.721
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:20.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:20.778
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-c7e7c1d4-2ad1-4468-b881-26dcf0cac19d 08/30/23 06:42:20.787
STEP: Creating a pod to test consume configMaps 08/30/23 06:42:20.807
Aug 30 06:42:20.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678" in namespace "configmap-1968" to be "Succeeded or Failed"
Aug 30 06:42:20.905: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Pending", Reason="", readiness=false. Elapsed: 37.810135ms
Aug 30 06:42:22.919: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051136971s
Aug 30 06:42:24.917: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049167895s
STEP: Saw pod success 08/30/23 06:42:24.917
Aug 30 06:42:24.917: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678" satisfied condition "Succeeded or Failed"
Aug 30 06:42:24.930: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 06:42:24.95
Aug 30 06:42:24.989: INFO: Waiting for pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 to disappear
Aug 30 06:42:25.002: INFO: Pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1968" for this suite. 08/30/23 06:42:25.017
------------------------------
• [4.320 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:20.72
    Aug 30 06:42:20.720: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:42:20.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:20.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:20.778
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-c7e7c1d4-2ad1-4468-b881-26dcf0cac19d 08/30/23 06:42:20.787
    STEP: Creating a pod to test consume configMaps 08/30/23 06:42:20.807
    Aug 30 06:42:20.868: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678" in namespace "configmap-1968" to be "Succeeded or Failed"
    Aug 30 06:42:20.905: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Pending", Reason="", readiness=false. Elapsed: 37.810135ms
    Aug 30 06:42:22.919: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051136971s
    Aug 30 06:42:24.917: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049167895s
    STEP: Saw pod success 08/30/23 06:42:24.917
    Aug 30 06:42:24.917: INFO: Pod "pod-configmaps-fb429590-c171-4499-9513-998122be5678" satisfied condition "Succeeded or Failed"
    Aug 30 06:42:24.930: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 06:42:24.95
    Aug 30 06:42:24.989: INFO: Waiting for pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 to disappear
    Aug 30 06:42:25.002: INFO: Pod pod-configmaps-fb429590-c171-4499-9513-998122be5678 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:25.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1968" for this suite. 08/30/23 06:42:25.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:25.043
Aug 30 06:42:25.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:42:25.044
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:25.103
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:25.109
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 30 06:42:25.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 06:42:31.43
Aug 30 06:42:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 create -f -'
Aug 30 06:42:34.915: INFO: stderr: ""
Aug 30 06:42:34.915: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 06:42:34.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 delete e2e-test-crd-publish-openapi-1104-crds test-cr'
Aug 30 06:42:35.030: INFO: stderr: ""
Aug 30 06:42:35.030: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 30 06:42:35.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 apply -f -'
Aug 30 06:42:35.592: INFO: stderr: ""
Aug 30 06:42:35.592: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 30 06:42:35.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 delete e2e-test-crd-publish-openapi-1104-crds test-cr'
Aug 30 06:42:35.727: INFO: stderr: ""
Aug 30 06:42:35.727: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/30/23 06:42:35.727
Aug 30 06:42:35.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 explain e2e-test-crd-publish-openapi-1104-crds'
Aug 30 06:42:36.232: INFO: stderr: ""
Aug 30 06:42:36.232: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1104-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:42.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8330" for this suite. 08/30/23 06:42:42.675
------------------------------
• [SLOW TEST] [17.660 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:25.043
    Aug 30 06:42:25.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:42:25.044
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:25.103
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:25.109
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 30 06:42:25.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 06:42:31.43
    Aug 30 06:42:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 create -f -'
    Aug 30 06:42:34.915: INFO: stderr: ""
    Aug 30 06:42:34.915: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 30 06:42:34.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 delete e2e-test-crd-publish-openapi-1104-crds test-cr'
    Aug 30 06:42:35.030: INFO: stderr: ""
    Aug 30 06:42:35.030: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 30 06:42:35.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 apply -f -'
    Aug 30 06:42:35.592: INFO: stderr: ""
    Aug 30 06:42:35.592: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 30 06:42:35.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 --namespace=crd-publish-openapi-8330 delete e2e-test-crd-publish-openapi-1104-crds test-cr'
    Aug 30 06:42:35.727: INFO: stderr: ""
    Aug 30 06:42:35.727: INFO: stdout: "e2e-test-crd-publish-openapi-1104-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/30/23 06:42:35.727
    Aug 30 06:42:35.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-8330 explain e2e-test-crd-publish-openapi-1104-crds'
    Aug 30 06:42:36.232: INFO: stderr: ""
    Aug 30 06:42:36.232: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1104-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:42.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8330" for this suite. 08/30/23 06:42:42.675
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:42.717
Aug 30 06:42:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename watch 08/30/23 06:42:42.719
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:42.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:42.806
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/30/23 06:42:42.827
STEP: creating a new configmap 08/30/23 06:42:42.832
STEP: modifying the configmap once 08/30/23 06:42:42.908
STEP: closing the watch once it receives two notifications 08/30/23 06:42:43.019
Aug 30 06:42:43.019: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83087 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:42:43.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83097 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/30/23 06:42:43.02
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/30/23 06:42:43.092
STEP: deleting the configmap 08/30/23 06:42:43.183
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/30/23 06:42:43.225
Aug 30 06:42:43.232: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83100 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:42:43.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83102 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:43.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-4525" for this suite. 08/30/23 06:42:43.259
------------------------------
• [0.595 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:42.717
    Aug 30 06:42:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename watch 08/30/23 06:42:42.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:42.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:42.806
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/30/23 06:42:42.827
    STEP: creating a new configmap 08/30/23 06:42:42.832
    STEP: modifying the configmap once 08/30/23 06:42:42.908
    STEP: closing the watch once it receives two notifications 08/30/23 06:42:43.019
    Aug 30 06:42:43.019: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83087 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:42:43.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83097 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/30/23 06:42:43.02
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/30/23 06:42:43.092
    STEP: deleting the configmap 08/30/23 06:42:43.183
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/30/23 06:42:43.225
    Aug 30 06:42:43.232: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83100 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:42:43.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-4525  77a91e98-30f7-4cf6-8afb-e8cdd38c303b 83102 0 2023-08-30 06:42:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-30 06:42:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:43.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-4525" for this suite. 08/30/23 06:42:43.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:43.314
Aug 30 06:42:43.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:42:43.315
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:43.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:43.397
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-e2641769-eaf1-4139-acce-cad5cc595df7 08/30/23 06:42:43.409
STEP: Creating a pod to test consume secrets 08/30/23 06:42:43.458
Aug 30 06:42:43.546: INFO: Waiting up to 5m0s for pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644" in namespace "secrets-6131" to be "Succeeded or Failed"
Aug 30 06:42:43.576: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Pending", Reason="", readiness=false. Elapsed: 29.749801ms
Aug 30 06:42:45.593: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046060061s
Aug 30 06:42:47.596: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049105952s
STEP: Saw pod success 08/30/23 06:42:47.596
Aug 30 06:42:47.596: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644" satisfied condition "Succeeded or Failed"
Aug 30 06:42:47.609: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:42:47.77
Aug 30 06:42:47.816: INFO: Waiting for pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 to disappear
Aug 30 06:42:47.825: INFO: Pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:47.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6131" for this suite. 08/30/23 06:42:47.871
------------------------------
• [4.588 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:43.314
    Aug 30 06:42:43.314: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:42:43.315
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:43.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:43.397
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-e2641769-eaf1-4139-acce-cad5cc595df7 08/30/23 06:42:43.409
    STEP: Creating a pod to test consume secrets 08/30/23 06:42:43.458
    Aug 30 06:42:43.546: INFO: Waiting up to 5m0s for pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644" in namespace "secrets-6131" to be "Succeeded or Failed"
    Aug 30 06:42:43.576: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Pending", Reason="", readiness=false. Elapsed: 29.749801ms
    Aug 30 06:42:45.593: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046060061s
    Aug 30 06:42:47.596: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049105952s
    STEP: Saw pod success 08/30/23 06:42:47.596
    Aug 30 06:42:47.596: INFO: Pod "pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644" satisfied condition "Succeeded or Failed"
    Aug 30 06:42:47.609: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:42:47.77
    Aug 30 06:42:47.816: INFO: Waiting for pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 to disappear
    Aug 30 06:42:47.825: INFO: Pod pod-secrets-a922ef5f-0834-42d6-810d-ecba7e880644 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:47.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6131" for this suite. 08/30/23 06:42:47.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:47.94
Aug 30 06:42:47.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 06:42:47.942
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:48.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:48.039
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 30 06:42:48.174: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 06:42:53.185: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 06:42:53.185
STEP: Scaling up "test-rs" replicaset  08/30/23 06:42:53.185
Aug 30 06:42:53.214: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/30/23 06:42:53.214
W0830 06:42:53.232726      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 30 06:42:53.237: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 06:42:53.278: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 06:42:53.317: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 06:42:53.336: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
Aug 30 06:42:55.088: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 2, AvailableReplicas 2
Aug 30 06:42:55.910: INFO: observed Replicaset test-rs in namespace replicaset-1285 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:42:55.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1285" for this suite. 08/30/23 06:42:55.926
------------------------------
• [SLOW TEST] [8.006 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:47.94
    Aug 30 06:42:47.940: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 06:42:47.942
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:48.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:48.039
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 30 06:42:48.174: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 30 06:42:53.185: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 06:42:53.185
    STEP: Scaling up "test-rs" replicaset  08/30/23 06:42:53.185
    Aug 30 06:42:53.214: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/30/23 06:42:53.214
    W0830 06:42:53.232726      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 30 06:42:53.237: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
    Aug 30 06:42:53.278: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
    Aug 30 06:42:53.317: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
    Aug 30 06:42:53.336: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 1, AvailableReplicas 1
    Aug 30 06:42:55.088: INFO: observed ReplicaSet test-rs in namespace replicaset-1285 with ReadyReplicas 2, AvailableReplicas 2
    Aug 30 06:42:55.910: INFO: observed Replicaset test-rs in namespace replicaset-1285 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:42:55.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1285" for this suite. 08/30/23 06:42:55.926
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:42:55.947
Aug 30 06:42:55.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 06:42:55.948
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:55.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:56
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/30/23 06:42:56.027
W0830 06:42:56.045789      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: create the rc2 08/30/23 06:42:56.045
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/30/23 06:43:01.098
STEP: delete the rc simpletest-rc-to-be-deleted 08/30/23 06:43:03.463
STEP: wait for the rc to be deleted 08/30/23 06:43:03.531
STEP: Gathering metrics 08/30/23 06:43:08.604
W0830 06:43:08.688366      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 06:43:08.688: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 30 06:43:08.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-22hgx" in namespace "gc-8889"
Aug 30 06:43:08.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-25kfn" in namespace "gc-8889"
Aug 30 06:43:08.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qs7m" in namespace "gc-8889"
Aug 30 06:43:08.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vvpk" in namespace "gc-8889"
Aug 30 06:43:08.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xnmj" in namespace "gc-8889"
Aug 30 06:43:08.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-447mp" in namespace "gc-8889"
Aug 30 06:43:08.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hlmv" in namespace "gc-8889"
Aug 30 06:43:09.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-554h8" in namespace "gc-8889"
Aug 30 06:43:09.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jv95" in namespace "gc-8889"
Aug 30 06:43:09.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kn7z" in namespace "gc-8889"
Aug 30 06:43:09.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nz8s" in namespace "gc-8889"
Aug 30 06:43:09.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-694fd" in namespace "gc-8889"
Aug 30 06:43:09.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bn2q" in namespace "gc-8889"
Aug 30 06:43:09.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-75w45" in namespace "gc-8889"
Aug 30 06:43:09.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-772c6" in namespace "gc-8889"
Aug 30 06:43:10.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dr7s" in namespace "gc-8889"
Aug 30 06:43:10.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gx74" in namespace "gc-8889"
Aug 30 06:43:10.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sgdp" in namespace "gc-8889"
Aug 30 06:43:10.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-82xf2" in namespace "gc-8889"
Aug 30 06:43:10.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g24p" in namespace "gc-8889"
Aug 30 06:43:10.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ht89" in namespace "gc-8889"
Aug 30 06:43:10.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thq9" in namespace "gc-8889"
Aug 30 06:43:10.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-98n95" in namespace "gc-8889"
Aug 30 06:43:10.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jg79" in namespace "gc-8889"
Aug 30 06:43:10.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mm6m" in namespace "gc-8889"
Aug 30 06:43:10.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z7h8" in namespace "gc-8889"
Aug 30 06:43:11.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd9dn" in namespace "gc-8889"
Aug 30 06:43:11.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxzqt" in namespace "gc-8889"
Aug 30 06:43:11.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj6js" in namespace "gc-8889"
Aug 30 06:43:11.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr6xm" in namespace "gc-8889"
Aug 30 06:43:11.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-czrnb" in namespace "gc-8889"
Aug 30 06:43:11.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5nb9" in namespace "gc-8889"
Aug 30 06:43:11.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9wg2" in namespace "gc-8889"
Aug 30 06:43:11.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd4br" in namespace "gc-8889"
Aug 30 06:43:11.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6cx" in namespace "gc-8889"
Aug 30 06:43:11.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzc8g" in namespace "gc-8889"
Aug 30 06:43:11.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg7zs" in namespace "gc-8889"
Aug 30 06:43:11.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkv4d" in namespace "gc-8889"
Aug 30 06:43:11.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxhv8" in namespace "gc-8889"
Aug 30 06:43:11.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzf88" in namespace "gc-8889"
Aug 30 06:43:11.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4kjt" in namespace "gc-8889"
Aug 30 06:43:11.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5pf2" in namespace "gc-8889"
Aug 30 06:43:12.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-g97mn" in namespace "gc-8889"
Aug 30 06:43:12.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-gklrg" in namespace "gc-8889"
Aug 30 06:43:12.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmcdq" in namespace "gc-8889"
Aug 30 06:43:12.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmr7s" in namespace "gc-8889"
Aug 30 06:43:12.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgb8" in namespace "gc-8889"
Aug 30 06:43:12.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgwsj" in namespace "gc-8889"
Aug 30 06:43:12.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlj94" in namespace "gc-8889"
Aug 30 06:43:12.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsdmh" in namespace "gc-8889"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:12.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8889" for this suite. 08/30/23 06:43:12.452
------------------------------
• [SLOW TEST] [16.548 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:42:55.947
    Aug 30 06:42:55.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 06:42:55.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:42:55.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:42:56
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/30/23 06:42:56.027
    W0830 06:42:56.045789      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: create the rc2 08/30/23 06:42:56.045
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/30/23 06:43:01.098
    STEP: delete the rc simpletest-rc-to-be-deleted 08/30/23 06:43:03.463
    STEP: wait for the rc to be deleted 08/30/23 06:43:03.531
    STEP: Gathering metrics 08/30/23 06:43:08.604
    W0830 06:43:08.688366      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 06:43:08.688: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 30 06:43:08.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-22hgx" in namespace "gc-8889"
    Aug 30 06:43:08.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-25kfn" in namespace "gc-8889"
    Aug 30 06:43:08.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qs7m" in namespace "gc-8889"
    Aug 30 06:43:08.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vvpk" in namespace "gc-8889"
    Aug 30 06:43:08.855: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xnmj" in namespace "gc-8889"
    Aug 30 06:43:08.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-447mp" in namespace "gc-8889"
    Aug 30 06:43:08.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hlmv" in namespace "gc-8889"
    Aug 30 06:43:09.053: INFO: Deleting pod "simpletest-rc-to-be-deleted-554h8" in namespace "gc-8889"
    Aug 30 06:43:09.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jv95" in namespace "gc-8889"
    Aug 30 06:43:09.469: INFO: Deleting pod "simpletest-rc-to-be-deleted-5kn7z" in namespace "gc-8889"
    Aug 30 06:43:09.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nz8s" in namespace "gc-8889"
    Aug 30 06:43:09.791: INFO: Deleting pod "simpletest-rc-to-be-deleted-694fd" in namespace "gc-8889"
    Aug 30 06:43:09.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bn2q" in namespace "gc-8889"
    Aug 30 06:43:09.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-75w45" in namespace "gc-8889"
    Aug 30 06:43:09.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-772c6" in namespace "gc-8889"
    Aug 30 06:43:10.044: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dr7s" in namespace "gc-8889"
    Aug 30 06:43:10.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gx74" in namespace "gc-8889"
    Aug 30 06:43:10.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sgdp" in namespace "gc-8889"
    Aug 30 06:43:10.176: INFO: Deleting pod "simpletest-rc-to-be-deleted-82xf2" in namespace "gc-8889"
    Aug 30 06:43:10.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-8g24p" in namespace "gc-8889"
    Aug 30 06:43:10.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ht89" in namespace "gc-8889"
    Aug 30 06:43:10.489: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thq9" in namespace "gc-8889"
    Aug 30 06:43:10.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-98n95" in namespace "gc-8889"
    Aug 30 06:43:10.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jg79" in namespace "gc-8889"
    Aug 30 06:43:10.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mm6m" in namespace "gc-8889"
    Aug 30 06:43:10.935: INFO: Deleting pod "simpletest-rc-to-be-deleted-9z7h8" in namespace "gc-8889"
    Aug 30 06:43:11.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-bd9dn" in namespace "gc-8889"
    Aug 30 06:43:11.121: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxzqt" in namespace "gc-8889"
    Aug 30 06:43:11.222: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj6js" in namespace "gc-8889"
    Aug 30 06:43:11.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr6xm" in namespace "gc-8889"
    Aug 30 06:43:11.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-czrnb" in namespace "gc-8889"
    Aug 30 06:43:11.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5nb9" in namespace "gc-8889"
    Aug 30 06:43:11.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9wg2" in namespace "gc-8889"
    Aug 30 06:43:11.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd4br" in namespace "gc-8889"
    Aug 30 06:43:11.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh6cx" in namespace "gc-8889"
    Aug 30 06:43:11.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzc8g" in namespace "gc-8889"
    Aug 30 06:43:11.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg7zs" in namespace "gc-8889"
    Aug 30 06:43:11.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkv4d" in namespace "gc-8889"
    Aug 30 06:43:11.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxhv8" in namespace "gc-8889"
    Aug 30 06:43:11.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzf88" in namespace "gc-8889"
    Aug 30 06:43:11.926: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4kjt" in namespace "gc-8889"
    Aug 30 06:43:11.984: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5pf2" in namespace "gc-8889"
    Aug 30 06:43:12.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-g97mn" in namespace "gc-8889"
    Aug 30 06:43:12.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-gklrg" in namespace "gc-8889"
    Aug 30 06:43:12.154: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmcdq" in namespace "gc-8889"
    Aug 30 06:43:12.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-gmr7s" in namespace "gc-8889"
    Aug 30 06:43:12.228: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdgb8" in namespace "gc-8889"
    Aug 30 06:43:12.264: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgwsj" in namespace "gc-8889"
    Aug 30 06:43:12.329: INFO: Deleting pod "simpletest-rc-to-be-deleted-hlj94" in namespace "gc-8889"
    Aug 30 06:43:12.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsdmh" in namespace "gc-8889"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:12.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8889" for this suite. 08/30/23 06:43:12.452
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:12.496
Aug 30 06:43:12.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:43:12.499
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:12.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:12.744
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/30/23 06:43:12.843
Aug 30 06:43:12.910: INFO: Waiting up to 5m0s for pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c" in namespace "emptydir-5870" to be "Succeeded or Failed"
Aug 30 06:43:12.923: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.728079ms
Aug 30 06:43:14.939: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028839129s
Aug 30 06:43:16.932: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021947058s
Aug 30 06:43:18.938: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027699962s
STEP: Saw pod success 08/30/23 06:43:18.938
Aug 30 06:43:18.938: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c" satisfied condition "Succeeded or Failed"
Aug 30 06:43:18.952: INFO: Trying to get logs from node 10.135.139.190 pod pod-52e02507-aae6-466c-9b62-a98b91b9566c container test-container: <nil>
STEP: delete the pod 08/30/23 06:43:18.988
Aug 30 06:43:19.140: INFO: Waiting for pod pod-52e02507-aae6-466c-9b62-a98b91b9566c to disappear
Aug 30 06:43:19.156: INFO: Pod pod-52e02507-aae6-466c-9b62-a98b91b9566c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:19.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5870" for this suite. 08/30/23 06:43:19.178
------------------------------
• [SLOW TEST] [6.743 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:12.496
    Aug 30 06:43:12.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:43:12.499
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:12.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:12.744
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/30/23 06:43:12.843
    Aug 30 06:43:12.910: INFO: Waiting up to 5m0s for pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c" in namespace "emptydir-5870" to be "Succeeded or Failed"
    Aug 30 06:43:12.923: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.728079ms
    Aug 30 06:43:14.939: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028839129s
    Aug 30 06:43:16.932: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021947058s
    Aug 30 06:43:18.938: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027699962s
    STEP: Saw pod success 08/30/23 06:43:18.938
    Aug 30 06:43:18.938: INFO: Pod "pod-52e02507-aae6-466c-9b62-a98b91b9566c" satisfied condition "Succeeded or Failed"
    Aug 30 06:43:18.952: INFO: Trying to get logs from node 10.135.139.190 pod pod-52e02507-aae6-466c-9b62-a98b91b9566c container test-container: <nil>
    STEP: delete the pod 08/30/23 06:43:18.988
    Aug 30 06:43:19.140: INFO: Waiting for pod pod-52e02507-aae6-466c-9b62-a98b91b9566c to disappear
    Aug 30 06:43:19.156: INFO: Pod pod-52e02507-aae6-466c-9b62-a98b91b9566c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:19.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5870" for this suite. 08/30/23 06:43:19.178
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:19.239
Aug 30 06:43:19.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 06:43:19.241
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:19.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:19.435
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/30/23 06:43:19.445
W0830 06:43:20.465335      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:43:20.495: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 06:43:25.507: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 06:43:25.507
STEP: getting scale subresource 08/30/23 06:43:25.508
STEP: updating a scale subresource 08/30/23 06:43:25.521
STEP: verifying the replicaset Spec.Replicas was modified 08/30/23 06:43:25.539
STEP: Patch a scale subresource 08/30/23 06:43:25.555
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:25.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1914" for this suite. 08/30/23 06:43:25.65
------------------------------
• [SLOW TEST] [6.437 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:19.239
    Aug 30 06:43:19.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 06:43:19.241
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:19.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:19.435
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/30/23 06:43:19.445
    W0830 06:43:20.465335      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:43:20.495: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 30 06:43:25.507: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 06:43:25.507
    STEP: getting scale subresource 08/30/23 06:43:25.508
    STEP: updating a scale subresource 08/30/23 06:43:25.521
    STEP: verifying the replicaset Spec.Replicas was modified 08/30/23 06:43:25.539
    STEP: Patch a scale subresource 08/30/23 06:43:25.555
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:25.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1914" for this suite. 08/30/23 06:43:25.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:25.678
Aug 30 06:43:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:43:25.679
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:25.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:25.738
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/30/23 06:43:25.746
Aug 30 06:43:25.812: INFO: Waiting up to 5m0s for pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba" in namespace "emptydir-6930" to be "Succeeded or Failed"
Aug 30 06:43:25.829: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.44233ms
Aug 30 06:43:27.839: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027120352s
Aug 30 06:43:29.842: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030026611s
STEP: Saw pod success 08/30/23 06:43:29.842
Aug 30 06:43:29.844: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba" satisfied condition "Succeeded or Failed"
Aug 30 06:43:29.856: INFO: Trying to get logs from node 10.135.139.190 pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba container test-container: <nil>
STEP: delete the pod 08/30/23 06:43:29.887
Aug 30 06:43:29.947: INFO: Waiting for pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba to disappear
Aug 30 06:43:29.961: INFO: Pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:29.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6930" for this suite. 08/30/23 06:43:29.974
------------------------------
• [4.316 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:25.678
    Aug 30 06:43:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:43:25.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:25.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:25.738
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/30/23 06:43:25.746
    Aug 30 06:43:25.812: INFO: Waiting up to 5m0s for pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba" in namespace "emptydir-6930" to be "Succeeded or Failed"
    Aug 30 06:43:25.829: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.44233ms
    Aug 30 06:43:27.839: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027120352s
    Aug 30 06:43:29.842: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030026611s
    STEP: Saw pod success 08/30/23 06:43:29.842
    Aug 30 06:43:29.844: INFO: Pod "pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba" satisfied condition "Succeeded or Failed"
    Aug 30 06:43:29.856: INFO: Trying to get logs from node 10.135.139.190 pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba container test-container: <nil>
    STEP: delete the pod 08/30/23 06:43:29.887
    Aug 30 06:43:29.947: INFO: Waiting for pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba to disappear
    Aug 30 06:43:29.961: INFO: Pod pod-ba6499bd-470a-4032-ada4-d535b3e4c6ba no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:29.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6930" for this suite. 08/30/23 06:43:29.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:30
Aug 30 06:43:30.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 06:43:30.001
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:30.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:30.05
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 08/30/23 06:43:30.166
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:43:30.212
Aug 30 06:43:30.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:43:30.252: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:43:31.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:43:31.295: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:43:32.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 06:43:32.290: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 06:43:33.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 06:43:33.307: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/30/23 06:43:33.317
Aug 30 06:43:33.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 06:43:33.370: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 06:43:34.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 06:43:34.397: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 06:43:35.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 06:43:35.402: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 06:43:36.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 06:43:36.397: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 06:43:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 06:43:37.397: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 06:43:37.407
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8375, will wait for the garbage collector to delete the pods 08/30/23 06:43:37.407
Aug 30 06:43:37.484: INFO: Deleting DaemonSet.extensions daemon-set took: 17.496629ms
Aug 30 06:43:37.585: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.024704ms
Aug 30 06:43:41.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 06:43:41.398: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 06:43:41.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85993"},"items":null}

Aug 30 06:43:41.416: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85993"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8375" for this suite. 08/30/23 06:43:41.475
------------------------------
• [SLOW TEST] [11.494 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:30
    Aug 30 06:43:30.000: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 06:43:30.001
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:30.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:30.05
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 08/30/23 06:43:30.166
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 06:43:30.212
    Aug 30 06:43:30.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:43:30.252: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:43:31.295: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:43:31.295: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:43:32.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 30 06:43:32.290: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 06:43:33.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 06:43:33.307: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/30/23 06:43:33.317
    Aug 30 06:43:33.370: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 06:43:33.370: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 06:43:34.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 06:43:34.397: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 06:43:35.402: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 06:43:35.402: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 06:43:36.397: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 06:43:36.397: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 06:43:37.396: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 06:43:37.397: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 06:43:37.407
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8375, will wait for the garbage collector to delete the pods 08/30/23 06:43:37.407
    Aug 30 06:43:37.484: INFO: Deleting DaemonSet.extensions daemon-set took: 17.496629ms
    Aug 30 06:43:37.585: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.024704ms
    Aug 30 06:43:41.398: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 06:43:41.398: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 06:43:41.407: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"85993"},"items":null}

    Aug 30 06:43:41.416: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"85993"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8375" for this suite. 08/30/23 06:43:41.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:41.496
Aug 30 06:43:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 06:43:41.497
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:41.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:41.587
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/30/23 06:43:41.597
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:41.668
STEP: Creating a service in the namespace 08/30/23 06:43:41.677
STEP: Deleting the namespace 08/30/23 06:43:41.719
STEP: Waiting for the namespace to be removed. 08/30/23 06:43:41.752
STEP: Recreating the namespace 08/30/23 06:43:49.78
STEP: Verifying there is no service in the namespace 08/30/23 06:43:49.84
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:43:49.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7049" for this suite. 08/30/23 06:43:49.885
STEP: Destroying namespace "nsdeletetest-2186" for this suite. 08/30/23 06:43:49.908
Aug 30 06:43:49.939: INFO: Namespace nsdeletetest-2186 was already deleted
STEP: Destroying namespace "nsdeletetest-1881" for this suite. 08/30/23 06:43:49.94
------------------------------
• [SLOW TEST] [8.479 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:41.496
    Aug 30 06:43:41.496: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 06:43:41.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:41.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:41.587
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/30/23 06:43:41.597
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:41.668
    STEP: Creating a service in the namespace 08/30/23 06:43:41.677
    STEP: Deleting the namespace 08/30/23 06:43:41.719
    STEP: Waiting for the namespace to be removed. 08/30/23 06:43:41.752
    STEP: Recreating the namespace 08/30/23 06:43:49.78
    STEP: Verifying there is no service in the namespace 08/30/23 06:43:49.84
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:43:49.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7049" for this suite. 08/30/23 06:43:49.885
    STEP: Destroying namespace "nsdeletetest-2186" for this suite. 08/30/23 06:43:49.908
    Aug 30 06:43:49.939: INFO: Namespace nsdeletetest-2186 was already deleted
    STEP: Destroying namespace "nsdeletetest-1881" for this suite. 08/30/23 06:43:49.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:43:49.975
Aug 30 06:43:49.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 06:43:49.977
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:50.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:50.038
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f 08/30/23 06:43:50.051
W0830 06:43:51.080509      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:43:51.117: INFO: Pod name my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Found 0 pods out of 1
Aug 30 06:43:56.128: INFO: Pod name my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Found 1 pods out of 1
Aug 30 06:43:56.128: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" are running
Aug 30 06:43:56.128: INFO: Waiting up to 5m0s for pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" in namespace "replication-controller-6133" to be "running"
Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z": Phase="Running", Reason="", readiness=true. Elapsed: 11.970006ms
Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" satisfied condition "running"
Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:51 +0000 UTC Reason: Message:}])
Aug 30 06:43:56.140: INFO: Trying to dial the pod
Aug 30 06:44:01.185: INFO: Controller my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Got expected result from replica 1 [my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z]: "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:01.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6133" for this suite. 08/30/23 06:44:01.201
------------------------------
• [SLOW TEST] [11.265 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:43:49.975
    Aug 30 06:43:49.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 06:43:49.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:43:50.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:43:50.038
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f 08/30/23 06:43:50.051
    W0830 06:43:51.080509      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:43:51.117: INFO: Pod name my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Found 0 pods out of 1
    Aug 30 06:43:56.128: INFO: Pod name my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Found 1 pods out of 1
    Aug 30 06:43:56.128: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f" are running
    Aug 30 06:43:56.128: INFO: Waiting up to 5m0s for pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" in namespace "replication-controller-6133" to be "running"
    Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z": Phase="Running", Reason="", readiness=true. Elapsed: 11.970006ms
    Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" satisfied condition "running"
    Aug 30 06:43:56.140: INFO: Pod "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 06:43:51 +0000 UTC Reason: Message:}])
    Aug 30 06:43:56.140: INFO: Trying to dial the pod
    Aug 30 06:44:01.185: INFO: Controller my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f: Got expected result from replica 1 [my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z]: "my-hostname-basic-7dac8d12-1aa9-4749-9def-0c697bd65a5f-82l4z", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:01.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6133" for this suite. 08/30/23 06:44:01.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:01.241
Aug 30 06:44:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 06:44:01.242
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:01.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:01.296
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/30/23 06:44:01.304
STEP: submitting the pod to kubernetes 08/30/23 06:44:01.304
Aug 30 06:44:01.357: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" in namespace "pods-4667" to be "running and ready"
Aug 30 06:44:01.377: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.433402ms
Aug 30 06:44:01.377: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:44:03.387: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030261931s
Aug 30 06:44:03.387: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:44:05.386: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=true. Elapsed: 4.029283875s
Aug 30 06:44:05.387: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Running (Ready = true)
Aug 30 06:44:05.387: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/30/23 06:44:05.396
STEP: updating the pod 08/30/23 06:44:05.407
Aug 30 06:44:05.947: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e"
Aug 30 06:44:05.947: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" in namespace "pods-4667" to be "terminated with reason DeadlineExceeded"
Aug 30 06:44:05.958: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=true. Elapsed: 10.509763ms
Aug 30 06:44:07.969: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=false. Elapsed: 2.021444858s
Aug 30 06:44:09.971: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.023293908s
Aug 30 06:44:09.971: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4667" for this suite. 08/30/23 06:44:09.986
------------------------------
• [SLOW TEST] [8.762 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:01.241
    Aug 30 06:44:01.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 06:44:01.242
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:01.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:01.296
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/30/23 06:44:01.304
    STEP: submitting the pod to kubernetes 08/30/23 06:44:01.304
    Aug 30 06:44:01.357: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" in namespace "pods-4667" to be "running and ready"
    Aug 30 06:44:01.377: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.433402ms
    Aug 30 06:44:01.377: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:44:03.387: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030261931s
    Aug 30 06:44:03.387: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:44:05.386: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=true. Elapsed: 4.029283875s
    Aug 30 06:44:05.387: INFO: The phase of Pod pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e is Running (Ready = true)
    Aug 30 06:44:05.387: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/30/23 06:44:05.396
    STEP: updating the pod 08/30/23 06:44:05.407
    Aug 30 06:44:05.947: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e"
    Aug 30 06:44:05.947: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" in namespace "pods-4667" to be "terminated with reason DeadlineExceeded"
    Aug 30 06:44:05.958: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=true. Elapsed: 10.509763ms
    Aug 30 06:44:07.969: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Running", Reason="", readiness=false. Elapsed: 2.021444858s
    Aug 30 06:44:09.971: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.023293908s
    Aug 30 06:44:09.971: INFO: Pod "pod-update-activedeadlineseconds-a59ab732-e88e-4d2b-b06a-1ff88e92593e" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:09.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4667" for this suite. 08/30/23 06:44:09.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:10.006
Aug 30 06:44:10.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:44:10.007
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:10.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:10.081
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:44:10.143
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:44:10.587
STEP: Deploying the webhook pod 08/30/23 06:44:10.633
STEP: Wait for the deployment to be ready 08/30/23 06:44:10.671
Aug 30 06:44:10.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:44:12.729
STEP: Verifying the service has paired with the endpoint 08/30/23 06:44:12.768
Aug 30 06:44:13.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/30/23 06:44:13.975
STEP: Creating a configMap that should be mutated 08/30/23 06:44:14.026
STEP: Deleting the collection of validation webhooks 08/30/23 06:44:14.128
STEP: Creating a configMap that should not be mutated 08/30/23 06:44:14.272
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:14.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-813" for this suite. 08/30/23 06:44:14.564
STEP: Destroying namespace "webhook-813-markers" for this suite. 08/30/23 06:44:14.587
------------------------------
• [4.602 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:10.006
    Aug 30 06:44:10.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:44:10.007
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:10.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:10.081
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:44:10.143
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:44:10.587
    STEP: Deploying the webhook pod 08/30/23 06:44:10.633
    STEP: Wait for the deployment to be ready 08/30/23 06:44:10.671
    Aug 30 06:44:10.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:44:12.729
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:44:12.768
    Aug 30 06:44:13.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/30/23 06:44:13.975
    STEP: Creating a configMap that should be mutated 08/30/23 06:44:14.026
    STEP: Deleting the collection of validation webhooks 08/30/23 06:44:14.128
    STEP: Creating a configMap that should not be mutated 08/30/23 06:44:14.272
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:14.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-813" for this suite. 08/30/23 06:44:14.564
    STEP: Destroying namespace "webhook-813-markers" for this suite. 08/30/23 06:44:14.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:14.611
Aug 30 06:44:14.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename events 08/30/23 06:44:14.612
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:14.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:14.672
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/30/23 06:44:14.681
STEP: get a list of Events with a label in the current namespace 08/30/23 06:44:14.736
STEP: delete a list of events 08/30/23 06:44:14.752
Aug 30 06:44:14.752: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/30/23 06:44:14.824
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:14.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1243" for this suite. 08/30/23 06:44:14.85
------------------------------
• [0.255 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:14.611
    Aug 30 06:44:14.611: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename events 08/30/23 06:44:14.612
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:14.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:14.672
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/30/23 06:44:14.681
    STEP: get a list of Events with a label in the current namespace 08/30/23 06:44:14.736
    STEP: delete a list of events 08/30/23 06:44:14.752
    Aug 30 06:44:14.752: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/30/23 06:44:14.824
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:14.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1243" for this suite. 08/30/23 06:44:14.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:14.869
Aug 30 06:44:14.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename subpath 08/30/23 06:44:14.87
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:14.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:14.927
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/30/23 06:44:14.935
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-95mc 08/30/23 06:44:14.963
STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:44:14.963
Aug 30 06:44:14.996: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-95mc" in namespace "subpath-6343" to be "Succeeded or Failed"
Aug 30 06:44:15.021: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.284356ms
Aug 30 06:44:17.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.035352726s
Aug 30 06:44:19.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 4.037189007s
Aug 30 06:44:21.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 6.035322661s
Aug 30 06:44:23.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 8.036159456s
Aug 30 06:44:25.035: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 10.038822336s
Aug 30 06:44:27.035: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 12.038526667s
Aug 30 06:44:29.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 14.036058704s
Aug 30 06:44:31.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 16.037001165s
Aug 30 06:44:33.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 18.036315583s
Aug 30 06:44:35.030: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 20.033329543s
Aug 30 06:44:37.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=false. Elapsed: 22.03608297s
Aug 30 06:44:39.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036186593s
STEP: Saw pod success 08/30/23 06:44:39.033
Aug 30 06:44:39.033: INFO: Pod "pod-subpath-test-projected-95mc" satisfied condition "Succeeded or Failed"
Aug 30 06:44:39.044: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-projected-95mc container test-container-subpath-projected-95mc: <nil>
STEP: delete the pod 08/30/23 06:44:39.109
Aug 30 06:44:39.155: INFO: Waiting for pod pod-subpath-test-projected-95mc to disappear
Aug 30 06:44:39.163: INFO: Pod pod-subpath-test-projected-95mc no longer exists
STEP: Deleting pod pod-subpath-test-projected-95mc 08/30/23 06:44:39.164
Aug 30 06:44:39.164: INFO: Deleting pod "pod-subpath-test-projected-95mc" in namespace "subpath-6343"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:39.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6343" for this suite. 08/30/23 06:44:39.19
------------------------------
• [SLOW TEST] [24.347 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:14.869
    Aug 30 06:44:14.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename subpath 08/30/23 06:44:14.87
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:14.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:14.927
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/30/23 06:44:14.935
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-95mc 08/30/23 06:44:14.963
    STEP: Creating a pod to test atomic-volume-subpath 08/30/23 06:44:14.963
    Aug 30 06:44:14.996: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-95mc" in namespace "subpath-6343" to be "Succeeded or Failed"
    Aug 30 06:44:15.021: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.284356ms
    Aug 30 06:44:17.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 2.035352726s
    Aug 30 06:44:19.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 4.037189007s
    Aug 30 06:44:21.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 6.035322661s
    Aug 30 06:44:23.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 8.036159456s
    Aug 30 06:44:25.035: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 10.038822336s
    Aug 30 06:44:27.035: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 12.038526667s
    Aug 30 06:44:29.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 14.036058704s
    Aug 30 06:44:31.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 16.037001165s
    Aug 30 06:44:33.033: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 18.036315583s
    Aug 30 06:44:35.030: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=true. Elapsed: 20.033329543s
    Aug 30 06:44:37.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Running", Reason="", readiness=false. Elapsed: 22.03608297s
    Aug 30 06:44:39.032: INFO: Pod "pod-subpath-test-projected-95mc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.036186593s
    STEP: Saw pod success 08/30/23 06:44:39.033
    Aug 30 06:44:39.033: INFO: Pod "pod-subpath-test-projected-95mc" satisfied condition "Succeeded or Failed"
    Aug 30 06:44:39.044: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-projected-95mc container test-container-subpath-projected-95mc: <nil>
    STEP: delete the pod 08/30/23 06:44:39.109
    Aug 30 06:44:39.155: INFO: Waiting for pod pod-subpath-test-projected-95mc to disappear
    Aug 30 06:44:39.163: INFO: Pod pod-subpath-test-projected-95mc no longer exists
    STEP: Deleting pod pod-subpath-test-projected-95mc 08/30/23 06:44:39.164
    Aug 30 06:44:39.164: INFO: Deleting pod "pod-subpath-test-projected-95mc" in namespace "subpath-6343"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:39.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6343" for this suite. 08/30/23 06:44:39.19
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:39.217
Aug 30 06:44:39.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename events 08/30/23 06:44:39.218
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.29
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/30/23 06:44:39.299
Aug 30 06:44:39.341: INFO: created test-event-1
Aug 30 06:44:39.387: INFO: created test-event-2
Aug 30 06:44:39.445: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/30/23 06:44:39.445
STEP: delete collection of events 08/30/23 06:44:39.46
Aug 30 06:44:39.460: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/30/23 06:44:39.565
Aug 30 06:44:39.565: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:39.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6358" for this suite. 08/30/23 06:44:39.592
------------------------------
• [0.395 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:39.217
    Aug 30 06:44:39.217: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename events 08/30/23 06:44:39.218
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.29
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/30/23 06:44:39.299
    Aug 30 06:44:39.341: INFO: created test-event-1
    Aug 30 06:44:39.387: INFO: created test-event-2
    Aug 30 06:44:39.445: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/30/23 06:44:39.445
    STEP: delete collection of events 08/30/23 06:44:39.46
    Aug 30 06:44:39.460: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/30/23 06:44:39.565
    Aug 30 06:44:39.565: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:39.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6358" for this suite. 08/30/23 06:44:39.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:39.613
Aug 30 06:44:39.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:44:39.614
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.668
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/30/23 06:44:39.681
STEP: Getting a ResourceQuota 08/30/23 06:44:39.702
STEP: Updating a ResourceQuota 08/30/23 06:44:39.756
STEP: Verifying a ResourceQuota was modified 08/30/23 06:44:39.779
STEP: Deleting a ResourceQuota 08/30/23 06:44:39.795
STEP: Verifying the deleted ResourceQuota 08/30/23 06:44:39.845
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:39.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6058" for this suite. 08/30/23 06:44:39.893
------------------------------
• [0.298 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:39.613
    Aug 30 06:44:39.614: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:44:39.614
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.668
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/30/23 06:44:39.681
    STEP: Getting a ResourceQuota 08/30/23 06:44:39.702
    STEP: Updating a ResourceQuota 08/30/23 06:44:39.756
    STEP: Verifying a ResourceQuota was modified 08/30/23 06:44:39.779
    STEP: Deleting a ResourceQuota 08/30/23 06:44:39.795
    STEP: Verifying the deleted ResourceQuota 08/30/23 06:44:39.845
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:39.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6058" for this suite. 08/30/23 06:44:39.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:39.913
Aug 30 06:44:39.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:44:39.914
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.974
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:44:39.999
Aug 30 06:44:40.035: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2772" to be "running and ready"
Aug 30 06:44:40.050: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.737024ms
Aug 30 06:44:40.050: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:44:42.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028940666s
Aug 30 06:44:42.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:44:44.063: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.027780742s
Aug 30 06:44:44.063: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 30 06:44:44.063: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/30/23 06:44:44.072
Aug 30 06:44:44.121: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2772" to be "running and ready"
Aug 30 06:44:44.131: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848753ms
Aug 30 06:44:44.131: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:44:46.141: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019643155s
Aug 30 06:44:46.141: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 30 06:44:46.141: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/30/23 06:44:46.15
Aug 30 06:44:46.168: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 06:44:46.179: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 06:44:48.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 06:44:48.190: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 30 06:44:50.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 30 06:44:50.189: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/30/23 06:44:50.189
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:50.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-2772" for this suite. 08/30/23 06:44:50.257
------------------------------
• [SLOW TEST] [10.362 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:39.913
    Aug 30 06:44:39.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:44:39.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:39.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:39.974
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:44:39.999
    Aug 30 06:44:40.035: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2772" to be "running and ready"
    Aug 30 06:44:40.050: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.737024ms
    Aug 30 06:44:40.050: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:44:42.064: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028940666s
    Aug 30 06:44:42.064: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:44:44.063: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.027780742s
    Aug 30 06:44:44.063: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 30 06:44:44.063: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/30/23 06:44:44.072
    Aug 30 06:44:44.121: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2772" to be "running and ready"
    Aug 30 06:44:44.131: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848753ms
    Aug 30 06:44:44.131: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:44:46.141: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.019643155s
    Aug 30 06:44:46.141: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 30 06:44:46.141: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/30/23 06:44:46.15
    Aug 30 06:44:46.168: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 30 06:44:46.179: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 30 06:44:48.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 30 06:44:48.190: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 30 06:44:50.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 30 06:44:50.189: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/30/23 06:44:50.189
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:50.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-2772" for this suite. 08/30/23 06:44:50.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:50.277
Aug 30 06:44:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:44:50.278
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:50.36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:50.369
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/30/23 06:44:50.378
Aug 30 06:44:50.418: INFO: Waiting up to 5m0s for pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468" in namespace "emptydir-2918" to be "Succeeded or Failed"
Aug 30 06:44:50.433: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433567ms
Aug 30 06:44:52.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026050322s
Aug 30 06:44:54.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026632807s
Aug 30 06:44:56.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026353077s
STEP: Saw pod success 08/30/23 06:44:56.445
Aug 30 06:44:56.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468" satisfied condition "Succeeded or Failed"
Aug 30 06:44:56.455: INFO: Trying to get logs from node 10.135.139.190 pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 container test-container: <nil>
STEP: delete the pod 08/30/23 06:44:56.481
Aug 30 06:44:56.522: INFO: Waiting for pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 to disappear
Aug 30 06:44:56.537: INFO: Pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:44:56.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2918" for this suite. 08/30/23 06:44:56.554
------------------------------
• [SLOW TEST] [6.298 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:50.277
    Aug 30 06:44:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:44:50.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:50.36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:50.369
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/30/23 06:44:50.378
    Aug 30 06:44:50.418: INFO: Waiting up to 5m0s for pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468" in namespace "emptydir-2918" to be "Succeeded or Failed"
    Aug 30 06:44:50.433: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 14.433567ms
    Aug 30 06:44:52.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026050322s
    Aug 30 06:44:54.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026632807s
    Aug 30 06:44:56.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026353077s
    STEP: Saw pod success 08/30/23 06:44:56.445
    Aug 30 06:44:56.445: INFO: Pod "pod-ad26da29-5995-4182-86ed-7d3cc8669468" satisfied condition "Succeeded or Failed"
    Aug 30 06:44:56.455: INFO: Trying to get logs from node 10.135.139.190 pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 container test-container: <nil>
    STEP: delete the pod 08/30/23 06:44:56.481
    Aug 30 06:44:56.522: INFO: Waiting for pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 to disappear
    Aug 30 06:44:56.537: INFO: Pod pod-ad26da29-5995-4182-86ed-7d3cc8669468 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:44:56.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2918" for this suite. 08/30/23 06:44:56.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:44:56.58
Aug 30 06:44:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context 08/30/23 06:44:56.581
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:56.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:56.639
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/30/23 06:44:56.65
W0830 06:44:56.693901      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:44:56.694: INFO: Waiting up to 5m0s for pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb" in namespace "security-context-4326" to be "Succeeded or Failed"
Aug 30 06:44:56.710: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.784785ms
Aug 30 06:44:58.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027300244s
Aug 30 06:45:00.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02750392s
STEP: Saw pod success 08/30/23 06:45:00.721
Aug 30 06:45:00.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb" satisfied condition "Succeeded or Failed"
Aug 30 06:45:00.731: INFO: Trying to get logs from node 10.135.139.190 pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb container test-container: <nil>
STEP: delete the pod 08/30/23 06:45:00.837
Aug 30 06:45:00.869: INFO: Waiting for pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb to disappear
Aug 30 06:45:00.878: INFO: Pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 06:45:00.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4326" for this suite. 08/30/23 06:45:00.895
------------------------------
• [4.334 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:44:56.58
    Aug 30 06:44:56.580: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context 08/30/23 06:44:56.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:44:56.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:44:56.639
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/30/23 06:44:56.65
    W0830 06:44:56.693901      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:44:56.694: INFO: Waiting up to 5m0s for pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb" in namespace "security-context-4326" to be "Succeeded or Failed"
    Aug 30 06:44:56.710: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.784785ms
    Aug 30 06:44:58.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027300244s
    Aug 30 06:45:00.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02750392s
    STEP: Saw pod success 08/30/23 06:45:00.721
    Aug 30 06:45:00.721: INFO: Pod "security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb" satisfied condition "Succeeded or Failed"
    Aug 30 06:45:00.731: INFO: Trying to get logs from node 10.135.139.190 pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb container test-container: <nil>
    STEP: delete the pod 08/30/23 06:45:00.837
    Aug 30 06:45:00.869: INFO: Waiting for pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb to disappear
    Aug 30 06:45:00.878: INFO: Pod security-context-2a336d34-1a3a-461e-8dea-6e775e643ffb no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:45:00.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4326" for this suite. 08/30/23 06:45:00.895
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:45:00.914
Aug 30 06:45:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:45:00.915
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:00.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:01.004
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 30 06:45:01.195: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1278 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 30 06:45:01.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1278" for this suite. 08/30/23 06:45:01.286
------------------------------
• [0.393 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:45:00.914
    Aug 30 06:45:00.915: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename runtimeclass 08/30/23 06:45:00.915
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:00.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:01.004
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 30 06:45:01.195: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1278 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:45:01.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1278" for this suite. 08/30/23 06:45:01.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:45:01.309
Aug 30 06:45:01.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:45:01.31
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:01.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:01.368
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/30/23 06:45:01.38
W0830 06:45:01.489763      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:45:01.490: INFO: Waiting up to 5m0s for pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc" in namespace "emptydir-2778" to be "Succeeded or Failed"
Aug 30 06:45:01.582: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 91.788542ms
Aug 30 06:45:03.594: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103810265s
Aug 30 06:45:05.593: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102881482s
Aug 30 06:45:07.592: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102372077s
STEP: Saw pod success 08/30/23 06:45:07.592
Aug 30 06:45:07.593: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc" satisfied condition "Succeeded or Failed"
Aug 30 06:45:07.602: INFO: Trying to get logs from node 10.135.139.190 pod pod-c05e7318-d825-48e4-8eed-4379418c02bc container test-container: <nil>
STEP: delete the pod 08/30/23 06:45:07.635
Aug 30 06:45:07.672: INFO: Waiting for pod pod-c05e7318-d825-48e4-8eed-4379418c02bc to disappear
Aug 30 06:45:07.683: INFO: Pod pod-c05e7318-d825-48e4-8eed-4379418c02bc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:45:07.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2778" for this suite. 08/30/23 06:45:07.696
------------------------------
• [SLOW TEST] [6.407 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:45:01.309
    Aug 30 06:45:01.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:45:01.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:01.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:01.368
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/30/23 06:45:01.38
    W0830 06:45:01.489763      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:45:01.490: INFO: Waiting up to 5m0s for pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc" in namespace "emptydir-2778" to be "Succeeded or Failed"
    Aug 30 06:45:01.582: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 91.788542ms
    Aug 30 06:45:03.594: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103810265s
    Aug 30 06:45:05.593: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102881482s
    Aug 30 06:45:07.592: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.102372077s
    STEP: Saw pod success 08/30/23 06:45:07.592
    Aug 30 06:45:07.593: INFO: Pod "pod-c05e7318-d825-48e4-8eed-4379418c02bc" satisfied condition "Succeeded or Failed"
    Aug 30 06:45:07.602: INFO: Trying to get logs from node 10.135.139.190 pod pod-c05e7318-d825-48e4-8eed-4379418c02bc container test-container: <nil>
    STEP: delete the pod 08/30/23 06:45:07.635
    Aug 30 06:45:07.672: INFO: Waiting for pod pod-c05e7318-d825-48e4-8eed-4379418c02bc to disappear
    Aug 30 06:45:07.683: INFO: Pod pod-c05e7318-d825-48e4-8eed-4379418c02bc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:45:07.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2778" for this suite. 08/30/23 06:45:07.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:45:07.718
Aug 30 06:45:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pod-network-test 08/30/23 06:45:07.719
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:07.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:07.78
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-9845 08/30/23 06:45:07.79
STEP: creating a selector 08/30/23 06:45:07.79
STEP: Creating the service pods in kubernetes 08/30/23 06:45:07.79
Aug 30 06:45:07.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 30 06:45:07.969: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9845" to be "running and ready"
Aug 30 06:45:08.005: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012163ms
Aug 30 06:45:08.005: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:45:10.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.04630393s
Aug 30 06:45:10.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:12.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.047747495s
Aug 30 06:45:12.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:14.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.048735265s
Aug 30 06:45:14.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:16.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.047540771s
Aug 30 06:45:16.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:18.020: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051132225s
Aug 30 06:45:18.020: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:20.045: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.07643931s
Aug 30 06:45:20.045: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:22.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.058441584s
Aug 30 06:45:22.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:24.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.048245394s
Aug 30 06:45:24.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:26.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049661763s
Aug 30 06:45:26.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:28.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04695486s
Aug 30 06:45:28.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 06:45:30.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.048620526s
Aug 30 06:45:30.017: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 30 06:45:30.017: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 30 06:45:30.028: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9845" to be "running and ready"
Aug 30 06:45:30.038: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.960898ms
Aug 30 06:45:30.038: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 30 06:45:30.038: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 30 06:45:30.049: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9845" to be "running and ready"
Aug 30 06:45:30.059: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.045422ms
Aug 30 06:45:30.059: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 30 06:45:30.059: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/30/23 06:45:30.085
Aug 30 06:45:30.136: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9845" to be "running"
Aug 30 06:45:30.147: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.619445ms
Aug 30 06:45:32.161: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024524084s
Aug 30 06:45:32.161: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 30 06:45:32.170: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9845" to be "running"
Aug 30 06:45:32.180: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.025152ms
Aug 30 06:45:32.180: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 30 06:45:32.190: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 06:45:32.190: INFO: Going to poll 172.30.86.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:45:32.201: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.86.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:45:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:45:32.201: INFO: ExecWithOptions: Clientset creation
Aug 30 06:45:32.202: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.86.189+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:45:33.617: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 30 06:45:33.617: INFO: Going to poll 172.30.224.50 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:45:33.629: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.224.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:45:33.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:45:33.630: INFO: ExecWithOptions: Clientset creation
Aug 30 06:45:33.630: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.224.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:45:35.024: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 30 06:45:35.024: INFO: Going to poll 172.30.58.79 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Aug 30 06:45:35.042: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.58.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:45:35.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:45:35.042: INFO: ExecWithOptions: Clientset creation
Aug 30 06:45:35.042: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.58.79+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 30 06:45:36.187: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 30 06:45:36.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9845" for this suite. 08/30/23 06:45:36.204
------------------------------
• [SLOW TEST] [28.503 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:45:07.718
    Aug 30 06:45:07.718: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pod-network-test 08/30/23 06:45:07.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:07.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:07.78
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-9845 08/30/23 06:45:07.79
    STEP: creating a selector 08/30/23 06:45:07.79
    STEP: Creating the service pods in kubernetes 08/30/23 06:45:07.79
    Aug 30 06:45:07.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 30 06:45:07.969: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9845" to be "running and ready"
    Aug 30 06:45:08.005: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.012163ms
    Aug 30 06:45:08.005: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:45:10.015: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.04630393s
    Aug 30 06:45:10.015: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:12.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.047747495s
    Aug 30 06:45:12.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:14.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.048735265s
    Aug 30 06:45:14.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:16.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.047540771s
    Aug 30 06:45:16.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:18.020: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.051132225s
    Aug 30 06:45:18.020: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:20.045: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.07643931s
    Aug 30 06:45:20.045: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:22.027: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.058441584s
    Aug 30 06:45:22.027: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:24.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.048245394s
    Aug 30 06:45:24.017: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:26.018: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.049661763s
    Aug 30 06:45:26.018: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:28.016: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.04695486s
    Aug 30 06:45:28.016: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 06:45:30.017: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.048620526s
    Aug 30 06:45:30.017: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 30 06:45:30.017: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 30 06:45:30.028: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9845" to be "running and ready"
    Aug 30 06:45:30.038: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.960898ms
    Aug 30 06:45:30.038: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 30 06:45:30.038: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 30 06:45:30.049: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9845" to be "running and ready"
    Aug 30 06:45:30.059: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 10.045422ms
    Aug 30 06:45:30.059: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 30 06:45:30.059: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/30/23 06:45:30.085
    Aug 30 06:45:30.136: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9845" to be "running"
    Aug 30 06:45:30.147: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.619445ms
    Aug 30 06:45:32.161: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024524084s
    Aug 30 06:45:32.161: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 30 06:45:32.170: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-9845" to be "running"
    Aug 30 06:45:32.180: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.025152ms
    Aug 30 06:45:32.180: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 30 06:45:32.190: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 30 06:45:32.190: INFO: Going to poll 172.30.86.189 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:45:32.201: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.86.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:45:32.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:45:32.201: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:45:32.202: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.86.189+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:45:33.617: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 30 06:45:33.617: INFO: Going to poll 172.30.224.50 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:45:33.629: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.224.50 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:45:33.629: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:45:33.630: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:45:33.630: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.224.50+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:45:35.024: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 30 06:45:35.024: INFO: Going to poll 172.30.58.79 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Aug 30 06:45:35.042: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.58.79 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9845 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:45:35.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:45:35.042: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:45:35.042: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-9845/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.58.79+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 30 06:45:36.187: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:45:36.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9845" for this suite. 08/30/23 06:45:36.204
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:45:36.222
Aug 30 06:45:36.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:45:36.223
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:36.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:36.285
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
Aug 30 06:45:36.328: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-162d5649-efc1-4198-b3d0-14651907c96f 08/30/23 06:45:36.328
STEP: Creating the pod 08/30/23 06:45:36.397
Aug 30 06:45:36.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054" in namespace "configmap-2818" to be "running and ready"
Aug 30 06:45:36.452: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Pending", Reason="", readiness=false. Elapsed: 22.708757ms
Aug 30 06:45:36.452: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:45:38.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034157811s
Aug 30 06:45:38.464: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:45:40.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Running", Reason="", readiness=true. Elapsed: 4.03441033s
Aug 30 06:45:40.464: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Running (Ready = true)
Aug 30 06:45:40.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-162d5649-efc1-4198-b3d0-14651907c96f 08/30/23 06:45:40.528
STEP: waiting to observe update in volume 08/30/23 06:45:40.542
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:46:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2818" for this suite. 08/30/23 06:46:58.065
------------------------------
• [SLOW TEST] [81.887 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:45:36.222
    Aug 30 06:45:36.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:45:36.223
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:45:36.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:45:36.285
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    Aug 30 06:45:36.328: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-162d5649-efc1-4198-b3d0-14651907c96f 08/30/23 06:45:36.328
    STEP: Creating the pod 08/30/23 06:45:36.397
    Aug 30 06:45:36.429: INFO: Waiting up to 5m0s for pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054" in namespace "configmap-2818" to be "running and ready"
    Aug 30 06:45:36.452: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Pending", Reason="", readiness=false. Elapsed: 22.708757ms
    Aug 30 06:45:36.452: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:45:38.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034157811s
    Aug 30 06:45:38.464: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:45:40.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054": Phase="Running", Reason="", readiness=true. Elapsed: 4.03441033s
    Aug 30 06:45:40.464: INFO: The phase of Pod pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054 is Running (Ready = true)
    Aug 30 06:45:40.464: INFO: Pod "pod-configmaps-16847119-5266-4ceb-a0eb-c28fbbac3054" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-162d5649-efc1-4198-b3d0-14651907c96f 08/30/23 06:45:40.528
    STEP: waiting to observe update in volume 08/30/23 06:45:40.542
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:46:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2818" for this suite. 08/30/23 06:46:58.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:46:58.109
Aug 30 06:46:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 06:46:58.111
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:46:58.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:46:58.256
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
Aug 30 06:46:58.349: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-2f309f93-c886-4d66-9d3b-e5d6fca25de7 08/30/23 06:46:58.349
STEP: Creating the pod 08/30/23 06:46:58.382
Aug 30 06:46:58.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a" in namespace "configmap-8122" to be "running"
Aug 30 06:46:58.488: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.738457ms
Aug 30 06:47:00.507: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a": Phase="Running", Reason="", readiness=false. Elapsed: 2.030610937s
Aug 30 06:47:00.507: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a" satisfied condition "running"
STEP: Waiting for pod with text data 08/30/23 06:47:00.507
STEP: Waiting for pod with binary data 08/30/23 06:47:00.599
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:00.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8122" for this suite. 08/30/23 06:47:00.703
------------------------------
• [2.615 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:46:58.109
    Aug 30 06:46:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 06:46:58.111
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:46:58.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:46:58.256
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    Aug 30 06:46:58.349: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-2f309f93-c886-4d66-9d3b-e5d6fca25de7 08/30/23 06:46:58.349
    STEP: Creating the pod 08/30/23 06:46:58.382
    Aug 30 06:46:58.476: INFO: Waiting up to 5m0s for pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a" in namespace "configmap-8122" to be "running"
    Aug 30 06:46:58.488: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.738457ms
    Aug 30 06:47:00.507: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a": Phase="Running", Reason="", readiness=false. Elapsed: 2.030610937s
    Aug 30 06:47:00.507: INFO: Pod "pod-configmaps-45c0d9db-c145-4fb7-bf76-32750d6b252a" satisfied condition "running"
    STEP: Waiting for pod with text data 08/30/23 06:47:00.507
    STEP: Waiting for pod with binary data 08/30/23 06:47:00.599
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:00.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8122" for this suite. 08/30/23 06:47:00.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:00.726
Aug 30 06:47:00.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 06:47:00.727
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:00.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:00.781
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6139" for this suite. 08/30/23 06:47:00.985
------------------------------
• [0.276 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:00.726
    Aug 30 06:47:00.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 06:47:00.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:00.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:00.781
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:00.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6139" for this suite. 08/30/23 06:47:00.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:01.003
Aug 30 06:47:01.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 06:47:01.004
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:01.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:01.053
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/30/23 06:47:01.081
STEP: waiting for available Endpoint 08/30/23 06:47:01.1
STEP: listing all Endpoints 08/30/23 06:47:01.104
STEP: updating the Endpoint 08/30/23 06:47:01.128
STEP: fetching the Endpoint 08/30/23 06:47:01.153
STEP: patching the Endpoint 08/30/23 06:47:01.163
STEP: fetching the Endpoint 08/30/23 06:47:01.186
STEP: deleting the Endpoint by Collection 08/30/23 06:47:01.195
STEP: waiting for Endpoint deletion 08/30/23 06:47:01.218
STEP: fetching the Endpoint 08/30/23 06:47:01.238
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6273" for this suite. 08/30/23 06:47:01.263
------------------------------
• [0.279 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:01.003
    Aug 30 06:47:01.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 06:47:01.004
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:01.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:01.053
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/30/23 06:47:01.081
    STEP: waiting for available Endpoint 08/30/23 06:47:01.1
    STEP: listing all Endpoints 08/30/23 06:47:01.104
    STEP: updating the Endpoint 08/30/23 06:47:01.128
    STEP: fetching the Endpoint 08/30/23 06:47:01.153
    STEP: patching the Endpoint 08/30/23 06:47:01.163
    STEP: fetching the Endpoint 08/30/23 06:47:01.186
    STEP: deleting the Endpoint by Collection 08/30/23 06:47:01.195
    STEP: waiting for Endpoint deletion 08/30/23 06:47:01.218
    STEP: fetching the Endpoint 08/30/23 06:47:01.238
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:01.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6273" for this suite. 08/30/23 06:47:01.263
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:01.282
Aug 30 06:47:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:47:01.283
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:01.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:01.367
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:47:01.4
W0830 06:47:01.486998      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:47:01.487: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4329" to be "running and ready"
Aug 30 06:47:01.515: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 28.761113ms
Aug 30 06:47:01.515: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:47:03.528: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040882592s
Aug 30 06:47:03.528: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:47:05.531: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.043826663s
Aug 30 06:47:05.531: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 30 06:47:05.531: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/30/23 06:47:05.54
Aug 30 06:47:05.562: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4329" to be "running and ready"
Aug 30 06:47:05.575: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.852347ms
Aug 30 06:47:05.575: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:47:07.585: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023248935s
Aug 30 06:47:07.585: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:47:09.586: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.024280313s
Aug 30 06:47:09.586: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 30 06:47:09.586: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/30/23 06:47:09.596
STEP: delete the pod with lifecycle hook 08/30/23 06:47:09.648
Aug 30 06:47:09.681: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 06:47:09.729: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 30 06:47:11.730: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 30 06:47:11.743: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:11.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4329" for this suite. 08/30/23 06:47:11.758
------------------------------
• [SLOW TEST] [10.524 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:01.282
    Aug 30 06:47:01.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 06:47:01.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:01.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:01.367
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/30/23 06:47:01.4
    W0830 06:47:01.486998      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "container-handle-http-request", "container-handle-https-request" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "container-handle-http-request", "container-handle-https-request" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:47:01.487: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4329" to be "running and ready"
    Aug 30 06:47:01.515: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 28.761113ms
    Aug 30 06:47:01.515: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:47:03.528: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040882592s
    Aug 30 06:47:03.528: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:47:05.531: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.043826663s
    Aug 30 06:47:05.531: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 30 06:47:05.531: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/30/23 06:47:05.54
    Aug 30 06:47:05.562: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4329" to be "running and ready"
    Aug 30 06:47:05.575: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.852347ms
    Aug 30 06:47:05.575: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:47:07.585: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023248935s
    Aug 30 06:47:07.585: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:47:09.586: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.024280313s
    Aug 30 06:47:09.586: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 30 06:47:09.586: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/30/23 06:47:09.596
    STEP: delete the pod with lifecycle hook 08/30/23 06:47:09.648
    Aug 30 06:47:09.681: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 30 06:47:09.729: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 30 06:47:11.730: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 30 06:47:11.743: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:11.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4329" for this suite. 08/30/23 06:47:11.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:11.809
Aug 30 06:47:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 06:47:11.81
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:11.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:11.884
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/30/23 06:47:11.893
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:11.916
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:11.916
STEP: creating a pod to probe DNS 08/30/23 06:47:11.916
STEP: submitting the pod to kubernetes 08/30/23 06:47:11.916
Aug 30 06:47:11.968: INFO: Waiting up to 15m0s for pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9" in namespace "dns-6271" to be "running"
Aug 30 06:47:11.987: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.590052ms
Aug 30 06:47:14.004: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035820851s
Aug 30 06:47:15.998: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Running", Reason="", readiness=true. Elapsed: 4.029903726s
Aug 30 06:47:15.998: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:47:15.998
STEP: looking for the results for each expected name from probers 08/30/23 06:47:16.009
Aug 30 06:47:16.044: INFO: DNS probes using dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9 succeeded

STEP: deleting the pod 08/30/23 06:47:16.044
STEP: changing the externalName to bar.example.com 08/30/23 06:47:16.079
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:16.109
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:16.11
STEP: creating a second pod to probe DNS 08/30/23 06:47:16.11
STEP: submitting the pod to kubernetes 08/30/23 06:47:16.11
Aug 30 06:47:16.145: INFO: Waiting up to 15m0s for pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5" in namespace "dns-6271" to be "running"
Aug 30 06:47:16.162: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.494066ms
Aug 30 06:47:18.173: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.0285785s
Aug 30 06:47:18.173: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:47:18.173
STEP: looking for the results for each expected name from probers 08/30/23 06:47:18.185
Aug 30 06:47:18.204: INFO: File wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local from pod  dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 30 06:47:18.217: INFO: File jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local from pod  dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 30 06:47:18.217: INFO: Lookups using dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 failed for: [wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local]

Aug 30 06:47:23.248: INFO: DNS probes using dns-test-2497a906-e411-491f-a650-15482f0f5cb5 succeeded

STEP: deleting the pod 08/30/23 06:47:23.248
STEP: changing the service to type=ClusterIP 08/30/23 06:47:23.28
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:23.333
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
 08/30/23 06:47:23.333
STEP: creating a third pod to probe DNS 08/30/23 06:47:23.333
STEP: submitting the pod to kubernetes 08/30/23 06:47:23.342
Aug 30 06:47:23.371: INFO: Waiting up to 15m0s for pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c" in namespace "dns-6271" to be "running"
Aug 30 06:47:23.385: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.306851ms
Aug 30 06:47:25.395: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02379649s
Aug 30 06:47:27.397: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Running", Reason="", readiness=true. Elapsed: 4.025344804s
Aug 30 06:47:27.397: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:47:27.397
STEP: looking for the results for each expected name from probers 08/30/23 06:47:27.41
Aug 30 06:47:27.446: INFO: DNS probes using dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c succeeded

STEP: deleting the pod 08/30/23 06:47:27.446
STEP: deleting the test externalName service 08/30/23 06:47:27.473
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:27.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6271" for this suite. 08/30/23 06:47:27.526
------------------------------
• [SLOW TEST] [15.740 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:11.809
    Aug 30 06:47:11.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 06:47:11.81
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:11.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:11.884
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/30/23 06:47:11.893
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:11.916
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:11.916
    STEP: creating a pod to probe DNS 08/30/23 06:47:11.916
    STEP: submitting the pod to kubernetes 08/30/23 06:47:11.916
    Aug 30 06:47:11.968: INFO: Waiting up to 15m0s for pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9" in namespace "dns-6271" to be "running"
    Aug 30 06:47:11.987: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 19.590052ms
    Aug 30 06:47:14.004: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035820851s
    Aug 30 06:47:15.998: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9": Phase="Running", Reason="", readiness=true. Elapsed: 4.029903726s
    Aug 30 06:47:15.998: INFO: Pod "dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:47:15.998
    STEP: looking for the results for each expected name from probers 08/30/23 06:47:16.009
    Aug 30 06:47:16.044: INFO: DNS probes using dns-test-7b8cae69-886c-47bf-9cfc-5f3e0b1e3bd9 succeeded

    STEP: deleting the pod 08/30/23 06:47:16.044
    STEP: changing the externalName to bar.example.com 08/30/23 06:47:16.079
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:16.109
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:16.11
    STEP: creating a second pod to probe DNS 08/30/23 06:47:16.11
    STEP: submitting the pod to kubernetes 08/30/23 06:47:16.11
    Aug 30 06:47:16.145: INFO: Waiting up to 15m0s for pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5" in namespace "dns-6271" to be "running"
    Aug 30 06:47:16.162: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.494066ms
    Aug 30 06:47:18.173: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5": Phase="Running", Reason="", readiness=true. Elapsed: 2.0285785s
    Aug 30 06:47:18.173: INFO: Pod "dns-test-2497a906-e411-491f-a650-15482f0f5cb5" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:47:18.173
    STEP: looking for the results for each expected name from probers 08/30/23 06:47:18.185
    Aug 30 06:47:18.204: INFO: File wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local from pod  dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 30 06:47:18.217: INFO: File jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local from pod  dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 30 06:47:18.217: INFO: Lookups using dns-6271/dns-test-2497a906-e411-491f-a650-15482f0f5cb5 failed for: [wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local]

    Aug 30 06:47:23.248: INFO: DNS probes using dns-test-2497a906-e411-491f-a650-15482f0f5cb5 succeeded

    STEP: deleting the pod 08/30/23 06:47:23.248
    STEP: changing the service to type=ClusterIP 08/30/23 06:47:23.28
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:23.333
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6271.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6271.svc.cluster.local; sleep 1; done
     08/30/23 06:47:23.333
    STEP: creating a third pod to probe DNS 08/30/23 06:47:23.333
    STEP: submitting the pod to kubernetes 08/30/23 06:47:23.342
    Aug 30 06:47:23.371: INFO: Waiting up to 15m0s for pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c" in namespace "dns-6271" to be "running"
    Aug 30 06:47:23.385: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.306851ms
    Aug 30 06:47:25.395: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02379649s
    Aug 30 06:47:27.397: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c": Phase="Running", Reason="", readiness=true. Elapsed: 4.025344804s
    Aug 30 06:47:27.397: INFO: Pod "dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:47:27.397
    STEP: looking for the results for each expected name from probers 08/30/23 06:47:27.41
    Aug 30 06:47:27.446: INFO: DNS probes using dns-test-69f6cf00-d666-4f93-9371-fdccf1044f1c succeeded

    STEP: deleting the pod 08/30/23 06:47:27.446
    STEP: deleting the test externalName service 08/30/23 06:47:27.473
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:27.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6271" for this suite. 08/30/23 06:47:27.526
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:27.55
Aug 30 06:47:27.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 06:47:27.551
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:27.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:27.609
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/30/23 06:47:27.648
STEP: waiting for Deployment to be created 08/30/23 06:47:27.667
STEP: waiting for all Replicas to be Ready 08/30/23 06:47:27.671
Aug 30 06:47:27.676: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.676: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.717: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.717: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.745: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.745: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.811: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:27.811: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 30 06:47:29.247: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 30 06:47:29.247: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 30 06:47:29.469: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/30/23 06:47:29.469
W0830 06:47:29.487429      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 30 06:47:29.492: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/30/23 06:47:29.493
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.542: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.542: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.587: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.587: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:29.621: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:29.621: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:29.649: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:29.649: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:31.396: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:31.397: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:31.562: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
STEP: listing Deployments 08/30/23 06:47:31.562
Aug 30 06:47:31.608: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/30/23 06:47:31.608
Aug 30 06:47:31.645: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/30/23 06:47:31.645
Aug 30 06:47:31.684: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:31.712: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:31.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:31.943: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:32.004: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:32.059: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:33.392: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:33.463: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:33.552: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 30 06:47:35.700: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/30/23 06:47:35.804
STEP: fetching the DeploymentStatus 08/30/23 06:47:35.824
Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 3
STEP: deleting the Deployment 08/30/23 06:47:35.84
Aug 30 06:47:35.868: INFO: observed event type MODIFIED
Aug 30 06:47:35.868: INFO: observed event type MODIFIED
Aug 30 06:47:35.868: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
Aug 30 06:47:35.869: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 06:47:35.882: INFO: Log out all the ReplicaSets if there is no deployment created
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:35.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-7545" for this suite. 08/30/23 06:47:35.915
------------------------------
• [SLOW TEST] [8.391 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:27.55
    Aug 30 06:47:27.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 06:47:27.551
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:27.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:27.609
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/30/23 06:47:27.648
    STEP: waiting for Deployment to be created 08/30/23 06:47:27.667
    STEP: waiting for all Replicas to be Ready 08/30/23 06:47:27.671
    Aug 30 06:47:27.676: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.676: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.717: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.717: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.745: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.745: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.811: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:27.811: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 30 06:47:29.247: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 30 06:47:29.247: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 30 06:47:29.469: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/30/23 06:47:29.469
    W0830 06:47:29.487429      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 30 06:47:29.492: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/30/23 06:47:29.493
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.497: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 0
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.498: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.542: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.542: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.587: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.587: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:29.621: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:29.621: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:29.649: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:29.649: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:31.396: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:31.397: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:31.562: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    STEP: listing Deployments 08/30/23 06:47:31.562
    Aug 30 06:47:31.608: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/30/23 06:47:31.608
    Aug 30 06:47:31.645: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/30/23 06:47:31.645
    Aug 30 06:47:31.684: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:31.712: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:31.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:31.943: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:32.004: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:32.059: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:33.392: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:33.463: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:33.552: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 30 06:47:35.700: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/30/23 06:47:35.804
    STEP: fetching the DeploymentStatus 08/30/23 06:47:35.824
    Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.839: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 1
    Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 2
    Aug 30 06:47:35.840: INFO: observed Deployment test-deployment in namespace deployment-7545 with ReadyReplicas 3
    STEP: deleting the Deployment 08/30/23 06:47:35.84
    Aug 30 06:47:35.868: INFO: observed event type MODIFIED
    Aug 30 06:47:35.868: INFO: observed event type MODIFIED
    Aug 30 06:47:35.868: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    Aug 30 06:47:35.869: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 06:47:35.882: INFO: Log out all the ReplicaSets if there is no deployment created
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:35.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-7545" for this suite. 08/30/23 06:47:35.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:35.941
Aug 30 06:47:35.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 06:47:35.942
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:35.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:36.005
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 30 06:47:36.019: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0830 06:47:36.038093      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:47:36.051: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 30 06:47:41.065: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 06:47:41.065
Aug 30 06:47:41.066: INFO: Creating deployment "test-rolling-update-deployment"
Aug 30 06:47:41.083: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 30 06:47:41.103: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 30 06:47:43.154: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 30 06:47:43.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 06:47:45.207: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 06:47:45.281: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3049  575a8ad2-1875-4a06-9ee5-c7e757db1181 89349 1 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b69948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 06:47:41 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-30 06:47:43 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 06:47:45.344: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3049  2028a7ea-cda6-4429-b634-6767d369e100 89336 1 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 575a8ad2-1875-4a06-9ee5-c7e757db1181 0xc004e56c67 0xc004e56c68}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575a8ad2-1875-4a06-9ee5-c7e757db1181\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e56d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 06:47:45.344: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 30 06:47:45.344: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3049  0bb4ebfe-cddf-4a92-b6ba-023b7b819642 89348 2 2023-08-30 06:47:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 575a8ad2-1875-4a06-9ee5-c7e757db1181 0xc004e56b27 0xc004e56b28}] [] [{e2e.test Update apps/v1 2023-08-30 06:47:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575a8ad2-1875-4a06-9ee5-c7e757db1181\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e56bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 06:47:45.365: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8xgb9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8xgb9 test-rolling-update-deployment-7549d9f46d- deployment-3049  fbf43bad-abb6-4122-9a0d-fb0cbd60a966 89334 0 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:524d6dfed4741936b8cdecfbdde4e9e9b3768387400dcc0ef50ae5d12d6695f3 cni.projectcalico.org/podIP:172.30.58.117/32 cni.projectcalico.org/podIPs:172.30.58.117/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.117"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2028a7ea-cda6-4429-b634-6767d369e100 0xc004b69d57 0xc004b69d58}] [] [{calico Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2028a7ea-cda6-4429-b634-6767d369e100\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7hjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7hjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c44,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jrw26,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.117,StartTime:2023-08-30 06:47:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:47:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://a3b58b2e6734bfed6537b0799d4c8de9acda97ecf70d1126ce2b9840de70390c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:45.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3049" for this suite. 08/30/23 06:47:45.39
------------------------------
• [SLOW TEST] [9.469 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:35.941
    Aug 30 06:47:35.941: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 06:47:35.942
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:35.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:36.005
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 30 06:47:36.019: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0830 06:47:36.038093      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:47:36.051: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 30 06:47:41.065: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 06:47:41.065
    Aug 30 06:47:41.066: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 30 06:47:41.083: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 30 06:47:41.103: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 30 06:47:43.154: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 30 06:47:43.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 6, 47, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7549d9f46d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 06:47:45.207: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 06:47:45.281: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3049  575a8ad2-1875-4a06-9ee5-c7e757db1181 89349 1 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b69948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 06:47:41 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-30 06:47:43 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 30 06:47:45.344: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-3049  2028a7ea-cda6-4429-b634-6767d369e100 89336 1 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 575a8ad2-1875-4a06-9ee5-c7e757db1181 0xc004e56c67 0xc004e56c68}] [] [{kube-controller-manager Update apps/v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575a8ad2-1875-4a06-9ee5-c7e757db1181\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e56d28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 06:47:45.344: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 30 06:47:45.344: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3049  0bb4ebfe-cddf-4a92-b6ba-023b7b819642 89348 2 2023-08-30 06:47:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 575a8ad2-1875-4a06-9ee5-c7e757db1181 0xc004e56b27 0xc004e56b28}] [] [{e2e.test Update apps/v1 2023-08-30 06:47:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"575a8ad2-1875-4a06-9ee5-c7e757db1181\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004e56bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 06:47:45.365: INFO: Pod "test-rolling-update-deployment-7549d9f46d-8xgb9" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-8xgb9 test-rolling-update-deployment-7549d9f46d- deployment-3049  fbf43bad-abb6-4122-9a0d-fb0cbd60a966 89334 0 2023-08-30 06:47:41 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:524d6dfed4741936b8cdecfbdde4e9e9b3768387400dcc0ef50ae5d12d6695f3 cni.projectcalico.org/podIP:172.30.58.117/32 cni.projectcalico.org/podIPs:172.30.58.117/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.117"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 2028a7ea-cda6-4429-b634-6767d369e100 0xc004b69d57 0xc004b69d58}] [] [{calico Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2028a7ea-cda6-4429-b634-6767d369e100\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-08-30 06:47:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 06:47:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7hjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7hjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c49,c44,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jrw26,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 06:47:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.117,StartTime:2023-08-30 06:47:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 06:47:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://a3b58b2e6734bfed6537b0799d4c8de9acda97ecf70d1126ce2b9840de70390c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.117,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:45.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3049" for this suite. 08/30/23 06:47:45.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:45.412
Aug 30 06:47:45.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 06:47:45.413
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:45.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:45.526
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 06:47:45.536
Aug 30 06:47:45.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 30 06:47:45.807: INFO: stderr: ""
Aug 30 06:47:45.807: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/30/23 06:47:45.807
STEP: verifying the pod e2e-test-httpd-pod was created 08/30/23 06:47:50.86
Aug 30 06:47:50.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 get pod e2e-test-httpd-pod -o json'
Aug 30 06:47:50.947: INFO: stderr: ""
Aug 30 06:47:50.947: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"be33dfcd240245eb7a6f47ac3e2e3737426aa1129c793126364e3942e9e54031\",\n            \"cni.projectcalico.org/podIP\": \"172.30.58.95/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.58.95/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.58.95\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-08-30T06:47:45Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5843\",\n        \"resourceVersion\": \"89446\",\n        \"uid\": \"d48d2f39-bd0f-43d5-b64b-87dd7106dd2a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qnzk6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.135.139.190\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c50,c0\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qnzk6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:45Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://1b12c811f11948cf80ff9d86502b3ed1b1a8e7e5340cfe4946eac010f2a61cb8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-30T06:47:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.135.139.190\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.58.95\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.58.95\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-30T06:47:45Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/30/23 06:47:50.948
Aug 30 06:47:50.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 replace -f -'
Aug 30 06:47:52.592: INFO: stderr: ""
Aug 30 06:47:52.593: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/30/23 06:47:52.593
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 30 06:47:52.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 delete pods e2e-test-httpd-pod'
Aug 30 06:47:54.621: INFO: stderr: ""
Aug 30 06:47:54.622: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 06:47:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5843" for this suite. 08/30/23 06:47:54.636
------------------------------
• [SLOW TEST] [9.245 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:45.412
    Aug 30 06:47:45.412: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 06:47:45.413
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:45.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:45.526
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 06:47:45.536
    Aug 30 06:47:45.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 30 06:47:45.807: INFO: stderr: ""
    Aug 30 06:47:45.807: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/30/23 06:47:45.807
    STEP: verifying the pod e2e-test-httpd-pod was created 08/30/23 06:47:50.86
    Aug 30 06:47:50.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 get pod e2e-test-httpd-pod -o json'
    Aug 30 06:47:50.947: INFO: stderr: ""
    Aug 30 06:47:50.947: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"be33dfcd240245eb7a6f47ac3e2e3737426aa1129c793126364e3942e9e54031\",\n            \"cni.projectcalico.org/podIP\": \"172.30.58.95/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.58.95/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.58.95\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-08-30T06:47:45Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5843\",\n        \"resourceVersion\": \"89446\",\n        \"uid\": \"d48d2f39-bd0f-43d5-b64b-87dd7106dd2a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qnzk6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.135.139.190\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c50,c0\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qnzk6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:45Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:47Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:47Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-30T06:47:45Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://1b12c811f11948cf80ff9d86502b3ed1b1a8e7e5340cfe4946eac010f2a61cb8\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-30T06:47:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.135.139.190\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.58.95\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.58.95\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-30T06:47:45Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/30/23 06:47:50.948
    Aug 30 06:47:50.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 replace -f -'
    Aug 30 06:47:52.592: INFO: stderr: ""
    Aug 30 06:47:52.593: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/30/23 06:47:52.593
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 30 06:47:52.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5843 delete pods e2e-test-httpd-pod'
    Aug 30 06:47:54.621: INFO: stderr: ""
    Aug 30 06:47:54.622: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:47:54.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5843" for this suite. 08/30/23 06:47:54.636
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:47:54.657
Aug 30 06:47:54.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:47:54.658
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:54.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:54.726
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:47:54.733
W0830 06:47:54.796284      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:47:54.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7" in namespace "projected-7825" to be "Succeeded or Failed"
Aug 30 06:47:54.889: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 92.866483ms
Aug 30 06:47:56.923: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126618026s
Aug 30 06:47:58.918: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122222557s
Aug 30 06:48:00.917: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.120647984s
STEP: Saw pod success 08/30/23 06:48:00.917
Aug 30 06:48:00.917: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7" satisfied condition "Succeeded or Failed"
Aug 30 06:48:00.952: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 container client-container: <nil>
STEP: delete the pod 08/30/23 06:48:01.056
Aug 30 06:48:01.089: INFO: Waiting for pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 to disappear
Aug 30 06:48:01.104: INFO: Pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:48:01.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7825" for this suite. 08/30/23 06:48:01.171
------------------------------
• [SLOW TEST] [6.540 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:47:54.657
    Aug 30 06:47:54.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:47:54.658
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:47:54.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:47:54.726
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:47:54.733
    W0830 06:47:54.796284      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:47:54.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7" in namespace "projected-7825" to be "Succeeded or Failed"
    Aug 30 06:47:54.889: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 92.866483ms
    Aug 30 06:47:56.923: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126618026s
    Aug 30 06:47:58.918: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122222557s
    Aug 30 06:48:00.917: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.120647984s
    STEP: Saw pod success 08/30/23 06:48:00.917
    Aug 30 06:48:00.917: INFO: Pod "downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7" satisfied condition "Succeeded or Failed"
    Aug 30 06:48:00.952: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 container client-container: <nil>
    STEP: delete the pod 08/30/23 06:48:01.056
    Aug 30 06:48:01.089: INFO: Waiting for pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 to disappear
    Aug 30 06:48:01.104: INFO: Pod downwardapi-volume-146a310e-38da-4908-b008-813683f2a2b7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:48:01.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7825" for this suite. 08/30/23 06:48:01.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:48:01.214
Aug 30 06:48:01.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:48:01.217
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:48:01.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:48:01.28
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b in namespace container-probe-1383 08/30/23 06:48:01.289
W0830 06:48:01.325773      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:48:01.326: INFO: Waiting up to 5m0s for pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b" in namespace "container-probe-1383" to be "not pending"
Aug 30 06:48:01.342: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.063717ms
Aug 30 06:48:03.354: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027981905s
Aug 30 06:48:05.355: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Running", Reason="", readiness=true. Elapsed: 4.029296429s
Aug 30 06:48:05.355: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b" satisfied condition "not pending"
Aug 30 06:48:05.355: INFO: Started pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b in namespace container-probe-1383
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:48:05.355
Aug 30 06:48:05.374: INFO: Initial restart count of pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b is 0
Aug 30 06:48:53.733: INFO: Restart count of pod container-probe-1383/busybox-6c880a41-f08b-4f17-b148-c837cb6a989b is now 1 (48.359061184s elapsed)
STEP: deleting the pod 08/30/23 06:48:53.733
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 06:48:53.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1383" for this suite. 08/30/23 06:48:53.796
------------------------------
• [SLOW TEST] [52.610 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:48:01.214
    Aug 30 06:48:01.214: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:48:01.217
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:48:01.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:48:01.28
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b in namespace container-probe-1383 08/30/23 06:48:01.289
    W0830 06:48:01.325773      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:48:01.326: INFO: Waiting up to 5m0s for pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b" in namespace "container-probe-1383" to be "not pending"
    Aug 30 06:48:01.342: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.063717ms
    Aug 30 06:48:03.354: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027981905s
    Aug 30 06:48:05.355: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b": Phase="Running", Reason="", readiness=true. Elapsed: 4.029296429s
    Aug 30 06:48:05.355: INFO: Pod "busybox-6c880a41-f08b-4f17-b148-c837cb6a989b" satisfied condition "not pending"
    Aug 30 06:48:05.355: INFO: Started pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b in namespace container-probe-1383
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:48:05.355
    Aug 30 06:48:05.374: INFO: Initial restart count of pod busybox-6c880a41-f08b-4f17-b148-c837cb6a989b is 0
    Aug 30 06:48:53.733: INFO: Restart count of pod container-probe-1383/busybox-6c880a41-f08b-4f17-b148-c837cb6a989b is now 1 (48.359061184s elapsed)
    STEP: deleting the pod 08/30/23 06:48:53.733
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:48:53.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1383" for this suite. 08/30/23 06:48:53.796
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:48:53.826
Aug 30 06:48:53.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:48:53.828
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:48:53.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:48:53.892
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/30/23 06:48:53.9
Aug 30 06:48:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:49:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:49:22.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3572" for this suite. 08/30/23 06:49:22.825
------------------------------
• [SLOW TEST] [29.036 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:48:53.826
    Aug 30 06:48:53.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 06:48:53.828
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:48:53.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:48:53.892
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/30/23 06:48:53.9
    Aug 30 06:48:53.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:49:00.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:49:22.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3572" for this suite. 08/30/23 06:49:22.825
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:49:22.863
Aug 30 06:49:22.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:49:22.864
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:49:22.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:49:23.021
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/30/23 06:49:40.05
STEP: Creating a ResourceQuota 08/30/23 06:49:45.06
STEP: Ensuring resource quota status is calculated 08/30/23 06:49:45.074
STEP: Creating a ConfigMap 08/30/23 06:49:47.083
STEP: Ensuring resource quota status captures configMap creation 08/30/23 06:49:47.113
STEP: Deleting a ConfigMap 08/30/23 06:49:49.125
STEP: Ensuring resource quota status released usage 08/30/23 06:49:49.154
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:49:51.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9232" for this suite. 08/30/23 06:49:51.255
------------------------------
• [SLOW TEST] [28.511 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:49:22.863
    Aug 30 06:49:22.863: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:49:22.864
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:49:22.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:49:23.021
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/30/23 06:49:40.05
    STEP: Creating a ResourceQuota 08/30/23 06:49:45.06
    STEP: Ensuring resource quota status is calculated 08/30/23 06:49:45.074
    STEP: Creating a ConfigMap 08/30/23 06:49:47.083
    STEP: Ensuring resource quota status captures configMap creation 08/30/23 06:49:47.113
    STEP: Deleting a ConfigMap 08/30/23 06:49:49.125
    STEP: Ensuring resource quota status released usage 08/30/23 06:49:49.154
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:49:51.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9232" for this suite. 08/30/23 06:49:51.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:49:51.374
Aug 30 06:49:51.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:49:51.375
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:49:51.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:49:51.508
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/30/23 06:49:51.523
STEP: Creating a ResourceQuota 08/30/23 06:49:56.547
STEP: Ensuring resource quota status is calculated 08/30/23 06:49:56.564
STEP: Creating a ReplicationController 08/30/23 06:49:58.575
STEP: Ensuring resource quota status captures replication controller creation 08/30/23 06:49:58.678
STEP: Deleting a ReplicationController 08/30/23 06:50:00.69
STEP: Ensuring resource quota status released usage 08/30/23 06:50:00.711
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:50:02.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5552" for this suite. 08/30/23 06:50:02.734
------------------------------
• [SLOW TEST] [11.381 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:49:51.374
    Aug 30 06:49:51.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:49:51.375
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:49:51.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:49:51.508
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/30/23 06:49:51.523
    STEP: Creating a ResourceQuota 08/30/23 06:49:56.547
    STEP: Ensuring resource quota status is calculated 08/30/23 06:49:56.564
    STEP: Creating a ReplicationController 08/30/23 06:49:58.575
    STEP: Ensuring resource quota status captures replication controller creation 08/30/23 06:49:58.678
    STEP: Deleting a ReplicationController 08/30/23 06:50:00.69
    STEP: Ensuring resource quota status released usage 08/30/23 06:50:00.711
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:50:02.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5552" for this suite. 08/30/23 06:50:02.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:50:02.757
Aug 30 06:50:02.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename watch 08/30/23 06:50:02.758
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:50:02.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:50:02.853
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/30/23 06:50:02.863
STEP: creating a new configmap 08/30/23 06:50:02.868
STEP: modifying the configmap once 08/30/23 06:50:02.902
STEP: changing the label value of the configmap 08/30/23 06:50:02.949
STEP: Expecting to observe a delete notification for the watched object 08/30/23 06:50:03.014
Aug 30 06:50:03.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90476 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:50:03.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90480 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:50:03.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90487 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/30/23 06:50:03.016
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/30/23 06:50:03.05
STEP: changing the label value of the configmap back 08/30/23 06:50:13.05
STEP: modifying the configmap a third time 08/30/23 06:50:13.093
STEP: deleting the configmap 08/30/23 06:50:13.146
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/30/23 06:50:13.171
Aug 30 06:50:13.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90569 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:50:13.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90570 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 06:50:13.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90571 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 30 06:50:13.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9438" for this suite. 08/30/23 06:50:13.193
------------------------------
• [SLOW TEST] [10.465 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:50:02.757
    Aug 30 06:50:02.757: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename watch 08/30/23 06:50:02.758
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:50:02.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:50:02.853
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/30/23 06:50:02.863
    STEP: creating a new configmap 08/30/23 06:50:02.868
    STEP: modifying the configmap once 08/30/23 06:50:02.902
    STEP: changing the label value of the configmap 08/30/23 06:50:02.949
    STEP: Expecting to observe a delete notification for the watched object 08/30/23 06:50:03.014
    Aug 30 06:50:03.014: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90476 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:50:03.015: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90480 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:50:03.015: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90487 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:02 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/30/23 06:50:03.016
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/30/23 06:50:03.05
    STEP: changing the label value of the configmap back 08/30/23 06:50:13.05
    STEP: modifying the configmap a third time 08/30/23 06:50:13.093
    STEP: deleting the configmap 08/30/23 06:50:13.146
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/30/23 06:50:13.171
    Aug 30 06:50:13.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90569 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:50:13.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90570 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 06:50:13.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9438  0b7e3926-dad5-4f0c-b65d-1e0b8e2cfa69 90571 0 2023-08-30 06:50:02 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-30 06:50:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:50:13.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9438" for this suite. 08/30/23 06:50:13.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:50:13.232
Aug 30 06:50:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 06:50:13.233
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:50:13.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:50:13.311
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-cr8xz" 08/30/23 06:50:13.335
Aug 30 06:50:13.362: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard cpu limit of 500m
Aug 30 06:50:13.362: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.362
STEP: Confirm /status for "e2e-rq-status-cr8xz" resourceQuota via watch 08/30/23 06:50:13.387
Aug 30 06:50:13.392: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList(nil)
Aug 30 06:50:13.392: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 30 06:50:13.392: INFO: ResourceQuota "e2e-rq-status-cr8xz" /status was updated
STEP: Patching hard spec values for cpu & memory 08/30/23 06:50:13.405
Aug 30 06:50:13.445: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard cpu limit of 1
Aug 30 06:50:13.445: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.445
STEP: Confirm /status for "e2e-rq-status-cr8xz" resourceQuota via watch 08/30/23 06:50:13.47
Aug 30 06:50:13.475: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 30 06:50:13.475: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 30 06:50:13.475: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 30 06:50:13.475: INFO: ResourceQuota "e2e-rq-status-cr8xz" /status was patched
STEP: Get "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.475
Aug 30 06:50:13.490: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard cpu of 1
Aug 30 06:50:13.490: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-cr8xz" /status before checking Spec is unchanged 08/30/23 06:50:13.5
Aug 30 06:50:13.534: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard cpu of 2
Aug 30 06:50:13.534: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard memory of 2Gi
Aug 30 06:50:13.539: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 30 06:52:38.564: INFO: ResourceQuota "e2e-rq-status-cr8xz" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 06:52:38.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1253" for this suite. 08/30/23 06:52:38.579
------------------------------
• [SLOW TEST] [145.391 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:50:13.232
    Aug 30 06:50:13.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 06:50:13.233
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:50:13.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:50:13.311
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-cr8xz" 08/30/23 06:50:13.335
    Aug 30 06:50:13.362: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard cpu limit of 500m
    Aug 30 06:50:13.362: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.362
    STEP: Confirm /status for "e2e-rq-status-cr8xz" resourceQuota via watch 08/30/23 06:50:13.387
    Aug 30 06:50:13.392: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList(nil)
    Aug 30 06:50:13.392: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 30 06:50:13.392: INFO: ResourceQuota "e2e-rq-status-cr8xz" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/30/23 06:50:13.405
    Aug 30 06:50:13.445: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard cpu limit of 1
    Aug 30 06:50:13.445: INFO: Resource quota "e2e-rq-status-cr8xz" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.445
    STEP: Confirm /status for "e2e-rq-status-cr8xz" resourceQuota via watch 08/30/23 06:50:13.47
    Aug 30 06:50:13.475: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 30 06:50:13.475: INFO: observed resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 30 06:50:13.475: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 30 06:50:13.475: INFO: ResourceQuota "e2e-rq-status-cr8xz" /status was patched
    STEP: Get "e2e-rq-status-cr8xz" /status 08/30/23 06:50:13.475
    Aug 30 06:50:13.490: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard cpu of 1
    Aug 30 06:50:13.490: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-cr8xz" /status before checking Spec is unchanged 08/30/23 06:50:13.5
    Aug 30 06:50:13.534: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard cpu of 2
    Aug 30 06:50:13.534: INFO: Resourcequota "e2e-rq-status-cr8xz" reports status: hard memory of 2Gi
    Aug 30 06:50:13.539: INFO: Found resourceQuota "e2e-rq-status-cr8xz" in namespace "resourcequota-1253" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 30 06:52:38.564: INFO: ResourceQuota "e2e-rq-status-cr8xz" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:52:38.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1253" for this suite. 08/30/23 06:52:38.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:52:38.624
Aug 30 06:52:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sysctl 08/30/23 06:52:38.625
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:52:38.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:52:38.728
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/30/23 06:52:38.739
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:52:38.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4561" for this suite. 08/30/23 06:52:38.773
------------------------------
• [0.200 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:52:38.624
    Aug 30 06:52:38.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sysctl 08/30/23 06:52:38.625
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:52:38.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:52:38.728
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/30/23 06:52:38.739
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:52:38.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4561" for this suite. 08/30/23 06:52:38.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:52:38.825
Aug 30 06:52:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir-wrapper 08/30/23 06:52:38.826
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:52:38.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:52:38.903
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/30/23 06:52:38.915
STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:52:40.263
Aug 30 06:52:40.318: INFO: Pod name wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95: Found 0 pods out of 5
Aug 30 06:52:45.357: INFO: Pod name wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/30/23 06:52:45.357
Aug 30 06:52:45.357: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:45.378: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm": Phase="Running", Reason="", readiness=true. Elapsed: 20.976134ms
Aug 30 06:52:45.378: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm" satisfied condition "running"
Aug 30 06:52:45.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:45.432: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh": Phase="Pending", Reason="", readiness=false. Elapsed: 53.926083ms
Aug 30 06:52:47.450: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.072540478s
Aug 30 06:52:47.450: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh" satisfied condition "running"
Aug 30 06:52:47.450: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:47.467: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp": Phase="Running", Reason="", readiness=true. Elapsed: 16.491797ms
Aug 30 06:52:47.467: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp" satisfied condition "running"
Aug 30 06:52:47.467: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:47.488: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn": Phase="Running", Reason="", readiness=true. Elapsed: 20.98033ms
Aug 30 06:52:47.488: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn" satisfied condition "running"
Aug 30 06:52:47.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:47.512: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv": Phase="Running", Reason="", readiness=true. Elapsed: 24.054869ms
Aug 30 06:52:47.513: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:52:47.513
Aug 30 06:52:47.606: INFO: Deleting ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 took: 25.213868ms
Aug 30 06:52:47.807: INFO: Terminating ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 pods took: 201.16544ms
STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:52:51.021
Aug 30 06:52:51.065: INFO: Pod name wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f: Found 0 pods out of 5
Aug 30 06:52:56.092: INFO: Pod name wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/30/23 06:52:56.093
Aug 30 06:52:56.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:56.105: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs": Phase="Running", Reason="", readiness=true. Elapsed: 12.731906ms
Aug 30 06:52:56.105: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs" satisfied condition "running"
Aug 30 06:52:56.105: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:56.120: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555": Phase="Running", Reason="", readiness=true. Elapsed: 14.272538ms
Aug 30 06:52:56.120: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555" satisfied condition "running"
Aug 30 06:52:56.120: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:56.137: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.973783ms
Aug 30 06:52:56.137: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv" satisfied condition "running"
Aug 30 06:52:56.137: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:56.149: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6": Phase="Running", Reason="", readiness=true. Elapsed: 12.228572ms
Aug 30 06:52:56.149: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6" satisfied condition "running"
Aug 30 06:52:56.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:52:56.169: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx": Phase="Running", Reason="", readiness=true. Elapsed: 19.79435ms
Aug 30 06:52:56.169: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:52:56.169
Aug 30 06:52:56.267: INFO: Deleting ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f took: 28.792497ms
Aug 30 06:52:56.468: INFO: Terminating ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f pods took: 200.984706ms
STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:53:01.211
Aug 30 06:53:01.382: INFO: Pod name wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7: Found 0 pods out of 5
Aug 30 06:53:06.423: INFO: Pod name wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/30/23 06:53:06.423
Aug 30 06:53:06.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:53:06.438: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c": Phase="Running", Reason="", readiness=true. Elapsed: 14.358912ms
Aug 30 06:53:06.438: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c" satisfied condition "running"
Aug 30 06:53:06.438: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:53:06.465: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf": Phase="Running", Reason="", readiness=true. Elapsed: 27.830476ms
Aug 30 06:53:06.466: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf" satisfied condition "running"
Aug 30 06:53:06.466: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:53:06.484: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx": Phase="Running", Reason="", readiness=true. Elapsed: 18.262114ms
Aug 30 06:53:06.484: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx" satisfied condition "running"
Aug 30 06:53:06.484: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:53:06.507: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9": Phase="Running", Reason="", readiness=true. Elapsed: 23.618843ms
Aug 30 06:53:06.508: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9" satisfied condition "running"
Aug 30 06:53:06.508: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5" in namespace "emptydir-wrapper-8022" to be "running"
Aug 30 06:53:06.539: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5": Phase="Running", Reason="", readiness=true. Elapsed: 31.681329ms
Aug 30 06:53:06.539: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:53:06.539
Aug 30 06:53:06.673: INFO: Deleting ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 took: 29.582088ms
Aug 30 06:53:06.878: INFO: Terminating ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 pods took: 205.483343ms
STEP: Cleaning up the configMaps 08/30/23 06:53:10.279
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:53:11.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-8022" for this suite. 08/30/23 06:53:12.004
------------------------------
• [SLOW TEST] [33.203 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:52:38.825
    Aug 30 06:52:38.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir-wrapper 08/30/23 06:52:38.826
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:52:38.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:52:38.903
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/30/23 06:52:38.915
    STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:52:40.263
    Aug 30 06:52:40.318: INFO: Pod name wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95: Found 0 pods out of 5
    Aug 30 06:52:45.357: INFO: Pod name wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/30/23 06:52:45.357
    Aug 30 06:52:45.357: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:45.378: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm": Phase="Running", Reason="", readiness=true. Elapsed: 20.976134ms
    Aug 30 06:52:45.378: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tbxm" satisfied condition "running"
    Aug 30 06:52:45.378: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:45.432: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh": Phase="Pending", Reason="", readiness=false. Elapsed: 53.926083ms
    Aug 30 06:52:47.450: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.072540478s
    Aug 30 06:52:47.450: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-5tsdh" satisfied condition "running"
    Aug 30 06:52:47.450: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:47.467: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp": Phase="Running", Reason="", readiness=true. Elapsed: 16.491797ms
    Aug 30 06:52:47.467: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-7cbgp" satisfied condition "running"
    Aug 30 06:52:47.467: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:47.488: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn": Phase="Running", Reason="", readiness=true. Elapsed: 20.98033ms
    Aug 30 06:52:47.488: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-l8mqn" satisfied condition "running"
    Aug 30 06:52:47.488: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:47.512: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv": Phase="Running", Reason="", readiness=true. Elapsed: 24.054869ms
    Aug 30 06:52:47.513: INFO: Pod "wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95-zpbrv" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:52:47.513
    Aug 30 06:52:47.606: INFO: Deleting ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 took: 25.213868ms
    Aug 30 06:52:47.807: INFO: Terminating ReplicationController wrapped-volume-race-9fafd45e-1f63-437d-9c43-65af28cbcd95 pods took: 201.16544ms
    STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:52:51.021
    Aug 30 06:52:51.065: INFO: Pod name wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f: Found 0 pods out of 5
    Aug 30 06:52:56.092: INFO: Pod name wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/30/23 06:52:56.093
    Aug 30 06:52:56.093: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:56.105: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs": Phase="Running", Reason="", readiness=true. Elapsed: 12.731906ms
    Aug 30 06:52:56.105: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-5h7cs" satisfied condition "running"
    Aug 30 06:52:56.105: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:56.120: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555": Phase="Running", Reason="", readiness=true. Elapsed: 14.272538ms
    Aug 30 06:52:56.120: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-dn555" satisfied condition "running"
    Aug 30 06:52:56.120: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:56.137: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv": Phase="Running", Reason="", readiness=true. Elapsed: 16.973783ms
    Aug 30 06:52:56.137: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-gvwkv" satisfied condition "running"
    Aug 30 06:52:56.137: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:56.149: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6": Phase="Running", Reason="", readiness=true. Elapsed: 12.228572ms
    Aug 30 06:52:56.149: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pczw6" satisfied condition "running"
    Aug 30 06:52:56.149: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:52:56.169: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx": Phase="Running", Reason="", readiness=true. Elapsed: 19.79435ms
    Aug 30 06:52:56.169: INFO: Pod "wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f-pmbcx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:52:56.169
    Aug 30 06:52:56.267: INFO: Deleting ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f took: 28.792497ms
    Aug 30 06:52:56.468: INFO: Terminating ReplicationController wrapped-volume-race-725a2251-733a-4326-bb98-46fbeead3e8f pods took: 200.984706ms
    STEP: Creating RC which spawns configmap-volume pods 08/30/23 06:53:01.211
    Aug 30 06:53:01.382: INFO: Pod name wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7: Found 0 pods out of 5
    Aug 30 06:53:06.423: INFO: Pod name wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/30/23 06:53:06.423
    Aug 30 06:53:06.423: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:53:06.438: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c": Phase="Running", Reason="", readiness=true. Elapsed: 14.358912ms
    Aug 30 06:53:06.438: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-25b7c" satisfied condition "running"
    Aug 30 06:53:06.438: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:53:06.465: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf": Phase="Running", Reason="", readiness=true. Elapsed: 27.830476ms
    Aug 30 06:53:06.466: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-cs5cf" satisfied condition "running"
    Aug 30 06:53:06.466: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:53:06.484: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx": Phase="Running", Reason="", readiness=true. Elapsed: 18.262114ms
    Aug 30 06:53:06.484: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-gcvkx" satisfied condition "running"
    Aug 30 06:53:06.484: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:53:06.507: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9": Phase="Running", Reason="", readiness=true. Elapsed: 23.618843ms
    Aug 30 06:53:06.508: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-hkgr9" satisfied condition "running"
    Aug 30 06:53:06.508: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5" in namespace "emptydir-wrapper-8022" to be "running"
    Aug 30 06:53:06.539: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5": Phase="Running", Reason="", readiness=true. Elapsed: 31.681329ms
    Aug 30 06:53:06.539: INFO: Pod "wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7-xtnd5" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 in namespace emptydir-wrapper-8022, will wait for the garbage collector to delete the pods 08/30/23 06:53:06.539
    Aug 30 06:53:06.673: INFO: Deleting ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 took: 29.582088ms
    Aug 30 06:53:06.878: INFO: Terminating ReplicationController wrapped-volume-race-abdcc531-6c46-4bd6-8f37-bc92f82a3bb7 pods took: 205.483343ms
    STEP: Cleaning up the configMaps 08/30/23 06:53:10.279
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:53:11.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-8022" for this suite. 08/30/23 06:53:12.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:53:12.029
Aug 30 06:53:12.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename server-version 08/30/23 06:53:12.03
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:53:12.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:53:12.219
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/30/23 06:53:12.231
STEP: Confirm major version 08/30/23 06:53:12.239
Aug 30 06:53:12.239: INFO: Major version: 1
STEP: Confirm minor version 08/30/23 06:53:12.239
Aug 30 06:53:12.239: INFO: cleanMinorVersion: 26
Aug 30 06:53:12.239: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 30 06:53:12.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9338" for this suite. 08/30/23 06:53:12.257
------------------------------
• [0.270 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:53:12.029
    Aug 30 06:53:12.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename server-version 08/30/23 06:53:12.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:53:12.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:53:12.219
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/30/23 06:53:12.231
    STEP: Confirm major version 08/30/23 06:53:12.239
    Aug 30 06:53:12.239: INFO: Major version: 1
    STEP: Confirm minor version 08/30/23 06:53:12.239
    Aug 30 06:53:12.239: INFO: cleanMinorVersion: 26
    Aug 30 06:53:12.239: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:53:12.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9338" for this suite. 08/30/23 06:53:12.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:53:12.302
Aug 30 06:53:12.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-watch 08/30/23 06:53:12.303
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:53:12.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:53:12.405
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 30 06:53:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Creating first CR  08/30/23 06:53:15.065
Aug 30 06:53:15.110: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:15Z]] name:name1 resourceVersion:92179 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/30/23 06:53:25.113
Aug 30 06:53:25.130: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:25Z]] name:name2 resourceVersion:92356 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/30/23 06:53:35.135
Aug 30 06:53:35.156: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:35Z]] name:name1 resourceVersion:92401 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/30/23 06:53:45.156
Aug 30 06:53:45.172: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:45Z]] name:name2 resourceVersion:92442 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/30/23 06:53:55.173
Aug 30 06:53:55.221: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:35Z]] name:name1 resourceVersion:92481 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/30/23 06:54:05.222
Aug 30 06:54:05.239: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:45Z]] name:name2 resourceVersion:92521 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:54:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-5937" for this suite. 08/30/23 06:54:15.795
------------------------------
• [SLOW TEST] [63.552 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:53:12.302
    Aug 30 06:53:12.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-watch 08/30/23 06:53:12.303
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:53:12.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:53:12.405
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 30 06:53:12.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Creating first CR  08/30/23 06:53:15.065
    Aug 30 06:53:15.110: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:15Z]] name:name1 resourceVersion:92179 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/30/23 06:53:25.113
    Aug 30 06:53:25.130: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:25Z]] name:name2 resourceVersion:92356 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/30/23 06:53:35.135
    Aug 30 06:53:35.156: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:35Z]] name:name1 resourceVersion:92401 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/30/23 06:53:45.156
    Aug 30 06:53:45.172: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:45Z]] name:name2 resourceVersion:92442 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/30/23 06:53:55.173
    Aug 30 06:53:55.221: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:35Z]] name:name1 resourceVersion:92481 uid:711f5f0a-e2ec-4923-98dc-c12fea5642d4] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/30/23 06:54:05.222
    Aug 30 06:54:05.239: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-30T06:53:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-30T06:53:45Z]] name:name2 resourceVersion:92521 uid:82648339-d144-45bf-85d0-8fbe42d83733] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:54:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-5937" for this suite. 08/30/23 06:54:15.795
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:54:15.855
Aug 30 06:54:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 06:54:15.857
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:54:15.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:54:15.981
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 06:54:16.112
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:54:16.649
STEP: Deploying the webhook pod 08/30/23 06:54:16.725
STEP: Wait for the deployment to be ready 08/30/23 06:54:16.803
Aug 30 06:54:16.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 06:54:18.866
STEP: Verifying the service has paired with the endpoint 08/30/23 06:54:18.918
Aug 30 06:54:19.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/30/23 06:54:19.931
STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:19.931
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/30/23 06:54:19.983
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/30/23 06:54:21.013
STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:21.013
STEP: Having no error when timeout is longer than webhook latency 08/30/23 06:54:22.129
STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:22.129
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/30/23 06:54:27.242
STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:27.242
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:54:32.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4883" for this suite. 08/30/23 06:54:32.485
STEP: Destroying namespace "webhook-4883-markers" for this suite. 08/30/23 06:54:32.511
------------------------------
• [SLOW TEST] [16.678 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:54:15.855
    Aug 30 06:54:15.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 06:54:15.857
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:54:15.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:54:15.981
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 06:54:16.112
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 06:54:16.649
    STEP: Deploying the webhook pod 08/30/23 06:54:16.725
    STEP: Wait for the deployment to be ready 08/30/23 06:54:16.803
    Aug 30 06:54:16.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 06:54:18.866
    STEP: Verifying the service has paired with the endpoint 08/30/23 06:54:18.918
    Aug 30 06:54:19.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/30/23 06:54:19.931
    STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:19.931
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/30/23 06:54:19.983
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/30/23 06:54:21.013
    STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:21.013
    STEP: Having no error when timeout is longer than webhook latency 08/30/23 06:54:22.129
    STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:22.129
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/30/23 06:54:27.242
    STEP: Registering slow webhook via the AdmissionRegistration API 08/30/23 06:54:27.242
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:54:32.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4883" for this suite. 08/30/23 06:54:32.485
    STEP: Destroying namespace "webhook-4883-markers" for this suite. 08/30/23 06:54:32.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:54:32.535
Aug 30 06:54:32.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:54:32.537
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:54:32.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:54:32.605
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe in namespace container-probe-2916 08/30/23 06:54:32.621
Aug 30 06:54:32.650: INFO: Waiting up to 5m0s for pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe" in namespace "container-probe-2916" to be "not pending"
Aug 30 06:54:32.662: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.959024ms
Aug 30 06:54:34.676: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025971318s
Aug 30 06:54:34.676: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe" satisfied condition "not pending"
Aug 30 06:54:34.676: INFO: Started pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe in namespace container-probe-2916
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:54:34.676
Aug 30 06:54:34.688: INFO: Initial restart count of pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe is 0
Aug 30 06:54:54.859: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 1 (20.171396822s elapsed)
Aug 30 06:55:15.015: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 2 (40.326959615s elapsed)
Aug 30 06:55:35.201: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 3 (1m0.513156236s elapsed)
Aug 30 06:55:55.349: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 4 (1m20.661217184s elapsed)
Aug 30 06:57:08.011: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 5 (2m33.322725819s elapsed)
STEP: deleting the pod 08/30/23 06:57:08.011
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:08.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2916" for this suite. 08/30/23 06:57:08.072
------------------------------
• [SLOW TEST] [155.560 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:54:32.535
    Aug 30 06:54:32.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:54:32.537
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:54:32.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:54:32.605
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe in namespace container-probe-2916 08/30/23 06:54:32.621
    Aug 30 06:54:32.650: INFO: Waiting up to 5m0s for pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe" in namespace "container-probe-2916" to be "not pending"
    Aug 30 06:54:32.662: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe": Phase="Pending", Reason="", readiness=false. Elapsed: 11.959024ms
    Aug 30 06:54:34.676: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe": Phase="Running", Reason="", readiness=true. Elapsed: 2.025971318s
    Aug 30 06:54:34.676: INFO: Pod "liveness-642eb280-8920-4374-b1c9-3ce88800dffe" satisfied condition "not pending"
    Aug 30 06:54:34.676: INFO: Started pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe in namespace container-probe-2916
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:54:34.676
    Aug 30 06:54:34.688: INFO: Initial restart count of pod liveness-642eb280-8920-4374-b1c9-3ce88800dffe is 0
    Aug 30 06:54:54.859: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 1 (20.171396822s elapsed)
    Aug 30 06:55:15.015: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 2 (40.326959615s elapsed)
    Aug 30 06:55:35.201: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 3 (1m0.513156236s elapsed)
    Aug 30 06:55:55.349: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 4 (1m20.661217184s elapsed)
    Aug 30 06:57:08.011: INFO: Restart count of pod container-probe-2916/liveness-642eb280-8920-4374-b1c9-3ce88800dffe is now 5 (2m33.322725819s elapsed)
    STEP: deleting the pod 08/30/23 06:57:08.011
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:08.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2916" for this suite. 08/30/23 06:57:08.072
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:08.096
Aug 30 06:57:08.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:57:08.097
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:08.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:08.167
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/30/23 06:57:08.176
W0830 06:57:08.276791      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:57:08.276: INFO: Waiting up to 5m0s for pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f" in namespace "emptydir-8775" to be "Succeeded or Failed"
Aug 30 06:57:08.300: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 23.518361ms
Aug 30 06:57:10.314: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03786298s
Aug 30 06:57:12.319: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04268046s
STEP: Saw pod success 08/30/23 06:57:12.319
Aug 30 06:57:12.319: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f" satisfied condition "Succeeded or Failed"
Aug 30 06:57:12.339: INFO: Trying to get logs from node 10.135.139.190 pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f container test-container: <nil>
STEP: delete the pod 08/30/23 06:57:12.394
Aug 30 06:57:12.436: INFO: Waiting for pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f to disappear
Aug 30 06:57:12.446: INFO: Pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8775" for this suite. 08/30/23 06:57:12.458
------------------------------
• [4.384 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:08.096
    Aug 30 06:57:08.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:57:08.097
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:08.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:08.167
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/30/23 06:57:08.176
    W0830 06:57:08.276791      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:57:08.276: INFO: Waiting up to 5m0s for pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f" in namespace "emptydir-8775" to be "Succeeded or Failed"
    Aug 30 06:57:08.300: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 23.518361ms
    Aug 30 06:57:10.314: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03786298s
    Aug 30 06:57:12.319: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04268046s
    STEP: Saw pod success 08/30/23 06:57:12.319
    Aug 30 06:57:12.319: INFO: Pod "pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f" satisfied condition "Succeeded or Failed"
    Aug 30 06:57:12.339: INFO: Trying to get logs from node 10.135.139.190 pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f container test-container: <nil>
    STEP: delete the pod 08/30/23 06:57:12.394
    Aug 30 06:57:12.436: INFO: Waiting for pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f to disappear
    Aug 30 06:57:12.446: INFO: Pod pod-bba0ff7b-1bc7-4b00-8b77-b9f41e1d1d8f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:12.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8775" for this suite. 08/30/23 06:57:12.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:12.485
Aug 30 06:57:12.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:57:12.486
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:12.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:12.557
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/30/23 06:57:12.566
Aug 30 06:57:12.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c" in namespace "projected-7265" to be "Succeeded or Failed"
Aug 30 06:57:12.616: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.653001ms
Aug 30 06:57:14.631: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028503982s
Aug 30 06:57:16.636: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033463637s
STEP: Saw pod success 08/30/23 06:57:16.636
Aug 30 06:57:16.636: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c" satisfied condition "Succeeded or Failed"
Aug 30 06:57:16.647: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c container client-container: <nil>
STEP: delete the pod 08/30/23 06:57:16.666
Aug 30 06:57:16.723: INFO: Waiting for pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c to disappear
Aug 30 06:57:16.734: INFO: Pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:16.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7265" for this suite. 08/30/23 06:57:16.746
------------------------------
• [4.284 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:12.485
    Aug 30 06:57:12.485: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:57:12.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:12.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:12.557
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/30/23 06:57:12.566
    Aug 30 06:57:12.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c" in namespace "projected-7265" to be "Succeeded or Failed"
    Aug 30 06:57:12.616: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.653001ms
    Aug 30 06:57:14.631: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028503982s
    Aug 30 06:57:16.636: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033463637s
    STEP: Saw pod success 08/30/23 06:57:16.636
    Aug 30 06:57:16.636: INFO: Pod "downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c" satisfied condition "Succeeded or Failed"
    Aug 30 06:57:16.647: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c container client-container: <nil>
    STEP: delete the pod 08/30/23 06:57:16.666
    Aug 30 06:57:16.723: INFO: Waiting for pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c to disappear
    Aug 30 06:57:16.734: INFO: Pod downwardapi-volume-44d62b7e-45e6-4961-b6b4-bd4e7305006c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:16.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7265" for this suite. 08/30/23 06:57:16.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:16.772
Aug 30 06:57:16.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 06:57:16.773
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:16.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:16.835
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-5468408d-1b3c-495d-b518-473ed83f88cd 08/30/23 06:57:16.85
STEP: Creating a pod to test consume secrets 08/30/23 06:57:16.89
Aug 30 06:57:16.918: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05" in namespace "projected-8271" to be "Succeeded or Failed"
Aug 30 06:57:16.941: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Pending", Reason="", readiness=false. Elapsed: 23.471105ms
Aug 30 06:57:18.998: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080814699s
Aug 30 06:57:20.954: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036143973s
STEP: Saw pod success 08/30/23 06:57:20.954
Aug 30 06:57:20.954: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05" satisfied condition "Succeeded or Failed"
Aug 30 06:57:20.966: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/30/23 06:57:20.985
Aug 30 06:57:21.037: INFO: Waiting for pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 to disappear
Aug 30 06:57:21.049: INFO: Pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:21.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8271" for this suite. 08/30/23 06:57:21.067
------------------------------
• [4.321 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:16.772
    Aug 30 06:57:16.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 06:57:16.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:16.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:16.835
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-5468408d-1b3c-495d-b518-473ed83f88cd 08/30/23 06:57:16.85
    STEP: Creating a pod to test consume secrets 08/30/23 06:57:16.89
    Aug 30 06:57:16.918: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05" in namespace "projected-8271" to be "Succeeded or Failed"
    Aug 30 06:57:16.941: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Pending", Reason="", readiness=false. Elapsed: 23.471105ms
    Aug 30 06:57:18.998: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080814699s
    Aug 30 06:57:20.954: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036143973s
    STEP: Saw pod success 08/30/23 06:57:20.954
    Aug 30 06:57:20.954: INFO: Pod "pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05" satisfied condition "Succeeded or Failed"
    Aug 30 06:57:20.966: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 06:57:20.985
    Aug 30 06:57:21.037: INFO: Waiting for pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 to disappear
    Aug 30 06:57:21.049: INFO: Pod pod-projected-secrets-3dfc532e-6c2c-4040-9375-18e70ebf7d05 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:21.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8271" for this suite. 08/30/23 06:57:21.067
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:21.093
Aug 30 06:57:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename csistoragecapacity 08/30/23 06:57:21.095
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:21.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:21.187
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/30/23 06:57:21.197
STEP: getting /apis/storage.k8s.io 08/30/23 06:57:21.207
STEP: getting /apis/storage.k8s.io/v1 08/30/23 06:57:21.211
STEP: creating 08/30/23 06:57:21.215
STEP: watching 08/30/23 06:57:21.319
Aug 30 06:57:21.319: INFO: starting watch
STEP: getting 08/30/23 06:57:21.372
STEP: listing in namespace 08/30/23 06:57:21.383
STEP: listing across namespaces 08/30/23 06:57:21.427
STEP: patching 08/30/23 06:57:21.439
STEP: updating 08/30/23 06:57:21.457
Aug 30 06:57:21.478: INFO: waiting for watch events with expected annotations in namespace
Aug 30 06:57:21.478: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/30/23 06:57:21.478
STEP: deleting a collection 08/30/23 06:57:21.521
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:21.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-2826" for this suite. 08/30/23 06:57:21.592
------------------------------
• [0.527 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:21.093
    Aug 30 06:57:21.094: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename csistoragecapacity 08/30/23 06:57:21.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:21.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:21.187
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/30/23 06:57:21.197
    STEP: getting /apis/storage.k8s.io 08/30/23 06:57:21.207
    STEP: getting /apis/storage.k8s.io/v1 08/30/23 06:57:21.211
    STEP: creating 08/30/23 06:57:21.215
    STEP: watching 08/30/23 06:57:21.319
    Aug 30 06:57:21.319: INFO: starting watch
    STEP: getting 08/30/23 06:57:21.372
    STEP: listing in namespace 08/30/23 06:57:21.383
    STEP: listing across namespaces 08/30/23 06:57:21.427
    STEP: patching 08/30/23 06:57:21.439
    STEP: updating 08/30/23 06:57:21.457
    Aug 30 06:57:21.478: INFO: waiting for watch events with expected annotations in namespace
    Aug 30 06:57:21.478: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/30/23 06:57:21.478
    STEP: deleting a collection 08/30/23 06:57:21.521
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:21.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-2826" for this suite. 08/30/23 06:57:21.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:21.622
Aug 30 06:57:21.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:57:21.623
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:21.713
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:21.727
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 30 06:57:21.849: INFO: Waiting up to 5m0s for pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3" in namespace "svcaccounts-4946" to be "running"
Aug 30 06:57:21.888: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 39.005144ms
Aug 30 06:57:23.902: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.0527499s
Aug 30 06:57:23.902: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3" satisfied condition "running"
STEP: reading a file in the container 08/30/23 06:57:23.902
Aug 30 06:57:23.902: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/30/23 06:57:24.165
Aug 30 06:57:24.165: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/30/23 06:57:24.406
Aug 30 06:57:24.406: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 30 06:57:24.857: INFO: Got root ca configmap in namespace "svcaccounts-4946"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:24.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4946" for this suite. 08/30/23 06:57:24.882
------------------------------
• [3.335 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:21.622
    Aug 30 06:57:21.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 06:57:21.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:21.713
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:21.727
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 30 06:57:21.849: INFO: Waiting up to 5m0s for pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3" in namespace "svcaccounts-4946" to be "running"
    Aug 30 06:57:21.888: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 39.005144ms
    Aug 30 06:57:23.902: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.0527499s
    Aug 30 06:57:23.902: INFO: Pod "pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3" satisfied condition "running"
    STEP: reading a file in the container 08/30/23 06:57:23.902
    Aug 30 06:57:23.902: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/30/23 06:57:24.165
    Aug 30 06:57:24.165: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/30/23 06:57:24.406
    Aug 30 06:57:24.406: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4946 pod-service-account-768acdef-09cc-4b27-a662-78ed1cb18fa3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 30 06:57:24.857: INFO: Got root ca configmap in namespace "svcaccounts-4946"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:24.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4946" for this suite. 08/30/23 06:57:24.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:24.958
Aug 30 06:57:24.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:57:24.959
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:25.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:25.099
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 30 06:57:25.192: INFO: Waiting up to 5m0s for pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775" in namespace "container-probe-8651" to be "running and ready"
Aug 30 06:57:25.209: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Pending", Reason="", readiness=false. Elapsed: 17.121553ms
Aug 30 06:57:25.209: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 06:57:27.240: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 2.048109289s
Aug 30 06:57:27.240: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:29.221: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 4.028924914s
Aug 30 06:57:29.221: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:31.221: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 6.029311905s
Aug 30 06:57:31.221: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:33.223: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 8.031817526s
Aug 30 06:57:33.224: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:35.226: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 10.034111616s
Aug 30 06:57:35.226: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:37.232: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 12.040808719s
Aug 30 06:57:37.233: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:39.229: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 14.037201732s
Aug 30 06:57:39.229: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:41.228: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 16.036407977s
Aug 30 06:57:41.228: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:43.233: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 18.041063451s
Aug 30 06:57:43.233: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:45.245: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 20.053594389s
Aug 30 06:57:45.245: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
Aug 30 06:57:47.231: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=true. Elapsed: 22.039085395s
Aug 30 06:57:47.231: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = true)
Aug 30 06:57:47.231: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775" satisfied condition "running and ready"
Aug 30 06:57:47.292: INFO: Container started at 2023-08-30 06:57:26 +0000 UTC, pod became ready at 2023-08-30 06:57:45 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:47.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8651" for this suite. 08/30/23 06:57:47.329
------------------------------
• [SLOW TEST] [22.397 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:24.958
    Aug 30 06:57:24.958: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:57:24.959
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:25.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:25.099
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 30 06:57:25.192: INFO: Waiting up to 5m0s for pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775" in namespace "container-probe-8651" to be "running and ready"
    Aug 30 06:57:25.209: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Pending", Reason="", readiness=false. Elapsed: 17.121553ms
    Aug 30 06:57:25.209: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 06:57:27.240: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 2.048109289s
    Aug 30 06:57:27.240: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:29.221: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 4.028924914s
    Aug 30 06:57:29.221: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:31.221: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 6.029311905s
    Aug 30 06:57:31.221: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:33.223: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 8.031817526s
    Aug 30 06:57:33.224: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:35.226: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 10.034111616s
    Aug 30 06:57:35.226: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:37.232: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 12.040808719s
    Aug 30 06:57:37.233: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:39.229: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 14.037201732s
    Aug 30 06:57:39.229: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:41.228: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 16.036407977s
    Aug 30 06:57:41.228: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:43.233: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 18.041063451s
    Aug 30 06:57:43.233: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:45.245: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=false. Elapsed: 20.053594389s
    Aug 30 06:57:45.245: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = false)
    Aug 30 06:57:47.231: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775": Phase="Running", Reason="", readiness=true. Elapsed: 22.039085395s
    Aug 30 06:57:47.231: INFO: The phase of Pod test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775 is Running (Ready = true)
    Aug 30 06:57:47.231: INFO: Pod "test-webserver-8469ca9d-20f1-428a-b485-efc7640b2775" satisfied condition "running and ready"
    Aug 30 06:57:47.292: INFO: Container started at 2023-08-30 06:57:26 +0000 UTC, pod became ready at 2023-08-30 06:57:45 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:47.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8651" for this suite. 08/30/23 06:57:47.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:47.359
Aug 30 06:57:47.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 06:57:47.36
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:47.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:47.564
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/30/23 06:57:47.585
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local;sleep 1; done
 08/30/23 06:57:47.621
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local;sleep 1; done
 08/30/23 06:57:47.621
STEP: creating a pod to probe DNS 08/30/23 06:57:47.621
STEP: submitting the pod to kubernetes 08/30/23 06:57:47.622
Aug 30 06:57:47.815: INFO: Waiting up to 15m0s for pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1" in namespace "dns-8801" to be "running"
Aug 30 06:57:47.930: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 115.825681ms
Aug 30 06:57:49.953: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138383867s
Aug 30 06:57:51.951: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Running", Reason="", readiness=true. Elapsed: 4.135984907s
Aug 30 06:57:51.951: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1" satisfied condition "running"
STEP: retrieving the pod 08/30/23 06:57:51.951
STEP: looking for the results for each expected name from probers 08/30/23 06:57:51.963
Aug 30 06:57:51.987: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.003: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.023: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.039: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.056: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.071: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.087: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.101: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
Aug 30 06:57:52.101: INFO: Lookups using dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local]

Aug 30 06:57:57.251: INFO: DNS probes using dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1 succeeded

STEP: deleting the pod 08/30/23 06:57:57.251
STEP: deleting the test headless service 08/30/23 06:57:57.296
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 06:57:57.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8801" for this suite. 08/30/23 06:57:57.429
------------------------------
• [SLOW TEST] [10.129 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:47.359
    Aug 30 06:57:47.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 06:57:47.36
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:47.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:47.564
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/30/23 06:57:47.585
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local;sleep 1; done
     08/30/23 06:57:47.621
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8801.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local;sleep 1; done
     08/30/23 06:57:47.621
    STEP: creating a pod to probe DNS 08/30/23 06:57:47.621
    STEP: submitting the pod to kubernetes 08/30/23 06:57:47.622
    Aug 30 06:57:47.815: INFO: Waiting up to 15m0s for pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1" in namespace "dns-8801" to be "running"
    Aug 30 06:57:47.930: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 115.825681ms
    Aug 30 06:57:49.953: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138383867s
    Aug 30 06:57:51.951: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1": Phase="Running", Reason="", readiness=true. Elapsed: 4.135984907s
    Aug 30 06:57:51.951: INFO: Pod "dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 06:57:51.951
    STEP: looking for the results for each expected name from probers 08/30/23 06:57:51.963
    Aug 30 06:57:51.987: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.003: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.023: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.039: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.056: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.071: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.087: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.101: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local from pod dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1: the server could not find the requested resource (get pods dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1)
    Aug 30 06:57:52.101: INFO: Lookups using dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8801.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8801.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8801.svc.cluster.local jessie_udp@dns-test-service-2.dns-8801.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8801.svc.cluster.local]

    Aug 30 06:57:57.251: INFO: DNS probes using dns-8801/dns-test-2265c07f-fb61-4c89-b9b7-e5da31171fc1 succeeded

    STEP: deleting the pod 08/30/23 06:57:57.251
    STEP: deleting the test headless service 08/30/23 06:57:57.296
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:57:57.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8801" for this suite. 08/30/23 06:57:57.429
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:57:57.503
Aug 30 06:57:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 06:57:57.504
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:57.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:57.603
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/30/23 06:57:57.615
STEP: When the matched label of one of its pods change 08/30/23 06:57:57.631
Aug 30 06:57:57.652: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 30 06:58:02.670: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/30/23 06:58:02.746
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 06:58:03.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8207" for this suite. 08/30/23 06:58:03.807
------------------------------
• [SLOW TEST] [6.349 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:57:57.503
    Aug 30 06:57:57.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 06:57:57.504
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:57:57.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:57:57.603
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/30/23 06:57:57.615
    STEP: When the matched label of one of its pods change 08/30/23 06:57:57.631
    Aug 30 06:57:57.652: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 30 06:58:02.670: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/30/23 06:58:02.746
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:58:03.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8207" for this suite. 08/30/23 06:58:03.807
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:58:03.853
Aug 30 06:58:03.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-pred 08/30/23 06:58:03.857
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:03.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:04.022
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 30 06:58:04.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 06:58:04.230: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 06:58:04.342: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.183 before test
Aug 30 06:58:04.434: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.434: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 06:58:04.434: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.434: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:58:04.435: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 06:58:04.435: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:58:04.435: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Aug 30 06:58:04.435: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:58:04.435: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 06:58:04.435: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container vpn ready: true, restart count 1
Aug 30 06:58:04.435: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.435: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 06:58:04.436: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:58:04.436: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 06:58:04.436: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 06:58:04.436: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 06:58:04.436: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:58:04.436: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 06:58:04.436: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:58:04.436: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 06:58:04.436: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Aug 30 06:58:04.436: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container console ready: true, restart count 0
Aug 30 06:58:04.436: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:58:04.436: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 06:58:04.436: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.436: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:58:04.436: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.436: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:58:04.436: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.436: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 06:58:04.436: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:58:04.437: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:58:04.437: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container router ready: true, restart count 0
Aug 30 06:58:04.437: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 06:58:04.437: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 06:58:04.437: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 06:58:04.437: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:58:04.437: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 06:58:04.437: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.437: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:58:04.437: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:58:04.437: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.437: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:58:04.438: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:58:04.438: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:58:04.438: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:58:04.438: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:58:04.438: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:58:04.438: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.438: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:58:04.438: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 06:58:04.438: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:58:04.438: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 06:58:04.438: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 06:58:04.438: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.438: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 06:58:04.438: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.439: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:58:04.439: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.439: INFO: 	Container metrics ready: true, restart count 2
Aug 30 06:58:04.439: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.439: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 06:58:04.439: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.439: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 06:58:04.439: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:58:04.439: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:58:04.439: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.185 before test
Aug 30 06:58:04.748: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:58:04.748: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 06:58:04.748: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:58:04.748: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:58:04.748: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:58:04.748: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:58:04.748: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 06:58:04.748: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container webhook ready: true, restart count 0
Aug 30 06:58:04.748: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container console ready: true, restart count 0
Aug 30 06:58:04.748: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container download-server ready: true, restart count 0
Aug 30 06:58:04.748: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:58:04.748: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container registry ready: true, restart count 0
Aug 30 06:58:04.748: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:58:04.748: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:58:04.748: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container router ready: true, restart count 0
Aug 30 06:58:04.748: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container migrator ready: true, restart count 0
Aug 30 06:58:04.748: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:58:04.748: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:58:04.748: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:58:04.748: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 06:58:04.748: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 06:58:04.748: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 06:58:04.748: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:58:04.748: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 06:58:04.748: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 06:58:04.748: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 06:58:04.748: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 06:58:04.748: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 06:58:04.748: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container reload ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 06:58:04.748: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 06:58:04.748: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:58:04.748: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 06:58:04.748: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:58:04.748: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:58:04.748: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:58:04.748: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container network-operator ready: true, restart count 0
Aug 30 06:58:04.748: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:58:04.748: INFO: collect-profiles-28222950-256v9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:30:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:58:04.748: INFO: collect-profiles-28222965-gxwc6 from openshift-operator-lifecycle-manager started at 2023-08-30 06:45:00 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 06:58:04.748: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 06:58:04.748: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container service-ca-controller ready: true, restart count 0
Aug 30 06:58:04.748: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container e2e ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:58:04.748: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:58:04.748: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 06:58:04.748: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.748: INFO: 	Container tigera-operator ready: true, restart count 6
Aug 30 06:58:04.748: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.190 before test
Aug 30 06:58:04.806: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 06:58:04.806: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 06:58:04.806: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 06:58:04.806: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container pause ready: true, restart count 0
Aug 30 06:58:04.806: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 06:58:04.806: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container tuned ready: true, restart count 0
Aug 30 06:58:04.806: INFO: dns-default-n295h from openshift-dns started at 2023-08-30 06:28:53 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container dns ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 06:58:04.806: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 06:58:04.806: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 06:58:04.806: INFO: ingress-canary-h6ntm from openshift-ingress-canary started at 2023-08-30 06:28:53 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 06:58:04.806: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 06:58:04.806: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 06:58:04.806: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 06:58:04.806: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 06:58:04.806: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 06:58:04.806: INFO: pod-release-96h7h from replication-controller-8207 started at 2023-08-30 06:57:57 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container pod-release ready: true, restart count 0
Aug 30 06:58:04.806: INFO: pod-release-h2z5h from replication-controller-8207 started at 2023-08-30 06:58:02 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container pod-release ready: true, restart count 0
Aug 30 06:58:04.806: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 06:58:04.806: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 06:58:04.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 06:58:04.806: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node 10.135.139.183 08/30/23 06:58:04.998
STEP: verifying the node has the label node 10.135.139.185 08/30/23 06:58:05.051
STEP: verifying the node has the label node 10.135.139.190 08/30/23 06:58:05.227
Aug 30 06:58:05.393: INFO: Pod calico-kube-controllers-8c94dd78-pv85v requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod calico-node-2nwc8 requesting resource cpu=250m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod calico-node-95g5x requesting resource cpu=250m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod calico-node-rkdbq requesting resource cpu=250m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod calico-typha-6668d4cdd9-hl2qp requesting resource cpu=250m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod calico-typha-6668d4cdd9-n2znw requesting resource cpu=250m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-687wz requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-fqb6w requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-vxc59 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc requesting resource cpu=5m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w requesting resource cpu=5m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ibm-file-plugin-77c56989c6-nkjrh requesting resource cpu=50m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-bvskn requesting resource cpu=5m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-g6vmc requesting resource cpu=5m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-j9sbd requesting resource cpu=5m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.183 requesting resource cpu=26m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.185 requesting resource cpu=26m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.190 requesting resource cpu=26m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod ibm-storage-metrics-agent-66b6778cfb-7cpg6 requesting resource cpu=60m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibm-storage-watcher-569f8b7c46-vxtjw requesting resource cpu=50m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-92fnq requesting resource cpu=50m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-llp66 requesting resource cpu=50m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-xtxbl requesting resource cpu=50m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-plugin-7556db7ff5-s5xzr requesting resource cpu=50m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod vpn-5cf898c745-hk9nt requesting resource cpu=5m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod cluster-node-tuning-operator-6f7b6884b9-2qg9l requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod tuned-j5t4h requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod tuned-xkwnz requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod tuned-zzmsd requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod cluster-samples-operator-5db6d764c6-9s952 requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod cluster-storage-operator-848968879c-br4hq requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-b5685b8b7-s7d7c requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-b5685b8b7-x2252 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-operator-85b4b8fdc8-htd5f requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod csi-snapshot-webhook-645cd76dd7-bhc7s requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod csi-snapshot-webhook-645cd76dd7-dmm7c requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod console-operator-77698fb45f-7tq8z requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod console-76ccb968f7-h5h2q requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod console-76ccb968f7-xrkzz requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod downloads-55ff47758f-g7rfm requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod downloads-55ff47758f-p9bfz requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod dns-operator-54bdb67d9f-8cjg2 requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod dns-default-5mmgg requesting resource cpu=60m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod dns-default-5rrtf requesting resource cpu=60m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod dns-default-n295h requesting resource cpu=60m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod node-resolver-b2rmk requesting resource cpu=5m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod node-resolver-nnr9x requesting resource cpu=5m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod node-resolver-vhd6d requesting resource cpu=5m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod cluster-image-registry-operator-77f67cc94-8p5p5 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod image-registry-6f96b7475f-n4rwx requesting resource cpu=100m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod node-ca-dq2gx requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod node-ca-kjt5c requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod node-ca-zh287 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ingress-canary-85cph requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod ingress-canary-h6ntm requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod ingress-canary-jfxbv requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod ingress-operator-cb44b8bc7-2rjr2 requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod router-default-9c97f6b97-jfjfq requesting resource cpu=100m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod router-default-9c97f6b97-tkkf8 requesting resource cpu=100m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod insights-operator-7f9b7d96b4-9cv5s requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-5hwpc requesting resource cpu=110m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-fsndc requesting resource cpu=110m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-p9frm requesting resource cpu=110m on Node 10.135.139.190
Aug 30 06:58:05.393: INFO: Pod kube-storage-version-migrator-operator-854564dc54-mj7zl requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.393: INFO: Pod migrator-75dd49656f-g9cr8 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod certified-operators-dw8qh requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod community-operators-cmrk9 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.393: INFO: Pod marketplace-operator-dcc4b747b-4bcck requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.398: INFO: Pod redhat-marketplace-9tdl9 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod redhat-operators-d6hnk requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.135.139.183
Aug 30 06:58:05.398: INFO: Pod cluster-monitoring-operator-7bc996789-qqt52 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.398: INFO: Pod kube-state-metrics-5fdc98dfcd-qf85n requesting resource cpu=4m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod node-exporter-9pzcl requesting resource cpu=9m on Node 10.135.139.183
Aug 30 06:58:05.398: INFO: Pod node-exporter-bwpv7 requesting resource cpu=9m on Node 10.135.139.190
Aug 30 06:58:05.398: INFO: Pod node-exporter-r4rwz requesting resource cpu=9m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod openshift-state-metrics-7d998cd668-snkcx requesting resource cpu=3m on Node 10.135.139.185
Aug 30 06:58:05.398: INFO: Pod prometheus-adapter-5d4fdc4794-48ncq requesting resource cpu=1m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod prometheus-adapter-5d4fdc4794-gct89 requesting resource cpu=1m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod prometheus-operator-77dbfbbd6f-bm4pb requesting resource cpu=6m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod prometheus-operator-admission-webhook-748bd6896b-544b2 requesting resource cpu=5m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod prometheus-operator-admission-webhook-748bd6896b-8b8tk requesting resource cpu=5m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod telemeter-client-cc9864968-9jhnm requesting resource cpu=3m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod thanos-querier-7bd6db4456-6vdlm requesting resource cpu=15m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod thanos-querier-7bd6db4456-jqzhk requesting resource cpu=15m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-d5v92 requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-fd7l2 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-mpkt8 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod multus-admission-controller-68f648d7b7-65hkj requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod multus-admission-controller-68f648d7b7-74l5c requesting resource cpu=20m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod multus-pt4x5 requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod multus-vllh7 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod multus-znp7c requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-78ggq requesting resource cpu=20m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-d65fw requesting resource cpu=20m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-lk2w7 requesting resource cpu=20m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod network-check-source-8dd86ffb8-k4c5v requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod network-check-target-grps8 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod network-check-target-rsbp5 requesting resource cpu=10m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod network-check-target-zvrg2 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod network-operator-68ffb666f9-kw748 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod catalog-operator-56d6f4596f-fzjbx requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod olm-operator-79d7d96656-gs9jc requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod package-server-manager-54dcf5867b-z6ksr requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod packageserver-56d55d9ff4-489b6 requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod packageserver-56d55d9ff4-g9wz7 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod metrics-7d985d4645-ntm6t requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod push-gateway-86f4464769-8nhh2 requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod service-ca-operator-fdbd6d689-hvb8m requesting resource cpu=10m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod service-ca-6bf49cb844-8pjfw requesting resource cpu=10m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod pod-release-96h7h requesting resource cpu=0m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod pod-release-h2z5h requesting resource cpu=0m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod sonobuoy-e2e-job-f8d5f988a13f45ff requesting resource cpu=0m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 requesting resource cpu=0m on Node 10.135.139.183
Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 requesting resource cpu=0m on Node 10.135.139.185
Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz requesting resource cpu=0m on Node 10.135.139.190
Aug 30 06:58:05.403: INFO: Pod tigera-operator-7f6598444c-rhbbz requesting resource cpu=100m on Node 10.135.139.185
STEP: Starting Pods to consume most of the cluster CPU. 08/30/23 06:58:05.403
Aug 30 06:58:05.403: INFO: Creating a pod which consumes cpu=1610m on Node 10.135.139.183
Aug 30 06:58:05.467: INFO: Creating a pod which consumes cpu=1924m on Node 10.135.139.185
Aug 30 06:58:05.490: INFO: Creating a pod which consumes cpu=2145m on Node 10.135.139.190
Aug 30 06:58:05.516: INFO: Waiting up to 5m0s for pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5" in namespace "sched-pred-3576" to be "running"
Aug 30 06:58:05.536: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.880448ms
Aug 30 06:58:07.557: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041443779s
Aug 30 06:58:09.548: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Running", Reason="", readiness=true. Elapsed: 4.032280872s
Aug 30 06:58:09.548: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5" satisfied condition "running"
Aug 30 06:58:09.548: INFO: Waiting up to 5m0s for pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09" in namespace "sched-pred-3576" to be "running"
Aug 30 06:58:09.560: INFO: Pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09": Phase="Running", Reason="", readiness=true. Elapsed: 11.807514ms
Aug 30 06:58:09.560: INFO: Pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09" satisfied condition "running"
Aug 30 06:58:09.560: INFO: Waiting up to 5m0s for pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e" in namespace "sched-pred-3576" to be "running"
Aug 30 06:58:09.573: INFO: Pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e": Phase="Running", Reason="", readiness=true. Elapsed: 12.783911ms
Aug 30 06:58:09.573: INFO: Pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/30/23 06:58:09.573
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f14a422d4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5 to 10.135.139.183] 08/30/23 06:58:09.582
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1833a7fea], Reason = [AddedInterface], Message = [Add eth0 [172.30.86.165/32] from k8s-pod-network] 08/30/23 06:58:09.582
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f196015097], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.582
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1a509e052], Reason = [Created], Message = [Created container filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1a7a0ab4c], Reason = [Started], Message = [Started container filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f14d6acfa0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e to 10.135.139.190] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f17ec766d4], Reason = [AddedInterface], Message = [Add eth0 [172.30.58.125/32] from k8s-pod-network] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f18ce5fa99], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f196db3643], Reason = [Created], Message = [Created container filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f198efda44], Reason = [Started], Message = [Started container filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f14bf448c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09 to 10.135.139.185] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f180855188], Reason = [AddedInterface], Message = [Add eth0 [172.30.224.54/32] from k8s-pod-network] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f19025f466], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f19ee1323c], Reason = [Created], Message = [Created container filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f1a1f7b02b], Reason = [Started], Message = [Started container filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09] 08/30/23 06:58:09.583
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.178016f2405c03ea], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/30/23 06:58:09.628
STEP: removing the label node off the node 10.135.139.183 08/30/23 06:58:10.633
STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.677
STEP: removing the label node off the node 10.135.139.185 08/30/23 06:58:10.719
STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.771
STEP: removing the label node off the node 10.135.139.190 08/30/23 06:58:10.781
STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.838
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 06:58:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3576" for this suite. 08/30/23 06:58:10.868
------------------------------
• [SLOW TEST] [7.039 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:58:03.853
    Aug 30 06:58:03.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-pred 08/30/23 06:58:03.857
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:03.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:04.022
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 30 06:58:04.118: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 30 06:58:04.230: INFO: Waiting for terminating namespaces to be deleted...
    Aug 30 06:58:04.342: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.183 before test
    Aug 30 06:58:04.434: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.434: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 30 06:58:04.434: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.434: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 06:58:04.435: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Aug 30 06:58:04.435: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container vpn ready: true, restart count 1
    Aug 30 06:58:04.435: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.435: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Aug 30 06:58:04.436: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container console-operator ready: true, restart count 1
    Aug 30 06:58:04.436: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Aug 30 06:58:04.436: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container dns-operator ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.436: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Aug 30 06:58:04.436: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container ingress-operator ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container insights-operator ready: true, restart count 1
    Aug 30 06:58:04.437: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Aug 30 06:58:04.437: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container marketplace-operator ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:58:04.437: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:58:04.437: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.437: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container check-endpoints ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container catalog-operator ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container olm-operator ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.438: INFO: 	Container package-server-manager ready: true, restart count 0
    Aug 30 06:58:04.438: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.439: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:58:04.439: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.439: INFO: 	Container metrics ready: true, restart count 2
    Aug 30 06:58:04.439: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.439: INFO: 	Container push-gateway ready: true, restart count 0
    Aug 30 06:58:04.439: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.439: INFO: 	Container service-ca-operator ready: true, restart count 1
    Aug 30 06:58:04.439: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:58:04.439: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:58:04.439: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.185 before test
    Aug 30 06:58:04.748: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container console ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container registry ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container router ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container migrator ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 06:58:04.748: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container reload ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container telemeter-client ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container network-operator ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: collect-profiles-28222935-2cwf9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:15:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:58:04.748: INFO: collect-profiles-28222950-256v9 from openshift-operator-lifecycle-manager started at 2023-08-30 06:30:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:58:04.748: INFO: collect-profiles-28222965-gxwc6 from openshift-operator-lifecycle-manager started at 2023-08-30 06:45:00 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 06:58:04.748: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container service-ca-controller ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container e2e ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 06:58:04.748: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.748: INFO: 	Container tigera-operator ready: true, restart count 6
    Aug 30 06:58:04.748: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.190 before test
    Aug 30 06:58:04.806: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container pause ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: dns-default-n295h from openshift-dns started at 2023-08-30 06:28:53 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container dns ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container pvc-permissions ready: false, restart count 0
    Aug 30 06:58:04.806: INFO: ingress-canary-h6ntm from openshift-ingress-canary started at 2023-08-30 06:28:53 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: pod-release-96h7h from replication-controller-8207 started at 2023-08-30 06:57:57 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container pod-release ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: pod-release-h2z5h from replication-controller-8207 started at 2023-08-30 06:58:02 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container pod-release ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 06:58:04.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 06:58:04.806: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node 10.135.139.183 08/30/23 06:58:04.998
    STEP: verifying the node has the label node 10.135.139.185 08/30/23 06:58:05.051
    STEP: verifying the node has the label node 10.135.139.190 08/30/23 06:58:05.227
    Aug 30 06:58:05.393: INFO: Pod calico-kube-controllers-8c94dd78-pv85v requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod calico-node-2nwc8 requesting resource cpu=250m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod calico-node-95g5x requesting resource cpu=250m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod calico-node-rkdbq requesting resource cpu=250m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod calico-typha-6668d4cdd9-hl2qp requesting resource cpu=250m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod calico-typha-6668d4cdd9-n2znw requesting resource cpu=250m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-687wz requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-fqb6w requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod managed-storage-validation-webhooks-5445c9f55f-vxc59 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc requesting resource cpu=5m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w requesting resource cpu=5m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ibm-file-plugin-77c56989c6-nkjrh requesting resource cpu=50m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-bvskn requesting resource cpu=5m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-g6vmc requesting resource cpu=5m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ibm-keepalived-watcher-j9sbd requesting resource cpu=5m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.183 requesting resource cpu=26m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.185 requesting resource cpu=26m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ibm-master-proxy-static-10.135.139.190 requesting resource cpu=26m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod ibm-storage-metrics-agent-66b6778cfb-7cpg6 requesting resource cpu=60m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibm-storage-watcher-569f8b7c46-vxtjw requesting resource cpu=50m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-92fnq requesting resource cpu=50m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-llp66 requesting resource cpu=50m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-driver-xtxbl requesting resource cpu=50m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ibmcloud-block-storage-plugin-7556db7ff5-s5xzr requesting resource cpu=50m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod vpn-5cf898c745-hk9nt requesting resource cpu=5m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod cluster-node-tuning-operator-6f7b6884b9-2qg9l requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod tuned-j5t4h requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod tuned-xkwnz requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod tuned-zzmsd requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod cluster-samples-operator-5db6d764c6-9s952 requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod cluster-storage-operator-848968879c-br4hq requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-b5685b8b7-s7d7c requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-b5685b8b7-x2252 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod csi-snapshot-controller-operator-85b4b8fdc8-htd5f requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod csi-snapshot-webhook-645cd76dd7-bhc7s requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod csi-snapshot-webhook-645cd76dd7-dmm7c requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod console-operator-77698fb45f-7tq8z requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod console-76ccb968f7-h5h2q requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod console-76ccb968f7-xrkzz requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod downloads-55ff47758f-g7rfm requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod downloads-55ff47758f-p9bfz requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod dns-operator-54bdb67d9f-8cjg2 requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod dns-default-5mmgg requesting resource cpu=60m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod dns-default-5rrtf requesting resource cpu=60m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod dns-default-n295h requesting resource cpu=60m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod node-resolver-b2rmk requesting resource cpu=5m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod node-resolver-nnr9x requesting resource cpu=5m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod node-resolver-vhd6d requesting resource cpu=5m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod cluster-image-registry-operator-77f67cc94-8p5p5 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod image-registry-6f96b7475f-n4rwx requesting resource cpu=100m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod node-ca-dq2gx requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod node-ca-kjt5c requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod node-ca-zh287 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ingress-canary-85cph requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod ingress-canary-h6ntm requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod ingress-canary-jfxbv requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod ingress-operator-cb44b8bc7-2rjr2 requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod router-default-9c97f6b97-jfjfq requesting resource cpu=100m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod router-default-9c97f6b97-tkkf8 requesting resource cpu=100m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod insights-operator-7f9b7d96b4-9cv5s requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-5hwpc requesting resource cpu=110m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-fsndc requesting resource cpu=110m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod openshift-kube-proxy-p9frm requesting resource cpu=110m on Node 10.135.139.190
    Aug 30 06:58:05.393: INFO: Pod kube-storage-version-migrator-operator-854564dc54-mj7zl requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.393: INFO: Pod migrator-75dd49656f-g9cr8 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod certified-operators-dw8qh requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod community-operators-cmrk9 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.393: INFO: Pod marketplace-operator-dcc4b747b-4bcck requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.398: INFO: Pod redhat-marketplace-9tdl9 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod redhat-operators-d6hnk requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.135.139.183
    Aug 30 06:58:05.398: INFO: Pod cluster-monitoring-operator-7bc996789-qqt52 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.398: INFO: Pod kube-state-metrics-5fdc98dfcd-qf85n requesting resource cpu=4m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod node-exporter-9pzcl requesting resource cpu=9m on Node 10.135.139.183
    Aug 30 06:58:05.398: INFO: Pod node-exporter-bwpv7 requesting resource cpu=9m on Node 10.135.139.190
    Aug 30 06:58:05.398: INFO: Pod node-exporter-r4rwz requesting resource cpu=9m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod openshift-state-metrics-7d998cd668-snkcx requesting resource cpu=3m on Node 10.135.139.185
    Aug 30 06:58:05.398: INFO: Pod prometheus-adapter-5d4fdc4794-48ncq requesting resource cpu=1m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod prometheus-adapter-5d4fdc4794-gct89 requesting resource cpu=1m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod prometheus-operator-77dbfbbd6f-bm4pb requesting resource cpu=6m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod prometheus-operator-admission-webhook-748bd6896b-544b2 requesting resource cpu=5m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod prometheus-operator-admission-webhook-748bd6896b-8b8tk requesting resource cpu=5m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod telemeter-client-cc9864968-9jhnm requesting resource cpu=3m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod thanos-querier-7bd6db4456-6vdlm requesting resource cpu=15m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod thanos-querier-7bd6db4456-jqzhk requesting resource cpu=15m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-d5v92 requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-fd7l2 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod multus-additional-cni-plugins-mpkt8 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod multus-admission-controller-68f648d7b7-65hkj requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod multus-admission-controller-68f648d7b7-74l5c requesting resource cpu=20m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod multus-pt4x5 requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod multus-vllh7 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod multus-znp7c requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-78ggq requesting resource cpu=20m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-d65fw requesting resource cpu=20m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod network-metrics-daemon-lk2w7 requesting resource cpu=20m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod network-check-source-8dd86ffb8-k4c5v requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod network-check-target-grps8 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod network-check-target-rsbp5 requesting resource cpu=10m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod network-check-target-zvrg2 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod network-operator-68ffb666f9-kw748 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod catalog-operator-56d6f4596f-fzjbx requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod olm-operator-79d7d96656-gs9jc requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod package-server-manager-54dcf5867b-z6ksr requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod packageserver-56d55d9ff4-489b6 requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod packageserver-56d55d9ff4-g9wz7 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod metrics-7d985d4645-ntm6t requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod push-gateway-86f4464769-8nhh2 requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod service-ca-operator-fdbd6d689-hvb8m requesting resource cpu=10m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod service-ca-6bf49cb844-8pjfw requesting resource cpu=10m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod pod-release-96h7h requesting resource cpu=0m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod pod-release-h2z5h requesting resource cpu=0m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod sonobuoy-e2e-job-f8d5f988a13f45ff requesting resource cpu=0m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 requesting resource cpu=0m on Node 10.135.139.183
    Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 requesting resource cpu=0m on Node 10.135.139.185
    Aug 30 06:58:05.403: INFO: Pod sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz requesting resource cpu=0m on Node 10.135.139.190
    Aug 30 06:58:05.403: INFO: Pod tigera-operator-7f6598444c-rhbbz requesting resource cpu=100m on Node 10.135.139.185
    STEP: Starting Pods to consume most of the cluster CPU. 08/30/23 06:58:05.403
    Aug 30 06:58:05.403: INFO: Creating a pod which consumes cpu=1610m on Node 10.135.139.183
    Aug 30 06:58:05.467: INFO: Creating a pod which consumes cpu=1924m on Node 10.135.139.185
    Aug 30 06:58:05.490: INFO: Creating a pod which consumes cpu=2145m on Node 10.135.139.190
    Aug 30 06:58:05.516: INFO: Waiting up to 5m0s for pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5" in namespace "sched-pred-3576" to be "running"
    Aug 30 06:58:05.536: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Pending", Reason="", readiness=false. Elapsed: 19.880448ms
    Aug 30 06:58:07.557: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041443779s
    Aug 30 06:58:09.548: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5": Phase="Running", Reason="", readiness=true. Elapsed: 4.032280872s
    Aug 30 06:58:09.548: INFO: Pod "filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5" satisfied condition "running"
    Aug 30 06:58:09.548: INFO: Waiting up to 5m0s for pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09" in namespace "sched-pred-3576" to be "running"
    Aug 30 06:58:09.560: INFO: Pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09": Phase="Running", Reason="", readiness=true. Elapsed: 11.807514ms
    Aug 30 06:58:09.560: INFO: Pod "filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09" satisfied condition "running"
    Aug 30 06:58:09.560: INFO: Waiting up to 5m0s for pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e" in namespace "sched-pred-3576" to be "running"
    Aug 30 06:58:09.573: INFO: Pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e": Phase="Running", Reason="", readiness=true. Elapsed: 12.783911ms
    Aug 30 06:58:09.573: INFO: Pod "filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/30/23 06:58:09.573
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f14a422d4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5 to 10.135.139.183] 08/30/23 06:58:09.582
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1833a7fea], Reason = [AddedInterface], Message = [Add eth0 [172.30.86.165/32] from k8s-pod-network] 08/30/23 06:58:09.582
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f196015097], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.582
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1a509e052], Reason = [Created], Message = [Created container filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5.178016f1a7a0ab4c], Reason = [Started], Message = [Started container filler-pod-5319710e-607b-4193-a5d6-72f2786b61d5] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f14d6acfa0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e to 10.135.139.190] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f17ec766d4], Reason = [AddedInterface], Message = [Add eth0 [172.30.58.125/32] from k8s-pod-network] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f18ce5fa99], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f196db3643], Reason = [Created], Message = [Created container filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e.178016f198efda44], Reason = [Started], Message = [Started container filler-pod-9ebd69eb-28ce-4d34-b81c-fdcc2feed09e] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f14bf448c5], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3576/filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09 to 10.135.139.185] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f180855188], Reason = [AddedInterface], Message = [Add eth0 [172.30.224.54/32] from k8s-pod-network] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f19025f466], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f19ee1323c], Reason = [Created], Message = [Created container filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09.178016f1a1f7b02b], Reason = [Started], Message = [Started container filler-pod-b5fecbab-5f5f-47f2-8be2-19c9006c2e09] 08/30/23 06:58:09.583
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.178016f2405c03ea], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 08/30/23 06:58:09.628
    STEP: removing the label node off the node 10.135.139.183 08/30/23 06:58:10.633
    STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.677
    STEP: removing the label node off the node 10.135.139.185 08/30/23 06:58:10.719
    STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.771
    STEP: removing the label node off the node 10.135.139.190 08/30/23 06:58:10.781
    STEP: verifying the node doesn't have the label node 08/30/23 06:58:10.838
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:58:10.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3576" for this suite. 08/30/23 06:58:10.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:58:10.893
Aug 30 06:58:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 06:58:10.894
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:10.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:11.002
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/30/23 06:58:11.019
Aug 30 06:58:11.099: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557" in namespace "emptydir-1748" to be "running"
Aug 30 06:58:11.155: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Pending", Reason="", readiness=false. Elapsed: 56.418413ms
Aug 30 06:58:13.170: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071155936s
Aug 30 06:58:15.169: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Running", Reason="", readiness=false. Elapsed: 4.07081243s
Aug 30 06:58:15.169: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/30/23 06:58:15.169
Aug 30 06:58:15.170: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1748 PodName:pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 06:58:15.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 06:58:15.170: INFO: ExecWithOptions: Clientset creation
Aug 30 06:58:15.170: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1748/pods/pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 30 06:58:15.305: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 06:58:15.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1748" for this suite. 08/30/23 06:58:15.318
------------------------------
• [4.458 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:58:10.893
    Aug 30 06:58:10.893: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 06:58:10.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:10.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:11.002
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/30/23 06:58:11.019
    Aug 30 06:58:11.099: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557" in namespace "emptydir-1748" to be "running"
    Aug 30 06:58:11.155: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Pending", Reason="", readiness=false. Elapsed: 56.418413ms
    Aug 30 06:58:13.170: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071155936s
    Aug 30 06:58:15.169: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557": Phase="Running", Reason="", readiness=false. Elapsed: 4.07081243s
    Aug 30 06:58:15.169: INFO: Pod "pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/30/23 06:58:15.169
    Aug 30 06:58:15.170: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1748 PodName:pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 06:58:15.170: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 06:58:15.170: INFO: ExecWithOptions: Clientset creation
    Aug 30 06:58:15.170: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-1748/pods/pod-sharedvolume-a948b7c8-ba12-4fbd-8ab8-7c8d27226557/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 30 06:58:15.305: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 06:58:15.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1748" for this suite. 08/30/23 06:58:15.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 06:58:15.351
Aug 30 06:58:15.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 06:58:15.352
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:15.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:15.42
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 in namespace container-probe-9856 08/30/23 06:58:15.43
W0830 06:58:15.533095      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 06:58:15.533: INFO: Waiting up to 5m0s for pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797" in namespace "container-probe-9856" to be "not pending"
Aug 30 06:58:15.553: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797": Phase="Pending", Reason="", readiness=false. Elapsed: 20.727014ms
Aug 30 06:58:17.568: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797": Phase="Running", Reason="", readiness=true. Elapsed: 2.034822989s
Aug 30 06:58:17.568: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797" satisfied condition "not pending"
Aug 30 06:58:17.568: INFO: Started pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 in namespace container-probe-9856
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:58:17.568
Aug 30 06:58:17.582: INFO: Initial restart count of pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 is 0
STEP: deleting the pod 08/30/23 07:02:18.052
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 07:02:18.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9856" for this suite. 08/30/23 07:02:18.101
------------------------------
• [SLOW TEST] [242.772 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 06:58:15.351
    Aug 30 06:58:15.351: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 06:58:15.352
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 06:58:15.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 06:58:15.42
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 in namespace container-probe-9856 08/30/23 06:58:15.43
    W0830 06:58:15.533095      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 06:58:15.533: INFO: Waiting up to 5m0s for pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797" in namespace "container-probe-9856" to be "not pending"
    Aug 30 06:58:15.553: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797": Phase="Pending", Reason="", readiness=false. Elapsed: 20.727014ms
    Aug 30 06:58:17.568: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797": Phase="Running", Reason="", readiness=true. Elapsed: 2.034822989s
    Aug 30 06:58:17.568: INFO: Pod "liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797" satisfied condition "not pending"
    Aug 30 06:58:17.568: INFO: Started pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 in namespace container-probe-9856
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 06:58:17.568
    Aug 30 06:58:17.582: INFO: Initial restart count of pod liveness-ff5f4ac6-6083-4444-bc69-09343d4f4797 is 0
    STEP: deleting the pod 08/30/23 07:02:18.052
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:02:18.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9856" for this suite. 08/30/23 07:02:18.101
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:02:18.123
Aug 30 07:02:18.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 07:02:18.124
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:18.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:18.191
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 08/30/23 07:02:18.312
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:02:18.338
Aug 30 07:02:18.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:02:18.362: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:02:19.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:02:19.405: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:02:20.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:02:20.389: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 08/30/23 07:02:20.401
STEP: DeleteCollection of the DaemonSets 08/30/23 07:02:20.429
STEP: Verify that ReplicaSets have been deleted 08/30/23 07:02:20.46
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Aug 30 07:02:20.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96150"},"items":null}

Aug 30 07:02:20.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96150"},"items":[{"metadata":{"name":"daemon-set-k6gbl","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"9c9d15a6-c88f-4655-92fd-e71ba5551ae4","resourceVersion":"96142","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c47ae6810ac6e0ce062dc6239007b1cd6fa1ab32e431b19b1f46831d27c5ecc8","cni.projectcalico.org/podIP":"172.30.58.116/32","cni.projectcalico.org/podIPs":"172.30.58.116/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.58.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zgj6f","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zgj6f","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.190","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.190"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.190","podIP":"172.30.58.116","podIPs":[{"ip":"172.30.58.116"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ccd3fe0ee51452dc2d88a232bc9c6c9c968bb85a7c00c409b0fb221a4e4fec00","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-l2zgw","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"54555179-571c-4cdb-862e-ae3c08a5f30f","resourceVersion":"96145","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"52e840a88bd6d32dac33db654ed3e3d9370437828987e80a00ad3b232178c431","cni.projectcalico.org/podIP":"172.30.86.191/32","cni.projectcalico.org/podIPs":"172.30.86.191/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.86.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9cp8v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9cp8v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.183","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.183"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.183","podIP":"172.30.86.191","podIPs":[{"ip":"172.30.86.191"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://5f99a5af367320574e8f46c8c319836eb81b56e40257b4e13d49c074d0c33627","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tql7p","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"00b4e3fe-fe9d-440d-8c96-4ea5dc4db048","resourceVersion":"96147","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04f3324028623cf05e0ad27ffbc64fae2b0394fbb6bed4b16a0f5442a07e3f96","cni.projectcalico.org/podIP":"172.30.224.53/32","cni.projectcalico.org/podIPs":"172.30.224.53/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.53\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-svpr7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-svpr7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.185","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.185"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.185","podIP":"172.30.224.53","podIPs":[{"ip":"172.30.224.53"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://fdf58979a36cf04f876c0c7a216dfa6b128723fe7e57315132553d037f58aba8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:02:20.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3744" for this suite. 08/30/23 07:02:20.608
------------------------------
• [2.527 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:02:18.123
    Aug 30 07:02:18.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 07:02:18.124
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:18.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:18.191
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 08/30/23 07:02:18.312
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:02:18.338
    Aug 30 07:02:18.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:02:18.362: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:02:19.404: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:02:19.405: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:02:20.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:02:20.389: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 08/30/23 07:02:20.401
    STEP: DeleteCollection of the DaemonSets 08/30/23 07:02:20.429
    STEP: Verify that ReplicaSets have been deleted 08/30/23 07:02:20.46
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Aug 30 07:02:20.509: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96150"},"items":null}

    Aug 30 07:02:20.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96150"},"items":[{"metadata":{"name":"daemon-set-k6gbl","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"9c9d15a6-c88f-4655-92fd-e71ba5551ae4","resourceVersion":"96142","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c47ae6810ac6e0ce062dc6239007b1cd6fa1ab32e431b19b1f46831d27c5ecc8","cni.projectcalico.org/podIP":"172.30.58.116/32","cni.projectcalico.org/podIPs":"172.30.58.116/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.58.116\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zgj6f","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zgj6f","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.190","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.190"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.190","podIP":"172.30.58.116","podIPs":[{"ip":"172.30.58.116"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://ccd3fe0ee51452dc2d88a232bc9c6c9c968bb85a7c00c409b0fb221a4e4fec00","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-l2zgw","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"54555179-571c-4cdb-862e-ae3c08a5f30f","resourceVersion":"96145","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"52e840a88bd6d32dac33db654ed3e3d9370437828987e80a00ad3b232178c431","cni.projectcalico.org/podIP":"172.30.86.191/32","cni.projectcalico.org/podIPs":"172.30.86.191/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.86.191\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.86.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9cp8v","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9cp8v","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.183","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.183"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.183","podIP":"172.30.86.191","podIPs":[{"ip":"172.30.86.191"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://5f99a5af367320574e8f46c8c319836eb81b56e40257b4e13d49c074d0c33627","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tql7p","generateName":"daemon-set-","namespace":"daemonsets-3744","uid":"00b4e3fe-fe9d-440d-8c96-4ea5dc4db048","resourceVersion":"96147","creationTimestamp":"2023-08-30T07:02:18Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04f3324028623cf05e0ad27ffbc64fae2b0394fbb6bed4b16a0f5442a07e3f96","cni.projectcalico.org/podIP":"172.30.224.53/32","cni.projectcalico.org/podIPs":"172.30.224.53/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.224.53\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e9f381e6-7bce-49a5-a0ab-0dd4ef48bab3\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-30T07:02:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.224.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-svpr7","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-svpr7","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.135.139.185","securityContext":{"seLinuxOptions":{"level":"s0:c52,c29"}},"imagePullSecrets":[{"name":"default-dockercfg-84n2d"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.135.139.185"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:20Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-30T07:02:18Z"}],"hostIP":"10.135.139.185","podIP":"172.30.224.53","podIPs":[{"ip":"172.30.224.53"}],"startTime":"2023-08-30T07:02:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-30T07:02:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://fdf58979a36cf04f876c0c7a216dfa6b128723fe7e57315132553d037f58aba8","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:02:20.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3744" for this suite. 08/30/23 07:02:20.608
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:02:20.652
Aug 30 07:02:20.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:02:20.653
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:20.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:20.731
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:02:20.827
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:02:21.088
STEP: Deploying the webhook pod 08/30/23 07:02:21.118
STEP: Wait for the deployment to be ready 08/30/23 07:02:21.158
Aug 30 07:02:21.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 07:02:23.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 07:02:25.238
STEP: Verifying the service has paired with the endpoint 08/30/23 07:02:25.274
Aug 30 07:02:26.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/30/23 07:02:26.296
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/30/23 07:02:26.338
STEP: Creating a dummy validating-webhook-configuration object 08/30/23 07:02:26.375
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/30/23 07:02:26.4
STEP: Creating a dummy mutating-webhook-configuration object 08/30/23 07:02:26.418
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/30/23 07:02:26.441
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:02:26.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7074" for this suite. 08/30/23 07:02:26.668
STEP: Destroying namespace "webhook-7074-markers" for this suite. 08/30/23 07:02:26.697
------------------------------
• [SLOW TEST] [6.068 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:02:20.652
    Aug 30 07:02:20.652: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:02:20.653
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:20.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:20.731
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:02:20.827
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:02:21.088
    STEP: Deploying the webhook pod 08/30/23 07:02:21.118
    STEP: Wait for the deployment to be ready 08/30/23 07:02:21.158
    Aug 30 07:02:21.180: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 07:02:23.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 07:02:25.238
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:02:25.274
    Aug 30 07:02:26.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/30/23 07:02:26.296
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/30/23 07:02:26.338
    STEP: Creating a dummy validating-webhook-configuration object 08/30/23 07:02:26.375
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/30/23 07:02:26.4
    STEP: Creating a dummy mutating-webhook-configuration object 08/30/23 07:02:26.418
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/30/23 07:02:26.441
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:02:26.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7074" for this suite. 08/30/23 07:02:26.668
    STEP: Destroying namespace "webhook-7074-markers" for this suite. 08/30/23 07:02:26.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:02:26.723
Aug 30 07:02:26.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 07:02:26.724
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:26.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:26.806
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 30 07:02:26.816: INFO: Creating ReplicaSet my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561
Aug 30 07:02:26.858: INFO: Pod name my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Found 0 pods out of 1
Aug 30 07:02:31.872: INFO: Pod name my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Found 1 pods out of 1
Aug 30 07:02:31.872: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561" is running
Aug 30 07:02:31.872: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" in namespace "replicaset-4214" to be "running"
Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559": Phase="Running", Reason="", readiness=true. Elapsed: 13.308053ms
Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" satisfied condition "running"
Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:26 +0000 UTC Reason: Message:}])
Aug 30 07:02:31.885: INFO: Trying to dial the pod
Aug 30 07:02:36.936: INFO: Controller my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Got expected result from replica 1 [my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559]: "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:02:36.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4214" for this suite. 08/30/23 07:02:36.949
------------------------------
• [SLOW TEST] [10.247 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:02:26.723
    Aug 30 07:02:26.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 07:02:26.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:26.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:26.806
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 30 07:02:26.816: INFO: Creating ReplicaSet my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561
    Aug 30 07:02:26.858: INFO: Pod name my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Found 0 pods out of 1
    Aug 30 07:02:31.872: INFO: Pod name my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Found 1 pods out of 1
    Aug 30 07:02:31.872: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561" is running
    Aug 30 07:02:31.872: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" in namespace "replicaset-4214" to be "running"
    Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559": Phase="Running", Reason="", readiness=true. Elapsed: 13.308053ms
    Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" satisfied condition "running"
    Aug 30 07:02:31.885: INFO: Pod "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:26 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-30 07:02:26 +0000 UTC Reason: Message:}])
    Aug 30 07:02:31.885: INFO: Trying to dial the pod
    Aug 30 07:02:36.936: INFO: Controller my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561: Got expected result from replica 1 [my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559]: "my-hostname-basic-f2385594-0089-4f4c-afa2-63527d31c561-2x559", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:02:36.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4214" for this suite. 08/30/23 07:02:36.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:02:36.971
Aug 30 07:02:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:02:36.972
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:37.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:37.034
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/30/23 07:02:37.046
Aug 30 07:02:37.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: mark a version not serverd 08/30/23 07:02:49.884
STEP: check the unserved version gets removed 08/30/23 07:02:49.961
STEP: check the other version is not changed 08/30/23 07:02:54.244
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:03:08.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-122" for this suite. 08/30/23 07:03:08.267
------------------------------
• [SLOW TEST] [31.337 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:02:36.971
    Aug 30 07:02:36.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:02:36.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:02:37.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:02:37.034
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/30/23 07:02:37.046
    Aug 30 07:02:37.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: mark a version not serverd 08/30/23 07:02:49.884
    STEP: check the unserved version gets removed 08/30/23 07:02:49.961
    STEP: check the other version is not changed 08/30/23 07:02:54.244
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:03:08.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-122" for this suite. 08/30/23 07:03:08.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:03:08.309
Aug 30 07:03:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename cronjob 08/30/23 07:03:08.311
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:03:08.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:03:08.451
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/30/23 07:03:08.462
STEP: Ensuring a job is scheduled 08/30/23 07:03:08.519
STEP: Ensuring exactly one is scheduled 08/30/23 07:04:00.594
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/30/23 07:04:00.609
STEP: Ensuring no more jobs are scheduled 08/30/23 07:04:00.618
STEP: Removing cronjob 08/30/23 07:09:00.639
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:00.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1698" for this suite. 08/30/23 07:09:00.666
------------------------------
• [SLOW TEST] [352.413 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:03:08.309
    Aug 30 07:03:08.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename cronjob 08/30/23 07:03:08.311
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:03:08.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:03:08.451
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/30/23 07:03:08.462
    STEP: Ensuring a job is scheduled 08/30/23 07:03:08.519
    STEP: Ensuring exactly one is scheduled 08/30/23 07:04:00.594
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/30/23 07:04:00.609
    STEP: Ensuring no more jobs are scheduled 08/30/23 07:04:00.618
    STEP: Removing cronjob 08/30/23 07:09:00.639
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:00.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1698" for this suite. 08/30/23 07:09:00.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:00.723
Aug 30 07:09:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:09:00.724
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:00.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:00.923
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/30/23 07:09:00.929
W0830 07:09:00.963697      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:09:00.964: INFO: Waiting up to 5m0s for pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483" in namespace "emptydir-7118" to be "Succeeded or Failed"
Aug 30 07:09:00.971: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 7.844626ms
Aug 30 07:09:02.982: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018149012s
Aug 30 07:09:04.980: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015974978s
Aug 30 07:09:06.980: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01662244s
STEP: Saw pod success 08/30/23 07:09:06.981
Aug 30 07:09:06.982: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483" satisfied condition "Succeeded or Failed"
Aug 30 07:09:06.990: INFO: Trying to get logs from node 10.135.139.190 pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 container test-container: <nil>
STEP: delete the pod 08/30/23 07:09:07.029
Aug 30 07:09:07.050: INFO: Waiting for pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 to disappear
Aug 30 07:09:07.057: INFO: Pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7118" for this suite. 08/30/23 07:09:07.068
------------------------------
• [SLOW TEST] [6.364 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:00.723
    Aug 30 07:09:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:09:00.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:00.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:00.923
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/30/23 07:09:00.929
    W0830 07:09:00.963697      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:09:00.964: INFO: Waiting up to 5m0s for pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483" in namespace "emptydir-7118" to be "Succeeded or Failed"
    Aug 30 07:09:00.971: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 7.844626ms
    Aug 30 07:09:02.982: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018149012s
    Aug 30 07:09:04.980: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015974978s
    Aug 30 07:09:06.980: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01662244s
    STEP: Saw pod success 08/30/23 07:09:06.981
    Aug 30 07:09:06.982: INFO: Pod "pod-6aea7f53-2c47-4459-b43b-a108da62b483" satisfied condition "Succeeded or Failed"
    Aug 30 07:09:06.990: INFO: Trying to get logs from node 10.135.139.190 pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 container test-container: <nil>
    STEP: delete the pod 08/30/23 07:09:07.029
    Aug 30 07:09:07.050: INFO: Waiting for pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 to disappear
    Aug 30 07:09:07.057: INFO: Pod pod-6aea7f53-2c47-4459-b43b-a108da62b483 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7118" for this suite. 08/30/23 07:09:07.068
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:07.092
Aug 30 07:09:07.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 07:09:07.093
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:07.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:07.148
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/30/23 07:09:07.154
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/30/23 07:09:07.154
STEP: creating a pod to probe DNS 08/30/23 07:09:07.154
STEP: submitting the pod to kubernetes 08/30/23 07:09:07.155
Aug 30 07:09:07.191: INFO: Waiting up to 15m0s for pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b" in namespace "dns-7877" to be "running"
Aug 30 07:09:07.199: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821543ms
Aug 30 07:09:09.208: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016671389s
Aug 30 07:09:11.206: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Running", Reason="", readiness=true. Elapsed: 4.014704993s
Aug 30 07:09:11.206: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b" satisfied condition "running"
STEP: retrieving the pod 08/30/23 07:09:11.206
STEP: looking for the results for each expected name from probers 08/30/23 07:09:11.213
Aug 30 07:09:11.258: INFO: DNS probes using dns-7877/dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b succeeded

STEP: deleting the pod 08/30/23 07:09:11.258
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:11.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7877" for this suite. 08/30/23 07:09:11.294
------------------------------
• [4.224 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:07.092
    Aug 30 07:09:07.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 07:09:07.093
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:07.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:07.148
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/30/23 07:09:07.154
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/30/23 07:09:07.154
    STEP: creating a pod to probe DNS 08/30/23 07:09:07.154
    STEP: submitting the pod to kubernetes 08/30/23 07:09:07.155
    Aug 30 07:09:07.191: INFO: Waiting up to 15m0s for pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b" in namespace "dns-7877" to be "running"
    Aug 30 07:09:07.199: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.821543ms
    Aug 30 07:09:09.208: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016671389s
    Aug 30 07:09:11.206: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b": Phase="Running", Reason="", readiness=true. Elapsed: 4.014704993s
    Aug 30 07:09:11.206: INFO: Pod "dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 07:09:11.206
    STEP: looking for the results for each expected name from probers 08/30/23 07:09:11.213
    Aug 30 07:09:11.258: INFO: DNS probes using dns-7877/dns-test-03ec78d7-6b88-443e-aa18-0114b7c93e1b succeeded

    STEP: deleting the pod 08/30/23 07:09:11.258
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:11.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7877" for this suite. 08/30/23 07:09:11.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:11.328
Aug 30 07:09:11.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:09:11.329
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:11.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:11.385
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:09:11.436
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:09:11.718
STEP: Deploying the webhook pod 08/30/23 07:09:11.746
STEP: Wait for the deployment to be ready 08/30/23 07:09:11.806
Aug 30 07:09:11.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 07:09:13.86
STEP: Verifying the service has paired with the endpoint 08/30/23 07:09:13.912
Aug 30 07:09:14.912: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 30 07:09:14.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5568-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 07:09:15.441
STEP: Creating a custom resource that should be mutated by the webhook 08/30/23 07:09:15.486
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:18.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4941" for this suite. 08/30/23 07:09:18.26
STEP: Destroying namespace "webhook-4941-markers" for this suite. 08/30/23 07:09:18.279
------------------------------
• [SLOW TEST] [6.977 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:11.328
    Aug 30 07:09:11.328: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:09:11.329
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:11.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:11.385
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:09:11.436
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:09:11.718
    STEP: Deploying the webhook pod 08/30/23 07:09:11.746
    STEP: Wait for the deployment to be ready 08/30/23 07:09:11.806
    Aug 30 07:09:11.823: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 07:09:13.86
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:09:13.912
    Aug 30 07:09:14.912: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 30 07:09:14.919: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5568-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 07:09:15.441
    STEP: Creating a custom resource that should be mutated by the webhook 08/30/23 07:09:15.486
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:18.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4941" for this suite. 08/30/23 07:09:18.26
    STEP: Destroying namespace "webhook-4941-markers" for this suite. 08/30/23 07:09:18.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:18.305
Aug 30 07:09:18.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:09:18.309
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:18.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:18.366
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:09:18.373
W0830 07:09:18.401866      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:09:18.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947" in namespace "projected-1465" to be "Succeeded or Failed"
Aug 30 07:09:18.417: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 15.914111ms
Aug 30 07:09:20.425: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023833546s
Aug 30 07:09:22.427: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025715939s
Aug 30 07:09:24.429: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027543029s
STEP: Saw pod success 08/30/23 07:09:24.429
Aug 30 07:09:24.430: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947" satisfied condition "Succeeded or Failed"
Aug 30 07:09:24.436: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 container client-container: <nil>
STEP: delete the pod 08/30/23 07:09:24.452
Aug 30 07:09:24.478: INFO: Waiting for pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 to disappear
Aug 30 07:09:24.485: INFO: Pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:24.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1465" for this suite. 08/30/23 07:09:24.495
------------------------------
• [SLOW TEST] [6.211 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:18.305
    Aug 30 07:09:18.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:09:18.309
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:18.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:18.366
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:09:18.373
    W0830 07:09:18.401866      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:09:18.402: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947" in namespace "projected-1465" to be "Succeeded or Failed"
    Aug 30 07:09:18.417: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 15.914111ms
    Aug 30 07:09:20.425: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023833546s
    Aug 30 07:09:22.427: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025715939s
    Aug 30 07:09:24.429: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027543029s
    STEP: Saw pod success 08/30/23 07:09:24.429
    Aug 30 07:09:24.430: INFO: Pod "downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947" satisfied condition "Succeeded or Failed"
    Aug 30 07:09:24.436: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:09:24.452
    Aug 30 07:09:24.478: INFO: Waiting for pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 to disappear
    Aug 30 07:09:24.485: INFO: Pod downwardapi-volume-56c32b9e-5def-426c-8e80-ab85e8e35947 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:24.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1465" for this suite. 08/30/23 07:09:24.495
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:24.516
Aug 30 07:09:24.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename endpointslice 08/30/23 07:09:24.517
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:24.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:24.581
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 30 07:09:24.632: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Aug 30 07:09:24.632: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:24.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2248" for this suite. 08/30/23 07:09:24.648
------------------------------
• [0.161 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:24.516
    Aug 30 07:09:24.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename endpointslice 08/30/23 07:09:24.517
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:24.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:24.581
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 30 07:09:24.632: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Aug 30 07:09:24.632: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:24.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2248" for this suite. 08/30/23 07:09:24.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:24.683
Aug 30 07:09:24.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename conformance-tests 08/30/23 07:09:24.684
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:24.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:24.821
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/30/23 07:09:24.829
Aug 30 07:09:24.829: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4417" for this suite. 08/30/23 07:09:24.912
------------------------------
• [0.317 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:24.683
    Aug 30 07:09:24.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename conformance-tests 08/30/23 07:09:24.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:24.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:24.821
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/30/23 07:09:24.829
    Aug 30 07:09:24.829: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:24.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4417" for this suite. 08/30/23 07:09:24.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:25.002
Aug 30 07:09:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:09:25.003
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:25.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:25.163
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/30/23 07:09:25.17
Aug 30 07:09:25.195: INFO: Waiting up to 5m0s for pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895" in namespace "pods-3204" to be "running and ready"
Aug 30 07:09:25.208: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895": Phase="Pending", Reason="", readiness=false. Elapsed: 12.991422ms
Aug 30 07:09:25.208: INFO: The phase of Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:09:27.216: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895": Phase="Running", Reason="", readiness=true. Elapsed: 2.021649336s
Aug 30 07:09:27.217: INFO: The phase of Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 is Running (Ready = true)
Aug 30 07:09:27.217: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895" satisfied condition "running and ready"
Aug 30 07:09:27.230: INFO: Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 has hostIP: 10.135.139.190
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:27.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3204" for this suite. 08/30/23 07:09:27.239
------------------------------
• [2.254 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:25.002
    Aug 30 07:09:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:09:25.003
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:25.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:25.163
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/30/23 07:09:25.17
    Aug 30 07:09:25.195: INFO: Waiting up to 5m0s for pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895" in namespace "pods-3204" to be "running and ready"
    Aug 30 07:09:25.208: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895": Phase="Pending", Reason="", readiness=false. Elapsed: 12.991422ms
    Aug 30 07:09:25.208: INFO: The phase of Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:09:27.216: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895": Phase="Running", Reason="", readiness=true. Elapsed: 2.021649336s
    Aug 30 07:09:27.217: INFO: The phase of Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 is Running (Ready = true)
    Aug 30 07:09:27.217: INFO: Pod "pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895" satisfied condition "running and ready"
    Aug 30 07:09:27.230: INFO: Pod pod-hostip-c9e4c0d0-bdc7-424a-8749-b845997c4895 has hostIP: 10.135.139.190
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:27.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3204" for this suite. 08/30/23 07:09:27.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:27.259
Aug 30 07:09:27.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:09:27.26
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:27.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:27.314
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-b3a686a7-6cb2-4e3a-9ddb-cdabc24401c3 08/30/23 07:09:27.32
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:27.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-879" for this suite. 08/30/23 07:09:27.335
------------------------------
• [0.127 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:27.259
    Aug 30 07:09:27.259: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:09:27.26
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:27.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:27.314
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-b3a686a7-6cb2-4e3a-9ddb-cdabc24401c3 08/30/23 07:09:27.32
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:27.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-879" for this suite. 08/30/23 07:09:27.335
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:27.387
Aug 30 07:09:27.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:09:27.388
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:27.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:27.492
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/30/23 07:09:27.497
STEP: Creating a ResourceQuota 08/30/23 07:09:32.508
STEP: Ensuring resource quota status is calculated 08/30/23 07:09:32.537
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3194" for this suite. 08/30/23 07:09:34.568
------------------------------
• [SLOW TEST] [7.198 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:27.387
    Aug 30 07:09:27.387: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:09:27.388
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:27.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:27.492
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/30/23 07:09:27.497
    STEP: Creating a ResourceQuota 08/30/23 07:09:32.508
    STEP: Ensuring resource quota status is calculated 08/30/23 07:09:32.537
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3194" for this suite. 08/30/23 07:09:34.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:34.587
Aug 30 07:09:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 07:09:34.588
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:34.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:34.637
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7212 08/30/23 07:09:34.643
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
W0830 07:09:34.681052      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:09:34.688: INFO: Found 0 stateful pods, waiting for 1
Aug 30 07:09:44.699: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/30/23 07:09:44.748
W0830 07:09:44.793201      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 30 07:09:44.814: INFO: Found 1 stateful pods, waiting for 2
Aug 30 07:09:54.825: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 07:09:54.825: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/30/23 07:09:54.841
STEP: Delete all of the StatefulSets 08/30/23 07:09:54.854
STEP: Verify that StatefulSets have been deleted 08/30/23 07:09:54.877
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 07:09:54.888: INFO: Deleting all statefulset in ns statefulset-7212
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:54.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7212" for this suite. 08/30/23 07:09:54.942
------------------------------
• [SLOW TEST] [20.381 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:34.587
    Aug 30 07:09:34.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 07:09:34.588
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:34.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:34.637
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7212 08/30/23 07:09:34.643
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    W0830 07:09:34.681052      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:09:34.688: INFO: Found 0 stateful pods, waiting for 1
    Aug 30 07:09:44.699: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/30/23 07:09:44.748
    W0830 07:09:44.793201      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 30 07:09:44.814: INFO: Found 1 stateful pods, waiting for 2
    Aug 30 07:09:54.825: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 07:09:54.825: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/30/23 07:09:54.841
    STEP: Delete all of the StatefulSets 08/30/23 07:09:54.854
    STEP: Verify that StatefulSets have been deleted 08/30/23 07:09:54.877
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 07:09:54.888: INFO: Deleting all statefulset in ns statefulset-7212
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:54.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7212" for this suite. 08/30/23 07:09:54.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:54.969
Aug 30 07:09:54.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:09:54.971
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:55.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:55.063
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/30/23 07:09:55.069
W0830 07:09:55.096099      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:09:55.096: INFO: Waiting up to 5m0s for pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4" in namespace "downward-api-9323" to be "Succeeded or Failed"
Aug 30 07:09:55.103: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.552262ms
Aug 30 07:09:57.114: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018467473s
Aug 30 07:09:59.111: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015141181s
STEP: Saw pod success 08/30/23 07:09:59.111
Aug 30 07:09:59.111: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4" satisfied condition "Succeeded or Failed"
Aug 30 07:09:59.119: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:09:59.136
Aug 30 07:09:59.163: INFO: Waiting for pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 to disappear
Aug 30 07:09:59.170: INFO: Pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:09:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9323" for this suite. 08/30/23 07:09:59.179
------------------------------
• [4.228 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:54.969
    Aug 30 07:09:54.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:09:54.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:55.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:55.063
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/30/23 07:09:55.069
    W0830 07:09:55.096099      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:09:55.096: INFO: Waiting up to 5m0s for pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4" in namespace "downward-api-9323" to be "Succeeded or Failed"
    Aug 30 07:09:55.103: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.552262ms
    Aug 30 07:09:57.114: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018467473s
    Aug 30 07:09:59.111: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015141181s
    STEP: Saw pod success 08/30/23 07:09:59.111
    Aug 30 07:09:59.111: INFO: Pod "downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4" satisfied condition "Succeeded or Failed"
    Aug 30 07:09:59.119: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:09:59.136
    Aug 30 07:09:59.163: INFO: Waiting for pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 to disappear
    Aug 30 07:09:59.170: INFO: Pod downward-api-49abb42d-5fc1-4d86-a22d-6d0b8673c5d4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:09:59.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9323" for this suite. 08/30/23 07:09:59.179
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:09:59.199
Aug 30 07:09:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename ephemeral-containers-test 08/30/23 07:09:59.2
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:59.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:59.262
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/30/23 07:09:59.268
W0830 07:09:59.290128      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:09:59.290: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8062" to be "running and ready"
Aug 30 07:09:59.301: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.129281ms
Aug 30 07:09:59.301: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:10:01.325: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03532754s
Aug 30 07:10:01.325: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 30 07:10:01.325: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/30/23 07:10:01.332
Aug 30 07:10:01.357: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8062" to be "container debugger running"
Aug 30 07:10:01.366: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.313358ms
Aug 30 07:10:03.374: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016838991s
Aug 30 07:10:05.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018740652s
Aug 30 07:10:05.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/30/23 07:10:05.376
Aug 30 07:10:05.376: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8062 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:10:05.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:10:05.377: INFO: ExecWithOptions: Clientset creation
Aug 30 07:10:05.377: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-8062/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 30 07:10:05.548: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:05.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-8062" for this suite. 08/30/23 07:10:05.575
------------------------------
• [SLOW TEST] [6.400 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:09:59.199
    Aug 30 07:09:59.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/30/23 07:09:59.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:09:59.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:09:59.262
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/30/23 07:09:59.268
    W0830 07:09:59.290128      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container-1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container-1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container-1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container-1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:09:59.290: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8062" to be "running and ready"
    Aug 30 07:09:59.301: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 11.129281ms
    Aug 30 07:09:59.301: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:10:01.325: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03532754s
    Aug 30 07:10:01.325: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 30 07:10:01.325: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/30/23 07:10:01.332
    Aug 30 07:10:01.357: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-8062" to be "container debugger running"
    Aug 30 07:10:01.366: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.313358ms
    Aug 30 07:10:03.374: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016838991s
    Aug 30 07:10:05.376: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.018740652s
    Aug 30 07:10:05.376: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/30/23 07:10:05.376
    Aug 30 07:10:05.376: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8062 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:10:05.376: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:10:05.377: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:10:05.377: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-8062/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 30 07:10:05.548: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:05.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-8062" for this suite. 08/30/23 07:10:05.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:05.6
Aug 30 07:10:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename runtimeclass 08/30/23 07:10:05.603
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:05.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:05.666
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/30/23 07:10:05.672
STEP: getting /apis/node.k8s.io 08/30/23 07:10:05.677
STEP: getting /apis/node.k8s.io/v1 08/30/23 07:10:05.679
STEP: creating 08/30/23 07:10:05.681
STEP: watching 08/30/23 07:10:05.725
Aug 30 07:10:05.725: INFO: starting watch
STEP: getting 08/30/23 07:10:05.738
STEP: listing 08/30/23 07:10:05.754
STEP: patching 08/30/23 07:10:05.778
STEP: updating 08/30/23 07:10:05.787
Aug 30 07:10:05.801: INFO: waiting for watch events with expected annotations
STEP: deleting 08/30/23 07:10:05.802
STEP: deleting a collection 08/30/23 07:10:05.838
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1314" for this suite. 08/30/23 07:10:05.879
------------------------------
• [0.305 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:05.6
    Aug 30 07:10:05.600: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename runtimeclass 08/30/23 07:10:05.603
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:05.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:05.666
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/30/23 07:10:05.672
    STEP: getting /apis/node.k8s.io 08/30/23 07:10:05.677
    STEP: getting /apis/node.k8s.io/v1 08/30/23 07:10:05.679
    STEP: creating 08/30/23 07:10:05.681
    STEP: watching 08/30/23 07:10:05.725
    Aug 30 07:10:05.725: INFO: starting watch
    STEP: getting 08/30/23 07:10:05.738
    STEP: listing 08/30/23 07:10:05.754
    STEP: patching 08/30/23 07:10:05.778
    STEP: updating 08/30/23 07:10:05.787
    Aug 30 07:10:05.801: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/30/23 07:10:05.802
    STEP: deleting a collection 08/30/23 07:10:05.838
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:05.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1314" for this suite. 08/30/23 07:10:05.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:05.914
Aug 30 07:10:05.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:10:05.915
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:05.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:05.991
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-159c7b5a-f4b7-4e28-a25e-1ecc39eaab40 08/30/23 07:10:05.996
STEP: Creating a pod to test consume secrets 08/30/23 07:10:06.017
W0830 07:10:06.041979      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:10:06.042: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9" in namespace "projected-4979" to be "Succeeded or Failed"
Aug 30 07:10:06.055: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.794813ms
Aug 30 07:10:08.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022274059s
Aug 30 07:10:10.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021885285s
STEP: Saw pod success 08/30/23 07:10:10.064
Aug 30 07:10:10.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9" satisfied condition "Succeeded or Failed"
Aug 30 07:10:10.087: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:10:10.291
Aug 30 07:10:10.326: INFO: Waiting for pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 to disappear
Aug 30 07:10:10.336: INFO: Pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4979" for this suite. 08/30/23 07:10:10.356
------------------------------
• [4.474 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:05.914
    Aug 30 07:10:05.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:10:05.915
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:05.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:05.991
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-159c7b5a-f4b7-4e28-a25e-1ecc39eaab40 08/30/23 07:10:05.996
    STEP: Creating a pod to test consume secrets 08/30/23 07:10:06.017
    W0830 07:10:06.041979      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:10:06.042: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9" in namespace "projected-4979" to be "Succeeded or Failed"
    Aug 30 07:10:06.055: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.794813ms
    Aug 30 07:10:08.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022274059s
    Aug 30 07:10:10.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021885285s
    STEP: Saw pod success 08/30/23 07:10:10.064
    Aug 30 07:10:10.064: INFO: Pod "pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9" satisfied condition "Succeeded or Failed"
    Aug 30 07:10:10.087: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:10:10.291
    Aug 30 07:10:10.326: INFO: Waiting for pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 to disappear
    Aug 30 07:10:10.336: INFO: Pod pod-projected-secrets-0afee749-48db-496a-b79f-2fa0b39c00c9 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:10.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4979" for this suite. 08/30/23 07:10:10.356
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:10.389
Aug 30 07:10:10.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename csiinlinevolumes 08/30/23 07:10:10.39
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:10.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:10.47
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/30/23 07:10:10.475
W0830 07:10:10.586844      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 07:10:10.586937      21 warnings.go:70] pod-csi-inline-volumes uses an inline volume provided by CSIDriver e2e.example.com and namespace csiinlinevolumes-2463 has a pod security warn level that is lower than privileged
W0830 07:10:10.659055      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
W0830 07:10:10.659173      21 warnings.go:70] pod-csi-inline-volumes uses an inline volume provided by CSIDriver e2e.example.com and namespace csiinlinevolumes-2463 has a pod security warn level that is lower than privileged
STEP: getting 08/30/23 07:10:10.659
STEP: listing in namespace 08/30/23 07:10:10.728
STEP: patching 08/30/23 07:10:10.761
STEP: deleting 08/30/23 07:10:10.805
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:10.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2463" for this suite. 08/30/23 07:10:10.879
------------------------------
• [0.510 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:10.389
    Aug 30 07:10:10.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename csiinlinevolumes 08/30/23 07:10:10.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:10.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:10.47
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/30/23 07:10:10.475
    W0830 07:10:10.586844      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0830 07:10:10.586937      21 warnings.go:70] pod-csi-inline-volumes uses an inline volume provided by CSIDriver e2e.example.com and namespace csiinlinevolumes-2463 has a pod security warn level that is lower than privileged
    W0830 07:10:10.659055      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-csi-inline-volumes" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-csi-inline-volumes" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-csi-inline-volumes" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-csi-inline-volumes" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    W0830 07:10:10.659173      21 warnings.go:70] pod-csi-inline-volumes uses an inline volume provided by CSIDriver e2e.example.com and namespace csiinlinevolumes-2463 has a pod security warn level that is lower than privileged
    STEP: getting 08/30/23 07:10:10.659
    STEP: listing in namespace 08/30/23 07:10:10.728
    STEP: patching 08/30/23 07:10:10.761
    STEP: deleting 08/30/23 07:10:10.805
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:10.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2463" for this suite. 08/30/23 07:10:10.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:10.903
Aug 30 07:10:10.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 07:10:10.904
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:10.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:10.997
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/30/23 07:10:11.003
W0830 07:10:12.018862      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 08/30/23 07:10:12.018
STEP: delete the deployment 08/30/23 07:10:12.534
STEP: wait for all rs to be garbage collected 08/30/23 07:10:12.552
STEP: expected 0 rs, got 1 rs 08/30/23 07:10:12.572
STEP: expected 0 pods, got 2 pods 08/30/23 07:10:12.581
STEP: Gathering metrics 08/30/23 07:10:13.103
W0830 07:10:13.125878      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 07:10:13.125: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:13.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9056" for this suite. 08/30/23 07:10:13.138
------------------------------
• [2.258 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:10.903
    Aug 30 07:10:10.903: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 07:10:10.904
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:10.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:10.997
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/30/23 07:10:11.003
    W0830 07:10:12.018862      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 08/30/23 07:10:12.018
    STEP: delete the deployment 08/30/23 07:10:12.534
    STEP: wait for all rs to be garbage collected 08/30/23 07:10:12.552
    STEP: expected 0 rs, got 1 rs 08/30/23 07:10:12.572
    STEP: expected 0 pods, got 2 pods 08/30/23 07:10:12.581
    STEP: Gathering metrics 08/30/23 07:10:13.103
    W0830 07:10:13.125878      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 07:10:13.125: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:13.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9056" for this suite. 08/30/23 07:10:13.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:13.162
Aug 30 07:10:13.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:10:13.164
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.24
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 30 07:10:13.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-4087 version'
Aug 30 07:10:13.369: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 30 07:10:13.369: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6+73ac561\", GitCommit:\"cbbd0bdba10e0b612e32cdeb6462daa6909df4be\", GitTreeState:\"clean\", BuildDate:\"2023-07-14T11:07:31Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:13.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4087" for this suite. 08/30/23 07:10:13.398
------------------------------
• [0.262 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:13.162
    Aug 30 07:10:13.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:10:13.164
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.24
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 30 07:10:13.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-4087 version'
    Aug 30 07:10:13.369: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 30 07:10:13.369: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6\", GitCommit:\"11902a838028edef305dfe2f96be929bc4d114d8\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:56:58Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.6+73ac561\", GitCommit:\"cbbd0bdba10e0b612e32cdeb6462daa6909df4be\", GitTreeState:\"clean\", BuildDate:\"2023-07-14T11:07:31Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:13.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4087" for this suite. 08/30/23 07:10:13.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:13.425
Aug 30 07:10:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename events 08/30/23 07:10:13.427
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.502
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/30/23 07:10:13.509
STEP: listing all events in all namespaces 08/30/23 07:10:13.528
STEP: patching the test event 08/30/23 07:10:13.58
STEP: fetching the test event 08/30/23 07:10:13.601
STEP: updating the test event 08/30/23 07:10:13.611
STEP: getting the test event 08/30/23 07:10:13.637
STEP: deleting the test event 08/30/23 07:10:13.645
STEP: listing all events in all namespaces 08/30/23 07:10:13.669
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2973" for this suite. 08/30/23 07:10:13.741
------------------------------
• [0.347 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:13.425
    Aug 30 07:10:13.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename events 08/30/23 07:10:13.427
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.502
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/30/23 07:10:13.509
    STEP: listing all events in all namespaces 08/30/23 07:10:13.528
    STEP: patching the test event 08/30/23 07:10:13.58
    STEP: fetching the test event 08/30/23 07:10:13.601
    STEP: updating the test event 08/30/23 07:10:13.611
    STEP: getting the test event 08/30/23 07:10:13.637
    STEP: deleting the test event 08/30/23 07:10:13.645
    STEP: listing all events in all namespaces 08/30/23 07:10:13.669
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:13.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2973" for this suite. 08/30/23 07:10:13.741
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:13.773
Aug 30 07:10:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 07:10:13.775
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.86
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/30/23 07:10:13.903
STEP: delete the rc 08/30/23 07:10:18.944
STEP: wait for the rc to be deleted 08/30/23 07:10:18.961
STEP: Gathering metrics 08/30/23 07:10:20.019
W0830 07:10:20.085774      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 07:10:20.085: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:20.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1704" for this suite. 08/30/23 07:10:20.186
------------------------------
• [SLOW TEST] [6.451 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:13.773
    Aug 30 07:10:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 07:10:13.775
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:13.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:13.86
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/30/23 07:10:13.903
    STEP: delete the rc 08/30/23 07:10:18.944
    STEP: wait for the rc to be deleted 08/30/23 07:10:18.961
    STEP: Gathering metrics 08/30/23 07:10:20.019
    W0830 07:10:20.085774      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 07:10:20.085: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:20.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1704" for this suite. 08/30/23 07:10:20.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:20.225
Aug 30 07:10:20.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replicaset 08/30/23 07:10:20.227
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:20.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:20.318
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/30/23 07:10:20.323
STEP: Verify that the required pods have come up 08/30/23 07:10:20.343
Aug 30 07:10:20.355: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 30 07:10:25.363: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/30/23 07:10:25.363
Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-tcbt4" in namespace "replicaset-4002" to be "running"
Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-29fp7" in namespace "replicaset-4002" to be "running"
Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-bvhch" in namespace "replicaset-4002" to be "running"
Aug 30 07:10:25.370: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.743403ms
Aug 30 07:10:25.371: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200977ms
Aug 30 07:10:25.372: INFO: Pod "test-rs-29fp7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.489017ms
Aug 30 07:10:27.380: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01664526s
Aug 30 07:10:27.382: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018961116s
Aug 30 07:10:27.384: INFO: Pod "test-rs-29fp7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020981009s
Aug 30 07:10:29.382: INFO: Pod "test-rs-29fp7": Phase="Running", Reason="", readiness=true. Elapsed: 4.018920544s
Aug 30 07:10:29.382: INFO: Pod "test-rs-29fp7" satisfied condition "running"
Aug 30 07:10:29.382: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019239741s
Aug 30 07:10:29.382: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019224171s
Aug 30 07:10:31.377: INFO: Pod "test-rs-tcbt4": Phase="Running", Reason="", readiness=true. Elapsed: 6.014339841s
Aug 30 07:10:31.377: INFO: Pod "test-rs-tcbt4" satisfied condition "running"
Aug 30 07:10:31.381: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017622606s
Aug 30 07:10:33.380: INFO: Pod "test-rs-bvhch": Phase="Running", Reason="", readiness=true. Elapsed: 8.016616006s
Aug 30 07:10:33.380: INFO: Pod "test-rs-bvhch" satisfied condition "running"
Aug 30 07:10:33.390: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/30/23 07:10:33.39
STEP: DeleteCollection of the ReplicaSets 08/30/23 07:10:33.406
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/30/23 07:10:33.434
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:33.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4002" for this suite. 08/30/23 07:10:33.478
------------------------------
• [SLOW TEST] [13.308 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:20.225
    Aug 30 07:10:20.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replicaset 08/30/23 07:10:20.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:20.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:20.318
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/30/23 07:10:20.323
    STEP: Verify that the required pods have come up 08/30/23 07:10:20.343
    Aug 30 07:10:20.355: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 30 07:10:25.363: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/30/23 07:10:25.363
    Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-tcbt4" in namespace "replicaset-4002" to be "running"
    Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-29fp7" in namespace "replicaset-4002" to be "running"
    Aug 30 07:10:25.363: INFO: Waiting up to 5m0s for pod "test-rs-bvhch" in namespace "replicaset-4002" to be "running"
    Aug 30 07:10:25.370: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.743403ms
    Aug 30 07:10:25.371: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200977ms
    Aug 30 07:10:25.372: INFO: Pod "test-rs-29fp7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.489017ms
    Aug 30 07:10:27.380: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01664526s
    Aug 30 07:10:27.382: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018961116s
    Aug 30 07:10:27.384: INFO: Pod "test-rs-29fp7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020981009s
    Aug 30 07:10:29.382: INFO: Pod "test-rs-29fp7": Phase="Running", Reason="", readiness=true. Elapsed: 4.018920544s
    Aug 30 07:10:29.382: INFO: Pod "test-rs-29fp7" satisfied condition "running"
    Aug 30 07:10:29.382: INFO: Pod "test-rs-tcbt4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019239741s
    Aug 30 07:10:29.382: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019224171s
    Aug 30 07:10:31.377: INFO: Pod "test-rs-tcbt4": Phase="Running", Reason="", readiness=true. Elapsed: 6.014339841s
    Aug 30 07:10:31.377: INFO: Pod "test-rs-tcbt4" satisfied condition "running"
    Aug 30 07:10:31.381: INFO: Pod "test-rs-bvhch": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017622606s
    Aug 30 07:10:33.380: INFO: Pod "test-rs-bvhch": Phase="Running", Reason="", readiness=true. Elapsed: 8.016616006s
    Aug 30 07:10:33.380: INFO: Pod "test-rs-bvhch" satisfied condition "running"
    Aug 30 07:10:33.390: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/30/23 07:10:33.39
    STEP: DeleteCollection of the ReplicaSets 08/30/23 07:10:33.406
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/30/23 07:10:33.434
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:33.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4002" for this suite. 08/30/23 07:10:33.478
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:33.533
Aug 30 07:10:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption 08/30/23 07:10:33.535
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:33.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:33.667
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/30/23 07:10:33.676
STEP: Waiting for the pdb to be processed 08/30/23 07:10:33.692
STEP: First trying to evict a pod which shouldn't be evictable 08/30/23 07:10:35.741
STEP: Waiting for all pods to be running 08/30/23 07:10:35.741
Aug 30 07:10:35.766: INFO: pods: 0 < 3
STEP: locating a running pod 08/30/23 07:10:37.775
STEP: Updating the pdb to allow a pod to be evicted 08/30/23 07:10:37.795
STEP: Waiting for the pdb to be processed 08/30/23 07:10:37.812
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/30/23 07:10:39.827
STEP: Waiting for all pods to be running 08/30/23 07:10:39.827
STEP: Waiting for the pdb to observed all healthy pods 08/30/23 07:10:39.835
STEP: Patching the pdb to disallow a pod to be evicted 08/30/23 07:10:39.88
STEP: Waiting for the pdb to be processed 08/30/23 07:10:39.898
STEP: Waiting for all pods to be running 08/30/23 07:10:41.917
STEP: locating a running pod 08/30/23 07:10:41.926
STEP: Deleting the pdb to allow a pod to be evicted 08/30/23 07:10:41.95
STEP: Waiting for the pdb to be deleted 08/30/23 07:10:41.962
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/30/23 07:10:41.968
STEP: Waiting for all pods to be running 08/30/23 07:10:41.968
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 30 07:10:42.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7921" for this suite. 08/30/23 07:10:42.081
------------------------------
• [SLOW TEST] [8.616 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:33.533
    Aug 30 07:10:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption 08/30/23 07:10:33.535
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:33.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:33.667
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/30/23 07:10:33.676
    STEP: Waiting for the pdb to be processed 08/30/23 07:10:33.692
    STEP: First trying to evict a pod which shouldn't be evictable 08/30/23 07:10:35.741
    STEP: Waiting for all pods to be running 08/30/23 07:10:35.741
    Aug 30 07:10:35.766: INFO: pods: 0 < 3
    STEP: locating a running pod 08/30/23 07:10:37.775
    STEP: Updating the pdb to allow a pod to be evicted 08/30/23 07:10:37.795
    STEP: Waiting for the pdb to be processed 08/30/23 07:10:37.812
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/30/23 07:10:39.827
    STEP: Waiting for all pods to be running 08/30/23 07:10:39.827
    STEP: Waiting for the pdb to observed all healthy pods 08/30/23 07:10:39.835
    STEP: Patching the pdb to disallow a pod to be evicted 08/30/23 07:10:39.88
    STEP: Waiting for the pdb to be processed 08/30/23 07:10:39.898
    STEP: Waiting for all pods to be running 08/30/23 07:10:41.917
    STEP: locating a running pod 08/30/23 07:10:41.926
    STEP: Deleting the pdb to allow a pod to be evicted 08/30/23 07:10:41.95
    STEP: Waiting for the pdb to be deleted 08/30/23 07:10:41.962
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/30/23 07:10:41.968
    STEP: Waiting for all pods to be running 08/30/23 07:10:41.968
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:10:42.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7921" for this suite. 08/30/23 07:10:42.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:10:42.151
Aug 30 07:10:42.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 07:10:42.152
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:42.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:42.239
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7348 08/30/23 07:10:42.244
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/30/23 07:10:42.273
STEP: Creating pod with conflicting port in namespace statefulset-7348 08/30/23 07:10:42.294
STEP: Waiting until pod test-pod will start running in namespace statefulset-7348 08/30/23 07:10:42.318
Aug 30 07:10:42.318: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7348" to be "running"
Aug 30 07:10:42.336: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.805513ms
Aug 30 07:10:44.343: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025266675s
Aug 30 07:10:46.345: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.02691602s
Aug 30 07:10:46.345: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-7348 08/30/23 07:10:46.345
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7348 08/30/23 07:10:46.36
Aug 30 07:10:46.409: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Pending. Waiting for statefulset controller to delete.
Aug 30 07:10:46.441: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 07:10:46.462: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Failed. Waiting for statefulset controller to delete.
Aug 30 07:10:46.469: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7348
STEP: Removing pod with conflicting port in namespace statefulset-7348 08/30/23 07:10:46.47
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7348 and will be in running state 08/30/23 07:10:46.494
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 07:10:52.532: INFO: Deleting all statefulset in ns statefulset-7348
Aug 30 07:10:52.543: INFO: Scaling statefulset ss to 0
Aug 30 07:11:02.578: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 07:11:02.587: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:11:02.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7348" for this suite. 08/30/23 07:11:02.643
------------------------------
• [SLOW TEST] [20.514 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:10:42.151
    Aug 30 07:10:42.151: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 07:10:42.152
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:10:42.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:10:42.239
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7348 08/30/23 07:10:42.244
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/30/23 07:10:42.273
    STEP: Creating pod with conflicting port in namespace statefulset-7348 08/30/23 07:10:42.294
    STEP: Waiting until pod test-pod will start running in namespace statefulset-7348 08/30/23 07:10:42.318
    Aug 30 07:10:42.318: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-7348" to be "running"
    Aug 30 07:10:42.336: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.805513ms
    Aug 30 07:10:44.343: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025266675s
    Aug 30 07:10:46.345: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.02691602s
    Aug 30 07:10:46.345: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-7348 08/30/23 07:10:46.345
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7348 08/30/23 07:10:46.36
    Aug 30 07:10:46.409: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 30 07:10:46.441: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 30 07:10:46.462: INFO: Observed stateful pod in namespace: statefulset-7348, name: ss-0, uid: e1436f42-eb9c-4aa4-bbf3-6c46137d35e4, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 30 07:10:46.469: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7348
    STEP: Removing pod with conflicting port in namespace statefulset-7348 08/30/23 07:10:46.47
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7348 and will be in running state 08/30/23 07:10:46.494
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 07:10:52.532: INFO: Deleting all statefulset in ns statefulset-7348
    Aug 30 07:10:52.543: INFO: Scaling statefulset ss to 0
    Aug 30 07:11:02.578: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 07:11:02.587: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:11:02.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7348" for this suite. 08/30/23 07:11:02.643
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:11:02.666
Aug 30 07:11:02.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:11:02.667
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:11:02.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:11:02.724
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/30/23 07:11:02.729
Aug 30 07:11:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:11:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:11:30.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9911" for this suite. 08/30/23 07:11:30.638
------------------------------
• [SLOW TEST] [27.991 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:11:02.666
    Aug 30 07:11:02.666: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:11:02.667
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:11:02.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:11:02.724
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/30/23 07:11:02.729
    Aug 30 07:11:02.730: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:11:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:11:30.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9911" for this suite. 08/30/23 07:11:30.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:11:30.659
Aug 30 07:11:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption 08/30/23 07:11:30.659
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:11:30.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:11:30.722
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 30 07:11:30.759: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 07:12:30.887: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/30/23 07:12:30.903
Aug 30 07:12:30.984: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 30 07:12:30.996: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 30 07:12:31.028: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 30 07:12:31.039: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 30 07:12:31.075: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 30 07:12:31.089: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/30/23 07:12:31.089
Aug 30 07:12:31.089: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:31.105: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803176ms
Aug 30 07:12:33.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025023074s
Aug 30 07:12:35.113: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.023625527s
Aug 30 07:12:35.113: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 30 07:12:35.113: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.120: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.165925ms
Aug 30 07:12:35.120: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 07:12:35.120: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.127: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.684892ms
Aug 30 07:12:35.127: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 07:12:35.127: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.133: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.511489ms
Aug 30 07:12:35.133: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 07:12:35.133: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.139: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.477288ms
Aug 30 07:12:35.139: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 30 07:12:35.139: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.146: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.477794ms
Aug 30 07:12:35.146: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/30/23 07:12:35.146
Aug 30 07:12:35.158: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7985" to be "running"
Aug 30 07:12:35.164: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57899ms
Aug 30 07:12:37.173: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015214989s
Aug 30 07:12:39.223: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.06566569s
Aug 30 07:12:39.223: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:12:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7985" for this suite. 08/30/23 07:12:39.462
------------------------------
• [SLOW TEST] [68.832 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:11:30.659
    Aug 30 07:11:30.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption 08/30/23 07:11:30.659
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:11:30.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:11:30.722
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 30 07:11:30.759: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 07:12:30.887: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/30/23 07:12:30.903
    Aug 30 07:12:30.984: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 30 07:12:30.996: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 30 07:12:31.028: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 30 07:12:31.039: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 30 07:12:31.075: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 30 07:12:31.089: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/30/23 07:12:31.089
    Aug 30 07:12:31.089: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:31.105: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803176ms
    Aug 30 07:12:33.114: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025023074s
    Aug 30 07:12:35.113: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.023625527s
    Aug 30 07:12:35.113: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 30 07:12:35.113: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.120: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.165925ms
    Aug 30 07:12:35.120: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 07:12:35.120: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.127: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.684892ms
    Aug 30 07:12:35.127: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 07:12:35.127: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.133: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.511489ms
    Aug 30 07:12:35.133: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 07:12:35.133: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.139: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.477288ms
    Aug 30 07:12:35.139: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 30 07:12:35.139: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.146: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.477794ms
    Aug 30 07:12:35.146: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/30/23 07:12:35.146
    Aug 30 07:12:35.158: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7985" to be "running"
    Aug 30 07:12:35.164: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.57899ms
    Aug 30 07:12:37.173: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015214989s
    Aug 30 07:12:39.223: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.06566569s
    Aug 30 07:12:39.223: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:12:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7985" for this suite. 08/30/23 07:12:39.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:12:39.492
Aug 30 07:12:39.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:12:39.492
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:12:39.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:12:39.578
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:12:39.582
W0830 07:12:39.600476      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:12:39.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12" in namespace "downward-api-8928" to be "Succeeded or Failed"
Aug 30 07:12:39.610: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Pending", Reason="", readiness=false. Elapsed: 9.429654ms
Aug 30 07:12:41.630: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029514092s
Aug 30 07:12:43.637: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037015674s
STEP: Saw pod success 08/30/23 07:12:43.637
Aug 30 07:12:43.639: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12" satisfied condition "Succeeded or Failed"
Aug 30 07:12:43.659: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 container client-container: <nil>
STEP: delete the pod 08/30/23 07:12:43.736
Aug 30 07:12:43.762: INFO: Waiting for pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 to disappear
Aug 30 07:12:43.771: INFO: Pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:12:43.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8928" for this suite. 08/30/23 07:12:43.78
------------------------------
• [4.309 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:12:39.492
    Aug 30 07:12:39.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:12:39.492
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:12:39.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:12:39.578
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:12:39.582
    W0830 07:12:39.600476      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:12:39.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12" in namespace "downward-api-8928" to be "Succeeded or Failed"
    Aug 30 07:12:39.610: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Pending", Reason="", readiness=false. Elapsed: 9.429654ms
    Aug 30 07:12:41.630: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029514092s
    Aug 30 07:12:43.637: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037015674s
    STEP: Saw pod success 08/30/23 07:12:43.637
    Aug 30 07:12:43.639: INFO: Pod "downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12" satisfied condition "Succeeded or Failed"
    Aug 30 07:12:43.659: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:12:43.736
    Aug 30 07:12:43.762: INFO: Waiting for pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 to disappear
    Aug 30 07:12:43.771: INFO: Pod downwardapi-volume-aa4a9021-b251-4741-9874-b57cd7d03c12 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:12:43.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8928" for this suite. 08/30/23 07:12:43.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:12:43.803
Aug 30 07:12:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pod-network-test 08/30/23 07:12:43.804
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:12:43.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:12:43.896
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-597 08/30/23 07:12:43.93
STEP: creating a selector 08/30/23 07:12:43.93
STEP: Creating the service pods in kubernetes 08/30/23 07:12:43.931
Aug 30 07:12:43.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 30 07:12:44.039: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-597" to be "running and ready"
Aug 30 07:12:44.053: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.352925ms
Aug 30 07:12:44.053: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:12:46.073: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034232685s
Aug 30 07:12:46.073: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:12:48.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.035412635s
Aug 30 07:12:48.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:12:50.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.092479945s
Aug 30 07:12:50.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:12:52.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023758092s
Aug 30 07:12:52.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:12:54.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023313368s
Aug 30 07:12:54.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:12:56.060: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021739906s
Aug 30 07:12:56.060: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:12:58.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.044759877s
Aug 30 07:12:58.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:13:00.068: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029810935s
Aug 30 07:13:00.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:13:02.123: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.08403429s
Aug 30 07:13:02.123: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:13:04.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029936097s
Aug 30 07:13:04.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:13:06.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.044767661s
Aug 30 07:13:06.084: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 30 07:13:06.084: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 30 07:13:06.095: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-597" to be "running and ready"
Aug 30 07:13:06.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.268174ms
Aug 30 07:13:06.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 30 07:13:06.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 30 07:13:06.109: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-597" to be "running and ready"
Aug 30 07:13:06.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 70.033994ms
Aug 30 07:13:06.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 30 07:13:06.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/30/23 07:13:06.189
Aug 30 07:13:06.260: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-597" to be "running"
Aug 30 07:13:06.288: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 27.614998ms
Aug 30 07:13:08.296: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036034454s
Aug 30 07:13:08.296: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 30 07:13:08.303: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 07:13:08.303: INFO: Breadth first check of 172.30.86.189 on host 10.135.139.183...
Aug 30 07:13:08.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.86.189&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:13:08.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:13:08.316: INFO: ExecWithOptions: Clientset creation
Aug 30 07:13:08.316: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.86.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:13:08.457: INFO: Waiting for responses: map[]
Aug 30 07:13:08.457: INFO: reached 172.30.86.189 after 0/1 tries
Aug 30 07:13:08.457: INFO: Breadth first check of 172.30.224.40 on host 10.135.139.185...
Aug 30 07:13:08.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.224.40&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:13:08.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:13:08.465: INFO: ExecWithOptions: Clientset creation
Aug 30 07:13:08.465: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.224.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:13:08.588: INFO: Waiting for responses: map[]
Aug 30 07:13:08.588: INFO: reached 172.30.224.40 after 0/1 tries
Aug 30 07:13:08.588: INFO: Breadth first check of 172.30.58.125 on host 10.135.139.190...
Aug 30 07:13:08.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.58.125&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:13:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:13:08.598: INFO: ExecWithOptions: Clientset creation
Aug 30 07:13:08.598: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.58.125%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:13:08.749: INFO: Waiting for responses: map[]
Aug 30 07:13:08.749: INFO: reached 172.30.58.125 after 0/1 tries
Aug 30 07:13:08.749: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:08.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-597" for this suite. 08/30/23 07:13:08.76
------------------------------
• [SLOW TEST] [24.974 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:12:43.803
    Aug 30 07:12:43.803: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pod-network-test 08/30/23 07:12:43.804
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:12:43.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:12:43.896
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-597 08/30/23 07:12:43.93
    STEP: creating a selector 08/30/23 07:12:43.93
    STEP: Creating the service pods in kubernetes 08/30/23 07:12:43.931
    Aug 30 07:12:43.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 30 07:12:44.039: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-597" to be "running and ready"
    Aug 30 07:12:44.053: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.352925ms
    Aug 30 07:12:44.053: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:12:46.073: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034232685s
    Aug 30 07:12:46.073: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:12:48.074: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.035412635s
    Aug 30 07:12:48.080: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:12:50.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.092479945s
    Aug 30 07:12:50.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:12:52.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023758092s
    Aug 30 07:12:52.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:12:54.062: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023313368s
    Aug 30 07:12:54.062: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:12:56.060: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.021739906s
    Aug 30 07:12:56.060: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:12:58.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.044759877s
    Aug 30 07:12:58.084: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:13:00.068: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.029810935s
    Aug 30 07:13:00.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:13:02.123: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.08403429s
    Aug 30 07:13:02.123: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:13:04.069: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.029936097s
    Aug 30 07:13:04.069: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:13:06.083: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.044767661s
    Aug 30 07:13:06.084: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 30 07:13:06.084: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 30 07:13:06.095: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-597" to be "running and ready"
    Aug 30 07:13:06.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.268174ms
    Aug 30 07:13:06.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 30 07:13:06.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 30 07:13:06.109: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-597" to be "running and ready"
    Aug 30 07:13:06.179: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 70.033994ms
    Aug 30 07:13:06.179: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 30 07:13:06.179: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/30/23 07:13:06.189
    Aug 30 07:13:06.260: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-597" to be "running"
    Aug 30 07:13:06.288: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 27.614998ms
    Aug 30 07:13:08.296: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.036034454s
    Aug 30 07:13:08.296: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 30 07:13:08.303: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 30 07:13:08.303: INFO: Breadth first check of 172.30.86.189 on host 10.135.139.183...
    Aug 30 07:13:08.316: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.86.189&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:13:08.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:13:08.316: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:13:08.316: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.86.189%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:13:08.457: INFO: Waiting for responses: map[]
    Aug 30 07:13:08.457: INFO: reached 172.30.86.189 after 0/1 tries
    Aug 30 07:13:08.457: INFO: Breadth first check of 172.30.224.40 on host 10.135.139.185...
    Aug 30 07:13:08.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.224.40&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:13:08.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:13:08.465: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:13:08.465: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.224.40%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:13:08.588: INFO: Waiting for responses: map[]
    Aug 30 07:13:08.588: INFO: reached 172.30.224.40 after 0/1 tries
    Aug 30 07:13:08.588: INFO: Breadth first check of 172.30.58.125 on host 10.135.139.190...
    Aug 30 07:13:08.597: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.110:9080/dial?request=hostname&protocol=udp&host=172.30.58.125&port=8081&tries=1'] Namespace:pod-network-test-597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:13:08.597: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:13:08.598: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:13:08.598: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.110%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.58.125%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:13:08.749: INFO: Waiting for responses: map[]
    Aug 30 07:13:08.749: INFO: reached 172.30.58.125 after 0/1 tries
    Aug 30 07:13:08.749: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:08.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-597" for this suite. 08/30/23 07:13:08.76
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:08.777
Aug 30 07:13:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename subpath 08/30/23 07:13:08.778
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:08.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:08.831
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/30/23 07:13:08.836
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-zzc5 08/30/23 07:13:08.861
STEP: Creating a pod to test atomic-volume-subpath 08/30/23 07:13:08.861
Aug 30 07:13:08.881: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zzc5" in namespace "subpath-3680" to be "Succeeded or Failed"
Aug 30 07:13:08.891: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.144651ms
Aug 30 07:13:10.903: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.021613441s
Aug 30 07:13:12.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 4.017392237s
Aug 30 07:13:14.900: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 6.018996786s
Aug 30 07:13:16.918: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 8.036125019s
Aug 30 07:13:18.902: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.020789967s
Aug 30 07:13:20.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 12.017418541s
Aug 30 07:13:22.905: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 14.02360264s
Aug 30 07:13:24.902: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 16.020693981s
Aug 30 07:13:26.926: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 18.044184551s
Aug 30 07:13:28.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 20.018015394s
Aug 30 07:13:30.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=false. Elapsed: 22.017079532s
Aug 30 07:13:32.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017482944s
STEP: Saw pod success 08/30/23 07:13:32.899
Aug 30 07:13:32.899: INFO: Pod "pod-subpath-test-configmap-zzc5" satisfied condition "Succeeded or Failed"
Aug 30 07:13:32.908: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-configmap-zzc5 container test-container-subpath-configmap-zzc5: <nil>
STEP: delete the pod 08/30/23 07:13:32.926
Aug 30 07:13:32.952: INFO: Waiting for pod pod-subpath-test-configmap-zzc5 to disappear
Aug 30 07:13:32.961: INFO: Pod pod-subpath-test-configmap-zzc5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-zzc5 08/30/23 07:13:32.961
Aug 30 07:13:32.962: INFO: Deleting pod "pod-subpath-test-configmap-zzc5" in namespace "subpath-3680"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:32.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3680" for this suite. 08/30/23 07:13:32.978
------------------------------
• [SLOW TEST] [24.247 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:08.777
    Aug 30 07:13:08.777: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename subpath 08/30/23 07:13:08.778
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:08.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:08.831
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/30/23 07:13:08.836
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-zzc5 08/30/23 07:13:08.861
    STEP: Creating a pod to test atomic-volume-subpath 08/30/23 07:13:08.861
    Aug 30 07:13:08.881: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-zzc5" in namespace "subpath-3680" to be "Succeeded or Failed"
    Aug 30 07:13:08.891: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.144651ms
    Aug 30 07:13:10.903: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.021613441s
    Aug 30 07:13:12.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 4.017392237s
    Aug 30 07:13:14.900: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 6.018996786s
    Aug 30 07:13:16.918: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 8.036125019s
    Aug 30 07:13:18.902: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.020789967s
    Aug 30 07:13:20.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 12.017418541s
    Aug 30 07:13:22.905: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 14.02360264s
    Aug 30 07:13:24.902: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 16.020693981s
    Aug 30 07:13:26.926: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 18.044184551s
    Aug 30 07:13:28.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=true. Elapsed: 20.018015394s
    Aug 30 07:13:30.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Running", Reason="", readiness=false. Elapsed: 22.017079532s
    Aug 30 07:13:32.899: INFO: Pod "pod-subpath-test-configmap-zzc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.017482944s
    STEP: Saw pod success 08/30/23 07:13:32.899
    Aug 30 07:13:32.899: INFO: Pod "pod-subpath-test-configmap-zzc5" satisfied condition "Succeeded or Failed"
    Aug 30 07:13:32.908: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-configmap-zzc5 container test-container-subpath-configmap-zzc5: <nil>
    STEP: delete the pod 08/30/23 07:13:32.926
    Aug 30 07:13:32.952: INFO: Waiting for pod pod-subpath-test-configmap-zzc5 to disappear
    Aug 30 07:13:32.961: INFO: Pod pod-subpath-test-configmap-zzc5 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-zzc5 08/30/23 07:13:32.961
    Aug 30 07:13:32.962: INFO: Deleting pod "pod-subpath-test-configmap-zzc5" in namespace "subpath-3680"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:32.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3680" for this suite. 08/30/23 07:13:32.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:33.031
Aug 30 07:13:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:13:33.032
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:33.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:33.114
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/30/23 07:13:33.121
Aug 30 07:13:33.121: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-604 proxy --unix-socket=/tmp/kubectl-proxy-unix1358789097/test'
STEP: retrieving proxy /api/ output 08/30/23 07:13:33.182
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:33.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-604" for this suite. 08/30/23 07:13:33.196
------------------------------
• [0.202 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:33.031
    Aug 30 07:13:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:13:33.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:33.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:33.114
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/30/23 07:13:33.121
    Aug 30 07:13:33.121: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-604 proxy --unix-socket=/tmp/kubectl-proxy-unix1358789097/test'
    STEP: retrieving proxy /api/ output 08/30/23 07:13:33.182
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:33.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-604" for this suite. 08/30/23 07:13:33.196
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:33.235
Aug 30 07:13:33.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 07:13:33.236
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:33.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:33.308
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 30 07:13:33.372: INFO: Waiting up to 2m0s for pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" in namespace "var-expansion-2151" to be "container 0 failed with reason CreateContainerConfigError"
Aug 30 07:13:33.382: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003286ms
Aug 30 07:13:35.408: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03655622s
Aug 30 07:13:35.408: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 30 07:13:35.408: INFO: Deleting pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" in namespace "var-expansion-2151"
Aug 30 07:13:35.482: INFO: Wait up to 5m0s for pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:39.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2151" for this suite. 08/30/23 07:13:39.539
------------------------------
• [SLOW TEST] [6.323 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:33.235
    Aug 30 07:13:33.235: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 07:13:33.236
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:33.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:33.308
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 30 07:13:33.372: INFO: Waiting up to 2m0s for pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" in namespace "var-expansion-2151" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 30 07:13:33.382: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393": Phase="Pending", Reason="", readiness=false. Elapsed: 10.003286ms
    Aug 30 07:13:35.408: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03655622s
    Aug 30 07:13:35.408: INFO: Pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 30 07:13:35.408: INFO: Deleting pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" in namespace "var-expansion-2151"
    Aug 30 07:13:35.482: INFO: Wait up to 5m0s for pod "var-expansion-6d9bfa4e-32c1-4d97-8e1d-d40551b8f393" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:39.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2151" for this suite. 08/30/23 07:13:39.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:39.559
Aug 30 07:13:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename disruption 08/30/23 07:13:39.56
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:39.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:39.621
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/30/23 07:13:39.627
STEP: Waiting for the pdb to be processed 08/30/23 07:13:39.639
STEP: updating the pdb 08/30/23 07:13:41.672
STEP: Waiting for the pdb to be processed 08/30/23 07:13:41.69
STEP: patching the pdb 08/30/23 07:13:43.706
STEP: Waiting for the pdb to be processed 08/30/23 07:13:43.722
STEP: Waiting for the pdb to be deleted 08/30/23 07:13:45.782
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:45.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6868" for this suite. 08/30/23 07:13:45.8
------------------------------
• [SLOW TEST] [6.260 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:39.559
    Aug 30 07:13:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename disruption 08/30/23 07:13:39.56
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:39.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:39.621
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/30/23 07:13:39.627
    STEP: Waiting for the pdb to be processed 08/30/23 07:13:39.639
    STEP: updating the pdb 08/30/23 07:13:41.672
    STEP: Waiting for the pdb to be processed 08/30/23 07:13:41.69
    STEP: patching the pdb 08/30/23 07:13:43.706
    STEP: Waiting for the pdb to be processed 08/30/23 07:13:43.722
    STEP: Waiting for the pdb to be deleted 08/30/23 07:13:45.782
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:45.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6868" for this suite. 08/30/23 07:13:45.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:45.83
Aug 30 07:13:45.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:13:45.831
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:45.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:45.887
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
Aug 30 07:13:45.903: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-d4d7b933-7d5e-419c-9151-f340a3e42aba 08/30/23 07:13:45.903
STEP: Creating secret with name s-test-opt-upd-5aac3048-ae4e-46d5-8ff8-f4b51753aea1 08/30/23 07:13:45.926
STEP: Creating the pod 08/30/23 07:13:45.939
Aug 30 07:13:45.964: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09" in namespace "projected-9919" to be "running and ready"
Aug 30 07:13:45.972: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.615976ms
Aug 30 07:13:45.972: INFO: The phase of Pod pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:13:47.980: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09": Phase="Running", Reason="", readiness=true. Elapsed: 2.015511386s
Aug 30 07:13:47.980: INFO: The phase of Pod pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09 is Running (Ready = true)
Aug 30 07:13:47.980: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-d4d7b933-7d5e-419c-9151-f340a3e42aba 08/30/23 07:13:48.034
STEP: Updating secret s-test-opt-upd-5aac3048-ae4e-46d5-8ff8-f4b51753aea1 08/30/23 07:13:48.048
STEP: Creating secret with name s-test-opt-create-a09d5de4-5d36-44b4-b6b3-6d932ca9ce3e 08/30/23 07:13:48.056
STEP: waiting to observe update in volume 08/30/23 07:13:48.065
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9919" for this suite. 08/30/23 07:13:50.14
------------------------------
• [4.329 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:45.83
    Aug 30 07:13:45.830: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:13:45.831
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:45.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:45.887
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    Aug 30 07:13:45.903: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-d4d7b933-7d5e-419c-9151-f340a3e42aba 08/30/23 07:13:45.903
    STEP: Creating secret with name s-test-opt-upd-5aac3048-ae4e-46d5-8ff8-f4b51753aea1 08/30/23 07:13:45.926
    STEP: Creating the pod 08/30/23 07:13:45.939
    Aug 30 07:13:45.964: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09" in namespace "projected-9919" to be "running and ready"
    Aug 30 07:13:45.972: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09": Phase="Pending", Reason="", readiness=false. Elapsed: 7.615976ms
    Aug 30 07:13:45.972: INFO: The phase of Pod pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:13:47.980: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09": Phase="Running", Reason="", readiness=true. Elapsed: 2.015511386s
    Aug 30 07:13:47.980: INFO: The phase of Pod pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09 is Running (Ready = true)
    Aug 30 07:13:47.980: INFO: Pod "pod-projected-secrets-7f0e36c6-e6f4-411e-8479-d6da9dd98e09" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-d4d7b933-7d5e-419c-9151-f340a3e42aba 08/30/23 07:13:48.034
    STEP: Updating secret s-test-opt-upd-5aac3048-ae4e-46d5-8ff8-f4b51753aea1 08/30/23 07:13:48.048
    STEP: Creating secret with name s-test-opt-create-a09d5de4-5d36-44b4-b6b3-6d932ca9ce3e 08/30/23 07:13:48.056
    STEP: waiting to observe update in volume 08/30/23 07:13:48.065
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9919" for this suite. 08/30/23 07:13:50.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:50.161
Aug 30 07:13:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:13:50.162
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:50.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:50.289
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1063 08/30/23 07:13:50.293
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/30/23 07:13:50.326
STEP: creating service externalsvc in namespace services-1063 08/30/23 07:13:50.326
STEP: creating replication controller externalsvc in namespace services-1063 08/30/23 07:13:50.347
I0830 07:13:50.370428      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1063, replica count: 2
I0830 07:13:53.421807      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/30/23 07:13:53.43
Aug 30 07:13:53.459: INFO: Creating new exec pod
Aug 30 07:13:53.482: INFO: Waiting up to 5m0s for pod "execpod7rt7f" in namespace "services-1063" to be "running"
Aug 30 07:13:53.491: INFO: Pod "execpod7rt7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.766697ms
Aug 30 07:13:55.499: INFO: Pod "execpod7rt7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016935618s
Aug 30 07:13:55.499: INFO: Pod "execpod7rt7f" satisfied condition "running"
Aug 30 07:13:55.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-1063 exec execpod7rt7f -- /bin/sh -x -c nslookup clusterip-service.services-1063.svc.cluster.local'
Aug 30 07:13:55.767: INFO: stderr: "+ nslookup clusterip-service.services-1063.svc.cluster.local\n"
Aug 30 07:13:55.768: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1063.svc.cluster.local\tcanonical name = externalsvc.services-1063.svc.cluster.local.\nName:\texternalsvc.services-1063.svc.cluster.local\nAddress: 172.21.41.148\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1063, will wait for the garbage collector to delete the pods 08/30/23 07:13:55.768
Aug 30 07:13:55.861: INFO: Deleting ReplicationController externalsvc took: 35.215882ms
Aug 30 07:13:55.962: INFO: Terminating ReplicationController externalsvc pods took: 100.752603ms
Aug 30 07:13:58.334: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1063" for this suite. 08/30/23 07:13:58.366
------------------------------
• [SLOW TEST] [8.254 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:50.161
    Aug 30 07:13:50.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:13:50.162
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:50.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:50.289
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1063 08/30/23 07:13:50.293
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/30/23 07:13:50.326
    STEP: creating service externalsvc in namespace services-1063 08/30/23 07:13:50.326
    STEP: creating replication controller externalsvc in namespace services-1063 08/30/23 07:13:50.347
    I0830 07:13:50.370428      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1063, replica count: 2
    I0830 07:13:53.421807      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/30/23 07:13:53.43
    Aug 30 07:13:53.459: INFO: Creating new exec pod
    Aug 30 07:13:53.482: INFO: Waiting up to 5m0s for pod "execpod7rt7f" in namespace "services-1063" to be "running"
    Aug 30 07:13:53.491: INFO: Pod "execpod7rt7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.766697ms
    Aug 30 07:13:55.499: INFO: Pod "execpod7rt7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016935618s
    Aug 30 07:13:55.499: INFO: Pod "execpod7rt7f" satisfied condition "running"
    Aug 30 07:13:55.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-1063 exec execpod7rt7f -- /bin/sh -x -c nslookup clusterip-service.services-1063.svc.cluster.local'
    Aug 30 07:13:55.767: INFO: stderr: "+ nslookup clusterip-service.services-1063.svc.cluster.local\n"
    Aug 30 07:13:55.768: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1063.svc.cluster.local\tcanonical name = externalsvc.services-1063.svc.cluster.local.\nName:\texternalsvc.services-1063.svc.cluster.local\nAddress: 172.21.41.148\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1063, will wait for the garbage collector to delete the pods 08/30/23 07:13:55.768
    Aug 30 07:13:55.861: INFO: Deleting ReplicationController externalsvc took: 35.215882ms
    Aug 30 07:13:55.962: INFO: Terminating ReplicationController externalsvc pods took: 100.752603ms
    Aug 30 07:13:58.334: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:58.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1063" for this suite. 08/30/23 07:13:58.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:58.417
Aug 30 07:13:58.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename podtemplate 08/30/23 07:13:58.42
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:58.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:58.482
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/30/23 07:13:58.487
W0830 07:13:59.499435      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:13:59.499: INFO: created test-podtemplate-1
Aug 30 07:13:59.513: INFO: created test-podtemplate-2
Aug 30 07:13:59.523: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/30/23 07:13:59.523
STEP: delete collection of pod templates 08/30/23 07:13:59.529
Aug 30 07:13:59.529: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/30/23 07:13:59.563
Aug 30 07:13:59.563: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 30 07:13:59.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8778" for this suite. 08/30/23 07:13:59.578
------------------------------
• [1.181 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:58.417
    Aug 30 07:13:58.419: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename podtemplate 08/30/23 07:13:58.42
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:58.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:58.482
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/30/23 07:13:58.487
    W0830 07:13:59.499435      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:13:59.499: INFO: created test-podtemplate-1
    Aug 30 07:13:59.513: INFO: created test-podtemplate-2
    Aug 30 07:13:59.523: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/30/23 07:13:59.523
    STEP: delete collection of pod templates 08/30/23 07:13:59.529
    Aug 30 07:13:59.529: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/30/23 07:13:59.563
    Aug 30 07:13:59.563: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:13:59.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8778" for this suite. 08/30/23 07:13:59.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:13:59.598
Aug 30 07:13:59.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:13:59.599
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:59.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:59.659
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/30/23 07:13:59.664
W0830 07:13:59.724244      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:13:59.724: INFO: Waiting up to 5m0s for pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1" in namespace "downward-api-8296" to be "Succeeded or Failed"
Aug 30 07:13:59.737: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.830434ms
Aug 30 07:14:01.764: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040466165s
Aug 30 07:14:03.745: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021314184s
STEP: Saw pod success 08/30/23 07:14:03.745
Aug 30 07:14:03.745: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1" satisfied condition "Succeeded or Failed"
Aug 30 07:14:03.752: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:14:03.772
Aug 30 07:14:03.795: INFO: Waiting for pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 to disappear
Aug 30 07:14:03.802: INFO: Pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:14:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8296" for this suite. 08/30/23 07:14:03.811
------------------------------
• [4.235 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:13:59.598
    Aug 30 07:13:59.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:13:59.599
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:13:59.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:13:59.659
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/30/23 07:13:59.664
    W0830 07:13:59.724244      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:13:59.724: INFO: Waiting up to 5m0s for pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1" in namespace "downward-api-8296" to be "Succeeded or Failed"
    Aug 30 07:13:59.737: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.830434ms
    Aug 30 07:14:01.764: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040466165s
    Aug 30 07:14:03.745: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021314184s
    STEP: Saw pod success 08/30/23 07:14:03.745
    Aug 30 07:14:03.745: INFO: Pod "downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1" satisfied condition "Succeeded or Failed"
    Aug 30 07:14:03.752: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:14:03.772
    Aug 30 07:14:03.795: INFO: Waiting for pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 to disappear
    Aug 30 07:14:03.802: INFO: Pod downward-api-4b00ea03-07d7-4015-8a11-a74c377df7e1 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:14:03.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8296" for this suite. 08/30/23 07:14:03.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:14:03.84
Aug 30 07:14:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:14:03.841
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:03.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:03.911
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3257 08/30/23 07:14:03.917
STEP: changing the ExternalName service to type=NodePort 08/30/23 07:14:03.93
STEP: creating replication controller externalname-service in namespace services-3257 08/30/23 07:14:03.995
I0830 07:14:04.028178      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3257, replica count: 2
I0830 07:14:07.079963      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:14:07.080: INFO: Creating new exec pod
Aug 30 07:14:07.100: INFO: Waiting up to 5m0s for pod "execpodn2xj7" in namespace "services-3257" to be "running"
Aug 30 07:14:07.107: INFO: Pod "execpodn2xj7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376757ms
Aug 30 07:14:09.114: INFO: Pod "execpodn2xj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013966869s
Aug 30 07:14:09.114: INFO: Pod "execpodn2xj7" satisfied condition "running"
Aug 30 07:14:10.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 30 07:14:10.332: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 07:14:10.332: INFO: stdout: ""
Aug 30 07:14:10.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 172.21.49.56 80'
Aug 30 07:14:10.611: INFO: stderr: "+ nc -v -z -w 2 172.21.49.56 80\nConnection to 172.21.49.56 80 port [tcp/http] succeeded!\n"
Aug 30 07:14:10.611: INFO: stdout: ""
Aug 30 07:14:10.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 32003'
Aug 30 07:14:10.900: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 32003\nConnection to 10.135.139.183 32003 port [tcp/*] succeeded!\n"
Aug 30 07:14:10.900: INFO: stdout: ""
Aug 30 07:14:10.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.185 32003'
Aug 30 07:14:11.164: INFO: stderr: "+ nc -v -z -w 2 10.135.139.185 32003\nConnection to 10.135.139.185 32003 port [tcp/*] succeeded!\n"
Aug 30 07:14:11.164: INFO: stdout: ""
Aug 30 07:14:11.164: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:14:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3257" for this suite. 08/30/23 07:14:11.224
------------------------------
• [SLOW TEST] [7.401 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:14:03.84
    Aug 30 07:14:03.840: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:14:03.841
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:03.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:03.911
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-3257 08/30/23 07:14:03.917
    STEP: changing the ExternalName service to type=NodePort 08/30/23 07:14:03.93
    STEP: creating replication controller externalname-service in namespace services-3257 08/30/23 07:14:03.995
    I0830 07:14:04.028178      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3257, replica count: 2
    I0830 07:14:07.079963      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:14:07.080: INFO: Creating new exec pod
    Aug 30 07:14:07.100: INFO: Waiting up to 5m0s for pod "execpodn2xj7" in namespace "services-3257" to be "running"
    Aug 30 07:14:07.107: INFO: Pod "execpodn2xj7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.376757ms
    Aug 30 07:14:09.114: INFO: Pod "execpodn2xj7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013966869s
    Aug 30 07:14:09.114: INFO: Pod "execpodn2xj7" satisfied condition "running"
    Aug 30 07:14:10.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 30 07:14:10.332: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 30 07:14:10.332: INFO: stdout: ""
    Aug 30 07:14:10.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 172.21.49.56 80'
    Aug 30 07:14:10.611: INFO: stderr: "+ nc -v -z -w 2 172.21.49.56 80\nConnection to 172.21.49.56 80 port [tcp/http] succeeded!\n"
    Aug 30 07:14:10.611: INFO: stdout: ""
    Aug 30 07:14:10.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 32003'
    Aug 30 07:14:10.900: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 32003\nConnection to 10.135.139.183 32003 port [tcp/*] succeeded!\n"
    Aug 30 07:14:10.900: INFO: stdout: ""
    Aug 30 07:14:10.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-3257 exec execpodn2xj7 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.185 32003'
    Aug 30 07:14:11.164: INFO: stderr: "+ nc -v -z -w 2 10.135.139.185 32003\nConnection to 10.135.139.185 32003 port [tcp/*] succeeded!\n"
    Aug 30 07:14:11.164: INFO: stdout: ""
    Aug 30 07:14:11.164: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:14:11.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3257" for this suite. 08/30/23 07:14:11.224
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:14:11.242
Aug 30 07:14:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename prestop 08/30/23 07:14:11.243
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:11.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:11.297
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-7536 08/30/23 07:14:11.303
W0830 07:14:11.323014      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pods to come up. 08/30/23 07:14:11.323
Aug 30 07:14:11.323: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7536" to be "running"
Aug 30 07:14:11.331: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078641ms
Aug 30 07:14:13.341: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.017981416s
Aug 30 07:14:13.341: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-7536 08/30/23 07:14:13.347
Aug 30 07:14:13.359: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7536" to be "running"
Aug 30 07:14:13.365: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239988ms
Aug 30 07:14:15.374: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.014885763s
Aug 30 07:14:15.374: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/30/23 07:14:15.374
Aug 30 07:14:20.403: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/30/23 07:14:20.403
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 30 07:14:20.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-7536" for this suite. 08/30/23 07:14:20.446
------------------------------
• [SLOW TEST] [9.225 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:14:11.242
    Aug 30 07:14:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename prestop 08/30/23 07:14:11.243
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:11.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:11.297
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-7536 08/30/23 07:14:11.303
    W0830 07:14:11.323014      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for pods to come up. 08/30/23 07:14:11.323
    Aug 30 07:14:11.323: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7536" to be "running"
    Aug 30 07:14:11.331: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078641ms
    Aug 30 07:14:13.341: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.017981416s
    Aug 30 07:14:13.341: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-7536 08/30/23 07:14:13.347
    Aug 30 07:14:13.359: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7536" to be "running"
    Aug 30 07:14:13.365: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239988ms
    Aug 30 07:14:15.374: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.014885763s
    Aug 30 07:14:15.374: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/30/23 07:14:15.374
    Aug 30 07:14:20.403: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/30/23 07:14:20.403
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:14:20.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-7536" for this suite. 08/30/23 07:14:20.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:14:20.469
Aug 30 07:14:20.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:14:20.47
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:20.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:20.532
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
Aug 30 07:14:20.552: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-82e1070f-5c48-40a9-b5e0-1ed6b10e6a23 08/30/23 07:14:20.552
STEP: Creating configMap with name cm-test-opt-upd-00c78ccb-4ce4-4510-b5a0-c0b1f339b99c 08/30/23 07:14:20.564
STEP: Creating the pod 08/30/23 07:14:20.61
Aug 30 07:14:20.630: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676" in namespace "projected-3523" to be "running and ready"
Aug 30 07:14:20.638: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Pending", Reason="", readiness=false. Elapsed: 7.641999ms
Aug 30 07:14:20.638: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:14:22.648: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017932374s
Aug 30 07:14:22.648: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:14:24.647: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Running", Reason="", readiness=true. Elapsed: 4.016710869s
Aug 30 07:14:24.647: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Running (Ready = true)
Aug 30 07:14:24.647: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-82e1070f-5c48-40a9-b5e0-1ed6b10e6a23 08/30/23 07:14:24.712
STEP: Updating configmap cm-test-opt-upd-00c78ccb-4ce4-4510-b5a0-c0b1f339b99c 08/30/23 07:14:24.729
STEP: Creating configMap with name cm-test-opt-create-57863562-1a8e-4359-920e-b202fa89ba72 08/30/23 07:14:24.746
STEP: waiting to observe update in volume 08/30/23 07:14:24.758
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:15:48.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3523" for this suite. 08/30/23 07:15:48.418
------------------------------
• [SLOW TEST] [87.991 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:14:20.469
    Aug 30 07:14:20.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:14:20.47
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:14:20.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:14:20.532
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    Aug 30 07:14:20.552: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-82e1070f-5c48-40a9-b5e0-1ed6b10e6a23 08/30/23 07:14:20.552
    STEP: Creating configMap with name cm-test-opt-upd-00c78ccb-4ce4-4510-b5a0-c0b1f339b99c 08/30/23 07:14:20.564
    STEP: Creating the pod 08/30/23 07:14:20.61
    Aug 30 07:14:20.630: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676" in namespace "projected-3523" to be "running and ready"
    Aug 30 07:14:20.638: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Pending", Reason="", readiness=false. Elapsed: 7.641999ms
    Aug 30 07:14:20.638: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:14:22.648: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017932374s
    Aug 30 07:14:22.648: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:14:24.647: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676": Phase="Running", Reason="", readiness=true. Elapsed: 4.016710869s
    Aug 30 07:14:24.647: INFO: The phase of Pod pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676 is Running (Ready = true)
    Aug 30 07:14:24.647: INFO: Pod "pod-projected-configmaps-f27d9c8f-9e8f-471a-b202-106522488676" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-82e1070f-5c48-40a9-b5e0-1ed6b10e6a23 08/30/23 07:14:24.712
    STEP: Updating configmap cm-test-opt-upd-00c78ccb-4ce4-4510-b5a0-c0b1f339b99c 08/30/23 07:14:24.729
    STEP: Creating configMap with name cm-test-opt-create-57863562-1a8e-4359-920e-b202fa89ba72 08/30/23 07:14:24.746
    STEP: waiting to observe update in volume 08/30/23 07:14:24.758
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:15:48.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3523" for this suite. 08/30/23 07:15:48.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:15:48.464
Aug 30 07:15:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 07:15:48.465
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:15:48.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:15:48.557
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/30/23 07:15:48.562
W0830 07:15:48.624663      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:15:48.625: INFO: Waiting up to 2m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595" to be "running"
Aug 30 07:15:48.642: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.23797ms
Aug 30 07:15:50.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027570888s
Aug 30 07:15:52.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024552794s
Aug 30 07:15:54.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025520726s
Aug 30 07:15:56.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025294975s
Aug 30 07:15:58.657: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031176168s
Aug 30 07:16:00.668: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 12.042154418s
Aug 30 07:16:02.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026553887s
Aug 30 07:16:04.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024495686s
Aug 30 07:16:06.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026227482s
Aug 30 07:16:08.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029205099s
Aug 30 07:16:10.665: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 22.039621186s
Aug 30 07:16:12.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 24.025302255s
Aug 30 07:16:14.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025945737s
Aug 30 07:16:16.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024638544s
Aug 30 07:16:18.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 30.025640823s
Aug 30 07:16:20.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024373335s
Aug 30 07:16:22.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024818002s
Aug 30 07:16:24.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025103149s
Aug 30 07:16:26.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02417354s
Aug 30 07:16:28.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025431778s
Aug 30 07:16:30.654: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 42.028093374s
Aug 30 07:16:32.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025348149s
Aug 30 07:16:34.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 46.027206194s
Aug 30 07:16:36.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024375367s
Aug 30 07:16:38.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024262084s
Aug 30 07:16:40.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 52.025050181s
Aug 30 07:16:42.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026450447s
Aug 30 07:16:44.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025313288s
Aug 30 07:16:46.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024107033s
Aug 30 07:16:48.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026629925s
Aug 30 07:16:50.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.026975353s
Aug 30 07:16:52.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02562664s
Aug 30 07:16:54.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026351063s
Aug 30 07:16:56.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025224835s
Aug 30 07:16:58.649: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023812559s
Aug 30 07:17:00.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025249762s
Aug 30 07:17:02.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.026115618s
Aug 30 07:17:04.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026697779s
Aug 30 07:17:06.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025085864s
Aug 30 07:17:08.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.026111329s
Aug 30 07:17:10.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025187197s
Aug 30 07:17:12.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027918184s
Aug 30 07:17:14.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.027645171s
Aug 30 07:17:16.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.025071843s
Aug 30 07:17:18.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025422956s
Aug 30 07:17:20.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.024650705s
Aug 30 07:17:22.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024746749s
Aug 30 07:17:24.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025751652s
Aug 30 07:17:26.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024892643s
Aug 30 07:17:28.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025175194s
Aug 30 07:17:30.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025655835s
Aug 30 07:17:32.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.024782774s
Aug 30 07:17:34.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02797555s
Aug 30 07:17:36.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025451703s
Aug 30 07:17:38.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029690853s
Aug 30 07:17:40.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.024372195s
Aug 30 07:17:42.691: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.065204897s
Aug 30 07:17:44.687: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.061915839s
Aug 30 07:17:46.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029363101s
Aug 30 07:17:48.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027964274s
Aug 30 07:17:48.663: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.037121582s
STEP: updating the pod 08/30/23 07:17:48.663
Aug 30 07:17:49.198: INFO: Successfully updated pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48"
STEP: waiting for pod running 08/30/23 07:17:49.198
Aug 30 07:17:49.198: INFO: Waiting up to 2m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595" to be "running"
Aug 30 07:17:49.212: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 13.54204ms
Aug 30 07:17:51.219: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Running", Reason="", readiness=true. Elapsed: 2.021296853s
Aug 30 07:17:51.219: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" satisfied condition "running"
STEP: deleting the pod gracefully 08/30/23 07:17:51.219
Aug 30 07:17:51.220: INFO: Deleting pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595"
Aug 30 07:17:51.248: INFO: Wait up to 5m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 07:18:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4595" for this suite. 08/30/23 07:18:23.275
------------------------------
• [SLOW TEST] [154.840 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:15:48.464
    Aug 30 07:15:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 07:15:48.465
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:15:48.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:15:48.557
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/30/23 07:15:48.562
    W0830 07:15:48.624663      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:15:48.625: INFO: Waiting up to 2m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595" to be "running"
    Aug 30 07:15:48.642: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.23797ms
    Aug 30 07:15:50.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027570888s
    Aug 30 07:15:52.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024552794s
    Aug 30 07:15:54.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.025520726s
    Aug 30 07:15:56.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025294975s
    Aug 30 07:15:58.657: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 10.031176168s
    Aug 30 07:16:00.668: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 12.042154418s
    Aug 30 07:16:02.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026553887s
    Aug 30 07:16:04.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024495686s
    Aug 30 07:16:06.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 18.026227482s
    Aug 30 07:16:08.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 20.029205099s
    Aug 30 07:16:10.665: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 22.039621186s
    Aug 30 07:16:12.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 24.025302255s
    Aug 30 07:16:14.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025945737s
    Aug 30 07:16:16.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 28.024638544s
    Aug 30 07:16:18.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 30.025640823s
    Aug 30 07:16:20.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 32.024373335s
    Aug 30 07:16:22.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 34.024818002s
    Aug 30 07:16:24.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025103149s
    Aug 30 07:16:26.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02417354s
    Aug 30 07:16:28.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025431778s
    Aug 30 07:16:30.654: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 42.028093374s
    Aug 30 07:16:32.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025348149s
    Aug 30 07:16:34.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 46.027206194s
    Aug 30 07:16:36.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024375367s
    Aug 30 07:16:38.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 50.024262084s
    Aug 30 07:16:40.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 52.025050181s
    Aug 30 07:16:42.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 54.026450447s
    Aug 30 07:16:44.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025313288s
    Aug 30 07:16:46.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 58.024107033s
    Aug 30 07:16:48.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026629925s
    Aug 30 07:16:50.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.026975353s
    Aug 30 07:16:52.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02562664s
    Aug 30 07:16:54.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.026351063s
    Aug 30 07:16:56.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.025224835s
    Aug 30 07:16:58.649: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.023812559s
    Aug 30 07:17:00.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.025249762s
    Aug 30 07:17:02.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.026115618s
    Aug 30 07:17:04.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.026697779s
    Aug 30 07:17:06.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025085864s
    Aug 30 07:17:08.652: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.026111329s
    Aug 30 07:17:10.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.025187197s
    Aug 30 07:17:12.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.027918184s
    Aug 30 07:17:14.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.027645171s
    Aug 30 07:17:16.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.025071843s
    Aug 30 07:17:18.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025422956s
    Aug 30 07:17:20.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.024650705s
    Aug 30 07:17:22.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024746749s
    Aug 30 07:17:24.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025751652s
    Aug 30 07:17:26.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.024892643s
    Aug 30 07:17:28.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025175194s
    Aug 30 07:17:30.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.025655835s
    Aug 30 07:17:32.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.024782774s
    Aug 30 07:17:34.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02797555s
    Aug 30 07:17:36.651: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025451703s
    Aug 30 07:17:38.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029690853s
    Aug 30 07:17:40.650: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.024372195s
    Aug 30 07:17:42.691: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.065204897s
    Aug 30 07:17:44.687: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.061915839s
    Aug 30 07:17:46.655: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.029363101s
    Aug 30 07:17:48.653: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027964274s
    Aug 30 07:17:48.663: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.037121582s
    STEP: updating the pod 08/30/23 07:17:48.663
    Aug 30 07:17:49.198: INFO: Successfully updated pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48"
    STEP: waiting for pod running 08/30/23 07:17:49.198
    Aug 30 07:17:49.198: INFO: Waiting up to 2m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595" to be "running"
    Aug 30 07:17:49.212: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Pending", Reason="", readiness=false. Elapsed: 13.54204ms
    Aug 30 07:17:51.219: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48": Phase="Running", Reason="", readiness=true. Elapsed: 2.021296853s
    Aug 30 07:17:51.219: INFO: Pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" satisfied condition "running"
    STEP: deleting the pod gracefully 08/30/23 07:17:51.219
    Aug 30 07:17:51.220: INFO: Deleting pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" in namespace "var-expansion-4595"
    Aug 30 07:17:51.248: INFO: Wait up to 5m0s for pod "var-expansion-3915ceb9-c74e-4859-a7b2-a759d73bcb48" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:18:23.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4595" for this suite. 08/30/23 07:18:23.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:18:23.305
Aug 30 07:18:23.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:18:23.307
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:23.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:23.403
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 30 07:18:23.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:18:26.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6949" for this suite. 08/30/23 07:18:26.709
------------------------------
• [3.444 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:18:23.305
    Aug 30 07:18:23.306: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:18:23.307
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:23.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:23.403
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 30 07:18:23.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:18:26.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6949" for this suite. 08/30/23 07:18:26.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:18:26.751
Aug 30 07:18:26.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename watch 08/30/23 07:18:26.752
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:26.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:26.827
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/30/23 07:18:26.833
STEP: creating a watch on configmaps with label B 08/30/23 07:18:26.835
STEP: creating a watch on configmaps with label A or B 08/30/23 07:18:26.838
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.84
Aug 30 07:18:26.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106423 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:26.869: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106423 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.869
Aug 30 07:18:26.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106426 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:26.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106426 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/30/23 07:18:26.892
Aug 30 07:18:26.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106428 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:26.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106428 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.935
Aug 30 07:18:26.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106434 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:26.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106434 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/30/23 07:18:26.976
Aug 30 07:18:26.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106439 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:26.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106439 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/30/23 07:18:36.991
Aug 30 07:18:37.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106529 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:18:37.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106529 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 30 07:18:47.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-7017" for this suite. 08/30/23 07:18:47.022
------------------------------
• [SLOW TEST] [20.329 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:18:26.751
    Aug 30 07:18:26.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename watch 08/30/23 07:18:26.752
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:26.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:26.827
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/30/23 07:18:26.833
    STEP: creating a watch on configmaps with label B 08/30/23 07:18:26.835
    STEP: creating a watch on configmaps with label A or B 08/30/23 07:18:26.838
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.84
    Aug 30 07:18:26.868: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106423 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:26.869: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106423 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.869
    Aug 30 07:18:26.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106426 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:26.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106426 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/30/23 07:18:26.892
    Aug 30 07:18:26.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106428 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:26.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106428 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/30/23 07:18:26.935
    Aug 30 07:18:26.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106434 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:26.976: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7017  daa4bd8c-16c5-43d0-a775-4ae1665cba7f 106434 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/30/23 07:18:26.976
    Aug 30 07:18:26.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106439 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:26.990: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106439 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/30/23 07:18:36.991
    Aug 30 07:18:37.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106529 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:18:37.010: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7017  304244d0-7c16-4245-ac12-3c4e00b6f2d4 106529 0 2023-08-30 07:18:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-30 07:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:18:47.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-7017" for this suite. 08/30/23 07:18:47.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:18:47.081
Aug 30 07:18:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:18:47.083
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:47.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:47.194
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/30/23 07:18:47.199
STEP: Creating a ResourceQuota 08/30/23 07:18:52.255
STEP: Ensuring resource quota status is calculated 08/30/23 07:18:52.282
STEP: Creating a Pod that fits quota 08/30/23 07:18:54.294
STEP: Ensuring ResourceQuota status captures the pod usage 08/30/23 07:18:54.36
STEP: Not allowing a pod to be created that exceeds remaining quota 08/30/23 07:18:56.405
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/30/23 07:18:56.412
STEP: Ensuring a pod cannot update its resource requirements 08/30/23 07:18:56.417
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/30/23 07:18:56.426
STEP: Deleting the pod 08/30/23 07:18:58.465
STEP: Ensuring resource quota status released the pod usage 08/30/23 07:18:58.605
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:19:00.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6865" for this suite. 08/30/23 07:19:00.632
------------------------------
• [SLOW TEST] [13.572 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:18:47.081
    Aug 30 07:18:47.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:18:47.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:18:47.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:18:47.194
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/30/23 07:18:47.199
    STEP: Creating a ResourceQuota 08/30/23 07:18:52.255
    STEP: Ensuring resource quota status is calculated 08/30/23 07:18:52.282
    STEP: Creating a Pod that fits quota 08/30/23 07:18:54.294
    STEP: Ensuring ResourceQuota status captures the pod usage 08/30/23 07:18:54.36
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/30/23 07:18:56.405
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/30/23 07:18:56.412
    STEP: Ensuring a pod cannot update its resource requirements 08/30/23 07:18:56.417
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/30/23 07:18:56.426
    STEP: Deleting the pod 08/30/23 07:18:58.465
    STEP: Ensuring resource quota status released the pod usage 08/30/23 07:18:58.605
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:19:00.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6865" for this suite. 08/30/23 07:19:00.632
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:19:00.654
Aug 30 07:19:00.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename proxy 08/30/23 07:19:00.655
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:00.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:00.791
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/30/23 07:19:00.849
STEP: creating replication controller proxy-service-wfgst in namespace proxy-8882 08/30/23 07:19:00.85
I0830 07:19:00.886651      21 runners.go:193] Created replication controller with name: proxy-service-wfgst, namespace: proxy-8882, replica count: 1
I0830 07:19:01.943513      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 07:19:02.943743      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 07:19:03.944859      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:19:03.968: INFO: setup took 3.171122749s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/30/23 07:19:03.968
Aug 30 07:19:03.987: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 19.244737ms)
Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 23.832436ms)
Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 23.835362ms)
Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 23.85802ms)
Aug 30 07:19:03.993: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 24.363002ms)
Aug 30 07:19:03.993: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 24.332149ms)
Aug 30 07:19:03.997: INFO: (0) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 28.579375ms)
Aug 30 07:19:03.997: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 28.128152ms)
Aug 30 07:19:04.000: INFO: (0) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 32.091546ms)
Aug 30 07:19:04.000: INFO: (0) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 32.257537ms)
Aug 30 07:19:04.001: INFO: (0) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 32.284931ms)
Aug 30 07:19:04.002: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 33.367016ms)
Aug 30 07:19:04.004: INFO: (0) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 35.424717ms)
Aug 30 07:19:04.004: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 35.915916ms)
Aug 30 07:19:04.005: INFO: (0) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 36.891908ms)
Aug 30 07:19:04.006: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 37.310536ms)
Aug 30 07:19:04.020: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 13.739531ms)
Aug 30 07:19:04.020: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.966331ms)
Aug 30 07:19:04.023: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.470427ms)
Aug 30 07:19:04.023: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 17.317638ms)
Aug 30 07:19:04.026: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 20.199477ms)
Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 20.641594ms)
Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 21.135948ms)
Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 21.079602ms)
Aug 30 07:19:04.028: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 21.680917ms)
Aug 30 07:19:04.028: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 21.406977ms)
Aug 30 07:19:04.029: INFO: (1) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.507348ms)
Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 23.849244ms)
Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 23.988538ms)
Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 24.248876ms)
Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 24.488432ms)
Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 24.475428ms)
Aug 30 07:19:04.041: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 10.858261ms)
Aug 30 07:19:04.042: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.91715ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.309006ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.146692ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.569178ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.135516ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 13.469589ms)
Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.844346ms)
Aug 30 07:19:04.045: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 13.524385ms)
Aug 30 07:19:04.045: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.691ms)
Aug 30 07:19:04.047: INFO: (2) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 16.287851ms)
Aug 30 07:19:04.051: INFO: (2) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.210609ms)
Aug 30 07:19:04.071: INFO: (2) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 40.716754ms)
Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 40.548165ms)
Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 40.725109ms)
Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 40.850187ms)
Aug 30 07:19:04.082: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 10.033857ms)
Aug 30 07:19:04.084: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 12.297323ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 16.089692ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.097815ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 16.221616ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 16.356068ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 16.18041ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 16.30145ms)
Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 16.784509ms)
Aug 30 07:19:04.090: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 17.422639ms)
Aug 30 07:19:04.092: INFO: (3) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 19.567541ms)
Aug 30 07:19:04.094: INFO: (3) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 21.297002ms)
Aug 30 07:19:04.098: INFO: (3) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 25.706564ms)
Aug 30 07:19:04.098: INFO: (3) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 26.006339ms)
Aug 30 07:19:04.099: INFO: (3) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 26.207475ms)
Aug 30 07:19:04.099: INFO: (3) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 26.055818ms)
Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.139701ms)
Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.31898ms)
Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.687981ms)
Aug 30 07:19:04.113: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.507878ms)
Aug 30 07:19:04.113: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 14.486219ms)
Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 15.09235ms)
Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.002566ms)
Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.091471ms)
Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.440178ms)
Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 15.599935ms)
Aug 30 07:19:04.118: INFO: (4) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 19.326959ms)
Aug 30 07:19:04.121: INFO: (4) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 22.347598ms)
Aug 30 07:19:04.121: INFO: (4) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.567499ms)
Aug 30 07:19:04.122: INFO: (4) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 23.771841ms)
Aug 30 07:19:04.123: INFO: (4) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.927961ms)
Aug 30 07:19:04.123: INFO: (4) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 24.118716ms)
Aug 30 07:19:04.135: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 11.570993ms)
Aug 30 07:19:04.138: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.530876ms)
Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 17.722554ms)
Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 17.093477ms)
Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 17.514048ms)
Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 17.572989ms)
Aug 30 07:19:04.142: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 17.992914ms)
Aug 30 07:19:04.142: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 18.709514ms)
Aug 30 07:19:04.143: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 19.143529ms)
Aug 30 07:19:04.143: INFO: (5) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 19.41198ms)
Aug 30 07:19:04.144: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 20.07247ms)
Aug 30 07:19:04.146: INFO: (5) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.649217ms)
Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 26.287935ms)
Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 26.159138ms)
Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 26.814924ms)
Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 26.624606ms)
Aug 30 07:19:04.160: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 9.490799ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.82142ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.322731ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.954186ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.232345ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.152947ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.618126ms)
Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.256874ms)
Aug 30 07:19:04.165: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.203884ms)
Aug 30 07:19:04.165: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.486607ms)
Aug 30 07:19:04.166: INFO: (6) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 15.826733ms)
Aug 30 07:19:04.167: INFO: (6) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 16.471359ms)
Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 18.608356ms)
Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 19.359387ms)
Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.87414ms)
Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 19.445317ms)
Aug 30 07:19:04.181: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 11.197881ms)
Aug 30 07:19:04.182: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 11.675994ms)
Aug 30 07:19:04.182: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 11.883869ms)
Aug 30 07:19:04.185: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.578103ms)
Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.208097ms)
Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.528293ms)
Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 15.718933ms)
Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 16.013036ms)
Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.901993ms)
Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.002673ms)
Aug 30 07:19:04.189: INFO: (7) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 18.706959ms)
Aug 30 07:19:04.190: INFO: (7) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 19.660638ms)
Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.189001ms)
Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.879965ms)
Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 21.50723ms)
Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.828357ms)
Aug 30 07:19:04.231: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 38.657533ms)
Aug 30 07:19:04.231: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 38.042662ms)
Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 39.975569ms)
Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 40.53827ms)
Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 40.22311ms)
Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 40.965504ms)
Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 40.685423ms)
Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 40.476156ms)
Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.524811ms)
Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 41.251049ms)
Aug 30 07:19:04.237: INFO: (8) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 44.156372ms)
Aug 30 07:19:04.237: INFO: (8) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 44.155602ms)
Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 46.121117ms)
Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 46.385358ms)
Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 46.267382ms)
Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 45.985091ms)
Aug 30 07:19:04.250: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.264193ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 43.610676ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 43.922946ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 44.078034ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 44.017897ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 44.427019ms)
Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 44.992333ms)
Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 44.707319ms)
Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 44.714914ms)
Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 44.790147ms)
Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 45.141153ms)
Aug 30 07:19:04.288: INFO: (9) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 48.043701ms)
Aug 30 07:19:04.289: INFO: (9) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 49.474263ms)
Aug 30 07:19:04.289: INFO: (9) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 49.3863ms)
Aug 30 07:19:04.290: INFO: (9) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 49.877672ms)
Aug 30 07:19:04.290: INFO: (9) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 49.705034ms)
Aug 30 07:19:04.300: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 10.063401ms)
Aug 30 07:19:04.304: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.524043ms)
Aug 30 07:19:04.304: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.927369ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.051279ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 14.167574ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.979287ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.385219ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 15.172765ms)
Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.940984ms)
Aug 30 07:19:04.306: INFO: (10) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 15.700036ms)
Aug 30 07:19:04.306: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 15.974509ms)
Aug 30 07:19:04.309: INFO: (10) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.637693ms)
Aug 30 07:19:04.310: INFO: (10) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 19.676936ms)
Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 20.518925ms)
Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.694119ms)
Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 20.858816ms)
Aug 30 07:19:04.322: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.623132ms)
Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 13.386782ms)
Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.552779ms)
Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.717703ms)
Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.365475ms)
Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.45166ms)
Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.355613ms)
Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.030419ms)
Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.371375ms)
Aug 30 07:19:04.327: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.253631ms)
Aug 30 07:19:04.327: INFO: (11) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 15.68149ms)
Aug 30 07:19:04.329: INFO: (11) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 17.056421ms)
Aug 30 07:19:04.332: INFO: (11) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.904366ms)
Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 20.747026ms)
Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 21.407784ms)
Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.485797ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 40.592118ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 40.532282ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 41.305567ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.112207ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 41.230059ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 42.195653ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 42.356047ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 42.150795ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.63453ms)
Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 42.026499ms)
Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 44.614458ms)
Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 44.997555ms)
Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 44.873705ms)
Aug 30 07:19:04.379: INFO: (12) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 45.200901ms)
Aug 30 07:19:04.383: INFO: (12) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 49.100834ms)
Aug 30 07:19:04.385: INFO: (12) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 50.548379ms)
Aug 30 07:19:04.395: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.175674ms)
Aug 30 07:19:04.398: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 12.364086ms)
Aug 30 07:19:04.398: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.268151ms)
Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.981083ms)
Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.187146ms)
Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.003346ms)
Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.537683ms)
Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.420432ms)
Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.955999ms)
Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.713038ms)
Aug 30 07:19:04.402: INFO: (13) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 16.594273ms)
Aug 30 07:19:04.407: INFO: (13) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.760895ms)
Aug 30 07:19:04.408: INFO: (13) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.262966ms)
Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.186946ms)
Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 23.222615ms)
Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 22.98291ms)
Aug 30 07:19:04.418: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 9.468377ms)
Aug 30 07:19:04.419: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 9.40758ms)
Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.843346ms)
Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.807425ms)
Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 10.952097ms)
Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 12.679348ms)
Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.060077ms)
Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 12.315697ms)
Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.020615ms)
Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.602441ms)
Aug 30 07:19:04.423: INFO: (14) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 13.84895ms)
Aug 30 07:19:04.424: INFO: (14) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 14.752991ms)
Aug 30 07:19:04.427: INFO: (14) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 18.121069ms)
Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 21.810241ms)
Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.604089ms)
Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 21.98488ms)
Aug 30 07:19:04.441: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 9.933434ms)
Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 10.653006ms)
Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 11.373144ms)
Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 11.664574ms)
Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.94007ms)
Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 11.885151ms)
Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 11.593056ms)
Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 11.972911ms)
Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.240674ms)
Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 12.535573ms)
Aug 30 07:19:04.453: INFO: (15) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.747956ms)
Aug 30 07:19:04.458: INFO: (15) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 25.59741ms)
Aug 30 07:19:04.459: INFO: (15) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 27.524621ms)
Aug 30 07:19:04.459: INFO: (15) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 27.442318ms)
Aug 30 07:19:04.460: INFO: (15) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 27.446725ms)
Aug 30 07:19:04.460: INFO: (15) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 28.242538ms)
Aug 30 07:19:04.470: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 9.865089ms)
Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.66838ms)
Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.951391ms)
Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 12.493145ms)
Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.874227ms)
Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.531436ms)
Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.158439ms)
Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.389334ms)
Aug 30 07:19:04.475: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.974838ms)
Aug 30 07:19:04.475: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.341217ms)
Aug 30 07:19:04.476: INFO: (16) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 16.37442ms)
Aug 30 07:19:04.479: INFO: (16) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 18.869579ms)
Aug 30 07:19:04.481: INFO: (16) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 20.505807ms)
Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.35325ms)
Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 21.716798ms)
Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.008418ms)
Aug 30 07:19:04.493: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 10.696264ms)
Aug 30 07:19:04.493: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.884627ms)
Aug 30 07:19:04.495: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.553731ms)
Aug 30 07:19:04.497: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.549255ms)
Aug 30 07:19:04.500: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 17.495388ms)
Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 18.080663ms)
Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.737663ms)
Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 18.016825ms)
Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 18.27989ms)
Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 18.585671ms)
Aug 30 07:19:04.502: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 19.535635ms)
Aug 30 07:19:04.502: INFO: (17) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 20.188479ms)
Aug 30 07:19:04.503: INFO: (17) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.802271ms)
Aug 30 07:19:04.505: INFO: (17) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.733431ms)
Aug 30 07:19:04.507: INFO: (17) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 24.491462ms)
Aug 30 07:19:04.508: INFO: (17) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 24.569027ms)
Aug 30 07:19:04.518: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.940924ms)
Aug 30 07:19:04.520: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 12.498486ms)
Aug 30 07:19:04.521: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.12275ms)
Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.159217ms)
Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.972278ms)
Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 14.890921ms)
Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.267686ms)
Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.506631ms)
Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 15.198208ms)
Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 15.268599ms)
Aug 30 07:19:04.526: INFO: (18) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.232811ms)
Aug 30 07:19:04.528: INFO: (18) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.075947ms)
Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.207385ms)
Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 23.80151ms)
Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 23.299017ms)
Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.422639ms)
Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 14.166024ms)
Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.790686ms)
Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.925107ms)
Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.906936ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.58085ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.663219ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.607073ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.923842ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 15.123695ms)
Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.991025ms)
Aug 30 07:19:04.548: INFO: (19) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 16.223028ms)
Aug 30 07:19:04.552: INFO: (19) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 19.331584ms)
Aug 30 07:19:04.554: INFO: (19) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.825724ms)
Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 22.168606ms)
Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 22.064133ms)
Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.524328ms)
STEP: deleting ReplicationController proxy-service-wfgst in namespace proxy-8882, will wait for the garbage collector to delete the pods 08/30/23 07:19:04.555
Aug 30 07:19:04.627: INFO: Deleting ReplicationController proxy-service-wfgst took: 15.384646ms
Aug 30 07:19:04.727: INFO: Terminating ReplicationController proxy-service-wfgst pods took: 100.200931ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 30 07:19:07.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8882" for this suite. 08/30/23 07:19:07.54
------------------------------
• [SLOW TEST] [6.910 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:19:00.654
    Aug 30 07:19:00.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename proxy 08/30/23 07:19:00.655
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:00.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:00.791
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/30/23 07:19:00.849
    STEP: creating replication controller proxy-service-wfgst in namespace proxy-8882 08/30/23 07:19:00.85
    I0830 07:19:00.886651      21 runners.go:193] Created replication controller with name: proxy-service-wfgst, namespace: proxy-8882, replica count: 1
    I0830 07:19:01.943513      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0830 07:19:02.943743      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0830 07:19:03.944859      21 runners.go:193] proxy-service-wfgst Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:19:03.968: INFO: setup took 3.171122749s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/30/23 07:19:03.968
    Aug 30 07:19:03.987: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 19.244737ms)
    Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 23.832436ms)
    Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 23.835362ms)
    Aug 30 07:19:03.992: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 23.85802ms)
    Aug 30 07:19:03.993: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 24.363002ms)
    Aug 30 07:19:03.993: INFO: (0) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 24.332149ms)
    Aug 30 07:19:03.997: INFO: (0) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 28.579375ms)
    Aug 30 07:19:03.997: INFO: (0) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 28.128152ms)
    Aug 30 07:19:04.000: INFO: (0) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 32.091546ms)
    Aug 30 07:19:04.000: INFO: (0) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 32.257537ms)
    Aug 30 07:19:04.001: INFO: (0) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 32.284931ms)
    Aug 30 07:19:04.002: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 33.367016ms)
    Aug 30 07:19:04.004: INFO: (0) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 35.424717ms)
    Aug 30 07:19:04.004: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 35.915916ms)
    Aug 30 07:19:04.005: INFO: (0) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 36.891908ms)
    Aug 30 07:19:04.006: INFO: (0) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 37.310536ms)
    Aug 30 07:19:04.020: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 13.739531ms)
    Aug 30 07:19:04.020: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.966331ms)
    Aug 30 07:19:04.023: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.470427ms)
    Aug 30 07:19:04.023: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 17.317638ms)
    Aug 30 07:19:04.026: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 20.199477ms)
    Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 20.641594ms)
    Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 21.135948ms)
    Aug 30 07:19:04.027: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 21.079602ms)
    Aug 30 07:19:04.028: INFO: (1) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 21.680917ms)
    Aug 30 07:19:04.028: INFO: (1) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 21.406977ms)
    Aug 30 07:19:04.029: INFO: (1) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.507348ms)
    Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 23.849244ms)
    Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 23.988538ms)
    Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 24.248876ms)
    Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 24.488432ms)
    Aug 30 07:19:04.030: INFO: (1) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 24.475428ms)
    Aug 30 07:19:04.041: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 10.858261ms)
    Aug 30 07:19:04.042: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.91715ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.309006ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.146692ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.569178ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.135516ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 13.469589ms)
    Aug 30 07:19:04.044: INFO: (2) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.844346ms)
    Aug 30 07:19:04.045: INFO: (2) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 13.524385ms)
    Aug 30 07:19:04.045: INFO: (2) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.691ms)
    Aug 30 07:19:04.047: INFO: (2) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 16.287851ms)
    Aug 30 07:19:04.051: INFO: (2) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.210609ms)
    Aug 30 07:19:04.071: INFO: (2) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 40.716754ms)
    Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 40.548165ms)
    Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 40.725109ms)
    Aug 30 07:19:04.072: INFO: (2) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 40.850187ms)
    Aug 30 07:19:04.082: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 10.033857ms)
    Aug 30 07:19:04.084: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 12.297323ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 16.089692ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.097815ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 16.221616ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 16.356068ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 16.18041ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 16.30145ms)
    Aug 30 07:19:04.089: INFO: (3) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 16.784509ms)
    Aug 30 07:19:04.090: INFO: (3) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 17.422639ms)
    Aug 30 07:19:04.092: INFO: (3) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 19.567541ms)
    Aug 30 07:19:04.094: INFO: (3) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 21.297002ms)
    Aug 30 07:19:04.098: INFO: (3) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 25.706564ms)
    Aug 30 07:19:04.098: INFO: (3) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 26.006339ms)
    Aug 30 07:19:04.099: INFO: (3) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 26.207475ms)
    Aug 30 07:19:04.099: INFO: (3) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 26.055818ms)
    Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.139701ms)
    Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.31898ms)
    Aug 30 07:19:04.112: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.687981ms)
    Aug 30 07:19:04.113: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.507878ms)
    Aug 30 07:19:04.113: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 14.486219ms)
    Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 15.09235ms)
    Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.002566ms)
    Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.091471ms)
    Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.440178ms)
    Aug 30 07:19:04.114: INFO: (4) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 15.599935ms)
    Aug 30 07:19:04.118: INFO: (4) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 19.326959ms)
    Aug 30 07:19:04.121: INFO: (4) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 22.347598ms)
    Aug 30 07:19:04.121: INFO: (4) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.567499ms)
    Aug 30 07:19:04.122: INFO: (4) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 23.771841ms)
    Aug 30 07:19:04.123: INFO: (4) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.927961ms)
    Aug 30 07:19:04.123: INFO: (4) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 24.118716ms)
    Aug 30 07:19:04.135: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 11.570993ms)
    Aug 30 07:19:04.138: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.530876ms)
    Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 17.722554ms)
    Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 17.093477ms)
    Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 17.514048ms)
    Aug 30 07:19:04.141: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 17.572989ms)
    Aug 30 07:19:04.142: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 17.992914ms)
    Aug 30 07:19:04.142: INFO: (5) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 18.709514ms)
    Aug 30 07:19:04.143: INFO: (5) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 19.143529ms)
    Aug 30 07:19:04.143: INFO: (5) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 19.41198ms)
    Aug 30 07:19:04.144: INFO: (5) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 20.07247ms)
    Aug 30 07:19:04.146: INFO: (5) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.649217ms)
    Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 26.287935ms)
    Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 26.159138ms)
    Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 26.814924ms)
    Aug 30 07:19:04.150: INFO: (5) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 26.624606ms)
    Aug 30 07:19:04.160: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 9.490799ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.82142ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.322731ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.954186ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.232345ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.152947ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.618126ms)
    Aug 30 07:19:04.164: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 13.256874ms)
    Aug 30 07:19:04.165: INFO: (6) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.203884ms)
    Aug 30 07:19:04.165: INFO: (6) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.486607ms)
    Aug 30 07:19:04.166: INFO: (6) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 15.826733ms)
    Aug 30 07:19:04.167: INFO: (6) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 16.471359ms)
    Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 18.608356ms)
    Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 19.359387ms)
    Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.87414ms)
    Aug 30 07:19:04.170: INFO: (6) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 19.445317ms)
    Aug 30 07:19:04.181: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 11.197881ms)
    Aug 30 07:19:04.182: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 11.675994ms)
    Aug 30 07:19:04.182: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 11.883869ms)
    Aug 30 07:19:04.185: INFO: (7) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.578103ms)
    Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.208097ms)
    Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.528293ms)
    Aug 30 07:19:04.186: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 15.718933ms)
    Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 16.013036ms)
    Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.901993ms)
    Aug 30 07:19:04.187: INFO: (7) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 16.002673ms)
    Aug 30 07:19:04.189: INFO: (7) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 18.706959ms)
    Aug 30 07:19:04.190: INFO: (7) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 19.660638ms)
    Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.189001ms)
    Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.879965ms)
    Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 21.50723ms)
    Aug 30 07:19:04.192: INFO: (7) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.828357ms)
    Aug 30 07:19:04.231: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 38.657533ms)
    Aug 30 07:19:04.231: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 38.042662ms)
    Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 39.975569ms)
    Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 40.53827ms)
    Aug 30 07:19:04.233: INFO: (8) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 40.22311ms)
    Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 40.965504ms)
    Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 40.685423ms)
    Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 40.476156ms)
    Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.524811ms)
    Aug 30 07:19:04.234: INFO: (8) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 41.251049ms)
    Aug 30 07:19:04.237: INFO: (8) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 44.156372ms)
    Aug 30 07:19:04.237: INFO: (8) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 44.155602ms)
    Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 46.121117ms)
    Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 46.385358ms)
    Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 46.267382ms)
    Aug 30 07:19:04.239: INFO: (8) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 45.985091ms)
    Aug 30 07:19:04.250: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.264193ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 43.610676ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 43.922946ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 44.078034ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 44.017897ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 44.427019ms)
    Aug 30 07:19:04.284: INFO: (9) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 44.992333ms)
    Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 44.707319ms)
    Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 44.714914ms)
    Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 44.790147ms)
    Aug 30 07:19:04.285: INFO: (9) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 45.141153ms)
    Aug 30 07:19:04.288: INFO: (9) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 48.043701ms)
    Aug 30 07:19:04.289: INFO: (9) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 49.474263ms)
    Aug 30 07:19:04.289: INFO: (9) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 49.3863ms)
    Aug 30 07:19:04.290: INFO: (9) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 49.877672ms)
    Aug 30 07:19:04.290: INFO: (9) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 49.705034ms)
    Aug 30 07:19:04.300: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 10.063401ms)
    Aug 30 07:19:04.304: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.524043ms)
    Aug 30 07:19:04.304: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.927369ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.051279ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 14.167574ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.979287ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.385219ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 15.172765ms)
    Aug 30 07:19:04.305: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.940984ms)
    Aug 30 07:19:04.306: INFO: (10) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 15.700036ms)
    Aug 30 07:19:04.306: INFO: (10) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 15.974509ms)
    Aug 30 07:19:04.309: INFO: (10) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.637693ms)
    Aug 30 07:19:04.310: INFO: (10) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 19.676936ms)
    Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 20.518925ms)
    Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.694119ms)
    Aug 30 07:19:04.311: INFO: (10) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 20.858816ms)
    Aug 30 07:19:04.322: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.623132ms)
    Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 13.386782ms)
    Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 13.552779ms)
    Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.717703ms)
    Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.365475ms)
    Aug 30 07:19:04.325: INFO: (11) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.45166ms)
    Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.355613ms)
    Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.030419ms)
    Aug 30 07:19:04.326: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.371375ms)
    Aug 30 07:19:04.327: INFO: (11) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.253631ms)
    Aug 30 07:19:04.327: INFO: (11) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 15.68149ms)
    Aug 30 07:19:04.329: INFO: (11) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 17.056421ms)
    Aug 30 07:19:04.332: INFO: (11) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.904366ms)
    Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 20.747026ms)
    Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 21.407784ms)
    Aug 30 07:19:04.333: INFO: (11) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.485797ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 40.592118ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 40.532282ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 41.305567ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.112207ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 41.230059ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 42.195653ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 42.356047ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 42.150795ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 41.63453ms)
    Aug 30 07:19:04.375: INFO: (12) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 42.026499ms)
    Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 44.614458ms)
    Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 44.997555ms)
    Aug 30 07:19:04.378: INFO: (12) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 44.873705ms)
    Aug 30 07:19:04.379: INFO: (12) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 45.200901ms)
    Aug 30 07:19:04.383: INFO: (12) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 49.100834ms)
    Aug 30 07:19:04.385: INFO: (12) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 50.548379ms)
    Aug 30 07:19:04.395: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.175674ms)
    Aug 30 07:19:04.398: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 12.364086ms)
    Aug 30 07:19:04.398: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.268151ms)
    Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.981083ms)
    Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.187146ms)
    Aug 30 07:19:04.399: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.003346ms)
    Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.537683ms)
    Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 14.420432ms)
    Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.955999ms)
    Aug 30 07:19:04.400: INFO: (13) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.713038ms)
    Aug 30 07:19:04.402: INFO: (13) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 16.594273ms)
    Aug 30 07:19:04.407: INFO: (13) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.760895ms)
    Aug 30 07:19:04.408: INFO: (13) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.262966ms)
    Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.186946ms)
    Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 23.222615ms)
    Aug 30 07:19:04.409: INFO: (13) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 22.98291ms)
    Aug 30 07:19:04.418: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 9.468377ms)
    Aug 30 07:19:04.419: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 9.40758ms)
    Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.843346ms)
    Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.807425ms)
    Aug 30 07:19:04.420: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 10.952097ms)
    Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 12.679348ms)
    Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.060077ms)
    Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 12.315697ms)
    Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.020615ms)
    Aug 30 07:19:04.422: INFO: (14) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.602441ms)
    Aug 30 07:19:04.423: INFO: (14) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 13.84895ms)
    Aug 30 07:19:04.424: INFO: (14) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 14.752991ms)
    Aug 30 07:19:04.427: INFO: (14) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 18.121069ms)
    Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 21.810241ms)
    Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.604089ms)
    Aug 30 07:19:04.431: INFO: (14) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 21.98488ms)
    Aug 30 07:19:04.441: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 9.933434ms)
    Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 10.653006ms)
    Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 11.373144ms)
    Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 11.664574ms)
    Aug 30 07:19:04.443: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 10.94007ms)
    Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 11.885151ms)
    Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 11.593056ms)
    Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 11.972911ms)
    Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.240674ms)
    Aug 30 07:19:04.444: INFO: (15) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 12.535573ms)
    Aug 30 07:19:04.453: INFO: (15) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.747956ms)
    Aug 30 07:19:04.458: INFO: (15) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 25.59741ms)
    Aug 30 07:19:04.459: INFO: (15) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 27.524621ms)
    Aug 30 07:19:04.459: INFO: (15) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 27.442318ms)
    Aug 30 07:19:04.460: INFO: (15) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 27.446725ms)
    Aug 30 07:19:04.460: INFO: (15) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 28.242538ms)
    Aug 30 07:19:04.470: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 9.865089ms)
    Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 12.66838ms)
    Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.951391ms)
    Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 12.493145ms)
    Aug 30 07:19:04.473: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.874227ms)
    Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.531436ms)
    Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 13.158439ms)
    Aug 30 07:19:04.474: INFO: (16) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.389334ms)
    Aug 30 07:19:04.475: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.974838ms)
    Aug 30 07:19:04.475: INFO: (16) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.341217ms)
    Aug 30 07:19:04.476: INFO: (16) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 16.37442ms)
    Aug 30 07:19:04.479: INFO: (16) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 18.869579ms)
    Aug 30 07:19:04.481: INFO: (16) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 20.505807ms)
    Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 21.35325ms)
    Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 21.716798ms)
    Aug 30 07:19:04.482: INFO: (16) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.008418ms)
    Aug 30 07:19:04.493: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 10.696264ms)
    Aug 30 07:19:04.493: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.884627ms)
    Aug 30 07:19:04.495: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 12.553731ms)
    Aug 30 07:19:04.497: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.549255ms)
    Aug 30 07:19:04.500: INFO: (17) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 17.495388ms)
    Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 18.080663ms)
    Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.737663ms)
    Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 18.016825ms)
    Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 18.27989ms)
    Aug 30 07:19:04.501: INFO: (17) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 18.585671ms)
    Aug 30 07:19:04.502: INFO: (17) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 19.535635ms)
    Aug 30 07:19:04.502: INFO: (17) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 20.188479ms)
    Aug 30 07:19:04.503: INFO: (17) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.802271ms)
    Aug 30 07:19:04.505: INFO: (17) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.733431ms)
    Aug 30 07:19:04.507: INFO: (17) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 24.491462ms)
    Aug 30 07:19:04.508: INFO: (17) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 24.569027ms)
    Aug 30 07:19:04.518: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 10.940924ms)
    Aug 30 07:19:04.520: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 12.498486ms)
    Aug 30 07:19:04.521: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 12.12275ms)
    Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.159217ms)
    Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.972278ms)
    Aug 30 07:19:04.523: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 14.890921ms)
    Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 15.267686ms)
    Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 15.506631ms)
    Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 15.198208ms)
    Aug 30 07:19:04.524: INFO: (18) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 15.268599ms)
    Aug 30 07:19:04.526: INFO: (18) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 18.232811ms)
    Aug 30 07:19:04.528: INFO: (18) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 20.075947ms)
    Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 23.207385ms)
    Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 23.80151ms)
    Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 23.299017ms)
    Aug 30 07:19:04.532: INFO: (18) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 23.422639ms)
    Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">test<... (200; 14.166024ms)
    Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:460/proxy/: tls baz (200; 13.790686ms)
    Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 13.925107ms)
    Aug 30 07:19:04.546: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 13.906936ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:1080/proxy/rewriteme">... (200; 14.58085ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:443/proxy/tlsrewritem... (200; 14.663219ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/http:proxy-service-wfgst-nx8tr:160/proxy/: foo (200; 14.607073ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/https:proxy-service-wfgst-nx8tr:462/proxy/: tls qux (200; 14.923842ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/: <a href="/api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr/proxy/rewriteme">test</a> (200; 15.123695ms)
    Aug 30 07:19:04.547: INFO: (19) /api/v1/namespaces/proxy-8882/pods/proxy-service-wfgst-nx8tr:162/proxy/: bar (200; 14.991025ms)
    Aug 30 07:19:04.548: INFO: (19) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname2/proxy/: tls qux (200; 16.223028ms)
    Aug 30 07:19:04.552: INFO: (19) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname2/proxy/: bar (200; 19.331584ms)
    Aug 30 07:19:04.554: INFO: (19) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname2/proxy/: bar (200; 21.825724ms)
    Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/https:proxy-service-wfgst:tlsportname1/proxy/: tls baz (200; 22.168606ms)
    Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/proxy-service-wfgst:portname1/proxy/: foo (200; 22.064133ms)
    Aug 30 07:19:04.555: INFO: (19) /api/v1/namespaces/proxy-8882/services/http:proxy-service-wfgst:portname1/proxy/: foo (200; 22.524328ms)
    STEP: deleting ReplicationController proxy-service-wfgst in namespace proxy-8882, will wait for the garbage collector to delete the pods 08/30/23 07:19:04.555
    Aug 30 07:19:04.627: INFO: Deleting ReplicationController proxy-service-wfgst took: 15.384646ms
    Aug 30 07:19:04.727: INFO: Terminating ReplicationController proxy-service-wfgst pods took: 100.200931ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:19:07.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8882" for this suite. 08/30/23 07:19:07.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:19:07.567
Aug 30 07:19:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:19:07.568
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:07.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:07.623
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-ee285eb6-877e-4b8e-903d-d84fe0191b48 08/30/23 07:19:07.63
STEP: Creating secret with name secret-projected-all-test-volume-f868b870-48c6-4a8c-b225-3041396cdb72 08/30/23 07:19:07.649
STEP: Creating a pod to test Check all projections for projected volume plugin 08/30/23 07:19:07.66
W0830 07:19:07.694778      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-all-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-all-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-all-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-all-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:19:07.694: INFO: Waiting up to 5m0s for pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05" in namespace "projected-9315" to be "Succeeded or Failed"
Aug 30 07:19:07.715: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Pending", Reason="", readiness=false. Elapsed: 20.658485ms
Aug 30 07:19:09.725: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030410008s
Aug 30 07:19:11.724: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029483529s
STEP: Saw pod success 08/30/23 07:19:11.724
Aug 30 07:19:11.724: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05" satisfied condition "Succeeded or Failed"
Aug 30 07:19:11.731: INFO: Trying to get logs from node 10.135.139.190 pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 container projected-all-volume-test: <nil>
STEP: delete the pod 08/30/23 07:19:11.758
Aug 30 07:19:11.814: INFO: Waiting for pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 to disappear
Aug 30 07:19:11.826: INFO: Pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 30 07:19:11.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9315" for this suite. 08/30/23 07:19:11.835
------------------------------
• [4.324 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:19:07.567
    Aug 30 07:19:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:19:07.568
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:07.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:07.623
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-ee285eb6-877e-4b8e-903d-d84fe0191b48 08/30/23 07:19:07.63
    STEP: Creating secret with name secret-projected-all-test-volume-f868b870-48c6-4a8c-b225-3041396cdb72 08/30/23 07:19:07.649
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/30/23 07:19:07.66
    W0830 07:19:07.694778      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-all-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-all-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-all-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-all-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:19:07.694: INFO: Waiting up to 5m0s for pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05" in namespace "projected-9315" to be "Succeeded or Failed"
    Aug 30 07:19:07.715: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Pending", Reason="", readiness=false. Elapsed: 20.658485ms
    Aug 30 07:19:09.725: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030410008s
    Aug 30 07:19:11.724: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029483529s
    STEP: Saw pod success 08/30/23 07:19:11.724
    Aug 30 07:19:11.724: INFO: Pod "projected-volume-83d73872-9474-4a86-a67b-44114e747a05" satisfied condition "Succeeded or Failed"
    Aug 30 07:19:11.731: INFO: Trying to get logs from node 10.135.139.190 pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 container projected-all-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:19:11.758
    Aug 30 07:19:11.814: INFO: Waiting for pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 to disappear
    Aug 30 07:19:11.826: INFO: Pod projected-volume-83d73872-9474-4a86-a67b-44114e747a05 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:19:11.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9315" for this suite. 08/30/23 07:19:11.835
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:19:11.89
Aug 30 07:19:11.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:19:11.891
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:11.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:11.981
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-4355 08/30/23 07:19:11.987
STEP: creating service affinity-nodeport in namespace services-4355 08/30/23 07:19:11.987
STEP: creating replication controller affinity-nodeport in namespace services-4355 08/30/23 07:19:12.091
I0830 07:19:12.108579      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4355, replica count: 3
I0830 07:19:15.160072      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:19:15.183: INFO: Creating new exec pod
Aug 30 07:19:15.199: INFO: Waiting up to 5m0s for pod "execpod-affinity7xgqs" in namespace "services-4355" to be "running"
Aug 30 07:19:15.207: INFO: Pod "execpod-affinity7xgqs": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03961ms
Aug 30 07:19:17.217: INFO: Pod "execpod-affinity7xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.017089702s
Aug 30 07:19:17.217: INFO: Pod "execpod-affinity7xgqs" satisfied condition "running"
Aug 30 07:19:18.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 30 07:19:18.522: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 30 07:19:18.522: INFO: stdout: ""
Aug 30 07:19:18.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 172.21.108.104 80'
Aug 30 07:19:18.784: INFO: stderr: "+ nc -v -z -w 2 172.21.108.104 80\nConnection to 172.21.108.104 80 port [tcp/http] succeeded!\n"
Aug 30 07:19:18.784: INFO: stdout: ""
Aug 30 07:19:18.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 31397'
Aug 30 07:19:19.061: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 31397\nConnection to 10.135.139.190 31397 port [tcp/*] succeeded!\n"
Aug 30 07:19:19.061: INFO: stdout: ""
Aug 30 07:19:19.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 31397'
Aug 30 07:19:19.303: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 31397\nConnection to 10.135.139.183 31397 port [tcp/*] succeeded!\n"
Aug 30 07:19:19.303: INFO: stdout: ""
Aug 30 07:19:19.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31397/ ; done'
Aug 30 07:19:19.658: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n"
Aug 30 07:19:19.658: INFO: stdout: "\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8"
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
Aug 30 07:19:19.658: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4355, will wait for the garbage collector to delete the pods 08/30/23 07:19:19.734
Aug 30 07:19:19.832: INFO: Deleting ReplicationController affinity-nodeport took: 38.707957ms
Aug 30 07:19:20.032: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.250498ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:19:22.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4355" for this suite. 08/30/23 07:19:23.001
------------------------------
• [SLOW TEST] [11.129 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:19:11.89
    Aug 30 07:19:11.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:19:11.891
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:11.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:11.981
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-4355 08/30/23 07:19:11.987
    STEP: creating service affinity-nodeport in namespace services-4355 08/30/23 07:19:11.987
    STEP: creating replication controller affinity-nodeport in namespace services-4355 08/30/23 07:19:12.091
    I0830 07:19:12.108579      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4355, replica count: 3
    I0830 07:19:15.160072      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:19:15.183: INFO: Creating new exec pod
    Aug 30 07:19:15.199: INFO: Waiting up to 5m0s for pod "execpod-affinity7xgqs" in namespace "services-4355" to be "running"
    Aug 30 07:19:15.207: INFO: Pod "execpod-affinity7xgqs": Phase="Pending", Reason="", readiness=false. Elapsed: 8.03961ms
    Aug 30 07:19:17.217: INFO: Pod "execpod-affinity7xgqs": Phase="Running", Reason="", readiness=true. Elapsed: 2.017089702s
    Aug 30 07:19:17.217: INFO: Pod "execpod-affinity7xgqs" satisfied condition "running"
    Aug 30 07:19:18.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 30 07:19:18.522: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 30 07:19:18.522: INFO: stdout: ""
    Aug 30 07:19:18.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 172.21.108.104 80'
    Aug 30 07:19:18.784: INFO: stderr: "+ nc -v -z -w 2 172.21.108.104 80\nConnection to 172.21.108.104 80 port [tcp/http] succeeded!\n"
    Aug 30 07:19:18.784: INFO: stdout: ""
    Aug 30 07:19:18.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 31397'
    Aug 30 07:19:19.061: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 31397\nConnection to 10.135.139.190 31397 port [tcp/*] succeeded!\n"
    Aug 30 07:19:19.061: INFO: stdout: ""
    Aug 30 07:19:19.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 31397'
    Aug 30 07:19:19.303: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 31397\nConnection to 10.135.139.183 31397 port [tcp/*] succeeded!\n"
    Aug 30 07:19:19.303: INFO: stdout: ""
    Aug 30 07:19:19.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-4355 exec execpod-affinity7xgqs -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.135.139.183:31397/ ; done'
    Aug 30 07:19:19.658: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.135.139.183:31397/\n"
    Aug 30 07:19:19.658: INFO: stdout: "\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8\naffinity-nodeport-vpnf8"
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Received response from host: affinity-nodeport-vpnf8
    Aug 30 07:19:19.658: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-4355, will wait for the garbage collector to delete the pods 08/30/23 07:19:19.734
    Aug 30 07:19:19.832: INFO: Deleting ReplicationController affinity-nodeport took: 38.707957ms
    Aug 30 07:19:20.032: INFO: Terminating ReplicationController affinity-nodeport pods took: 200.250498ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:19:22.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4355" for this suite. 08/30/23 07:19:23.001
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:19:23.02
Aug 30 07:19:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:19:23.021
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:23.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:23.092
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/30/23 07:19:23.099
Aug 30 07:19:24.121: INFO: Waiting up to 5m0s for pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c" in namespace "emptydir-2254" to be "Succeeded or Failed"
Aug 30 07:19:24.128: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.723403ms
Aug 30 07:19:26.137: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015882503s
Aug 30 07:19:28.139: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017866478s
Aug 30 07:19:30.136: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014785021s
STEP: Saw pod success 08/30/23 07:19:30.136
Aug 30 07:19:30.136: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c" satisfied condition "Succeeded or Failed"
Aug 30 07:19:30.143: INFO: Trying to get logs from node 10.135.139.190 pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c container test-container: <nil>
STEP: delete the pod 08/30/23 07:19:30.167
Aug 30 07:19:30.200: INFO: Waiting for pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c to disappear
Aug 30 07:19:30.207: INFO: Pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:19:30.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2254" for this suite. 08/30/23 07:19:30.219
------------------------------
• [SLOW TEST] [7.217 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:19:23.02
    Aug 30 07:19:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:19:23.021
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:23.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:23.092
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/30/23 07:19:23.099
    Aug 30 07:19:24.121: INFO: Waiting up to 5m0s for pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c" in namespace "emptydir-2254" to be "Succeeded or Failed"
    Aug 30 07:19:24.128: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.723403ms
    Aug 30 07:19:26.137: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015882503s
    Aug 30 07:19:28.139: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017866478s
    Aug 30 07:19:30.136: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014785021s
    STEP: Saw pod success 08/30/23 07:19:30.136
    Aug 30 07:19:30.136: INFO: Pod "pod-50a55daf-ec7a-43c3-9412-d77c170eee5c" satisfied condition "Succeeded or Failed"
    Aug 30 07:19:30.143: INFO: Trying to get logs from node 10.135.139.190 pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c container test-container: <nil>
    STEP: delete the pod 08/30/23 07:19:30.167
    Aug 30 07:19:30.200: INFO: Waiting for pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c to disappear
    Aug 30 07:19:30.207: INFO: Pod pod-50a55daf-ec7a-43c3-9412-d77c170eee5c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:19:30.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2254" for this suite. 08/30/23 07:19:30.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:19:30.237
Aug 30 07:19:30.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 07:19:30.239
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:30.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:30.288
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/30/23 07:19:30.293
W0830 07:19:30.310798      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for pod running 08/30/23 07:19:30.31
Aug 30 07:19:30.311: INFO: Waiting up to 2m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421" to be "running"
Aug 30 07:19:30.331: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.859387ms
Aug 30 07:19:32.340: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029038028s
Aug 30 07:19:32.340: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" satisfied condition "running"
STEP: creating a file in subpath 08/30/23 07:19:32.34
Aug 30 07:19:32.346: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3421 PodName:var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:19:32.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:19:32.347: INFO: ExecWithOptions: Clientset creation
Aug 30 07:19:32.347: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-3421/pods/var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/30/23 07:19:32.48
Aug 30 07:19:32.488: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3421 PodName:var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:19:32.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:19:32.489: INFO: ExecWithOptions: Clientset creation
Aug 30 07:19:32.489: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-3421/pods/var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/30/23 07:19:32.655
Aug 30 07:19:33.179: INFO: Successfully updated pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4"
STEP: waiting for annotated pod running 08/30/23 07:19:33.18
Aug 30 07:19:33.180: INFO: Waiting up to 2m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421" to be "running"
Aug 30 07:19:33.190: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.503036ms
Aug 30 07:19:33.191: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" satisfied condition "running"
STEP: deleting the pod gracefully 08/30/23 07:19:33.191
Aug 30 07:19:33.191: INFO: Deleting pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421"
Aug 30 07:19:33.210: INFO: Wait up to 5m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 07:20:07.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3421" for this suite. 08/30/23 07:20:07.239
------------------------------
• [SLOW TEST] [37.019 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:19:30.237
    Aug 30 07:19:30.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 07:19:30.239
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:19:30.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:19:30.288
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/30/23 07:19:30.293
    W0830 07:19:30.310798      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for pod running 08/30/23 07:19:30.31
    Aug 30 07:19:30.311: INFO: Waiting up to 2m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421" to be "running"
    Aug 30 07:19:30.331: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.859387ms
    Aug 30 07:19:32.340: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Running", Reason="", readiness=true. Elapsed: 2.029038028s
    Aug 30 07:19:32.340: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" satisfied condition "running"
    STEP: creating a file in subpath 08/30/23 07:19:32.34
    Aug 30 07:19:32.346: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3421 PodName:var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:19:32.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:19:32.347: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:19:32.347: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-3421/pods/var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/30/23 07:19:32.48
    Aug 30 07:19:32.488: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3421 PodName:var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:19:32.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:19:32.489: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:19:32.489: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-3421/pods/var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/30/23 07:19:32.655
    Aug 30 07:19:33.179: INFO: Successfully updated pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4"
    STEP: waiting for annotated pod running 08/30/23 07:19:33.18
    Aug 30 07:19:33.180: INFO: Waiting up to 2m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421" to be "running"
    Aug 30 07:19:33.190: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.503036ms
    Aug 30 07:19:33.191: INFO: Pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" satisfied condition "running"
    STEP: deleting the pod gracefully 08/30/23 07:19:33.191
    Aug 30 07:19:33.191: INFO: Deleting pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" in namespace "var-expansion-3421"
    Aug 30 07:19:33.210: INFO: Wait up to 5m0s for pod "var-expansion-fc45adff-f601-4e0f-9578-097178d23eb4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:20:07.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3421" for this suite. 08/30/23 07:20:07.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:20:07.259
Aug 30 07:20:07.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:20:07.26
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:07.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:07.313
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 30 07:20:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:20:14.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6762" for this suite. 08/30/23 07:20:14.474
------------------------------
• [SLOW TEST] [7.232 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:20:07.259
    Aug 30 07:20:07.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:20:07.26
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:07.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:07.313
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 30 07:20:07.322: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:20:14.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6762" for this suite. 08/30/23 07:20:14.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:20:14.493
Aug 30 07:20:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-runtime 08/30/23 07:20:14.494
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:14.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:14.544
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/30/23 07:20:14.568
W0830 07:20:14.587395      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 08/30/23 07:20:14.587
STEP: get the container status 08/30/23 07:20:18.653
STEP: the container should be terminated 08/30/23 07:20:18.66
STEP: the termination message should be set 08/30/23 07:20:18.66
Aug 30 07:20:18.660: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/30/23 07:20:18.66
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 30 07:20:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-9354" for this suite. 08/30/23 07:20:18.715
------------------------------
• [4.242 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:20:14.493
    Aug 30 07:20:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-runtime 08/30/23 07:20:14.494
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:14.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:14.544
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/30/23 07:20:14.568
    W0830 07:20:14.587395      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 08/30/23 07:20:14.587
    STEP: get the container status 08/30/23 07:20:18.653
    STEP: the container should be terminated 08/30/23 07:20:18.66
    STEP: the termination message should be set 08/30/23 07:20:18.66
    Aug 30 07:20:18.660: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/30/23 07:20:18.66
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:20:18.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-9354" for this suite. 08/30/23 07:20:18.715
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:20:18.736
Aug 30 07:20:18.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename cronjob 08/30/23 07:20:18.738
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:18.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:18.788
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/30/23 07:20:18.793
W0830 07:20:19.815342      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 08/30/23 07:20:19.831
STEP: Ensuring no job exists by listing jobs explicitly 08/30/23 07:25:19.858
STEP: Removing cronjob 08/30/23 07:25:19.868
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:19.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9653" for this suite. 08/30/23 07:25:19.895
------------------------------
• [SLOW TEST] [301.176 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:20:18.736
    Aug 30 07:20:18.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename cronjob 08/30/23 07:20:18.738
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:20:18.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:20:18.788
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/30/23 07:20:18.793
    W0830 07:20:19.815342      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 08/30/23 07:20:19.831
    STEP: Ensuring no job exists by listing jobs explicitly 08/30/23 07:25:19.858
    STEP: Removing cronjob 08/30/23 07:25:19.868
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:19.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9653" for this suite. 08/30/23 07:25:19.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:19.916
Aug 30 07:25:19.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context-test 08/30/23 07:25:19.917
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:19.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:19.995
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 30 07:25:21.028: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086" in namespace "security-context-test-477" to be "Succeeded or Failed"
Aug 30 07:25:21.035: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 7.097407ms
Aug 30 07:25:23.043: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015516091s
Aug 30 07:25:25.043: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015065967s
Aug 30 07:25:27.046: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018669761s
Aug 30 07:25:27.047: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-477" for this suite. 08/30/23 07:25:27.098
------------------------------
• [SLOW TEST] [7.202 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:19.916
    Aug 30 07:25:19.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context-test 08/30/23 07:25:19.917
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:19.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:19.995
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 30 07:25:21.028: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086" in namespace "security-context-test-477" to be "Succeeded or Failed"
    Aug 30 07:25:21.035: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 7.097407ms
    Aug 30 07:25:23.043: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015516091s
    Aug 30 07:25:25.043: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015065967s
    Aug 30 07:25:27.046: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018669761s
    Aug 30 07:25:27.047: INFO: Pod "alpine-nnp-false-552467de-8ae9-4b21-a920-2ef5f84f4086" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:27.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-477" for this suite. 08/30/23 07:25:27.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:27.119
Aug 30 07:25:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context-test 08/30/23 07:25:27.12
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:27.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:27.17
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 30 07:25:28.196: INFO: Waiting up to 5m0s for pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e" in namespace "security-context-test-3679" to be "Succeeded or Failed"
Aug 30 07:25:28.204: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.890987ms
Aug 30 07:25:30.212: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015868557s
Aug 30 07:25:32.212: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015933797s
Aug 30 07:25:34.220: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024034778s
Aug 30 07:25:34.220: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:34.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3679" for this suite. 08/30/23 07:25:34.229
------------------------------
• [SLOW TEST] [7.129 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:27.119
    Aug 30 07:25:27.119: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context-test 08/30/23 07:25:27.12
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:27.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:27.17
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 30 07:25:28.196: INFO: Waiting up to 5m0s for pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e" in namespace "security-context-test-3679" to be "Succeeded or Failed"
    Aug 30 07:25:28.204: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.890987ms
    Aug 30 07:25:30.212: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015868557s
    Aug 30 07:25:32.212: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015933797s
    Aug 30 07:25:34.220: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024034778s
    Aug 30 07:25:34.220: INFO: Pod "busybox-user-65534-edc376bd-7d91-4ff2-9ffa-be4808df9d8e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:34.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3679" for this suite. 08/30/23 07:25:34.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:34.249
Aug 30 07:25:34.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:25:34.251
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:34.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:34.308
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/30/23 07:25:34.313
STEP: Creating a ResourceQuota 08/30/23 07:25:39.328
STEP: Ensuring resource quota status is calculated 08/30/23 07:25:39.342
STEP: Creating a ReplicaSet 08/30/23 07:25:41.353
STEP: Ensuring resource quota status captures replicaset creation 08/30/23 07:25:41.413
STEP: Deleting a ReplicaSet 08/30/23 07:25:43.435
STEP: Ensuring resource quota status released usage 08/30/23 07:25:43.451
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8633" for this suite. 08/30/23 07:25:45.479
------------------------------
• [SLOW TEST] [11.250 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:34.249
    Aug 30 07:25:34.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:25:34.251
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:34.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:34.308
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/30/23 07:25:34.313
    STEP: Creating a ResourceQuota 08/30/23 07:25:39.328
    STEP: Ensuring resource quota status is calculated 08/30/23 07:25:39.342
    STEP: Creating a ReplicaSet 08/30/23 07:25:41.353
    STEP: Ensuring resource quota status captures replicaset creation 08/30/23 07:25:41.413
    STEP: Deleting a ReplicaSet 08/30/23 07:25:43.435
    STEP: Ensuring resource quota status released usage 08/30/23 07:25:43.451
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8633" for this suite. 08/30/23 07:25:45.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:45.501
Aug 30 07:25:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename podtemplate 08/30/23 07:25:45.503
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:45.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:45.553
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
W0830 07:25:45.579337      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:45.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8074" for this suite. 08/30/23 07:25:45.677
------------------------------
• [0.194 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:45.501
    Aug 30 07:25:45.502: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename podtemplate 08/30/23 07:25:45.503
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:45.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:45.553
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    W0830 07:25:45.579337      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:45.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8074" for this suite. 08/30/23 07:25:45.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:45.699
Aug 30 07:25:45.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 07:25:45.7
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:45.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:45.781
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/30/23 07:25:45.787
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +notcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_tcp@PTR;sleep 1; done
 08/30/23 07:25:45.823
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +notcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_tcp@PTR;sleep 1; done
 08/30/23 07:25:45.823
STEP: creating a pod to probe DNS 08/30/23 07:25:45.823
STEP: submitting the pod to kubernetes 08/30/23 07:25:45.823
Aug 30 07:25:45.842: INFO: Waiting up to 15m0s for pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b" in namespace "dns-468" to be "running"
Aug 30 07:25:45.865: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.94773ms
Aug 30 07:25:47.873: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030885077s
Aug 30 07:25:49.879: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Running", Reason="", readiness=true. Elapsed: 4.036726028s
Aug 30 07:25:49.879: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b" satisfied condition "running"
STEP: retrieving the pod 08/30/23 07:25:49.879
STEP: looking for the results for each expected name from probers 08/30/23 07:25:49.886
Aug 30 07:25:49.909: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.921: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.939: INFO: Unable to read wheezy_tcp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.949: INFO: Unable to read wheezy_udp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.971: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:49.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.048: INFO: Unable to read jessie_udp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.058: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.070: INFO: Unable to read jessie_udp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.124: INFO: Unable to read jessie_udp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.144: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.153: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
Aug 30 07:25:50.190: INFO: Lookups using dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-468 wheezy_tcp@dns-test-service.dns-468 wheezy_udp@dns-test-service.dns-468.svc wheezy_tcp@dns-test-service.dns-468.svc wheezy_udp@_http._tcp.dns-test-service.dns-468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-468 jessie_tcp@dns-test-service.dns-468 jessie_udp@dns-test-service.dns-468.svc jessie_tcp@dns-test-service.dns-468.svc jessie_udp@_http._tcp.dns-test-service.dns-468.svc jessie_tcp@_http._tcp.dns-test-service.dns-468.svc]

Aug 30 07:25:55.448: INFO: DNS probes using dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b succeeded

STEP: deleting the pod 08/30/23 07:25:55.448
STEP: deleting the test service 08/30/23 07:25:55.475
STEP: deleting the test headless service 08/30/23 07:25:55.56
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 07:25:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-468" for this suite. 08/30/23 07:25:55.597
------------------------------
• [SLOW TEST] [9.927 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:45.699
    Aug 30 07:25:45.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 07:25:45.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:45.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:45.781
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/30/23 07:25:45.787
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +notcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_tcp@PTR;sleep 1; done
     08/30/23 07:25:45.823
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-468;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-468;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-468.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-468.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-468.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-468.svc;check="$$(dig +notcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_udp@PTR;check="$$(dig +tcp +noall +answer +search 139.105.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.105.139_tcp@PTR;sleep 1; done
     08/30/23 07:25:45.823
    STEP: creating a pod to probe DNS 08/30/23 07:25:45.823
    STEP: submitting the pod to kubernetes 08/30/23 07:25:45.823
    Aug 30 07:25:45.842: INFO: Waiting up to 15m0s for pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b" in namespace "dns-468" to be "running"
    Aug 30 07:25:45.865: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.94773ms
    Aug 30 07:25:47.873: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030885077s
    Aug 30 07:25:49.879: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b": Phase="Running", Reason="", readiness=true. Elapsed: 4.036726028s
    Aug 30 07:25:49.879: INFO: Pod "dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 07:25:49.879
    STEP: looking for the results for each expected name from probers 08/30/23 07:25:49.886
    Aug 30 07:25:49.909: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.921: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.930: INFO: Unable to read wheezy_udp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.939: INFO: Unable to read wheezy_tcp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.949: INFO: Unable to read wheezy_udp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.960: INFO: Unable to read wheezy_tcp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.971: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:49.981: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.048: INFO: Unable to read jessie_udp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.058: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.070: INFO: Unable to read jessie_udp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.099: INFO: Unable to read jessie_tcp@dns-test-service.dns-468 from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.124: INFO: Unable to read jessie_udp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.144: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.153: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-468.svc from pod dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b: the server could not find the requested resource (get pods dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b)
    Aug 30 07:25:50.190: INFO: Lookups using dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-468 wheezy_tcp@dns-test-service.dns-468 wheezy_udp@dns-test-service.dns-468.svc wheezy_tcp@dns-test-service.dns-468.svc wheezy_udp@_http._tcp.dns-test-service.dns-468.svc wheezy_tcp@_http._tcp.dns-test-service.dns-468.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-468 jessie_tcp@dns-test-service.dns-468 jessie_udp@dns-test-service.dns-468.svc jessie_tcp@dns-test-service.dns-468.svc jessie_udp@_http._tcp.dns-test-service.dns-468.svc jessie_tcp@_http._tcp.dns-test-service.dns-468.svc]

    Aug 30 07:25:55.448: INFO: DNS probes using dns-468/dns-test-07b64f48-5f44-42d4-b87e-940ca73f052b succeeded

    STEP: deleting the pod 08/30/23 07:25:55.448
    STEP: deleting the test service 08/30/23 07:25:55.475
    STEP: deleting the test headless service 08/30/23 07:25:55.56
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:25:55.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-468" for this suite. 08/30/23 07:25:55.597
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:25:55.627
Aug 30 07:25:55.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pod-network-test 08/30/23 07:25:55.628
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:55.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:55.693
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5904 08/30/23 07:25:55.698
STEP: creating a selector 08/30/23 07:25:55.698
STEP: Creating the service pods in kubernetes 08/30/23 07:25:55.698
Aug 30 07:25:55.698: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 30 07:25:55.799: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5904" to be "running and ready"
Aug 30 07:25:55.808: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231272ms
Aug 30 07:25:55.808: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:25:57.816: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017560618s
Aug 30 07:25:57.816: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:25:59.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019564618s
Aug 30 07:25:59.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:01.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029689857s
Aug 30 07:26:01.828: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:03.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018393069s
Aug 30 07:26:03.817: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:05.815: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016438516s
Aug 30 07:26:05.815: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:07.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017969122s
Aug 30 07:26:07.817: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:09.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.027498958s
Aug 30 07:26:09.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:11.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017140471s
Aug 30 07:26:11.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:13.819: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019794217s
Aug 30 07:26:13.819: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:15.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019379678s
Aug 30 07:26:15.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 30 07:26:17.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017697868s
Aug 30 07:26:17.816: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 30 07:26:17.817: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 30 07:26:17.824: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5904" to be "running and ready"
Aug 30 07:26:17.830: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.391981ms
Aug 30 07:26:17.830: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 30 07:26:17.830: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 30 07:26:17.837: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5904" to be "running and ready"
Aug 30 07:26:17.844: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.441199ms
Aug 30 07:26:17.844: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 30 07:26:17.844: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 08/30/23 07:26:17.851
Aug 30 07:26:17.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5904" to be "running"
Aug 30 07:26:17.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.330439ms
Aug 30 07:26:19.890: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027605147s
Aug 30 07:26:19.890: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 30 07:26:19.898: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Aug 30 07:26:19.898: INFO: Breadth first check of 172.30.86.186 on host 10.135.139.183...
Aug 30 07:26:19.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.86.186&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:26:19.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:26:19.907: INFO: ExecWithOptions: Clientset creation
Aug 30 07:26:19.908: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.86.186%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:26:20.078: INFO: Waiting for responses: map[]
Aug 30 07:26:20.079: INFO: reached 172.30.86.186 after 0/1 tries
Aug 30 07:26:20.079: INFO: Breadth first check of 172.30.224.32 on host 10.135.139.185...
Aug 30 07:26:20.086: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.224.32&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:26:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:26:20.087: INFO: ExecWithOptions: Clientset creation
Aug 30 07:26:20.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.224.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:26:20.208: INFO: Waiting for responses: map[]
Aug 30 07:26:20.208: INFO: reached 172.30.224.32 after 0/1 tries
Aug 30 07:26:20.208: INFO: Breadth first check of 172.30.58.73 on host 10.135.139.190...
Aug 30 07:26:20.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.58.73&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:26:20.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:26:20.216: INFO: ExecWithOptions: Clientset creation
Aug 30 07:26:20.216: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.58.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 30 07:26:20.357: INFO: Waiting for responses: map[]
Aug 30 07:26:20.358: INFO: reached 172.30.58.73 after 0/1 tries
Aug 30 07:26:20.358: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 30 07:26:20.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5904" for this suite. 08/30/23 07:26:20.37
------------------------------
• [SLOW TEST] [24.763 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:25:55.627
    Aug 30 07:25:55.627: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pod-network-test 08/30/23 07:25:55.628
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:25:55.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:25:55.693
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5904 08/30/23 07:25:55.698
    STEP: creating a selector 08/30/23 07:25:55.698
    STEP: Creating the service pods in kubernetes 08/30/23 07:25:55.698
    Aug 30 07:25:55.698: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 30 07:25:55.799: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5904" to be "running and ready"
    Aug 30 07:25:55.808: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.231272ms
    Aug 30 07:25:55.808: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:25:57.816: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017560618s
    Aug 30 07:25:57.816: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:25:59.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.019564618s
    Aug 30 07:25:59.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:01.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.029689857s
    Aug 30 07:26:01.828: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:03.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018393069s
    Aug 30 07:26:03.817: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:05.815: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016438516s
    Aug 30 07:26:05.815: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:07.817: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.017969122s
    Aug 30 07:26:07.817: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:09.826: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.027498958s
    Aug 30 07:26:09.826: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:11.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.017140471s
    Aug 30 07:26:11.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:13.819: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.019794217s
    Aug 30 07:26:13.819: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:15.818: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.019379678s
    Aug 30 07:26:15.818: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 30 07:26:17.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017697868s
    Aug 30 07:26:17.816: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 30 07:26:17.817: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 30 07:26:17.824: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5904" to be "running and ready"
    Aug 30 07:26:17.830: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.391981ms
    Aug 30 07:26:17.830: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 30 07:26:17.830: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 30 07:26:17.837: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5904" to be "running and ready"
    Aug 30 07:26:17.844: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.441199ms
    Aug 30 07:26:17.844: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 30 07:26:17.844: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 08/30/23 07:26:17.851
    Aug 30 07:26:17.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5904" to be "running"
    Aug 30 07:26:17.882: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.330439ms
    Aug 30 07:26:19.890: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.027605147s
    Aug 30 07:26:19.890: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 30 07:26:19.898: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Aug 30 07:26:19.898: INFO: Breadth first check of 172.30.86.186 on host 10.135.139.183...
    Aug 30 07:26:19.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.86.186&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:26:19.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:26:19.907: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:26:19.908: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.86.186%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:26:20.078: INFO: Waiting for responses: map[]
    Aug 30 07:26:20.079: INFO: reached 172.30.86.186 after 0/1 tries
    Aug 30 07:26:20.079: INFO: Breadth first check of 172.30.224.32 on host 10.135.139.185...
    Aug 30 07:26:20.086: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.224.32&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:26:20.086: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:26:20.087: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:26:20.087: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.224.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:26:20.208: INFO: Waiting for responses: map[]
    Aug 30 07:26:20.208: INFO: reached 172.30.224.32 after 0/1 tries
    Aug 30 07:26:20.208: INFO: Breadth first check of 172.30.58.73 on host 10.135.139.190...
    Aug 30 07:26:20.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.58.91:9080/dial?request=hostname&protocol=http&host=172.30.58.73&port=8083&tries=1'] Namespace:pod-network-test-5904 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:26:20.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:26:20.216: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:26:20.216: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-5904/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.58.91%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.58.73%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 30 07:26:20.357: INFO: Waiting for responses: map[]
    Aug 30 07:26:20.358: INFO: reached 172.30.58.73 after 0/1 tries
    Aug 30 07:26:20.358: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:26:20.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5904" for this suite. 08/30/23 07:26:20.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:26:20.39
Aug 30 07:26:20.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename taint-single-pod 08/30/23 07:26:20.391
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:26:20.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:26:20.464
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 30 07:26:20.472: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 07:27:20.583: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 30 07:27:20.602: INFO: Starting informer...
STEP: Starting pod... 08/30/23 07:27:20.602
Aug 30 07:27:20.846: INFO: Pod is running on 10.135.139.190. Tainting Node
STEP: Trying to apply a taint on the Node 08/30/23 07:27:20.846
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 07:27:20.864
STEP: Waiting short time to make sure Pod is queued for deletion 08/30/23 07:27:20.871
Aug 30 07:27:20.871: INFO: Pod wasn't evicted. Proceeding
Aug 30 07:27:20.871: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 07:27:20.893
STEP: Waiting some time to make sure that toleration time passed. 08/30/23 07:27:20.905
Aug 30 07:28:35.905: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:28:35.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-9104" for this suite. 08/30/23 07:28:35.915
------------------------------
• [SLOW TEST] [135.545 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:26:20.39
    Aug 30 07:26:20.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename taint-single-pod 08/30/23 07:26:20.391
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:26:20.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:26:20.464
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 30 07:26:20.472: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 07:27:20.583: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 30 07:27:20.602: INFO: Starting informer...
    STEP: Starting pod... 08/30/23 07:27:20.602
    Aug 30 07:27:20.846: INFO: Pod is running on 10.135.139.190. Tainting Node
    STEP: Trying to apply a taint on the Node 08/30/23 07:27:20.846
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 07:27:20.864
    STEP: Waiting short time to make sure Pod is queued for deletion 08/30/23 07:27:20.871
    Aug 30 07:27:20.871: INFO: Pod wasn't evicted. Proceeding
    Aug 30 07:27:20.871: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/30/23 07:27:20.893
    STEP: Waiting some time to make sure that toleration time passed. 08/30/23 07:27:20.905
    Aug 30 07:28:35.905: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:28:35.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-9104" for this suite. 08/30/23 07:28:35.915
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:28:35.936
Aug 30 07:28:35.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 07:28:35.938
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:36.006
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Aug 30 07:28:36.098: INFO: Creating daemon "daemon-set" with a node selector
W0830 07:28:36.135553      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Initially, daemon pods should not be running on any nodes. 08/30/23 07:28:36.135
Aug 30 07:28:36.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:36.163: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/30/23 07:28:36.163
Aug 30 07:28:36.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:36.221: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:37.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:37.234: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:38.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:38.229: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:39.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 07:28:39.231: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/30/23 07:28:39.25
Aug 30 07:28:39.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 07:28:39.326: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 30 07:28:40.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:40.336: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/30/23 07:28:40.336
Aug 30 07:28:40.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:40.366: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:41.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:41.382: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:42.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:42.377: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:43.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:43.401: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:44.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:44.377: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:28:45.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 07:28:45.376: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:28:45.397
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7012, will wait for the garbage collector to delete the pods 08/30/23 07:28:45.397
Aug 30 07:28:45.486: INFO: Deleting DaemonSet.extensions daemon-set took: 28.291267ms
Aug 30 07:28:45.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.736108ms
Aug 30 07:28:48.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:28:48.007: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 07:28:48.016: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111001"},"items":null}

Aug 30 07:28:48.023: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111001"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:28:48.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7012" for this suite. 08/30/23 07:28:48.086
------------------------------
• [SLOW TEST] [12.193 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:28:35.936
    Aug 30 07:28:35.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 07:28:35.938
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:36.006
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Aug 30 07:28:36.098: INFO: Creating daemon "daemon-set" with a node selector
    W0830 07:28:36.135553      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "app" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "app" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "app" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "app" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Initially, daemon pods should not be running on any nodes. 08/30/23 07:28:36.135
    Aug 30 07:28:36.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:36.163: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/30/23 07:28:36.163
    Aug 30 07:28:36.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:36.221: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:37.234: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:37.234: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:38.229: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:38.229: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:39.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 30 07:28:39.231: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/30/23 07:28:39.25
    Aug 30 07:28:39.326: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 30 07:28:39.326: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 30 07:28:40.336: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:40.336: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/30/23 07:28:40.336
    Aug 30 07:28:40.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:40.366: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:41.382: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:41.382: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:42.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:42.377: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:43.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:43.401: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:44.377: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:44.377: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:28:45.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 30 07:28:45.376: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:28:45.397
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7012, will wait for the garbage collector to delete the pods 08/30/23 07:28:45.397
    Aug 30 07:28:45.486: INFO: Deleting DaemonSet.extensions daemon-set took: 28.291267ms
    Aug 30 07:28:45.687: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.736108ms
    Aug 30 07:28:48.006: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:28:48.007: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 07:28:48.016: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111001"},"items":null}

    Aug 30 07:28:48.023: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111001"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:28:48.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7012" for this suite. 08/30/23 07:28:48.086
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:28:48.135
Aug 30 07:28:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename endpointslicemirroring 08/30/23 07:28:48.136
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:48.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:48.202
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/30/23 07:28:48.246
Aug 30 07:28:48.265: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/30/23 07:28:50.276
Aug 30 07:28:50.292: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/30/23 07:28:52.326
Aug 30 07:28:52.345: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 30 07:28:54.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-4153" for this suite. 08/30/23 07:28:54.361
------------------------------
• [SLOW TEST] [6.245 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:28:48.135
    Aug 30 07:28:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename endpointslicemirroring 08/30/23 07:28:48.136
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:48.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:48.202
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/30/23 07:28:48.246
    Aug 30 07:28:48.265: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/30/23 07:28:50.276
    Aug 30 07:28:50.292: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/30/23 07:28:52.326
    Aug 30 07:28:52.345: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:28:54.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-4153" for this suite. 08/30/23 07:28:54.361
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:28:54.381
Aug 30 07:28:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:28:54.382
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:54.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:54.432
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-84bfbb22-2318-4be9-89d7-c0294aa4c2b5 08/30/23 07:28:54.437
STEP: Creating a pod to test consume configMaps 08/30/23 07:28:54.481
Aug 30 07:28:54.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835" in namespace "projected-7182" to be "Succeeded or Failed"
Aug 30 07:28:54.508: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Pending", Reason="", readiness=false. Elapsed: 7.678287ms
Aug 30 07:28:56.517: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01603253s
Aug 30 07:28:58.530: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029669451s
STEP: Saw pod success 08/30/23 07:28:58.53
Aug 30 07:28:58.530: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835" satisfied condition "Succeeded or Failed"
Aug 30 07:28:58.538: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:28:58.573
Aug 30 07:28:58.610: INFO: Waiting for pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 to disappear
Aug 30 07:28:58.619: INFO: Pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:28:58.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7182" for this suite. 08/30/23 07:28:58.632
------------------------------
• [4.282 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:28:54.381
    Aug 30 07:28:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:28:54.382
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:54.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:54.432
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-84bfbb22-2318-4be9-89d7-c0294aa4c2b5 08/30/23 07:28:54.437
    STEP: Creating a pod to test consume configMaps 08/30/23 07:28:54.481
    Aug 30 07:28:54.500: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835" in namespace "projected-7182" to be "Succeeded or Failed"
    Aug 30 07:28:54.508: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Pending", Reason="", readiness=false. Elapsed: 7.678287ms
    Aug 30 07:28:56.517: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01603253s
    Aug 30 07:28:58.530: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029669451s
    STEP: Saw pod success 08/30/23 07:28:58.53
    Aug 30 07:28:58.530: INFO: Pod "pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835" satisfied condition "Succeeded or Failed"
    Aug 30 07:28:58.538: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:28:58.573
    Aug 30 07:28:58.610: INFO: Waiting for pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 to disappear
    Aug 30 07:28:58.619: INFO: Pod pod-projected-configmaps-e7322e72-b149-4220-8197-4b6604467835 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:28:58.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7182" for this suite. 08/30/23 07:28:58.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:28:58.663
Aug 30 07:28:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename dns 08/30/23 07:28:58.665
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:58.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:58.736
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/30/23 07:28:58.741
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/30/23 07:28:58.763
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/30/23 07:28:58.763
STEP: creating a pod to probe DNS 08/30/23 07:28:58.763
STEP: submitting the pod to kubernetes 08/30/23 07:28:58.763
Aug 30 07:28:58.805: INFO: Waiting up to 15m0s for pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a" in namespace "dns-6072" to be "running"
Aug 30 07:28:58.812: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71822ms
Aug 30 07:29:00.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016256832s
Aug 30 07:29:02.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Running", Reason="", readiness=true. Elapsed: 4.015829909s
Aug 30 07:29:02.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a" satisfied condition "running"
STEP: retrieving the pod 08/30/23 07:29:02.821
STEP: looking for the results for each expected name from probers 08/30/23 07:29:02.828
Aug 30 07:29:02.877: INFO: DNS probes using dns-6072/dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a succeeded

STEP: deleting the pod 08/30/23 07:29:02.877
STEP: deleting the test headless service 08/30/23 07:29:02.903
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:02.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6072" for this suite. 08/30/23 07:29:02.944
------------------------------
• [4.310 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:28:58.663
    Aug 30 07:28:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename dns 08/30/23 07:28:58.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:28:58.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:28:58.736
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/30/23 07:28:58.741
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/30/23 07:28:58.763
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6072.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/30/23 07:28:58.763
    STEP: creating a pod to probe DNS 08/30/23 07:28:58.763
    STEP: submitting the pod to kubernetes 08/30/23 07:28:58.763
    Aug 30 07:28:58.805: INFO: Waiting up to 15m0s for pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a" in namespace "dns-6072" to be "running"
    Aug 30 07:28:58.812: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.71822ms
    Aug 30 07:29:00.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016256832s
    Aug 30 07:29:02.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a": Phase="Running", Reason="", readiness=true. Elapsed: 4.015829909s
    Aug 30 07:29:02.821: INFO: Pod "dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a" satisfied condition "running"
    STEP: retrieving the pod 08/30/23 07:29:02.821
    STEP: looking for the results for each expected name from probers 08/30/23 07:29:02.828
    Aug 30 07:29:02.877: INFO: DNS probes using dns-6072/dns-test-4c2b13bb-1408-47d4-be5e-c37da1a3176a succeeded

    STEP: deleting the pod 08/30/23 07:29:02.877
    STEP: deleting the test headless service 08/30/23 07:29:02.903
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:02.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6072" for this suite. 08/30/23 07:29:02.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:02.975
Aug 30 07:29:02.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename endpointslice 08/30/23 07:29:02.976
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:03.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:03.034
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/30/23 07:29:03.039
STEP: getting /apis/discovery.k8s.io 08/30/23 07:29:03.044
STEP: getting /apis/discovery.k8s.iov1 08/30/23 07:29:03.046
STEP: creating 08/30/23 07:29:03.048
STEP: getting 08/30/23 07:29:03.202
STEP: listing 08/30/23 07:29:03.23
STEP: watching 08/30/23 07:29:03.237
Aug 30 07:29:03.237: INFO: starting watch
STEP: cluster-wide listing 08/30/23 07:29:03.239
STEP: cluster-wide watching 08/30/23 07:29:03.249
Aug 30 07:29:03.249: INFO: starting watch
STEP: patching 08/30/23 07:29:03.251
STEP: updating 08/30/23 07:29:03.297
Aug 30 07:29:03.318: INFO: waiting for watch events with expected annotations
Aug 30 07:29:03.318: INFO: saw patched and updated annotations
STEP: deleting 08/30/23 07:29:03.318
STEP: deleting a collection 08/30/23 07:29:03.344
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:03.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3469" for this suite. 08/30/23 07:29:03.397
------------------------------
• [0.445 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:02.975
    Aug 30 07:29:02.975: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename endpointslice 08/30/23 07:29:02.976
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:03.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:03.034
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/30/23 07:29:03.039
    STEP: getting /apis/discovery.k8s.io 08/30/23 07:29:03.044
    STEP: getting /apis/discovery.k8s.iov1 08/30/23 07:29:03.046
    STEP: creating 08/30/23 07:29:03.048
    STEP: getting 08/30/23 07:29:03.202
    STEP: listing 08/30/23 07:29:03.23
    STEP: watching 08/30/23 07:29:03.237
    Aug 30 07:29:03.237: INFO: starting watch
    STEP: cluster-wide listing 08/30/23 07:29:03.239
    STEP: cluster-wide watching 08/30/23 07:29:03.249
    Aug 30 07:29:03.249: INFO: starting watch
    STEP: patching 08/30/23 07:29:03.251
    STEP: updating 08/30/23 07:29:03.297
    Aug 30 07:29:03.318: INFO: waiting for watch events with expected annotations
    Aug 30 07:29:03.318: INFO: saw patched and updated annotations
    STEP: deleting 08/30/23 07:29:03.318
    STEP: deleting a collection 08/30/23 07:29:03.344
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:03.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3469" for this suite. 08/30/23 07:29:03.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:03.422
Aug 30 07:29:03.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:03.423
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:03.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:03.484
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/30/23 07:29:03.489
W0830 07:29:03.514044      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:29:03.514: INFO: Waiting up to 5m0s for pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded" in namespace "downward-api-8098" to be "Succeeded or Failed"
Aug 30 07:29:03.529: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Pending", Reason="", readiness=false. Elapsed: 14.880292ms
Aug 30 07:29:05.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046721878s
Aug 30 07:29:07.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045813511s
STEP: Saw pod success 08/30/23 07:29:07.56
Aug 30 07:29:07.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded" satisfied condition "Succeeded or Failed"
Aug 30 07:29:07.567: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:29:07.593
Aug 30 07:29:07.668: INFO: Waiting for pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded to disappear
Aug 30 07:29:07.679: INFO: Pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:07.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8098" for this suite. 08/30/23 07:29:07.694
------------------------------
• [4.320 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:03.422
    Aug 30 07:29:03.422: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:03.423
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:03.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:03.484
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/30/23 07:29:03.489
    W0830 07:29:03.514044      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:29:03.514: INFO: Waiting up to 5m0s for pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded" in namespace "downward-api-8098" to be "Succeeded or Failed"
    Aug 30 07:29:03.529: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Pending", Reason="", readiness=false. Elapsed: 14.880292ms
    Aug 30 07:29:05.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046721878s
    Aug 30 07:29:07.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045813511s
    STEP: Saw pod success 08/30/23 07:29:07.56
    Aug 30 07:29:07.560: INFO: Pod "downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded" satisfied condition "Succeeded or Failed"
    Aug 30 07:29:07.567: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:29:07.593
    Aug 30 07:29:07.668: INFO: Waiting for pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded to disappear
    Aug 30 07:29:07.679: INFO: Pod downward-api-02e0f6a4-84a2-4ce5-90a3-7e152a90aded no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:07.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8098" for this suite. 08/30/23 07:29:07.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:07.743
Aug 30 07:29:07.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 07:29:07.744
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:07.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:07.871
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/30/23 07:29:07.876
W0830 07:29:07.904233      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:29:07.904: INFO: Waiting up to 5m0s for pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574" in namespace "var-expansion-7760" to be "Succeeded or Failed"
Aug 30 07:29:07.920: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 16.14698ms
Aug 30 07:29:09.929: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024720143s
Aug 30 07:29:11.929: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025534482s
Aug 30 07:29:13.930: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026559231s
STEP: Saw pod success 08/30/23 07:29:13.93
Aug 30 07:29:13.931: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574" satisfied condition "Succeeded or Failed"
Aug 30 07:29:13.938: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:29:13.957
Aug 30 07:29:13.984: INFO: Waiting for pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 to disappear
Aug 30 07:29:13.990: INFO: Pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7760" for this suite. 08/30/23 07:29:13.998
------------------------------
• [SLOW TEST] [6.298 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:07.743
    Aug 30 07:29:07.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 07:29:07.744
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:07.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:07.871
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/30/23 07:29:07.876
    W0830 07:29:07.904233      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:29:07.904: INFO: Waiting up to 5m0s for pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574" in namespace "var-expansion-7760" to be "Succeeded or Failed"
    Aug 30 07:29:07.920: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 16.14698ms
    Aug 30 07:29:09.929: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024720143s
    Aug 30 07:29:11.929: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025534482s
    Aug 30 07:29:13.930: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026559231s
    STEP: Saw pod success 08/30/23 07:29:13.93
    Aug 30 07:29:13.931: INFO: Pod "var-expansion-ba6d6098-f600-4637-8968-9525e32cc574" satisfied condition "Succeeded or Failed"
    Aug 30 07:29:13.938: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:29:13.957
    Aug 30 07:29:13.984: INFO: Waiting for pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 to disappear
    Aug 30 07:29:13.990: INFO: Pod var-expansion-ba6d6098-f600-4637-8968-9525e32cc574 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:13.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7760" for this suite. 08/30/23 07:29:13.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:14.043
Aug 30 07:29:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 07:29:14.044
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:14.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:14.113
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Aug 30 07:29:14.257: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:29:14.278
Aug 30 07:29:14.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:29:14.328: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:29:15.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:29:15.383: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:29:16.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 30 07:29:16.349: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:29:17.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:29:17.351: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 08/30/23 07:29:17.387
STEP: Check that daemon pods images are updated. 08/30/23 07:29:17.42
Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-k9zgr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:18.451: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:18.451: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:19.450: INFO: Pod daemon-set-lmwvm is not available
Aug 30 07:29:19.450: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:19.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:20.450: INFO: Pod daemon-set-lmwvm is not available
Aug 30 07:29:20.450: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:20.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:21.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:22.456: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:23.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:23.451: INFO: Pod daemon-set-rxhvh is not available
Aug 30 07:29:24.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:24.450: INFO: Pod daemon-set-rxhvh is not available
Aug 30 07:29:25.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 30 07:29:25.450: INFO: Pod daemon-set-rxhvh is not available
Aug 30 07:29:28.451: INFO: Pod daemon-set-5fttc is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/30/23 07:29:28.46
Aug 30 07:29:28.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 07:29:28.479: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:29:29.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 07:29:29.527: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:29:30.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:29:30.505: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:29:30.555
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3073, will wait for the garbage collector to delete the pods 08/30/23 07:29:30.556
Aug 30 07:29:30.639: INFO: Deleting DaemonSet.extensions daemon-set took: 21.58738ms
Aug 30 07:29:30.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.774068ms
Aug 30 07:29:33.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:29:33.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 07:29:33.957: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111855"},"items":null}

Aug 30 07:29:33.964: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111855"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3073" for this suite. 08/30/23 07:29:34.001
------------------------------
• [SLOW TEST] [19.977 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:14.043
    Aug 30 07:29:14.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 07:29:14.044
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:14.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:14.113
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Aug 30 07:29:14.257: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:29:14.278
    Aug 30 07:29:14.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:29:14.328: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:29:15.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:29:15.383: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:29:16.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 30 07:29:16.349: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:29:17.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:29:17.351: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 08/30/23 07:29:17.387
    STEP: Check that daemon pods images are updated. 08/30/23 07:29:17.42
    Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-k9zgr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:17.429: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:18.451: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:18.451: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:19.450: INFO: Pod daemon-set-lmwvm is not available
    Aug 30 07:29:19.450: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:19.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:20.450: INFO: Pod daemon-set-lmwvm is not available
    Aug 30 07:29:20.450: INFO: Wrong image for pod: daemon-set-m6xhj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:20.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:21.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:22.456: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:23.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:23.451: INFO: Pod daemon-set-rxhvh is not available
    Aug 30 07:29:24.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:24.450: INFO: Pod daemon-set-rxhvh is not available
    Aug 30 07:29:25.450: INFO: Wrong image for pod: daemon-set-nxkzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 30 07:29:25.450: INFO: Pod daemon-set-rxhvh is not available
    Aug 30 07:29:28.451: INFO: Pod daemon-set-5fttc is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/30/23 07:29:28.46
    Aug 30 07:29:28.479: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 07:29:28.479: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:29:29.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 07:29:29.527: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:29:30.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:29:30.505: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:29:30.555
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3073, will wait for the garbage collector to delete the pods 08/30/23 07:29:30.556
    Aug 30 07:29:30.639: INFO: Deleting DaemonSet.extensions daemon-set took: 21.58738ms
    Aug 30 07:29:30.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.774068ms
    Aug 30 07:29:33.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:29:33.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 07:29:33.957: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"111855"},"items":null}

    Aug 30 07:29:33.964: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"111855"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:33.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3073" for this suite. 08/30/23 07:29:34.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:34.022
Aug 30 07:29:34.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:29:34.024
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:34.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:34.077
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7667/secret-test-d5909ca4-55bb-48fb-8775-82f8dd814eca 08/30/23 07:29:34.086
STEP: Creating a pod to test consume secrets 08/30/23 07:29:34.098
W0830 07:29:34.118896      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:29:34.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f" in namespace "secrets-7667" to be "Succeeded or Failed"
Aug 30 07:29:34.126: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.436952ms
Aug 30 07:29:36.134: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015123116s
Aug 30 07:29:38.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01609541s
Aug 30 07:29:40.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016093324s
STEP: Saw pod success 08/30/23 07:29:40.135
Aug 30 07:29:40.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f" satisfied condition "Succeeded or Failed"
Aug 30 07:29:40.147: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f container env-test: <nil>
STEP: delete the pod 08/30/23 07:29:40.164
Aug 30 07:29:40.190: INFO: Waiting for pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f to disappear
Aug 30 07:29:40.197: INFO: Pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:40.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7667" for this suite. 08/30/23 07:29:40.212
------------------------------
• [SLOW TEST] [6.210 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:34.022
    Aug 30 07:29:34.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:29:34.024
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:34.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:34.077
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7667/secret-test-d5909ca4-55bb-48fb-8775-82f8dd814eca 08/30/23 07:29:34.086
    STEP: Creating a pod to test consume secrets 08/30/23 07:29:34.098
    W0830 07:29:34.118896      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "env-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "env-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "env-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "env-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:29:34.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f" in namespace "secrets-7667" to be "Succeeded or Failed"
    Aug 30 07:29:34.126: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.436952ms
    Aug 30 07:29:36.134: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015123116s
    Aug 30 07:29:38.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01609541s
    Aug 30 07:29:40.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016093324s
    STEP: Saw pod success 08/30/23 07:29:40.135
    Aug 30 07:29:40.135: INFO: Pod "pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f" satisfied condition "Succeeded or Failed"
    Aug 30 07:29:40.147: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f container env-test: <nil>
    STEP: delete the pod 08/30/23 07:29:40.164
    Aug 30 07:29:40.190: INFO: Waiting for pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f to disappear
    Aug 30 07:29:40.197: INFO: Pod pod-configmaps-637e7760-5166-4f15-b543-c14cf4d5d52f no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:40.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7667" for this suite. 08/30/23 07:29:40.212
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:40.233
Aug 30 07:29:40.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:40.234
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:40.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:40.296
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/30/23 07:29:40.301
Aug 30 07:29:41.320: INFO: Waiting up to 5m0s for pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801" in namespace "downward-api-4878" to be "Succeeded or Failed"
Aug 30 07:29:41.340: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Pending", Reason="", readiness=false. Elapsed: 20.643494ms
Aug 30 07:29:43.349: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028968963s
Aug 30 07:29:45.348: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028197258s
STEP: Saw pod success 08/30/23 07:29:45.348
Aug 30 07:29:45.348: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801" satisfied condition "Succeeded or Failed"
Aug 30 07:29:45.354: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:29:45.372
Aug 30 07:29:45.393: INFO: Waiting for pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 to disappear
Aug 30 07:29:45.399: INFO: Pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:45.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4878" for this suite. 08/30/23 07:29:45.412
------------------------------
• [SLOW TEST] [5.203 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:40.233
    Aug 30 07:29:40.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:40.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:40.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:40.296
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/30/23 07:29:40.301
    Aug 30 07:29:41.320: INFO: Waiting up to 5m0s for pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801" in namespace "downward-api-4878" to be "Succeeded or Failed"
    Aug 30 07:29:41.340: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Pending", Reason="", readiness=false. Elapsed: 20.643494ms
    Aug 30 07:29:43.349: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028968963s
    Aug 30 07:29:45.348: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028197258s
    STEP: Saw pod success 08/30/23 07:29:45.348
    Aug 30 07:29:45.348: INFO: Pod "downward-api-8ef1e717-d507-4604-af06-20bcb17c0801" satisfied condition "Succeeded or Failed"
    Aug 30 07:29:45.354: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:29:45.372
    Aug 30 07:29:45.393: INFO: Waiting for pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 to disappear
    Aug 30 07:29:45.399: INFO: Pod downward-api-8ef1e717-d507-4604-af06-20bcb17c0801 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:45.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4878" for this suite. 08/30/23 07:29:45.412
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:45.437
Aug 30 07:29:45.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:29:45.438
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:45.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:45.496
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 30 07:29:45.584: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81" in namespace "kubelet-test-2080" to be "running and ready"
Aug 30 07:29:45.603: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81": Phase="Pending", Reason="", readiness=false. Elapsed: 18.448455ms
Aug 30 07:29:45.603: INFO: The phase of Pod busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:29:47.612: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81": Phase="Running", Reason="", readiness=true. Elapsed: 2.027923369s
Aug 30 07:29:47.612: INFO: The phase of Pod busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81 is Running (Ready = true)
Aug 30 07:29:47.612: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:47.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2080" for this suite. 08/30/23 07:29:47.657
------------------------------
• [2.240 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:45.437
    Aug 30 07:29:45.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:29:45.438
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:45.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:45.496
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 30 07:29:45.584: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81" in namespace "kubelet-test-2080" to be "running and ready"
    Aug 30 07:29:45.603: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81": Phase="Pending", Reason="", readiness=false. Elapsed: 18.448455ms
    Aug 30 07:29:45.603: INFO: The phase of Pod busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:29:47.612: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81": Phase="Running", Reason="", readiness=true. Elapsed: 2.027923369s
    Aug 30 07:29:47.612: INFO: The phase of Pod busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81 is Running (Ready = true)
    Aug 30 07:29:47.612: INFO: Pod "busybox-readonly-fs035ad63e-c37b-4709-88b4-555aae37dc81" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:47.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2080" for this suite. 08/30/23 07:29:47.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:47.677
Aug 30 07:29:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename podtemplate 08/30/23 07:29:47.679
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:47.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:47.734
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/30/23 07:29:47.74
W0830 07:29:47.752523      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 08/30/23 07:29:47.752
Aug 30 07:29:47.770: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:47.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8334" for this suite. 08/30/23 07:29:47.778
------------------------------
• [0.133 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:47.677
    Aug 30 07:29:47.677: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename podtemplate 08/30/23 07:29:47.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:47.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:47.734
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/30/23 07:29:47.74
    W0830 07:29:47.752523      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 08/30/23 07:29:47.752
    Aug 30 07:29:47.770: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:47.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8334" for this suite. 08/30/23 07:29:47.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:47.813
Aug 30 07:29:47.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:29:47.814
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:47.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:47.893
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-zrhmj"  08/30/23 07:29:47.898
Aug 30 07:29:47.910: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-zrhmj"  08/30/23 07:29:47.91
Aug 30 07:29:47.962: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:47.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5349" for this suite. 08/30/23 07:29:47.973
------------------------------
• [0.203 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:47.813
    Aug 30 07:29:47.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:29:47.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:47.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:47.893
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-zrhmj"  08/30/23 07:29:47.898
    Aug 30 07:29:47.910: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-zrhmj"  08/30/23 07:29:47.91
    Aug 30 07:29:47.962: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:47.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5349" for this suite. 08/30/23 07:29:47.973
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:48.016
Aug 30 07:29:48.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:29:48.017
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:48.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:48.074
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/30/23 07:29:48.08
Aug 30 07:29:48.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5758 create -f -'
Aug 30 07:29:48.714: INFO: stderr: ""
Aug 30 07:29:48.714: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/30/23 07:29:48.714
Aug 30 07:29:49.755: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:29:49.755: INFO: Found 0 / 1
Aug 30 07:29:50.722: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:29:50.722: INFO: Found 1 / 1
Aug 30 07:29:50.722: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/30/23 07:29:50.722
Aug 30 07:29:50.732: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:29:50.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 07:29:50.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5758 patch pod agnhost-primary-zjsq8 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 30 07:29:50.918: INFO: stderr: ""
Aug 30 07:29:50.918: INFO: stdout: "pod/agnhost-primary-zjsq8 patched\n"
STEP: checking annotations 08/30/23 07:29:50.918
Aug 30 07:29:50.928: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:29:50.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:50.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5758" for this suite. 08/30/23 07:29:50.952
------------------------------
• [2.961 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:48.016
    Aug 30 07:29:48.017: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:29:48.017
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:48.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:48.074
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/30/23 07:29:48.08
    Aug 30 07:29:48.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5758 create -f -'
    Aug 30 07:29:48.714: INFO: stderr: ""
    Aug 30 07:29:48.714: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/30/23 07:29:48.714
    Aug 30 07:29:49.755: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:29:49.755: INFO: Found 0 / 1
    Aug 30 07:29:50.722: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:29:50.722: INFO: Found 1 / 1
    Aug 30 07:29:50.722: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/30/23 07:29:50.722
    Aug 30 07:29:50.732: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:29:50.732: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 30 07:29:50.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5758 patch pod agnhost-primary-zjsq8 -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 30 07:29:50.918: INFO: stderr: ""
    Aug 30 07:29:50.918: INFO: stdout: "pod/agnhost-primary-zjsq8 patched\n"
    STEP: checking annotations 08/30/23 07:29:50.918
    Aug 30 07:29:50.928: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:29:50.928: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:50.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5758" for this suite. 08/30/23 07:29:50.952
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:50.978
Aug 30 07:29:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:29:50.979
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:51.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:51.047
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:29:51.052
Aug 30 07:29:52.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787" in namespace "projected-3367" to be "Succeeded or Failed"
Aug 30 07:29:52.116: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 7.622049ms
Aug 30 07:29:54.126: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016791012s
Aug 30 07:29:56.125: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016671258s
Aug 30 07:29:58.125: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016565818s
STEP: Saw pod success 08/30/23 07:29:58.125
Aug 30 07:29:58.126: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787" satisfied condition "Succeeded or Failed"
Aug 30 07:29:58.134: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 container client-container: <nil>
STEP: delete the pod 08/30/23 07:29:58.155
Aug 30 07:29:58.177: INFO: Waiting for pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 to disappear
Aug 30 07:29:58.183: INFO: Pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:58.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3367" for this suite. 08/30/23 07:29:58.192
------------------------------
• [SLOW TEST] [7.235 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:50.978
    Aug 30 07:29:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:29:50.979
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:51.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:51.047
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:29:51.052
    Aug 30 07:29:52.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787" in namespace "projected-3367" to be "Succeeded or Failed"
    Aug 30 07:29:52.116: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 7.622049ms
    Aug 30 07:29:54.126: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016791012s
    Aug 30 07:29:56.125: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016671258s
    Aug 30 07:29:58.125: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016565818s
    STEP: Saw pod success 08/30/23 07:29:58.125
    Aug 30 07:29:58.126: INFO: Pod "downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787" satisfied condition "Succeeded or Failed"
    Aug 30 07:29:58.134: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:29:58.155
    Aug 30 07:29:58.177: INFO: Waiting for pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 to disappear
    Aug 30 07:29:58.183: INFO: Pod downwardapi-volume-98a45cd9-57f7-41c0-a3db-ecf1093f3787 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:58.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3367" for this suite. 08/30/23 07:29:58.192
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:58.213
Aug 30 07:29:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:29:58.214
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:58.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:58.28
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/30/23 07:29:58.308
STEP: watching for the Service to be added 08/30/23 07:29:58.329
Aug 30 07:29:58.331: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 30 07:29:58.331: INFO: Service test-service-bmkpv created
STEP: Getting /status 08/30/23 07:29:58.332
Aug 30 07:29:58.344: INFO: Service test-service-bmkpv has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/30/23 07:29:58.344
STEP: watching for the Service to be patched 08/30/23 07:29:58.367
Aug 30 07:29:58.372: INFO: observed Service test-service-bmkpv in namespace services-703 with annotations: map[] & LoadBalancer: {[]}
Aug 30 07:29:58.372: INFO: Found Service test-service-bmkpv in namespace services-703 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 30 07:29:58.372: INFO: Service test-service-bmkpv has service status patched
STEP: updating the ServiceStatus 08/30/23 07:29:58.372
Aug 30 07:29:58.396: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/30/23 07:29:58.396
Aug 30 07:29:58.399: INFO: Observed Service test-service-bmkpv in namespace services-703 with annotations: map[] & Conditions: {[]}
Aug 30 07:29:58.399: INFO: Observed event: &Service{ObjectMeta:{test-service-bmkpv  services-703  6adb2065-4abd-4453-a946-a019fba9a6f7 112477 0 2023-08-30 07:29:58 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-30 07:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-30 07:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.140.84,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.140.84],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 30 07:29:58.399: INFO: Found Service test-service-bmkpv in namespace services-703 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 30 07:29:58.399: INFO: Service test-service-bmkpv has service status updated
STEP: patching the service 08/30/23 07:29:58.399
STEP: watching for the Service to be patched 08/30/23 07:29:58.431
Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
Aug 30 07:29:58.439: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service:patched test-service-static:true]
Aug 30 07:29:58.439: INFO: Service test-service-bmkpv patched
STEP: deleting the service 08/30/23 07:29:58.439
STEP: watching for the Service to be deleted 08/30/23 07:29:58.48
Aug 30 07:29:58.485: INFO: Observed event: ADDED
Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
Aug 30 07:29:58.485: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 30 07:29:58.485: INFO: Service test-service-bmkpv deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:29:58.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-703" for this suite. 08/30/23 07:29:58.494
------------------------------
• [0.338 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:58.213
    Aug 30 07:29:58.213: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:29:58.214
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:58.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:58.28
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/30/23 07:29:58.308
    STEP: watching for the Service to be added 08/30/23 07:29:58.329
    Aug 30 07:29:58.331: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 30 07:29:58.331: INFO: Service test-service-bmkpv created
    STEP: Getting /status 08/30/23 07:29:58.332
    Aug 30 07:29:58.344: INFO: Service test-service-bmkpv has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/30/23 07:29:58.344
    STEP: watching for the Service to be patched 08/30/23 07:29:58.367
    Aug 30 07:29:58.372: INFO: observed Service test-service-bmkpv in namespace services-703 with annotations: map[] & LoadBalancer: {[]}
    Aug 30 07:29:58.372: INFO: Found Service test-service-bmkpv in namespace services-703 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 30 07:29:58.372: INFO: Service test-service-bmkpv has service status patched
    STEP: updating the ServiceStatus 08/30/23 07:29:58.372
    Aug 30 07:29:58.396: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/30/23 07:29:58.396
    Aug 30 07:29:58.399: INFO: Observed Service test-service-bmkpv in namespace services-703 with annotations: map[] & Conditions: {[]}
    Aug 30 07:29:58.399: INFO: Observed event: &Service{ObjectMeta:{test-service-bmkpv  services-703  6adb2065-4abd-4453-a946-a019fba9a6f7 112477 0 2023-08-30 07:29:58 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-30 07:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-30 07:29:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.140.84,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.140.84],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 30 07:29:58.399: INFO: Found Service test-service-bmkpv in namespace services-703 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 30 07:29:58.399: INFO: Service test-service-bmkpv has service status updated
    STEP: patching the service 08/30/23 07:29:58.399
    STEP: watching for the Service to be patched 08/30/23 07:29:58.431
    Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
    Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
    Aug 30 07:29:58.439: INFO: observed Service test-service-bmkpv in namespace services-703 with labels: map[test-service-static:true]
    Aug 30 07:29:58.439: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service:patched test-service-static:true]
    Aug 30 07:29:58.439: INFO: Service test-service-bmkpv patched
    STEP: deleting the service 08/30/23 07:29:58.439
    STEP: watching for the Service to be deleted 08/30/23 07:29:58.48
    Aug 30 07:29:58.485: INFO: Observed event: ADDED
    Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
    Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
    Aug 30 07:29:58.485: INFO: Observed event: MODIFIED
    Aug 30 07:29:58.485: INFO: Found Service test-service-bmkpv in namespace services-703 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 30 07:29:58.485: INFO: Service test-service-bmkpv deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:29:58.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-703" for this suite. 08/30/23 07:29:58.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:29:58.552
Aug 30 07:29:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:58.554
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:58.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:58.667
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:29:58.674
Aug 30 07:29:58.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53" in namespace "downward-api-405" to be "Succeeded or Failed"
Aug 30 07:29:58.719: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Pending", Reason="", readiness=false. Elapsed: 11.075326ms
Aug 30 07:30:00.727: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019153615s
Aug 30 07:30:02.727: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019302176s
STEP: Saw pod success 08/30/23 07:30:02.727
Aug 30 07:30:02.728: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53" satisfied condition "Succeeded or Failed"
Aug 30 07:30:02.734: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 container client-container: <nil>
STEP: delete the pod 08/30/23 07:30:02.749
Aug 30 07:30:02.775: INFO: Waiting for pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 to disappear
Aug 30 07:30:02.781: INFO: Pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:02.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-405" for this suite. 08/30/23 07:30:02.801
------------------------------
• [4.267 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:29:58.552
    Aug 30 07:29:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:29:58.554
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:29:58.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:29:58.667
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:29:58.674
    Aug 30 07:29:58.708: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53" in namespace "downward-api-405" to be "Succeeded or Failed"
    Aug 30 07:29:58.719: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Pending", Reason="", readiness=false. Elapsed: 11.075326ms
    Aug 30 07:30:00.727: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019153615s
    Aug 30 07:30:02.727: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019302176s
    STEP: Saw pod success 08/30/23 07:30:02.727
    Aug 30 07:30:02.728: INFO: Pod "downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53" satisfied condition "Succeeded or Failed"
    Aug 30 07:30:02.734: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:30:02.749
    Aug 30 07:30:02.775: INFO: Waiting for pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 to disappear
    Aug 30 07:30:02.781: INFO: Pod downwardapi-volume-2029b178-48f2-4495-a802-870fcd9e5f53 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:02.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-405" for this suite. 08/30/23 07:30:02.801
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:02.82
Aug 30 07:30:02.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:30:02.821
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:02.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:02.892
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-ac811fb2-afe5-450c-b30d-1f37e47a88fe 08/30/23 07:30:02.897
STEP: Creating a pod to test consume secrets 08/30/23 07:30:02.905
Aug 30 07:30:03.934: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2" in namespace "projected-2026" to be "Succeeded or Failed"
Aug 30 07:30:03.941: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216458ms
Aug 30 07:30:05.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016467709s
Aug 30 07:30:07.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015961697s
STEP: Saw pod success 08/30/23 07:30:07.95
Aug 30 07:30:07.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2" satisfied condition "Succeeded or Failed"
Aug 30 07:30:07.961: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:30:07.977
Aug 30 07:30:08.003: INFO: Waiting for pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 to disappear
Aug 30 07:30:08.011: INFO: Pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:08.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2026" for this suite. 08/30/23 07:30:08.021
------------------------------
• [SLOW TEST] [5.226 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:02.82
    Aug 30 07:30:02.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:30:02.821
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:02.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:02.892
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-ac811fb2-afe5-450c-b30d-1f37e47a88fe 08/30/23 07:30:02.897
    STEP: Creating a pod to test consume secrets 08/30/23 07:30:02.905
    Aug 30 07:30:03.934: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2" in namespace "projected-2026" to be "Succeeded or Failed"
    Aug 30 07:30:03.941: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.216458ms
    Aug 30 07:30:05.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016467709s
    Aug 30 07:30:07.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015961697s
    STEP: Saw pod success 08/30/23 07:30:07.95
    Aug 30 07:30:07.950: INFO: Pod "pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2" satisfied condition "Succeeded or Failed"
    Aug 30 07:30:07.961: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:30:07.977
    Aug 30 07:30:08.003: INFO: Waiting for pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 to disappear
    Aug 30 07:30:08.011: INFO: Pod pod-projected-secrets-7b4227d7-774a-487e-b66d-4fff109e4bd2 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:08.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2026" for this suite. 08/30/23 07:30:08.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:08.047
Aug 30 07:30:08.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:30:08.048
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:08.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:08.105
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/30/23 07:30:08.156
Aug 30 07:30:08.156: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d" in namespace "kubelet-test-3839" to be "completed"
Aug 30 07:30:08.164: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112895ms
Aug 30 07:30:10.173: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017418539s
Aug 30 07:30:12.173: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017428868s
Aug 30 07:30:14.171: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015201092s
Aug 30 07:30:14.171: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3839" for this suite. 08/30/23 07:30:14.198
------------------------------
• [SLOW TEST] [6.181 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:08.047
    Aug 30 07:30:08.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:30:08.048
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:08.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:08.105
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/30/23 07:30:08.156
    Aug 30 07:30:08.156: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d" in namespace "kubelet-test-3839" to be "completed"
    Aug 30 07:30:08.164: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.112895ms
    Aug 30 07:30:10.173: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017418539s
    Aug 30 07:30:12.173: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017428868s
    Aug 30 07:30:14.171: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015201092s
    Aug 30 07:30:14.171: INFO: Pod "agnhost-host-aliasesed20fdde-b92c-4770-8b2b-c7265dbf032d" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:14.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3839" for this suite. 08/30/23 07:30:14.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:14.229
Aug 30 07:30:14.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:30:14.23
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:14.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:14.296
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/30/23 07:30:14.302
W0830 07:30:14.331214      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:30:14.331: INFO: Waiting up to 5m0s for pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559" in namespace "downward-api-1874" to be "running and ready"
Aug 30 07:30:14.341: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Pending", Reason="", readiness=false. Elapsed: 9.554403ms
Aug 30 07:30:14.341: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:30:16.351: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019789141s
Aug 30 07:30:16.351: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:30:18.348: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Running", Reason="", readiness=true. Elapsed: 4.016874962s
Aug 30 07:30:18.348: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Running (Ready = true)
Aug 30 07:30:18.348: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559" satisfied condition "running and ready"
Aug 30 07:30:18.896: INFO: Successfully updated pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:20.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1874" for this suite. 08/30/23 07:30:20.952
------------------------------
• [SLOW TEST] [6.754 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:14.229
    Aug 30 07:30:14.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:30:14.23
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:14.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:14.296
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/30/23 07:30:14.302
    W0830 07:30:14.331214      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:30:14.331: INFO: Waiting up to 5m0s for pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559" in namespace "downward-api-1874" to be "running and ready"
    Aug 30 07:30:14.341: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Pending", Reason="", readiness=false. Elapsed: 9.554403ms
    Aug 30 07:30:14.341: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:30:16.351: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019789141s
    Aug 30 07:30:16.351: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:30:18.348: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559": Phase="Running", Reason="", readiness=true. Elapsed: 4.016874962s
    Aug 30 07:30:18.348: INFO: The phase of Pod labelsupdate540a78e5-17be-47fb-b377-fe827748b559 is Running (Ready = true)
    Aug 30 07:30:18.348: INFO: Pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559" satisfied condition "running and ready"
    Aug 30 07:30:18.896: INFO: Successfully updated pod "labelsupdate540a78e5-17be-47fb-b377-fe827748b559"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:20.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1874" for this suite. 08/30/23 07:30:20.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:20.985
Aug 30 07:30:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:30:20.986
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:21.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:21.133
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-ebfa9507-b3f7-4c66-a85c-00a889e61009 08/30/23 07:30:21.138
STEP: Creating a pod to test consume secrets 08/30/23 07:30:21.149
Aug 30 07:30:21.188: INFO: Waiting up to 5m0s for pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424" in namespace "secrets-1445" to be "Succeeded or Failed"
Aug 30 07:30:21.202: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 14.101031ms
Aug 30 07:30:23.211: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023675454s
Aug 30 07:30:25.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022088863s
Aug 30 07:30:27.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022004254s
STEP: Saw pod success 08/30/23 07:30:27.21
Aug 30 07:30:27.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424" satisfied condition "Succeeded or Failed"
Aug 30 07:30:27.217: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:30:27.319
Aug 30 07:30:27.350: INFO: Waiting for pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 to disappear
Aug 30 07:30:27.357: INFO: Pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:27.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1445" for this suite. 08/30/23 07:30:27.371
------------------------------
• [SLOW TEST] [6.405 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:20.985
    Aug 30 07:30:20.985: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:30:20.986
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:21.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:21.133
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-ebfa9507-b3f7-4c66-a85c-00a889e61009 08/30/23 07:30:21.138
    STEP: Creating a pod to test consume secrets 08/30/23 07:30:21.149
    Aug 30 07:30:21.188: INFO: Waiting up to 5m0s for pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424" in namespace "secrets-1445" to be "Succeeded or Failed"
    Aug 30 07:30:21.202: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 14.101031ms
    Aug 30 07:30:23.211: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023675454s
    Aug 30 07:30:25.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022088863s
    Aug 30 07:30:27.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022004254s
    STEP: Saw pod success 08/30/23 07:30:27.21
    Aug 30 07:30:27.210: INFO: Pod "pod-secrets-1f708554-21da-4c55-be7c-67a73a677424" satisfied condition "Succeeded or Failed"
    Aug 30 07:30:27.217: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:30:27.319
    Aug 30 07:30:27.350: INFO: Waiting for pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 to disappear
    Aug 30 07:30:27.357: INFO: Pod pod-secrets-1f708554-21da-4c55-be7c-67a73a677424 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:27.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1445" for this suite. 08/30/23 07:30:27.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:27.391
Aug 30 07:30:27.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 07:30:27.393
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:27.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:27.512
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
W0830 07:30:28.530787      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:30:28.538: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 30 07:30:33.547: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 07:30:33.547
Aug 30 07:30:33.547: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/30/23 07:30:33.608
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 07:30:33.637: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3249  c7825edc-83f9-45b9-b692-f7f6ee01ad90 113172 1 2023-08-30 07:30:33 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 07:30:33.645: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:33.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3249" for this suite. 08/30/23 07:30:33.686
------------------------------
• [SLOW TEST] [6.318 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:27.391
    Aug 30 07:30:27.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 07:30:27.393
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:27.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:27.512
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    W0830 07:30:28.530787      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:30:28.538: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 30 07:30:33.547: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 07:30:33.547
    Aug 30 07:30:33.547: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/30/23 07:30:33.608
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 07:30:33.637: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3249  c7825edc-83f9-45b9-b692-f7f6ee01ad90 113172 1 2023-08-30 07:30:33 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032ab668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 30 07:30:33.645: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:33.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3249" for this suite. 08/30/23 07:30:33.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:33.71
Aug 30 07:30:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:30:33.71
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:33.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:33.865
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-2239b3cd-4292-4e8a-83e4-c2b995191b98 08/30/23 07:30:33.872
STEP: Creating a pod to test consume configMaps 08/30/23 07:30:33.894
W0830 07:30:33.914156      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:30:33.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238" in namespace "configmap-8802" to be "Succeeded or Failed"
Aug 30 07:30:33.928: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684694ms
Aug 30 07:30:35.939: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024346901s
Aug 30 07:30:37.938: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023661765s
STEP: Saw pod success 08/30/23 07:30:37.939
Aug 30 07:30:37.939: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238" satisfied condition "Succeeded or Failed"
Aug 30 07:30:37.946: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:30:37.963
Aug 30 07:30:37.986: INFO: Waiting for pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 to disappear
Aug 30 07:30:37.992: INFO: Pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:37.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8802" for this suite. 08/30/23 07:30:38.001
------------------------------
• [4.310 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:33.71
    Aug 30 07:30:33.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:30:33.71
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:33.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:33.865
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-2239b3cd-4292-4e8a-83e4-c2b995191b98 08/30/23 07:30:33.872
    STEP: Creating a pod to test consume configMaps 08/30/23 07:30:33.894
    W0830 07:30:33.914156      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:30:33.915: INFO: Waiting up to 5m0s for pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238" in namespace "configmap-8802" to be "Succeeded or Failed"
    Aug 30 07:30:33.928: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Pending", Reason="", readiness=false. Elapsed: 13.684694ms
    Aug 30 07:30:35.939: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024346901s
    Aug 30 07:30:37.938: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023661765s
    STEP: Saw pod success 08/30/23 07:30:37.939
    Aug 30 07:30:37.939: INFO: Pod "pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238" satisfied condition "Succeeded or Failed"
    Aug 30 07:30:37.946: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:30:37.963
    Aug 30 07:30:37.986: INFO: Waiting for pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 to disappear
    Aug 30 07:30:37.992: INFO: Pod pod-configmaps-8017b59c-f025-491d-bbd7-ac0c81acd238 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:37.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8802" for this suite. 08/30/23 07:30:38.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:38.024
Aug 30 07:30:38.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename runtimeclass 08/30/23 07:30:38.025
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:38.065
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:38.07
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:38.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3021" for this suite. 08/30/23 07:30:38.107
------------------------------
• [0.114 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:38.024
    Aug 30 07:30:38.024: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename runtimeclass 08/30/23 07:30:38.025
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:38.065
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:38.07
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:38.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3021" for this suite. 08/30/23 07:30:38.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:38.141
Aug 30 07:30:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:30:38.141
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:38.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:38.293
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/30/23 07:30:38.312
Aug 30 07:30:38.331: INFO: Waiting up to 5m0s for pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22" in namespace "emptydir-8227" to be "Succeeded or Failed"
Aug 30 07:30:38.375: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 44.090921ms
Aug 30 07:30:40.384: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053012219s
Aug 30 07:30:42.386: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054982927s
Aug 30 07:30:44.383: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051710881s
STEP: Saw pod success 08/30/23 07:30:44.383
Aug 30 07:30:44.383: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22" satisfied condition "Succeeded or Failed"
Aug 30 07:30:44.390: INFO: Trying to get logs from node 10.135.139.190 pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 container test-container: <nil>
STEP: delete the pod 08/30/23 07:30:44.408
Aug 30 07:30:44.429: INFO: Waiting for pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 to disappear
Aug 30 07:30:44.438: INFO: Pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:44.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8227" for this suite. 08/30/23 07:30:44.447
------------------------------
• [SLOW TEST] [6.327 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:38.141
    Aug 30 07:30:38.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:30:38.141
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:38.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:38.293
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/30/23 07:30:38.312
    Aug 30 07:30:38.331: INFO: Waiting up to 5m0s for pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22" in namespace "emptydir-8227" to be "Succeeded or Failed"
    Aug 30 07:30:38.375: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 44.090921ms
    Aug 30 07:30:40.384: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053012219s
    Aug 30 07:30:42.386: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054982927s
    Aug 30 07:30:44.383: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051710881s
    STEP: Saw pod success 08/30/23 07:30:44.383
    Aug 30 07:30:44.383: INFO: Pod "pod-9655704f-04ff-4db5-915b-4c9e8ec02a22" satisfied condition "Succeeded or Failed"
    Aug 30 07:30:44.390: INFO: Trying to get logs from node 10.135.139.190 pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 container test-container: <nil>
    STEP: delete the pod 08/30/23 07:30:44.408
    Aug 30 07:30:44.429: INFO: Waiting for pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 to disappear
    Aug 30 07:30:44.438: INFO: Pod pod-9655704f-04ff-4db5-915b-4c9e8ec02a22 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:44.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8227" for this suite. 08/30/23 07:30:44.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:44.468
Aug 30 07:30:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 07:30:44.469
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:44.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:44.545
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 30 07:30:44.552: INFO: Creating deployment "test-recreate-deployment"
W0830 07:30:44.567647      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:30:44.567: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 30 07:30:44.589: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 30 07:30:46.624: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 30 07:30:46.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:30:48.656: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 30 07:30:48.682: INFO: Updating deployment test-recreate-deployment
Aug 30 07:30:48.682: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 07:30:48.987: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9913  21c2372c-76b0-4aa9-88b7-a0421b504cb2 113547 2 2023-08-30 07:30:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009fc1f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-30 07:30:48 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-30 07:30:48 +0000 UTC,LastTransitionTime:2023-08-30 07:30:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 30 07:30:48.995: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9913  4bab382e-8300-4c9c-b8a5-a25874e4a63a 113544 1 2023-08-30 07:30:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 21c2372c-76b0-4aa9-88b7-a0421b504cb2 0xc004ed0410 0xc004ed0411}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21c2372c-76b0-4aa9-88b7-a0421b504cb2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ed04a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:30:48.996: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 30 07:30:48.996: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9913  7ad86568-96a4-41d6-8051-a463a613896e 113533 2 2023-08-30 07:30:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 21c2372c-76b0-4aa9-88b7-a0421b504cb2 0xc004ed02f7 0xc004ed02f8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21c2372c-76b0-4aa9-88b7-a0421b504cb2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ed03a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:30:49.003: INFO: Pod "test-recreate-deployment-cff6dc657-52xh6" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-52xh6 test-recreate-deployment-cff6dc657- deployment-9913  3aa4e0b1-53c8-4c3e-a93f-abd5f14dfa88 113546 0 2023-08-30 07:30:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 4bab382e-8300-4c9c-b8a5-a25874e4a63a 0xc004e5a8a7 0xc004e5a8a8}] [] [{kube-controller-manager Update v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bab382e-8300-4c9c-b8a5-a25874e4a63a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45n7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45n7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nltv8,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 07:30:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:49.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9913" for this suite. 08/30/23 07:30:49.036
------------------------------
• [4.585 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:44.468
    Aug 30 07:30:44.468: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 07:30:44.469
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:44.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:44.545
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 30 07:30:44.552: INFO: Creating deployment "test-recreate-deployment"
    W0830 07:30:44.567647      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:30:44.567: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 30 07:30:44.589: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 30 07:30:46.624: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 30 07:30:46.645: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-795566c5cb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:30:48.656: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 30 07:30:48.682: INFO: Updating deployment test-recreate-deployment
    Aug 30 07:30:48.682: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 07:30:48.987: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9913  21c2372c-76b0-4aa9-88b7-a0421b504cb2 113547 2 2023-08-30 07:30:44 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009fc1f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-30 07:30:48 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-30 07:30:48 +0000 UTC,LastTransitionTime:2023-08-30 07:30:44 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 30 07:30:48.995: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9913  4bab382e-8300-4c9c-b8a5-a25874e4a63a 113544 1 2023-08-30 07:30:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 21c2372c-76b0-4aa9-88b7-a0421b504cb2 0xc004ed0410 0xc004ed0411}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21c2372c-76b0-4aa9-88b7-a0421b504cb2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ed04a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:30:48.996: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 30 07:30:48.996: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9913  7ad86568-96a4-41d6-8051-a463a613896e 113533 2 2023-08-30 07:30:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 21c2372c-76b0-4aa9-88b7-a0421b504cb2 0xc004ed02f7 0xc004ed02f8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"21c2372c-76b0-4aa9-88b7-a0421b504cb2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ed03a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:30:49.003: INFO: Pod "test-recreate-deployment-cff6dc657-52xh6" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-52xh6 test-recreate-deployment-cff6dc657- deployment-9913  3aa4e0b1-53c8-4c3e-a93f-abd5f14dfa88 113546 0 2023-08-30 07:30:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 4bab382e-8300-4c9c-b8a5-a25874e4a63a 0xc004e5a8a7 0xc004e5a8a8}] [] [{kube-controller-manager Update v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4bab382e-8300-4c9c-b8a5-a25874e4a63a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 07:30:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45n7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45n7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-nltv8,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:,StartTime:2023-08-30 07:30:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:49.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9913" for this suite. 08/30/23 07:30:49.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:49.056
Aug 30 07:30:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 07:30:49.058
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:49.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:49.112
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/30/23 07:30:49.145
Aug 30 07:30:49.145: INFO: Creating simple deployment test-deployment-2n4bw
Aug 30 07:30:49.179: INFO: deployment "test-deployment-2n4bw" doesn't have the required revision set
STEP: Getting /status 08/30/23 07:30:51.214
Aug 30 07:30:51.224: INFO: Deployment test-deployment-2n4bw has Conditions: [{Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/30/23 07:30:51.224
Aug 30 07:30:51.246: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2n4bw-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/30/23 07:30:51.246
Aug 30 07:30:51.249: INFO: Observed &Deployment event: ADDED
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2n4bw-54bc444df" is progressing.}
Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
Aug 30 07:30:51.251: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
Aug 30 07:30:51.251: INFO: Found Deployment test-deployment-2n4bw in namespace deployment-5725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 07:30:51.251: INFO: Deployment test-deployment-2n4bw has an updated status
STEP: patching the Statefulset Status 08/30/23 07:30:51.251
Aug 30 07:30:51.251: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 07:30:51.264: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/30/23 07:30:51.264
Aug 30 07:30:51.273: INFO: Observed &Deployment event: ADDED
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2n4bw-54bc444df" is progressing.}
Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
Aug 30 07:30:51.273: INFO: Found deployment test-deployment-2n4bw in namespace deployment-5725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 30 07:30:51.273: INFO: Deployment test-deployment-2n4bw has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 07:30:51.285: INFO: Deployment "test-deployment-2n4bw":
&Deployment{ObjectMeta:{test-deployment-2n4bw  deployment-5725  a77e3b6c-9140-466e-9d74-0961f5772d8c 113650 1 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-30 07:30:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-30 07:30:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051709b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-2n4bw-54bc444df",LastUpdateTime:2023-08-30 07:30:51 +0000 UTC,LastTransitionTime:2023-08-30 07:30:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 07:30:51.292: INFO: New ReplicaSet "test-deployment-2n4bw-54bc444df" of Deployment "test-deployment-2n4bw":
&ReplicaSet{ObjectMeta:{test-deployment-2n4bw-54bc444df  deployment-5725  18a71585-70e2-4e4a-98ce-160ec589adba 113623 1 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2n4bw a77e3b6c-9140-466e-9d74-0961f5772d8c 0xc004d66ef0 0xc004d66ef1}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a77e3b6c-9140-466e-9d74-0961f5772d8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d66f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:30:51.299: INFO: Pod "test-deployment-2n4bw-54bc444df-gl8vq" is available:
&Pod{ObjectMeta:{test-deployment-2n4bw-54bc444df-gl8vq test-deployment-2n4bw-54bc444df- deployment-5725  e61502a8-c0f0-48ae-802d-d8a9195e96c8 113622 0 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:2524577901c2ddfddafcfb8748a71be32993a4574e7bb0f9c5886f2cbdce5e12 cni.projectcalico.org/podIP:172.30.58.97/32 cni.projectcalico.org/podIPs:172.30.58.97/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.97"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-2n4bw-54bc444df 18a71585-70e2-4e4a-98ce-160ec589adba 0xc004d67337 0xc004d67338}] [] [{kube-controller-manager Update v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18a71585-70e2-4e4a-98ce-160ec589adba\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krqjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krqjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.97,StartTime:2023-08-30 07:30:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:30:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5ea93258aac2dbf6cdf1eb66e1db5758c7296ee8af6ca6a3478d65e9174cd86b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 07:30:51.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5725" for this suite. 08/30/23 07:30:51.307
------------------------------
• [2.284 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:49.056
    Aug 30 07:30:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 07:30:49.058
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:49.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:49.112
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/30/23 07:30:49.145
    Aug 30 07:30:49.145: INFO: Creating simple deployment test-deployment-2n4bw
    Aug 30 07:30:49.179: INFO: deployment "test-deployment-2n4bw" doesn't have the required revision set
    STEP: Getting /status 08/30/23 07:30:51.214
    Aug 30 07:30:51.224: INFO: Deployment test-deployment-2n4bw has Conditions: [{Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/30/23 07:30:51.224
    Aug 30 07:30:51.246: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 30, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 30, 49, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2n4bw-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/30/23 07:30:51.246
    Aug 30 07:30:51.249: INFO: Observed &Deployment event: ADDED
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
    Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2n4bw-54bc444df" is progressing.}
    Aug 30 07:30:51.250: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.250: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
    Aug 30 07:30:51.251: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 30 07:30:51.251: INFO: Observed Deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
    Aug 30 07:30:51.251: INFO: Found Deployment test-deployment-2n4bw in namespace deployment-5725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 30 07:30:51.251: INFO: Deployment test-deployment-2n4bw has an updated status
    STEP: patching the Statefulset Status 08/30/23 07:30:51.251
    Aug 30 07:30:51.251: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 30 07:30:51.264: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/30/23 07:30:51.264
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: ADDED
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2n4bw-54bc444df"}
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:49 +0000 UTC 2023-08-30 07:30:49 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2n4bw-54bc444df" is progressing.}
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:50 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-30 07:30:50 +0000 UTC 2023-08-30 07:30:49 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2n4bw-54bc444df" has successfully progressed.}
    Aug 30 07:30:51.273: INFO: Observed deployment test-deployment-2n4bw in namespace deployment-5725 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 30 07:30:51.273: INFO: Observed &Deployment event: MODIFIED
    Aug 30 07:30:51.273: INFO: Found deployment test-deployment-2n4bw in namespace deployment-5725 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 30 07:30:51.273: INFO: Deployment test-deployment-2n4bw has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 07:30:51.285: INFO: Deployment "test-deployment-2n4bw":
    &Deployment{ObjectMeta:{test-deployment-2n4bw  deployment-5725  a77e3b6c-9140-466e-9d74-0961f5772d8c 113650 1 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-30 07:30:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-30 07:30:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051709b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-2n4bw-54bc444df",LastUpdateTime:2023-08-30 07:30:51 +0000 UTC,LastTransitionTime:2023-08-30 07:30:51 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 30 07:30:51.292: INFO: New ReplicaSet "test-deployment-2n4bw-54bc444df" of Deployment "test-deployment-2n4bw":
    &ReplicaSet{ObjectMeta:{test-deployment-2n4bw-54bc444df  deployment-5725  18a71585-70e2-4e4a-98ce-160ec589adba 113623 1 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2n4bw a77e3b6c-9140-466e-9d74-0961f5772d8c 0xc004d66ef0 0xc004d66ef1}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a77e3b6c-9140-466e-9d74-0961f5772d8c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d66f98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:30:51.299: INFO: Pod "test-deployment-2n4bw-54bc444df-gl8vq" is available:
    &Pod{ObjectMeta:{test-deployment-2n4bw-54bc444df-gl8vq test-deployment-2n4bw-54bc444df- deployment-5725  e61502a8-c0f0-48ae-802d-d8a9195e96c8 113622 0 2023-08-30 07:30:49 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:2524577901c2ddfddafcfb8748a71be32993a4574e7bb0f9c5886f2cbdce5e12 cni.projectcalico.org/podIP:172.30.58.97/32 cni.projectcalico.org/podIPs:172.30.58.97/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.97"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-2n4bw-54bc444df 18a71585-70e2-4e4a-98ce-160ec589adba 0xc004d67337 0xc004d67338}] [] [{kube-controller-manager Update v1 2023-08-30 07:30:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"18a71585-70e2-4e4a-98ce-160ec589adba\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.97\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-08-30 07:30:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krqjj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krqjj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c30,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:30:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.97,StartTime:2023-08-30 07:30:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:30:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5ea93258aac2dbf6cdf1eb66e1db5758c7296ee8af6ca6a3478d65e9174cd86b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:30:51.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5725" for this suite. 08/30/23 07:30:51.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:30:51.346
Aug 30 07:30:51.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:30:51.347
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:51.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:51.409
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/30/23 07:30:51.414
STEP: Counting existing ResourceQuota 08/30/23 07:30:57.428
STEP: Creating a ResourceQuota 08/30/23 07:31:02.439
STEP: Ensuring resource quota status is calculated 08/30/23 07:31:02.451
STEP: Creating a Secret 08/30/23 07:31:04.479
STEP: Ensuring resource quota status captures secret creation 08/30/23 07:31:04.52
STEP: Deleting a secret 08/30/23 07:31:06.531
STEP: Ensuring resource quota status released usage 08/30/23 07:31:06.559
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:31:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1299" for this suite. 08/30/23 07:31:08.581
------------------------------
• [SLOW TEST] [17.256 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:30:51.346
    Aug 30 07:30:51.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:30:51.347
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:30:51.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:30:51.409
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/30/23 07:30:51.414
    STEP: Counting existing ResourceQuota 08/30/23 07:30:57.428
    STEP: Creating a ResourceQuota 08/30/23 07:31:02.439
    STEP: Ensuring resource quota status is calculated 08/30/23 07:31:02.451
    STEP: Creating a Secret 08/30/23 07:31:04.479
    STEP: Ensuring resource quota status captures secret creation 08/30/23 07:31:04.52
    STEP: Deleting a secret 08/30/23 07:31:06.531
    STEP: Ensuring resource quota status released usage 08/30/23 07:31:06.559
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:31:08.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1299" for this suite. 08/30/23 07:31:08.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:31:08.603
Aug 30 07:31:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 07:31:08.604
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:08.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:08.716
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 08/30/23 07:31:08.803
STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:31:08.822
Aug 30 07:31:08.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:31:08.870: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:31:09.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:31:09.913: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:31:10.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 30 07:31:10.887: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
Aug 30 07:31:11.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:31:11.888: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/30/23 07:31:11.896
Aug 30 07:31:11.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:31:11.934: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/30/23 07:31:11.934
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:31:11.984
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4314, will wait for the garbage collector to delete the pods 08/30/23 07:31:11.984
Aug 30 07:31:12.076: INFO: Deleting DaemonSet.extensions daemon-set took: 26.480282ms
Aug 30 07:31:12.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.953191ms
Aug 30 07:31:15.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:31:15.384: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 07:31:15.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"114045"},"items":null}

Aug 30 07:31:15.398: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"114045"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:31:15.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4314" for this suite. 08/30/23 07:31:15.435
------------------------------
• [SLOW TEST] [6.856 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:31:08.603
    Aug 30 07:31:08.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 07:31:08.604
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:08.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:08.716
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 08/30/23 07:31:08.803
    STEP: Check that daemon pods launch on every node of the cluster. 08/30/23 07:31:08.822
    Aug 30 07:31:08.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:31:08.870: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:31:09.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:31:09.913: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:31:10.886: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 30 07:31:10.887: INFO: Node 10.135.139.185 is running 0 daemon pod, expected 1
    Aug 30 07:31:11.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:31:11.888: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/30/23 07:31:11.896
    Aug 30 07:31:11.934: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:31:11.934: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/30/23 07:31:11.934
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:31:11.984
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4314, will wait for the garbage collector to delete the pods 08/30/23 07:31:11.984
    Aug 30 07:31:12.076: INFO: Deleting DaemonSet.extensions daemon-set took: 26.480282ms
    Aug 30 07:31:12.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.953191ms
    Aug 30 07:31:15.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:31:15.384: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 07:31:15.392: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"114045"},"items":null}

    Aug 30 07:31:15.398: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"114045"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:31:15.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4314" for this suite. 08/30/23 07:31:15.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:31:15.463
Aug 30 07:31:15.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:31:15.464
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:15.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:15.526
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/30/23 07:31:15.532
STEP: Getting a ResourceQuota 08/30/23 07:31:15.547
STEP: Listing all ResourceQuotas with LabelSelector 08/30/23 07:31:15.585
STEP: Patching the ResourceQuota 08/30/23 07:31:15.602
STEP: Deleting a Collection of ResourceQuotas 08/30/23 07:31:15.635
STEP: Verifying the deleted ResourceQuota 08/30/23 07:31:15.669
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:31:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9637" for this suite. 08/30/23 07:31:15.69
------------------------------
• [0.249 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:31:15.463
    Aug 30 07:31:15.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:31:15.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:15.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:15.526
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/30/23 07:31:15.532
    STEP: Getting a ResourceQuota 08/30/23 07:31:15.547
    STEP: Listing all ResourceQuotas with LabelSelector 08/30/23 07:31:15.585
    STEP: Patching the ResourceQuota 08/30/23 07:31:15.602
    STEP: Deleting a Collection of ResourceQuotas 08/30/23 07:31:15.635
    STEP: Verifying the deleted ResourceQuota 08/30/23 07:31:15.669
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:31:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9637" for this suite. 08/30/23 07:31:15.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:31:15.713
Aug 30 07:31:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-pred 08/30/23 07:31:15.714
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:15.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:15.775
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 30 07:31:15.780: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 30 07:31:15.802: INFO: Waiting for terminating namespaces to be deleted...
Aug 30 07:31:15.825: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.183 before test
Aug 30 07:31:15.886: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 30 07:31:15.886: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 07:31:15.886: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
Aug 30 07:31:15.886: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 07:31:15.886: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Aug 30 07:31:15.886: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 07:31:15.886: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.886: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 07:31:15.886: INFO: 	Container pause ready: true, restart count 0
Aug 30 07:31:15.886: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Aug 30 07:31:15.887: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Aug 30 07:31:15.887: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 07:31:15.887: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Aug 30 07:31:15.887: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container vpn ready: true, restart count 1
Aug 30 07:31:15.887: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container tuned ready: true, restart count 0
Aug 30 07:31:15.887: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Aug 30 07:31:15.887: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Aug 30 07:31:15.887: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 07:31:15.887: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container webhook ready: true, restart count 0
Aug 30 07:31:15.887: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container console-operator ready: true, restart count 1
Aug 30 07:31:15.887: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Aug 30 07:31:15.887: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container console ready: true, restart count 0
Aug 30 07:31:15.887: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container download-server ready: true, restart count 0
Aug 30 07:31:15.887: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container dns-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container dns ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 07:31:15.887: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 07:31:15.887: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 07:31:15.887: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container ingress-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container router ready: true, restart count 0
Aug 30 07:31:15.887: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container insights-operator ready: true, restart count 1
Aug 30 07:31:15.887: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Aug 30 07:31:15.887: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container marketplace-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 07:31:15.887: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Aug 30 07:31:15.887: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 07:31:15.887: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 07:31:15.887: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 07:31:15.887: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 07:31:15.887: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 07:31:15.887: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 07:31:15.887: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 07:31:15.887: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 07:31:15.887: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.887: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 07:31:15.887: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container check-endpoints ready: true, restart count 0
Aug 30 07:31:15.887: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.887: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 07:31:15.887: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container catalog-operator ready: true, restart count 0
Aug 30 07:31:15.888: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container olm-operator ready: true, restart count 0
Aug 30 07:31:15.888: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container package-server-manager ready: true, restart count 0
Aug 30 07:31:15.888: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 07:31:15.888: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container metrics ready: true, restart count 2
Aug 30 07:31:15.888: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container push-gateway ready: true, restart count 0
Aug 30 07:31:15.888: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container service-ca-operator ready: true, restart count 1
Aug 30 07:31:15.888: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 07:31:15.888: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 07:31:15.888: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.185 before test
Aug 30 07:31:15.931: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 07:31:15.931: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
Aug 30 07:31:15.931: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 07:31:15.931: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 07:31:15.931: INFO: 	Container pause ready: true, restart count 0
Aug 30 07:31:15.931: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 07:31:15.931: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container tuned ready: true, restart count 0
Aug 30 07:31:15.931: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 30 07:31:15.931: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.931: INFO: 	Container webhook ready: true, restart count 0
Aug 30 07:31:15.932: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container console ready: true, restart count 0
Aug 30 07:31:15.932: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container download-server ready: true, restart count 0
Aug 30 07:31:15.932: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container dns ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 07:31:15.932: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container registry ready: true, restart count 0
Aug 30 07:31:15.932: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 07:31:15.932: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 07:31:15.932: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container router ready: true, restart count 0
Aug 30 07:31:15.932: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container migrator ready: true, restart count 0
Aug 30 07:31:15.932: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 07:31:15.932: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 07:31:15.932: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 07:31:15.932: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
Aug 30 07:31:15.932: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container alertmanager ready: true, restart count 1
Aug 30 07:31:15.932: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 30 07:31:15.932: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 07:31:15.932: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Aug 30 07:31:15.932: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 30 07:31:15.932: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container config-reloader ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container prometheus ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container prometheus-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container thanos-sidecar ready: true, restart count 0
Aug 30 07:31:15.932: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 30 07:31:15.932: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Aug 30 07:31:15.932: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container reload ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container telemeter-client ready: true, restart count 0
Aug 30 07:31:15.932: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container oauth-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container prom-label-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container thanos-query ready: true, restart count 0
Aug 30 07:31:15.932: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 07:31:15.932: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container multus-admission-controller ready: true, restart count 0
Aug 30 07:31:15.932: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 07:31:15.932: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.932: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 07:31:15.932: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 07:31:15.932: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container network-operator ready: true, restart count 0
Aug 30 07:31:15.932: INFO: collect-profiles-28222980-qqqn7 from openshift-operator-lifecycle-manager started at 2023-08-30 07:00:00 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 07:31:15.932: INFO: collect-profiles-28222995-tdchr from openshift-operator-lifecycle-manager started at 2023-08-30 07:15:00 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 07:31:15.932: INFO: collect-profiles-28223010-npdcl from openshift-operator-lifecycle-manager started at 2023-08-30 07:30:00 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
Aug 30 07:31:15.932: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.933: INFO: 	Container packageserver ready: true, restart count 0
Aug 30 07:31:15.933: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.933: INFO: 	Container service-ca-controller ready: true, restart count 0
Aug 30 07:31:15.933: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.933: INFO: 	Container e2e ready: true, restart count 0
Aug 30 07:31:15.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 07:31:15.933: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 07:31:15.933: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 30 07:31:15.933: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.933: INFO: 	Container tigera-operator ready: true, restart count 6
Aug 30 07:31:15.933: INFO: 
Logging pods the apiserver thinks is on node 10.135.139.190 before test
Aug 30 07:31:15.967: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container calico-node ready: true, restart count 0
Aug 30 07:31:15.967: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container calico-typha ready: true, restart count 0
Aug 30 07:31:15.967: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container keepalived-watcher ready: true, restart count 0
Aug 30 07:31:15.967: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Aug 30 07:31:15.967: INFO: 	Container pause ready: true, restart count 0
Aug 30 07:31:15.967: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Aug 30 07:31:15.967: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container tuned ready: true, restart count 0
Aug 30 07:31:15.967: INFO: dns-default-78wgk from openshift-dns started at 2023-08-30 07:27:42 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container dns ready: true, restart count 0
Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.967: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container dns-node-resolver ready: true, restart count 0
Aug 30 07:31:15.967: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container node-ca ready: true, restart count 0
Aug 30 07:31:15.967: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container pvc-permissions ready: false, restart count 0
Aug 30 07:31:15.967: INFO: ingress-canary-5wsgc from openshift-ingress-canary started at 2023-08-30 07:27:22 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Aug 30 07:31:15.967: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.967: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.967: INFO: 	Container node-exporter ready: true, restart count 0
Aug 30 07:31:15.967: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Aug 30 07:31:15.968: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container kube-multus ready: true, restart count 0
Aug 30 07:31:15.968: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 30 07:31:15.968: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Aug 30 07:31:15.968: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container network-check-target-container ready: true, restart count 0
Aug 30 07:31:15.968: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 30 07:31:15.968: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
Aug 30 07:31:15.968: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 30 07:31:15.968: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 07:31:15.968
Aug 30 07:31:15.983: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3237" to be "running"
Aug 30 07:31:15.991: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.933859ms
Aug 30 07:31:18.000: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016512843s
Aug 30 07:31:18.000: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 07:31:18.019
STEP: Trying to apply a random label on the found node. 08/30/23 07:31:18.057
STEP: verifying the node has the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc 95 08/30/23 07:31:18.093
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/30/23 07:31:18.101
Aug 30 07:31:18.135: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3237" to be "not pending"
Aug 30 07:31:18.143: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981013ms
Aug 30 07:31:20.151: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015688202s
Aug 30 07:31:20.151: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.135.139.190 on the node which pod4 resides and expect not scheduled 08/30/23 07:31:20.151
Aug 30 07:31:20.163: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3237" to be "not pending"
Aug 30 07:31:20.171: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.433781ms
Aug 30 07:31:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015505901s
Aug 30 07:31:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017226435s
Aug 30 07:31:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015426757s
Aug 30 07:31:28.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0181275s
Aug 30 07:31:30.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016504809s
Aug 30 07:31:32.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014841403s
Aug 30 07:31:34.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01713692s
Aug 30 07:31:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015470788s
Aug 30 07:31:38.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017669844s
Aug 30 07:31:40.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016186327s
Aug 30 07:31:42.177: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014120906s
Aug 30 07:31:44.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015799977s
Aug 30 07:31:46.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015083347s
Aug 30 07:31:48.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015822384s
Aug 30 07:31:50.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01538537s
Aug 30 07:31:52.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019565439s
Aug 30 07:31:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016163048s
Aug 30 07:31:56.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014994356s
Aug 30 07:31:58.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016402769s
Aug 30 07:32:00.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01949062s
Aug 30 07:32:02.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02793065s
Aug 30 07:32:04.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015814059s
Aug 30 07:32:06.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016149817s
Aug 30 07:32:08.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019655213s
Aug 30 07:32:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016358855s
Aug 30 07:32:12.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.016289764s
Aug 30 07:32:14.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.018144901s
Aug 30 07:32:16.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.016069201s
Aug 30 07:32:18.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.039255665s
Aug 30 07:32:20.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016791088s
Aug 30 07:32:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.015518229s
Aug 30 07:32:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.0165434s
Aug 30 07:32:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015536519s
Aug 30 07:32:28.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018503083s
Aug 30 07:32:30.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01551797s
Aug 30 07:32:32.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016216155s
Aug 30 07:32:34.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.022035422s
Aug 30 07:32:36.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014606172s
Aug 30 07:32:38.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015528144s
Aug 30 07:32:40.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016238238s
Aug 30 07:32:42.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015643267s
Aug 30 07:32:44.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.050568348s
Aug 30 07:32:46.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015298526s
Aug 30 07:32:48.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017524441s
Aug 30 07:32:50.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.048339306s
Aug 30 07:32:52.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017445249s
Aug 30 07:32:54.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017206931s
Aug 30 07:32:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015783111s
Aug 30 07:32:58.209: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.046099289s
Aug 30 07:33:00.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.022111457s
Aug 30 07:33:02.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.030522109s
Aug 30 07:33:04.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017500992s
Aug 30 07:33:06.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.032333734s
Aug 30 07:33:08.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018470716s
Aug 30 07:33:10.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029680708s
Aug 30 07:33:12.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018560298s
Aug 30 07:33:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01730256s
Aug 30 07:33:16.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018600727s
Aug 30 07:33:18.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019446033s
Aug 30 07:33:20.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014785981s
Aug 30 07:33:22.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.016479333s
Aug 30 07:33:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017132077s
Aug 30 07:33:26.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.015294113s
Aug 30 07:33:28.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017212641s
Aug 30 07:33:30.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017202935s
Aug 30 07:33:32.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.016807187s
Aug 30 07:33:34.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.015606305s
Aug 30 07:33:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.015569439s
Aug 30 07:33:38.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016349325s
Aug 30 07:33:40.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01715764s
Aug 30 07:33:42.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.066577186s
Aug 30 07:33:44.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.017790707s
Aug 30 07:33:46.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.015906178s
Aug 30 07:33:48.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.015484809s
Aug 30 07:33:50.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.016859458s
Aug 30 07:33:52.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.016470613s
Aug 30 07:33:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016099984s
Aug 30 07:33:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015740827s
Aug 30 07:33:58.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.029667335s
Aug 30 07:34:00.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016330087s
Aug 30 07:34:02.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.016168211s
Aug 30 07:34:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.017340633s
Aug 30 07:34:06.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.017486607s
Aug 30 07:34:08.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017023001s
Aug 30 07:34:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015556807s
Aug 30 07:34:12.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.017594927s
Aug 30 07:34:14.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.034913858s
Aug 30 07:34:16.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018398489s
Aug 30 07:34:18.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016153986s
Aug 30 07:34:20.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.015386075s
Aug 30 07:34:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015599442s
Aug 30 07:34:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017207189s
Aug 30 07:34:26.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.021931521s
Aug 30 07:34:28.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.01767341s
Aug 30 07:34:30.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.015682323s
Aug 30 07:34:32.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.015562738s
Aug 30 07:34:34.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016601553s
Aug 30 07:34:36.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016399306s
Aug 30 07:34:38.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.017005397s
Aug 30 07:34:40.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.019025167s
Aug 30 07:34:42.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019334475s
Aug 30 07:34:44.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017195059s
Aug 30 07:34:46.190: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.02660632s
Aug 30 07:34:48.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.016865911s
Aug 30 07:34:50.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015866059s
Aug 30 07:34:52.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015699525s
Aug 30 07:34:54.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014998514s
Aug 30 07:34:56.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015282017s
Aug 30 07:34:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.058209707s
Aug 30 07:35:00.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.016243679s
Aug 30 07:35:02.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.017162463s
Aug 30 07:35:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016405487s
Aug 30 07:35:06.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016951418s
Aug 30 07:35:08.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.031985458s
Aug 30 07:35:10.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014578724s
Aug 30 07:35:12.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.032383505s
Aug 30 07:35:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016718173s
Aug 30 07:35:16.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019793164s
Aug 30 07:35:18.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01703833s
Aug 30 07:35:20.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016564403s
Aug 30 07:35:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015699415s
Aug 30 07:35:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.016548492s
Aug 30 07:35:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.016004051s
Aug 30 07:35:28.190: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.026576648s
Aug 30 07:35:30.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.01535314s
Aug 30 07:35:32.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017827721s
Aug 30 07:35:34.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.017440765s
Aug 30 07:35:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016060453s
Aug 30 07:35:38.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.018386872s
Aug 30 07:35:40.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.02074457s
Aug 30 07:35:42.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.016409747s
Aug 30 07:35:44.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015753529s
Aug 30 07:35:46.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01559438s
Aug 30 07:35:48.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016494837s
Aug 30 07:35:50.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.017222581s
Aug 30 07:35:52.218: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.0546485s
Aug 30 07:35:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016224538s
Aug 30 07:35:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015834498s
Aug 30 07:35:58.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021299258s
Aug 30 07:36:00.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.016772618s
Aug 30 07:36:02.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018894429s
Aug 30 07:36:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016991056s
Aug 30 07:36:06.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.017131403s
Aug 30 07:36:08.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.019885894s
Aug 30 07:36:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016229422s
Aug 30 07:36:12.189: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.025929272s
Aug 30 07:36:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016945s
Aug 30 07:36:16.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.019872936s
Aug 30 07:36:18.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.015498175s
Aug 30 07:36:20.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01495445s
Aug 30 07:36:20.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022578548s
STEP: removing the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc off the node 10.135.139.190 08/30/23 07:36:20.186
STEP: verifying the node doesn't have the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc 08/30/23 07:36:20.224
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:36:20.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3237" for this suite. 08/30/23 07:36:20.251
------------------------------
• [SLOW TEST] [304.557 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:31:15.713
    Aug 30 07:31:15.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-pred 08/30/23 07:31:15.714
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:31:15.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:31:15.775
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 30 07:31:15.780: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 30 07:31:15.802: INFO: Waiting for terminating namespaces to be deleted...
    Aug 30 07:31:15.825: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.183 before test
    Aug 30 07:31:15.886: INFO: calico-kube-controllers-8c94dd78-pv85v from calico-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: calico-node-rkdbq from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: calico-typha-6668d4cdd9-hl2qp from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-687wz from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-fqb6w from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: managed-storage-validation-webhooks-5445c9f55f-vxc59 from ibm-odf-validation-webhook started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 1
    Aug 30 07:31:15.886: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc from ibm-system started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: ibm-file-plugin-77c56989c6-nkjrh from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: ibm-keepalived-watcher-j9sbd from kube-system started at 2023-08-30 03:49:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: ibm-master-proxy-static-10.135.139.183 from kube-system started at 2023-08-30 03:49:22 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.886: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: 	Container pause ready: true, restart count 0
    Aug 30 07:31:15.886: INFO: ibm-storage-metrics-agent-66b6778cfb-7cpg6 from kube-system started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: ibm-storage-watcher-569f8b7c46-vxtjw from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: ibmcloud-block-storage-driver-xtxbl from kube-system started at 2023-08-30 03:49:30 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: ibmcloud-block-storage-plugin-7556db7ff5-s5xzr from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: vpn-5cf898c745-hk9nt from kube-system started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container vpn ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: cluster-node-tuning-operator-6f7b6884b9-2qg9l from openshift-cluster-node-tuning-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: tuned-j5t4h from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: cluster-samples-operator-5db6d764c6-9s952 from openshift-cluster-samples-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: cluster-storage-operator-848968879c-br4hq from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: csi-snapshot-controller-b5685b8b7-s7d7c from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: csi-snapshot-controller-operator-85b4b8fdc8-htd5f from openshift-cluster-storage-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: csi-snapshot-webhook-645cd76dd7-bhc7s from openshift-cluster-storage-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: console-operator-77698fb45f-7tq8z from openshift-console-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container console-operator ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Aug 30 07:31:15.887: INFO: console-76ccb968f7-h5h2q from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container console ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: downloads-55ff47758f-p9bfz from openshift-console started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: dns-operator-54bdb67d9f-8cjg2 from openshift-dns-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container dns-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: dns-default-5mmgg from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container dns ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: node-resolver-b2rmk from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: cluster-image-registry-operator-77f67cc94-8p5p5 from openshift-image-registry started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: node-ca-kjt5c from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: ingress-canary-jfxbv from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: ingress-operator-cb44b8bc7-2rjr2 from openshift-ingress-operator started at 2023-08-30 03:58:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container ingress-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: router-default-9c97f6b97-tkkf8 from openshift-ingress started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container router ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: insights-operator-7f9b7d96b4-9cv5s from openshift-insights started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container insights-operator ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: openshift-kube-proxy-5hwpc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: kube-storage-version-migrator-operator-854564dc54-mj7zl from openshift-kube-storage-version-migrator-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: marketplace-operator-dcc4b747b-4bcck from openshift-marketplace started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container marketplace-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 07:31:15.887: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: cluster-monitoring-operator-7bc996789-qqt52 from openshift-monitoring started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: node-exporter-9pzcl from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: prometheus-adapter-5d4fdc4794-gct89 from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-08-30 06:28:32 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: prometheus-operator-admission-webhook-748bd6896b-8b8tk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: thanos-querier-7bd6db4456-jqzhk from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: multus-additional-cni-plugins-fd7l2 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: multus-admission-controller-68f648d7b7-65hkj from openshift-multus started at 2023-08-30 06:28:27 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: multus-vllh7 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: network-metrics-daemon-lk2w7 from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: network-check-source-8dd86ffb8-k4c5v from openshift-network-diagnostics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container check-endpoints ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: network-check-target-zvrg2 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.887: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 07:31:15.887: INFO: catalog-operator-56d6f4596f-fzjbx from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container catalog-operator ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: olm-operator-79d7d96656-gs9jc from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container olm-operator ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: package-server-manager-54dcf5867b-z6ksr from openshift-operator-lifecycle-manager started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container package-server-manager ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: packageserver-56d55d9ff4-g9wz7 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: metrics-7d985d4645-ntm6t from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container metrics ready: true, restart count 2
    Aug 30 07:31:15.888: INFO: push-gateway-86f4464769-8nhh2 from openshift-roks-metrics started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container push-gateway ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: service-ca-operator-fdbd6d689-hvb8m from openshift-service-ca-operator started at 2023-08-30 03:58:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container service-ca-operator ready: true, restart count 1
    Aug 30 07:31:15.888: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.888: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 07:31:15.888: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.185 before test
    Aug 30 07:31:15.931: INFO: calico-node-2nwc8 from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-t5m5w from ibm-system started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container ibm-cloud-provider-ip-159-122-65-122 ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: ibm-keepalived-watcher-g6vmc from kube-system started at 2023-08-30 03:49:04 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: ibm-master-proxy-static-10.135.139.185 from kube-system started at 2023-08-30 03:49:03 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: 	Container pause ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: ibmcloud-block-storage-driver-llp66 from kube-system started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: tuned-zzmsd from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: csi-snapshot-controller-b5685b8b7-x2252 from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 30 07:31:15.931: INFO: csi-snapshot-webhook-645cd76dd7-dmm7c from openshift-cluster-storage-operator started at 2023-08-30 03:59:11 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.931: INFO: 	Container webhook ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: console-76ccb968f7-xrkzz from openshift-console started at 2023-08-30 04:06:21 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container console ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: downloads-55ff47758f-g7rfm from openshift-console started at 2023-08-30 03:59:22 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container download-server ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: dns-default-5rrtf from openshift-dns started at 2023-08-30 04:02:36 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container dns ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: node-resolver-nnr9x from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: image-registry-6f96b7475f-n4rwx from openshift-image-registry started at 2023-08-30 04:04:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container registry ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: node-ca-zh287 from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: ingress-canary-85cph from openshift-ingress-canary started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: router-default-9c97f6b97-jfjfq from openshift-ingress started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container router ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: openshift-kube-proxy-fsndc from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: migrator-75dd49656f-g9cr8 from openshift-kube-storage-version-migrator started at 2023-08-30 03:59:30 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container migrator ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: certified-operators-dw8qh from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: community-operators-cmrk9 from openshift-marketplace started at 2023-08-30 04:00:18 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: redhat-marketplace-9tdl9 from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: redhat-operators-d6hnk from openshift-marketplace started at 2023-08-30 04:00:19 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container registry-server ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-08-30 04:05:17 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 30 07:31:15.932: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: kube-state-metrics-5fdc98dfcd-qf85n from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: node-exporter-r4rwz from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: openshift-state-metrics-7d998cd668-snkcx from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (3 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: prometheus-adapter-5d4fdc4794-48ncq from openshift-monitoring started at 2023-08-30 04:03:32 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-08-30 04:05:18 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container prometheus ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: prometheus-operator-77dbfbbd6f-bm4pb from openshift-monitoring started at 2023-08-30 04:02:57 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: prometheus-operator-admission-webhook-748bd6896b-544b2 from openshift-monitoring started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: telemeter-client-cc9864968-9jhnm from openshift-monitoring started at 2023-08-30 06:28:27 +0000 UTC (3 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container reload ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container telemeter-client ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: thanos-querier-7bd6db4456-6vdlm from openshift-monitoring started at 2023-08-30 04:03:41 +0000 UTC (6 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container oauth-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container thanos-query ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: multus-additional-cni-plugins-mpkt8 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: multus-admission-controller-68f648d7b7-74l5c from openshift-multus started at 2023-08-30 04:02:35 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: multus-znp7c from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: network-metrics-daemon-78ggq from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: network-check-target-grps8 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: network-operator-68ffb666f9-kw748 from openshift-network-operator started at 2023-08-30 06:28:27 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container network-operator ready: true, restart count 0
    Aug 30 07:31:15.932: INFO: collect-profiles-28222980-qqqn7 from openshift-operator-lifecycle-manager started at 2023-08-30 07:00:00 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 07:31:15.932: INFO: collect-profiles-28222995-tdchr from openshift-operator-lifecycle-manager started at 2023-08-30 07:15:00 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 07:31:15.932: INFO: collect-profiles-28223010-npdcl from openshift-operator-lifecycle-manager started at 2023-08-30 07:30:00 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.932: INFO: 	Container collect-profiles ready: false, restart count 0
    Aug 30 07:31:15.932: INFO: packageserver-56d55d9ff4-489b6 from openshift-operator-lifecycle-manager started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.933: INFO: 	Container packageserver ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: service-ca-6bf49cb844-8pjfw from openshift-service-ca started at 2023-08-30 03:59:09 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.933: INFO: 	Container service-ca-controller ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: sonobuoy-e2e-job-f8d5f988a13f45ff from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.933: INFO: 	Container e2e ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-s9wd2 from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.933: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 30 07:31:15.933: INFO: tigera-operator-7f6598444c-rhbbz from tigera-operator started at 2023-08-30 03:49:16 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.933: INFO: 	Container tigera-operator ready: true, restart count 6
    Aug 30 07:31:15.933: INFO: 
    Logging pods the apiserver thinks is on node 10.135.139.190 before test
    Aug 30 07:31:15.967: INFO: calico-node-95g5x from calico-system started at 2023-08-30 03:56:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container calico-node ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: calico-typha-6668d4cdd9-n2znw from calico-system started at 2023-08-30 03:56:44 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: ibm-keepalived-watcher-bvskn from kube-system started at 2023-08-30 03:49:18 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: ibm-master-proxy-static-10.135.139.190 from kube-system started at 2023-08-30 03:49:17 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: 	Container pause ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: ibmcloud-block-storage-driver-92fnq from kube-system started at 2023-08-30 03:49:25 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: tuned-xkwnz from openshift-cluster-node-tuning-operator started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container tuned ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: dns-default-78wgk from openshift-dns started at 2023-08-30 07:27:42 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container dns ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: node-resolver-vhd6d from openshift-dns started at 2023-08-30 04:02:35 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: node-ca-dq2gx from openshift-image-registry started at 2023-08-30 04:02:36 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container node-ca ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: registry-pvc-permissions-lpxfg from openshift-image-registry started at 2023-08-30 04:04:42 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container pvc-permissions ready: false, restart count 0
    Aug 30 07:31:15.967: INFO: ingress-canary-5wsgc from openshift-ingress-canary started at 2023-08-30 07:27:22 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: openshift-kube-proxy-p9frm from openshift-kube-proxy started at 2023-08-30 03:54:25 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: node-exporter-bwpv7 from openshift-monitoring started at 2023-08-30 04:03:29 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.967: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 30 07:31:15.967: INFO: multus-additional-cni-plugins-d5v92 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: multus-pt4x5 from openshift-multus started at 2023-08-30 03:54:20 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container kube-multus ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: network-metrics-daemon-d65fw from openshift-multus started at 2023-08-30 03:54:21 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: network-check-target-rsbp5 from openshift-network-diagnostics started at 2023-08-30 03:54:28 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container network-check-target-container ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: sonobuoy from sonobuoy started at 2023-08-30 06:16:43 +0000 UTC (1 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-vjshz from sonobuoy started at 2023-08-30 06:16:50 +0000 UTC (2 container statuses recorded)
    Aug 30 07:31:15.968: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 30 07:31:15.968: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/30/23 07:31:15.968
    Aug 30 07:31:15.983: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3237" to be "running"
    Aug 30 07:31:15.991: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 7.933859ms
    Aug 30 07:31:18.000: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016512843s
    Aug 30 07:31:18.000: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/30/23 07:31:18.019
    STEP: Trying to apply a random label on the found node. 08/30/23 07:31:18.057
    STEP: verifying the node has the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc 95 08/30/23 07:31:18.093
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/30/23 07:31:18.101
    Aug 30 07:31:18.135: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3237" to be "not pending"
    Aug 30 07:31:18.143: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981013ms
    Aug 30 07:31:20.151: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015688202s
    Aug 30 07:31:20.151: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.135.139.190 on the node which pod4 resides and expect not scheduled 08/30/23 07:31:20.151
    Aug 30 07:31:20.163: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3237" to be "not pending"
    Aug 30 07:31:20.171: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.433781ms
    Aug 30 07:31:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015505901s
    Aug 30 07:31:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017226435s
    Aug 30 07:31:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015426757s
    Aug 30 07:31:28.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0181275s
    Aug 30 07:31:30.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016504809s
    Aug 30 07:31:32.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014841403s
    Aug 30 07:31:34.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01713692s
    Aug 30 07:31:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015470788s
    Aug 30 07:31:38.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017669844s
    Aug 30 07:31:40.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016186327s
    Aug 30 07:31:42.177: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.014120906s
    Aug 30 07:31:44.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.015799977s
    Aug 30 07:31:46.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.015083347s
    Aug 30 07:31:48.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.015822384s
    Aug 30 07:31:50.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.01538537s
    Aug 30 07:31:52.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019565439s
    Aug 30 07:31:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016163048s
    Aug 30 07:31:56.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.014994356s
    Aug 30 07:31:58.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016402769s
    Aug 30 07:32:00.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.01949062s
    Aug 30 07:32:02.191: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02793065s
    Aug 30 07:32:04.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.015814059s
    Aug 30 07:32:06.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.016149817s
    Aug 30 07:32:08.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.019655213s
    Aug 30 07:32:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016358855s
    Aug 30 07:32:12.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.016289764s
    Aug 30 07:32:14.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.018144901s
    Aug 30 07:32:16.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.016069201s
    Aug 30 07:32:18.202: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.039255665s
    Aug 30 07:32:20.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.016791088s
    Aug 30 07:32:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.015518229s
    Aug 30 07:32:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.0165434s
    Aug 30 07:32:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015536519s
    Aug 30 07:32:28.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018503083s
    Aug 30 07:32:30.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.01551797s
    Aug 30 07:32:32.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.016216155s
    Aug 30 07:32:34.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.022035422s
    Aug 30 07:32:36.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.014606172s
    Aug 30 07:32:38.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015528144s
    Aug 30 07:32:40.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.016238238s
    Aug 30 07:32:42.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015643267s
    Aug 30 07:32:44.214: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.050568348s
    Aug 30 07:32:46.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015298526s
    Aug 30 07:32:48.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017524441s
    Aug 30 07:32:50.211: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.048339306s
    Aug 30 07:32:52.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.017445249s
    Aug 30 07:32:54.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.017206931s
    Aug 30 07:32:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015783111s
    Aug 30 07:32:58.209: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.046099289s
    Aug 30 07:33:00.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.022111457s
    Aug 30 07:33:02.194: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.030522109s
    Aug 30 07:33:04.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.017500992s
    Aug 30 07:33:06.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.032333734s
    Aug 30 07:33:08.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018470716s
    Aug 30 07:33:10.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.029680708s
    Aug 30 07:33:12.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018560298s
    Aug 30 07:33:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01730256s
    Aug 30 07:33:16.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018600727s
    Aug 30 07:33:18.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019446033s
    Aug 30 07:33:20.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014785981s
    Aug 30 07:33:22.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.016479333s
    Aug 30 07:33:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017132077s
    Aug 30 07:33:26.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.015294113s
    Aug 30 07:33:28.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017212641s
    Aug 30 07:33:30.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017202935s
    Aug 30 07:33:32.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.016807187s
    Aug 30 07:33:34.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.015606305s
    Aug 30 07:33:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.015569439s
    Aug 30 07:33:38.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.016349325s
    Aug 30 07:33:40.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01715764s
    Aug 30 07:33:42.230: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.066577186s
    Aug 30 07:33:44.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.017790707s
    Aug 30 07:33:46.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.015906178s
    Aug 30 07:33:48.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.015484809s
    Aug 30 07:33:50.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.016859458s
    Aug 30 07:33:52.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.016470613s
    Aug 30 07:33:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016099984s
    Aug 30 07:33:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.015740827s
    Aug 30 07:33:58.193: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.029667335s
    Aug 30 07:34:00.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.016330087s
    Aug 30 07:34:02.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.016168211s
    Aug 30 07:34:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.017340633s
    Aug 30 07:34:06.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.017486607s
    Aug 30 07:34:08.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.017023001s
    Aug 30 07:34:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.015556807s
    Aug 30 07:34:12.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.017594927s
    Aug 30 07:34:14.198: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.034913858s
    Aug 30 07:34:16.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.018398489s
    Aug 30 07:34:18.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.016153986s
    Aug 30 07:34:20.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.015386075s
    Aug 30 07:34:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.015599442s
    Aug 30 07:34:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017207189s
    Aug 30 07:34:26.185: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.021931521s
    Aug 30 07:34:28.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.01767341s
    Aug 30 07:34:30.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.015682323s
    Aug 30 07:34:32.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.015562738s
    Aug 30 07:34:34.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.016601553s
    Aug 30 07:34:36.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.016399306s
    Aug 30 07:34:38.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.017005397s
    Aug 30 07:34:40.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.019025167s
    Aug 30 07:34:42.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019334475s
    Aug 30 07:34:44.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017195059s
    Aug 30 07:34:46.190: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.02660632s
    Aug 30 07:34:48.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.016865911s
    Aug 30 07:34:50.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.015866059s
    Aug 30 07:34:52.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.015699525s
    Aug 30 07:34:54.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.014998514s
    Aug 30 07:34:56.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.015282017s
    Aug 30 07:34:58.221: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.058209707s
    Aug 30 07:35:00.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.016243679s
    Aug 30 07:35:02.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.017162463s
    Aug 30 07:35:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016405487s
    Aug 30 07:35:06.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.016951418s
    Aug 30 07:35:08.195: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.031985458s
    Aug 30 07:35:10.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.014578724s
    Aug 30 07:35:12.196: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.032383505s
    Aug 30 07:35:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016718173s
    Aug 30 07:35:16.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.019793164s
    Aug 30 07:35:18.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.01703833s
    Aug 30 07:35:20.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016564403s
    Aug 30 07:35:22.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.015699415s
    Aug 30 07:35:24.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.016548492s
    Aug 30 07:35:26.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.016004051s
    Aug 30 07:35:28.190: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.026576648s
    Aug 30 07:35:30.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.01535314s
    Aug 30 07:35:32.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.017827721s
    Aug 30 07:35:34.181: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.017440765s
    Aug 30 07:35:36.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016060453s
    Aug 30 07:35:38.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.018386872s
    Aug 30 07:35:40.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.02074457s
    Aug 30 07:35:42.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.016409747s
    Aug 30 07:35:44.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.015753529s
    Aug 30 07:35:46.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.01559438s
    Aug 30 07:35:48.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.016494837s
    Aug 30 07:35:50.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.017222581s
    Aug 30 07:35:52.218: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.0546485s
    Aug 30 07:35:54.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.016224538s
    Aug 30 07:35:56.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015834498s
    Aug 30 07:35:58.184: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.021299258s
    Aug 30 07:36:00.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.016772618s
    Aug 30 07:36:02.182: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.018894429s
    Aug 30 07:36:04.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016991056s
    Aug 30 07:36:06.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.017131403s
    Aug 30 07:36:08.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.019885894s
    Aug 30 07:36:10.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.016229422s
    Aug 30 07:36:12.189: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.025929272s
    Aug 30 07:36:14.180: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.016945s
    Aug 30 07:36:16.183: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.019872936s
    Aug 30 07:36:18.179: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.015498175s
    Aug 30 07:36:20.178: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.01495445s
    Aug 30 07:36:20.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.022578548s
    STEP: removing the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc off the node 10.135.139.190 08/30/23 07:36:20.186
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-92b8d3d8-d0c4-4e3c-9709-b8830548d1bc 08/30/23 07:36:20.224
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:36:20.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3237" for this suite. 08/30/23 07:36:20.251
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:36:20.272
Aug 30 07:36:20.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:36:20.273
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:20.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:20.355
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/30/23 07:36:20.36
Aug 30 07:36:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 create -f -'
Aug 30 07:36:20.928: INFO: stderr: ""
Aug 30 07:36:20.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 07:36:20.929
Aug 30 07:36:20.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 07:36:21.026: INFO: stderr: ""
Aug 30 07:36:21.026: INFO: stdout: "update-demo-nautilus-qpllh "
STEP: Replicas for name=update-demo: expected=2 actual=1 08/30/23 07:36:21.026
Aug 30 07:36:26.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 07:36:26.149: INFO: stderr: ""
Aug 30 07:36:26.149: INFO: stdout: "update-demo-nautilus-9hwz5 update-demo-nautilus-qpllh "
Aug 30 07:36:26.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 07:36:26.375: INFO: stderr: ""
Aug 30 07:36:26.375: INFO: stdout: ""
Aug 30 07:36:26.375: INFO: update-demo-nautilus-9hwz5 is created but not running
Aug 30 07:36:31.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 30 07:36:31.573: INFO: stderr: ""
Aug 30 07:36:31.573: INFO: stdout: "update-demo-nautilus-9hwz5 update-demo-nautilus-qpllh "
Aug 30 07:36:31.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 07:36:31.761: INFO: stderr: ""
Aug 30 07:36:31.761: INFO: stdout: "true"
Aug 30 07:36:31.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 07:36:31.874: INFO: stderr: ""
Aug 30 07:36:31.874: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 07:36:31.874: INFO: validating pod update-demo-nautilus-9hwz5
Aug 30 07:36:31.891: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 07:36:31.891: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 07:36:31.891: INFO: update-demo-nautilus-9hwz5 is verified up and running
Aug 30 07:36:31.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-qpllh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 30 07:36:31.989: INFO: stderr: ""
Aug 30 07:36:31.989: INFO: stdout: "true"
Aug 30 07:36:31.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-qpllh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 30 07:36:32.115: INFO: stderr: ""
Aug 30 07:36:32.115: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 30 07:36:32.115: INFO: validating pod update-demo-nautilus-qpllh
Aug 30 07:36:32.136: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 30 07:36:32.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 30 07:36:32.136: INFO: update-demo-nautilus-qpllh is verified up and running
STEP: using delete to clean up resources 08/30/23 07:36:32.136
Aug 30 07:36:32.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 delete --grace-period=0 --force -f -'
Aug 30 07:36:32.272: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:36:32.272: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 30 07:36:32.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get rc,svc -l name=update-demo --no-headers'
Aug 30 07:36:32.392: INFO: stderr: "No resources found in kubectl-2234 namespace.\n"
Aug 30 07:36:32.392: INFO: stdout: ""
Aug 30 07:36:32.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 07:36:32.494: INFO: stderr: ""
Aug 30 07:36:32.495: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:36:32.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2234" for this suite. 08/30/23 07:36:32.505
------------------------------
• [SLOW TEST] [12.253 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:36:20.272
    Aug 30 07:36:20.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:36:20.273
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:20.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:20.355
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/30/23 07:36:20.36
    Aug 30 07:36:20.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 create -f -'
    Aug 30 07:36:20.928: INFO: stderr: ""
    Aug 30 07:36:20.929: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/30/23 07:36:20.929
    Aug 30 07:36:20.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 07:36:21.026: INFO: stderr: ""
    Aug 30 07:36:21.026: INFO: stdout: "update-demo-nautilus-qpllh "
    STEP: Replicas for name=update-demo: expected=2 actual=1 08/30/23 07:36:21.026
    Aug 30 07:36:26.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 07:36:26.149: INFO: stderr: ""
    Aug 30 07:36:26.149: INFO: stdout: "update-demo-nautilus-9hwz5 update-demo-nautilus-qpllh "
    Aug 30 07:36:26.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 07:36:26.375: INFO: stderr: ""
    Aug 30 07:36:26.375: INFO: stdout: ""
    Aug 30 07:36:26.375: INFO: update-demo-nautilus-9hwz5 is created but not running
    Aug 30 07:36:31.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 30 07:36:31.573: INFO: stderr: ""
    Aug 30 07:36:31.573: INFO: stdout: "update-demo-nautilus-9hwz5 update-demo-nautilus-qpllh "
    Aug 30 07:36:31.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 07:36:31.761: INFO: stderr: ""
    Aug 30 07:36:31.761: INFO: stdout: "true"
    Aug 30 07:36:31.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-9hwz5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 07:36:31.874: INFO: stderr: ""
    Aug 30 07:36:31.874: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 07:36:31.874: INFO: validating pod update-demo-nautilus-9hwz5
    Aug 30 07:36:31.891: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 07:36:31.891: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 07:36:31.891: INFO: update-demo-nautilus-9hwz5 is verified up and running
    Aug 30 07:36:31.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-qpllh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 30 07:36:31.989: INFO: stderr: ""
    Aug 30 07:36:31.989: INFO: stdout: "true"
    Aug 30 07:36:31.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods update-demo-nautilus-qpllh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 30 07:36:32.115: INFO: stderr: ""
    Aug 30 07:36:32.115: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 30 07:36:32.115: INFO: validating pod update-demo-nautilus-qpllh
    Aug 30 07:36:32.136: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 30 07:36:32.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 30 07:36:32.136: INFO: update-demo-nautilus-qpllh is verified up and running
    STEP: using delete to clean up resources 08/30/23 07:36:32.136
    Aug 30 07:36:32.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 delete --grace-period=0 --force -f -'
    Aug 30 07:36:32.272: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:36:32.272: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 30 07:36:32.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get rc,svc -l name=update-demo --no-headers'
    Aug 30 07:36:32.392: INFO: stderr: "No resources found in kubectl-2234 namespace.\n"
    Aug 30 07:36:32.392: INFO: stdout: ""
    Aug 30 07:36:32.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-2234 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 30 07:36:32.494: INFO: stderr: ""
    Aug 30 07:36:32.495: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:36:32.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2234" for this suite. 08/30/23 07:36:32.505
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:36:32.526
Aug 30 07:36:32.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:36:32.527
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:32.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:32.602
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-2195 08/30/23 07:36:32.608
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[] 08/30/23 07:36:32.642
Aug 30 07:36:32.652: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 30 07:36:33.668: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2195 08/30/23 07:36:33.668
Aug 30 07:36:33.684: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2195" to be "running and ready"
Aug 30 07:36:33.691: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.572078ms
Aug 30 07:36:33.691: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:36:35.700: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015258066s
Aug 30 07:36:35.700: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 30 07:36:35.700: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod1:[100]] 08/30/23 07:36:35.707
Aug 30 07:36:35.730: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2195 08/30/23 07:36:35.73
Aug 30 07:36:35.742: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2195" to be "running and ready"
Aug 30 07:36:35.749: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.996315ms
Aug 30 07:36:35.749: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:36:37.802: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.059671042s
Aug 30 07:36:37.802: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 30 07:36:37.802: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod1:[100] pod2:[101]] 08/30/23 07:36:37.81
Aug 30 07:36:37.844: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/30/23 07:36:37.844
Aug 30 07:36:37.844: INFO: Creating new exec pod
Aug 30 07:36:37.859: INFO: Waiting up to 5m0s for pod "execpod296hk" in namespace "services-2195" to be "running"
Aug 30 07:36:37.866: INFO: Pod "execpod296hk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988744ms
Aug 30 07:36:39.875: INFO: Pod "execpod296hk": Phase="Running", Reason="", readiness=true. Elapsed: 2.015610923s
Aug 30 07:36:39.875: INFO: Pod "execpod296hk" satisfied condition "running"
Aug 30 07:36:40.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 30 07:36:41.130: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 30 07:36:41.130: INFO: stdout: ""
Aug 30 07:36:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 172.21.111.7 80'
Aug 30 07:36:41.377: INFO: stderr: "+ nc -v -z -w 2 172.21.111.7 80\nConnection to 172.21.111.7 80 port [tcp/http] succeeded!\n"
Aug 30 07:36:41.377: INFO: stdout: ""
Aug 30 07:36:41.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 30 07:36:41.631: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 30 07:36:41.631: INFO: stdout: ""
Aug 30 07:36:41.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 172.21.111.7 81'
Aug 30 07:36:41.853: INFO: stderr: "+ nc -v -z -w 2 172.21.111.7 81\nConnection to 172.21.111.7 81 port [tcp/*] succeeded!\n"
Aug 30 07:36:41.853: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-2195 08/30/23 07:36:41.853
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod2:[101]] 08/30/23 07:36:41.902
Aug 30 07:36:42.969: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2195 08/30/23 07:36:42.969
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[] 08/30/23 07:36:43.007
Aug 30 07:36:43.053: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:36:43.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2195" for this suite. 08/30/23 07:36:43.103
------------------------------
• [SLOW TEST] [10.610 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:36:32.526
    Aug 30 07:36:32.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:36:32.527
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:32.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:32.602
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-2195 08/30/23 07:36:32.608
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[] 08/30/23 07:36:32.642
    Aug 30 07:36:32.652: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 30 07:36:33.668: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2195 08/30/23 07:36:33.668
    Aug 30 07:36:33.684: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2195" to be "running and ready"
    Aug 30 07:36:33.691: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.572078ms
    Aug 30 07:36:33.691: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:36:35.700: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015258066s
    Aug 30 07:36:35.700: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 30 07:36:35.700: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod1:[100]] 08/30/23 07:36:35.707
    Aug 30 07:36:35.730: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-2195 08/30/23 07:36:35.73
    Aug 30 07:36:35.742: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2195" to be "running and ready"
    Aug 30 07:36:35.749: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.996315ms
    Aug 30 07:36:35.749: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:36:37.802: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.059671042s
    Aug 30 07:36:37.802: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 30 07:36:37.802: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod1:[100] pod2:[101]] 08/30/23 07:36:37.81
    Aug 30 07:36:37.844: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/30/23 07:36:37.844
    Aug 30 07:36:37.844: INFO: Creating new exec pod
    Aug 30 07:36:37.859: INFO: Waiting up to 5m0s for pod "execpod296hk" in namespace "services-2195" to be "running"
    Aug 30 07:36:37.866: INFO: Pod "execpod296hk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.988744ms
    Aug 30 07:36:39.875: INFO: Pod "execpod296hk": Phase="Running", Reason="", readiness=true. Elapsed: 2.015610923s
    Aug 30 07:36:39.875: INFO: Pod "execpod296hk" satisfied condition "running"
    Aug 30 07:36:40.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 30 07:36:41.130: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 30 07:36:41.130: INFO: stdout: ""
    Aug 30 07:36:41.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 172.21.111.7 80'
    Aug 30 07:36:41.377: INFO: stderr: "+ nc -v -z -w 2 172.21.111.7 80\nConnection to 172.21.111.7 80 port [tcp/http] succeeded!\n"
    Aug 30 07:36:41.377: INFO: stdout: ""
    Aug 30 07:36:41.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 30 07:36:41.631: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 30 07:36:41.631: INFO: stdout: ""
    Aug 30 07:36:41.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-2195 exec execpod296hk -- /bin/sh -x -c nc -v -z -w 2 172.21.111.7 81'
    Aug 30 07:36:41.853: INFO: stderr: "+ nc -v -z -w 2 172.21.111.7 81\nConnection to 172.21.111.7 81 port [tcp/*] succeeded!\n"
    Aug 30 07:36:41.853: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-2195 08/30/23 07:36:41.853
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[pod2:[101]] 08/30/23 07:36:41.902
    Aug 30 07:36:42.969: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-2195 08/30/23 07:36:42.969
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2195 to expose endpoints map[] 08/30/23 07:36:43.007
    Aug 30 07:36:43.053: INFO: successfully validated that service multi-endpoint-test in namespace services-2195 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:36:43.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2195" for this suite. 08/30/23 07:36:43.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:36:43.138
Aug 30 07:36:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:36:43.139
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:43.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:43.199
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:36:43.278
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:36:43.974
STEP: Deploying the webhook pod 08/30/23 07:36:43.992
STEP: Wait for the deployment to be ready 08/30/23 07:36:44.02
Aug 30 07:36:44.036: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/30/23 07:36:46.075
STEP: Verifying the service has paired with the endpoint 08/30/23 07:36:46.113
Aug 30 07:36:47.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/30/23 07:36:47.121
STEP: Creating a custom resource definition that should be denied by the webhook 08/30/23 07:36:47.163
Aug 30 07:36:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:36:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1268" for this suite. 08/30/23 07:36:47.297
STEP: Destroying namespace "webhook-1268-markers" for this suite. 08/30/23 07:36:47.347
------------------------------
• [4.283 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:36:43.138
    Aug 30 07:36:43.138: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:36:43.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:43.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:43.199
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:36:43.278
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:36:43.974
    STEP: Deploying the webhook pod 08/30/23 07:36:43.992
    STEP: Wait for the deployment to be ready 08/30/23 07:36:44.02
    Aug 30 07:36:44.036: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/30/23 07:36:46.075
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:36:46.113
    Aug 30 07:36:47.113: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/30/23 07:36:47.121
    STEP: Creating a custom resource definition that should be denied by the webhook 08/30/23 07:36:47.163
    Aug 30 07:36:47.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:36:47.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1268" for this suite. 08/30/23 07:36:47.297
    STEP: Destroying namespace "webhook-1268-markers" for this suite. 08/30/23 07:36:47.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:36:47.423
Aug 30 07:36:47.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:36:47.425
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:47.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:47.562
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/30/23 07:36:47.567
Aug 30 07:36:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: rename a version 08/30/23 07:36:57.213
STEP: check the new version name is served 08/30/23 07:36:57.251
STEP: check the old version name is removed 08/30/23 07:37:03.247
STEP: check the other version is not changed 08/30/23 07:37:04.736
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7680" for this suite. 08/30/23 07:37:14.133
------------------------------
• [SLOW TEST] [26.731 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:36:47.423
    Aug 30 07:36:47.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:36:47.425
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:36:47.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:36:47.562
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/30/23 07:36:47.567
    Aug 30 07:36:47.568: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: rename a version 08/30/23 07:36:57.213
    STEP: check the new version name is served 08/30/23 07:36:57.251
    STEP: check the old version name is removed 08/30/23 07:37:03.247
    STEP: check the other version is not changed 08/30/23 07:37:04.736
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:14.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7680" for this suite. 08/30/23 07:37:14.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:14.156
Aug 30 07:37:14.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:37:14.157
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:14.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:14.245
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:37:14.327
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:37:14.628
STEP: Deploying the webhook pod 08/30/23 07:37:14.652
STEP: Wait for the deployment to be ready 08/30/23 07:37:14.687
Aug 30 07:37:14.715: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 07:37:16.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 07:37:18.75
STEP: Verifying the service has paired with the endpoint 08/30/23 07:37:18.775
Aug 30 07:37:19.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/30/23 07:37:19.789
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/30/23 07:37:19.908
STEP: Creating a configMap that should not be mutated 08/30/23 07:37:19.932
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/30/23 07:37:19.987
STEP: Creating a configMap that should be mutated 08/30/23 07:37:20
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:20.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8777" for this suite. 08/30/23 07:37:20.17
STEP: Destroying namespace "webhook-8777-markers" for this suite. 08/30/23 07:37:20.191
------------------------------
• [SLOW TEST] [6.061 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:14.156
    Aug 30 07:37:14.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:37:14.157
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:14.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:14.245
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:37:14.327
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:37:14.628
    STEP: Deploying the webhook pod 08/30/23 07:37:14.652
    STEP: Wait for the deployment to be ready 08/30/23 07:37:14.687
    Aug 30 07:37:14.715: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 07:37:16.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 37, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 07:37:18.75
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:37:18.775
    Aug 30 07:37:19.775: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/30/23 07:37:19.789
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/30/23 07:37:19.908
    STEP: Creating a configMap that should not be mutated 08/30/23 07:37:19.932
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/30/23 07:37:19.987
    STEP: Creating a configMap that should be mutated 08/30/23 07:37:20
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:20.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8777" for this suite. 08/30/23 07:37:20.17
    STEP: Destroying namespace "webhook-8777-markers" for this suite. 08/30/23 07:37:20.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:20.224
Aug 30 07:37:20.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:37:20.225
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:20.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:20.28
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:37:20.286
W0830 07:37:20.309578      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:37:20.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23" in namespace "projected-7056" to be "Succeeded or Failed"
Aug 30 07:37:20.321: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098736ms
Aug 30 07:37:22.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020460461s
Aug 30 07:37:24.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020763921s
STEP: Saw pod success 08/30/23 07:37:24.33
Aug 30 07:37:24.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23" satisfied condition "Succeeded or Failed"
Aug 30 07:37:24.337: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 container client-container: <nil>
STEP: delete the pod 08/30/23 07:37:24.372
Aug 30 07:37:24.402: INFO: Waiting for pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 to disappear
Aug 30 07:37:24.408: INFO: Pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7056" for this suite. 08/30/23 07:37:24.417
------------------------------
• [4.211 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:20.224
    Aug 30 07:37:20.224: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:37:20.225
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:20.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:20.28
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:37:20.286
    W0830 07:37:20.309578      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:37:20.309: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23" in namespace "projected-7056" to be "Succeeded or Failed"
    Aug 30 07:37:20.321: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098736ms
    Aug 30 07:37:22.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020460461s
    Aug 30 07:37:24.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020763921s
    STEP: Saw pod success 08/30/23 07:37:24.33
    Aug 30 07:37:24.330: INFO: Pod "downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23" satisfied condition "Succeeded or Failed"
    Aug 30 07:37:24.337: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:37:24.372
    Aug 30 07:37:24.402: INFO: Waiting for pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 to disappear
    Aug 30 07:37:24.408: INFO: Pod downwardapi-volume-85b680cb-8212-45ab-9a6f-5e15f8940e23 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:24.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7056" for this suite. 08/30/23 07:37:24.417
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:24.436
Aug 30 07:37:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 07:37:24.437
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:24.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:24.486
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/30/23 07:37:24.492
Aug 30 07:37:24.507: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/30/23 07:37:24.507
Aug 30 07:37:24.542: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/30/23 07:37:24.543
Aug 30 07:37:24.589: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5798" for this suite. 08/30/23 07:37:24.603
------------------------------
• [0.189 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:24.436
    Aug 30 07:37:24.436: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 07:37:24.437
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:24.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:24.486
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/30/23 07:37:24.492
    Aug 30 07:37:24.507: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/30/23 07:37:24.507
    Aug 30 07:37:24.542: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/30/23 07:37:24.543
    Aug 30 07:37:24.589: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:24.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5798" for this suite. 08/30/23 07:37:24.603
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:24.625
Aug 30 07:37:24.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:37:24.627
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:24.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:24.682
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 30 07:37:24.706: INFO: Waiting up to 5m0s for pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c" in namespace "pods-6469" to be "running and ready"
Aug 30 07:37:24.716: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57128ms
Aug 30 07:37:24.716: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:37:26.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017704533s
Aug 30 07:37:26.724: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:37:28.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Running", Reason="", readiness=true. Elapsed: 4.018302334s
Aug 30 07:37:28.724: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Running (Ready = true)
Aug 30 07:37:28.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c" satisfied condition "running and ready"
Aug 30 07:37:28.767: INFO: Waiting up to 5m0s for pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2" in namespace "pods-6469" to be "Succeeded or Failed"
Aug 30 07:37:28.775: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047633ms
Aug 30 07:37:30.785: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017947009s
Aug 30 07:37:32.811: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043803684s
Aug 30 07:37:34.784: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017035248s
STEP: Saw pod success 08/30/23 07:37:34.784
Aug 30 07:37:34.784: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2" satisfied condition "Succeeded or Failed"
Aug 30 07:37:34.791: INFO: Trying to get logs from node 10.135.139.190 pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 container env3cont: <nil>
STEP: delete the pod 08/30/23 07:37:34.809
Aug 30 07:37:34.830: INFO: Waiting for pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 to disappear
Aug 30 07:37:34.837: INFO: Pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6469" for this suite. 08/30/23 07:37:34.847
------------------------------
• [SLOW TEST] [10.240 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:24.625
    Aug 30 07:37:24.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:37:24.627
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:24.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:24.682
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 30 07:37:24.706: INFO: Waiting up to 5m0s for pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c" in namespace "pods-6469" to be "running and ready"
    Aug 30 07:37:24.716: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.57128ms
    Aug 30 07:37:24.716: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:37:26.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017704533s
    Aug 30 07:37:26.724: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:37:28.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c": Phase="Running", Reason="", readiness=true. Elapsed: 4.018302334s
    Aug 30 07:37:28.724: INFO: The phase of Pod server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c is Running (Ready = true)
    Aug 30 07:37:28.724: INFO: Pod "server-envvars-d94ca101-29dd-4753-856c-b8deba6d193c" satisfied condition "running and ready"
    Aug 30 07:37:28.767: INFO: Waiting up to 5m0s for pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2" in namespace "pods-6469" to be "Succeeded or Failed"
    Aug 30 07:37:28.775: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047633ms
    Aug 30 07:37:30.785: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017947009s
    Aug 30 07:37:32.811: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043803684s
    Aug 30 07:37:34.784: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017035248s
    STEP: Saw pod success 08/30/23 07:37:34.784
    Aug 30 07:37:34.784: INFO: Pod "client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2" satisfied condition "Succeeded or Failed"
    Aug 30 07:37:34.791: INFO: Trying to get logs from node 10.135.139.190 pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 container env3cont: <nil>
    STEP: delete the pod 08/30/23 07:37:34.809
    Aug 30 07:37:34.830: INFO: Waiting for pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 to disappear
    Aug 30 07:37:34.837: INFO: Pod client-envvars-c9cad37f-81fa-4b1e-aea7-7413424ad2a2 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6469" for this suite. 08/30/23 07:37:34.847
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:34.87
Aug 30 07:37:34.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:37:34.871
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:34.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:34.929
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-c3c9b68b-2e64-411a-87d6-743994f0df2e 08/30/23 07:37:34.936
STEP: Creating a pod to test consume configMaps 08/30/23 07:37:34.958
W0830 07:37:34.978608      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:37:34.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987" in namespace "configmap-8348" to be "Succeeded or Failed"
Aug 30 07:37:34.986: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 7.856947ms
Aug 30 07:37:36.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01893498s
Aug 30 07:37:38.995: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017194509s
Aug 30 07:37:40.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01884493s
STEP: Saw pod success 08/30/23 07:37:40.997
Aug 30 07:37:40.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987" satisfied condition "Succeeded or Failed"
Aug 30 07:37:41.004: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:37:41.02
Aug 30 07:37:41.104: INFO: Waiting for pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 to disappear
Aug 30 07:37:41.112: INFO: Pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:41.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8348" for this suite. 08/30/23 07:37:41.123
------------------------------
• [SLOW TEST] [6.332 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:34.87
    Aug 30 07:37:34.870: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:37:34.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:34.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:34.929
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-c3c9b68b-2e64-411a-87d6-743994f0df2e 08/30/23 07:37:34.936
    STEP: Creating a pod to test consume configMaps 08/30/23 07:37:34.958
    W0830 07:37:34.978608      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:37:34.978: INFO: Waiting up to 5m0s for pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987" in namespace "configmap-8348" to be "Succeeded or Failed"
    Aug 30 07:37:34.986: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 7.856947ms
    Aug 30 07:37:36.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01893498s
    Aug 30 07:37:38.995: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017194509s
    Aug 30 07:37:40.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01884493s
    STEP: Saw pod success 08/30/23 07:37:40.997
    Aug 30 07:37:40.997: INFO: Pod "pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987" satisfied condition "Succeeded or Failed"
    Aug 30 07:37:41.004: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:37:41.02
    Aug 30 07:37:41.104: INFO: Waiting for pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 to disappear
    Aug 30 07:37:41.112: INFO: Pod pod-configmaps-abaa3a8e-ef72-42aa-8e58-48d6a2199987 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:41.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8348" for this suite. 08/30/23 07:37:41.123
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:41.204
Aug 30 07:37:41.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename resourcequota 08/30/23 07:37:41.205
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:41.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:41.353
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/30/23 07:37:41.358
STEP: Ensuring ResourceQuota status is calculated 08/30/23 07:37:41.404
STEP: Creating a ResourceQuota with not best effort scope 08/30/23 07:37:43.465
STEP: Ensuring ResourceQuota status is calculated 08/30/23 07:37:43.561
STEP: Creating a best-effort pod 08/30/23 07:37:45.572
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/30/23 07:37:45.608
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/30/23 07:37:47.654
STEP: Deleting the pod 08/30/23 07:37:49.69
STEP: Ensuring resource quota status released the pod usage 08/30/23 07:37:49.751
STEP: Creating a not best-effort pod 08/30/23 07:37:51.766
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/30/23 07:37:51.789
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/30/23 07:37:53.817
STEP: Deleting the pod 08/30/23 07:37:55.832
STEP: Ensuring resource quota status released the pod usage 08/30/23 07:37:55.864
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 30 07:37:57.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5515" for this suite. 08/30/23 07:37:57.889
------------------------------
• [SLOW TEST] [16.703 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:41.204
    Aug 30 07:37:41.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename resourcequota 08/30/23 07:37:41.205
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:41.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:41.353
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/30/23 07:37:41.358
    STEP: Ensuring ResourceQuota status is calculated 08/30/23 07:37:41.404
    STEP: Creating a ResourceQuota with not best effort scope 08/30/23 07:37:43.465
    STEP: Ensuring ResourceQuota status is calculated 08/30/23 07:37:43.561
    STEP: Creating a best-effort pod 08/30/23 07:37:45.572
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/30/23 07:37:45.608
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/30/23 07:37:47.654
    STEP: Deleting the pod 08/30/23 07:37:49.69
    STEP: Ensuring resource quota status released the pod usage 08/30/23 07:37:49.751
    STEP: Creating a not best-effort pod 08/30/23 07:37:51.766
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/30/23 07:37:51.789
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/30/23 07:37:53.817
    STEP: Deleting the pod 08/30/23 07:37:55.832
    STEP: Ensuring resource quota status released the pod usage 08/30/23 07:37:55.864
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:37:57.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5515" for this suite. 08/30/23 07:37:57.889
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:37:57.909
Aug 30 07:37:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:37:57.911
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:57.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:58.003
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/30/23 07:37:58.015
Aug 30 07:37:58.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 30 07:37:58.503: INFO: stderr: ""
Aug 30 07:37:58.503: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/30/23 07:37:58.503
Aug 30 07:37:58.503: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 30 07:37:58.503: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8964" to be "running and ready, or succeeded"
Aug 30 07:37:58.518: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.865238ms
Aug 30 07:37:58.518: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.135.139.190' to be 'Running' but was 'Pending'
Aug 30 07:38:00.545: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.042120393s
Aug 30 07:38:00.545: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 30 07:38:00.545: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/30/23 07:38:00.545
Aug 30 07:38:00.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator'
Aug 30 07:38:00.857: INFO: stderr: ""
Aug 30 07:38:00.857: INFO: stdout: "I0830 07:37:59.858335       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/8pzj 579\nI0830 07:38:00.058578       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dsw 422\nI0830 07:38:00.259118       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/cfq2 327\nI0830 07:38:00.458457       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cdv 310\nI0830 07:38:00.658845       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/t8nx 543\n"
STEP: limiting log lines 08/30/23 07:38:00.857
Aug 30 07:38:00.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --tail=1'
Aug 30 07:38:01.179: INFO: stderr: ""
Aug 30 07:38:01.180: INFO: stdout: "I0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\n"
Aug 30 07:38:01.180: INFO: got output "I0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\n"
STEP: limiting log bytes 08/30/23 07:38:01.18
Aug 30 07:38:01.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --limit-bytes=1'
Aug 30 07:38:01.531: INFO: stderr: ""
Aug 30 07:38:01.531: INFO: stdout: "I"
Aug 30 07:38:01.531: INFO: got output "I"
STEP: exposing timestamps 08/30/23 07:38:01.531
Aug 30 07:38:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 30 07:38:01.908: INFO: stderr: ""
Aug 30 07:38:01.908: INFO: stdout: "2023-08-30T02:38:01.859323086-05:00 I0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\n"
Aug 30 07:38:01.908: INFO: got output "2023-08-30T02:38:01.859323086-05:00 I0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\n"
STEP: restricting to a time range 08/30/23 07:38:01.908
Aug 30 07:38:04.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --since=1s'
Aug 30 07:38:04.628: INFO: stderr: ""
Aug 30 07:38:04.628: INFO: stdout: "I0830 07:38:03.658524       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fx67 543\nI0830 07:38:03.858968       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/thh 481\nI0830 07:38:04.059387       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/zg7x 365\nI0830 07:38:04.258793       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tkzf 347\nI0830 07:38:04.459243       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/wphc 300\n"
Aug 30 07:38:04.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --since=24h'
Aug 30 07:38:05.470: INFO: stderr: ""
Aug 30 07:38:05.470: INFO: stdout: "I0830 07:37:59.858335       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/8pzj 579\nI0830 07:38:00.058578       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dsw 422\nI0830 07:38:00.259118       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/cfq2 327\nI0830 07:38:00.458457       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cdv 310\nI0830 07:38:00.658845       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/t8nx 543\nI0830 07:38:00.859226       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/k9gn 502\nI0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\nI0830 07:38:01.259054       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/btq7 534\nI0830 07:38:01.459464       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9lb2 354\nI0830 07:38:01.658758       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/qgnb 461\nI0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\nI0830 07:38:02.058603       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/nct 449\nI0830 07:38:02.258975       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/2dbv 247\nI0830 07:38:02.459357       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/2sst 235\nI0830 07:38:02.658772       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/cz8n 447\nI0830 07:38:02.859175       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/zsz4 466\nI0830 07:38:03.058515       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5tr 550\nI0830 07:38:03.258754       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f2rn 383\nI0830 07:38:03.459157       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/59m2 478\nI0830 07:38:03.658524       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fx67 543\nI0830 07:38:03.858968       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/thh 481\nI0830 07:38:04.059387       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/zg7x 365\nI0830 07:38:04.258793       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tkzf 347\nI0830 07:38:04.459243       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/wphc 300\nI0830 07:38:04.658580       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/57s 293\nI0830 07:38:04.858976       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/jbdl 297\nI0830 07:38:05.059439       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/gb7 521\nI0830 07:38:05.258793       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/cddd 377\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 30 07:38:05.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 delete pod logs-generator'
Aug 30 07:38:07.176: INFO: stderr: ""
Aug 30 07:38:07.176: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:07.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8964" for this suite. 08/30/23 07:38:07.185
------------------------------
• [SLOW TEST] [9.294 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:37:57.909
    Aug 30 07:37:57.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:37:57.911
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:37:57.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:37:58.003
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/30/23 07:37:58.015
    Aug 30 07:37:58.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 30 07:37:58.503: INFO: stderr: ""
    Aug 30 07:37:58.503: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/30/23 07:37:58.503
    Aug 30 07:37:58.503: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 30 07:37:58.503: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8964" to be "running and ready, or succeeded"
    Aug 30 07:37:58.518: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.865238ms
    Aug 30 07:37:58.518: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.135.139.190' to be 'Running' but was 'Pending'
    Aug 30 07:38:00.545: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.042120393s
    Aug 30 07:38:00.545: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 30 07:38:00.545: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/30/23 07:38:00.545
    Aug 30 07:38:00.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator'
    Aug 30 07:38:00.857: INFO: stderr: ""
    Aug 30 07:38:00.857: INFO: stdout: "I0830 07:37:59.858335       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/8pzj 579\nI0830 07:38:00.058578       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dsw 422\nI0830 07:38:00.259118       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/cfq2 327\nI0830 07:38:00.458457       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cdv 310\nI0830 07:38:00.658845       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/t8nx 543\n"
    STEP: limiting log lines 08/30/23 07:38:00.857
    Aug 30 07:38:00.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --tail=1'
    Aug 30 07:38:01.179: INFO: stderr: ""
    Aug 30 07:38:01.180: INFO: stdout: "I0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\n"
    Aug 30 07:38:01.180: INFO: got output "I0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\n"
    STEP: limiting log bytes 08/30/23 07:38:01.18
    Aug 30 07:38:01.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --limit-bytes=1'
    Aug 30 07:38:01.531: INFO: stderr: ""
    Aug 30 07:38:01.531: INFO: stdout: "I"
    Aug 30 07:38:01.531: INFO: got output "I"
    STEP: exposing timestamps 08/30/23 07:38:01.531
    Aug 30 07:38:01.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 30 07:38:01.908: INFO: stderr: ""
    Aug 30 07:38:01.908: INFO: stdout: "2023-08-30T02:38:01.859323086-05:00 I0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\n"
    Aug 30 07:38:01.908: INFO: got output "2023-08-30T02:38:01.859323086-05:00 I0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\n"
    STEP: restricting to a time range 08/30/23 07:38:01.908
    Aug 30 07:38:04.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --since=1s'
    Aug 30 07:38:04.628: INFO: stderr: ""
    Aug 30 07:38:04.628: INFO: stdout: "I0830 07:38:03.658524       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fx67 543\nI0830 07:38:03.858968       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/thh 481\nI0830 07:38:04.059387       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/zg7x 365\nI0830 07:38:04.258793       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tkzf 347\nI0830 07:38:04.459243       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/wphc 300\n"
    Aug 30 07:38:04.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 logs logs-generator logs-generator --since=24h'
    Aug 30 07:38:05.470: INFO: stderr: ""
    Aug 30 07:38:05.470: INFO: stdout: "I0830 07:37:59.858335       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/8pzj 579\nI0830 07:38:00.058578       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/dsw 422\nI0830 07:38:00.259118       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/default/pods/cfq2 327\nI0830 07:38:00.458457       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/cdv 310\nI0830 07:38:00.658845       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/t8nx 543\nI0830 07:38:00.859226       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/k9gn 502\nI0830 07:38:01.058678       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/jw6 400\nI0830 07:38:01.259054       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/btq7 534\nI0830 07:38:01.459464       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/9lb2 354\nI0830 07:38:01.658758       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/qgnb 461\nI0830 07:38:01.859205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/s65 549\nI0830 07:38:02.058603       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/nct 449\nI0830 07:38:02.258975       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/2dbv 247\nI0830 07:38:02.459357       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/2sst 235\nI0830 07:38:02.658772       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/cz8n 447\nI0830 07:38:02.859175       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/zsz4 466\nI0830 07:38:03.058515       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/5tr 550\nI0830 07:38:03.258754       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/f2rn 383\nI0830 07:38:03.459157       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/59m2 478\nI0830 07:38:03.658524       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fx67 543\nI0830 07:38:03.858968       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/thh 481\nI0830 07:38:04.059387       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/zg7x 365\nI0830 07:38:04.258793       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/tkzf 347\nI0830 07:38:04.459243       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/wphc 300\nI0830 07:38:04.658580       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/57s 293\nI0830 07:38:04.858976       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/jbdl 297\nI0830 07:38:05.059439       1 logs_generator.go:76] 26 GET /api/v1/namespaces/ns/pods/gb7 521\nI0830 07:38:05.258793       1 logs_generator.go:76] 27 GET /api/v1/namespaces/default/pods/cddd 377\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 30 07:38:05.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8964 delete pod logs-generator'
    Aug 30 07:38:07.176: INFO: stderr: ""
    Aug 30 07:38:07.176: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:07.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8964" for this suite. 08/30/23 07:38:07.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:07.206
Aug 30 07:38:07.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename replication-controller 08/30/23 07:38:07.207
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:07.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:07.262
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/30/23 07:38:07.274
W0830 07:38:07.299266      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:38:07.299: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6472" to be "running and ready"
Aug 30 07:38:07.307: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540917ms
Aug 30 07:38:07.307: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:38:09.317: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017648564s
Aug 30 07:38:09.317: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 30 07:38:09.317: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/30/23 07:38:09.324
STEP: Then the orphan pod is adopted 08/30/23 07:38:09.333
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6472" for this suite. 08/30/23 07:38:10.358
------------------------------
• [3.170 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:07.206
    Aug 30 07:38:07.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename replication-controller 08/30/23 07:38:07.207
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:07.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:07.262
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/30/23 07:38:07.274
    W0830 07:38:07.299266      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:38:07.299: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6472" to be "running and ready"
    Aug 30 07:38:07.307: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540917ms
    Aug 30 07:38:07.307: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:38:09.317: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.017648564s
    Aug 30 07:38:09.317: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 30 07:38:09.317: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/30/23 07:38:09.324
    STEP: Then the orphan pod is adopted 08/30/23 07:38:09.333
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:10.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6472" for this suite. 08/30/23 07:38:10.358
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:10.377
Aug 30 07:38:10.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename containers 08/30/23 07:38:10.378
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:10.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:10.432
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/30/23 07:38:10.436
Aug 30 07:38:11.483: INFO: Waiting up to 5m0s for pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2" in namespace "containers-4906" to be "Succeeded or Failed"
Aug 30 07:38:11.492: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.697465ms
Aug 30 07:38:13.500: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017361226s
Aug 30 07:38:15.504: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020684294s
STEP: Saw pod success 08/30/23 07:38:15.504
Aug 30 07:38:15.504: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2" satisfied condition "Succeeded or Failed"
Aug 30 07:38:15.522: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:38:15.55
Aug 30 07:38:15.636: INFO: Waiting for pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 to disappear
Aug 30 07:38:15.643: INFO: Pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:15.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4906" for this suite. 08/30/23 07:38:15.653
------------------------------
• [SLOW TEST] [5.300 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:10.377
    Aug 30 07:38:10.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename containers 08/30/23 07:38:10.378
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:10.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:10.432
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/30/23 07:38:10.436
    Aug 30 07:38:11.483: INFO: Waiting up to 5m0s for pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2" in namespace "containers-4906" to be "Succeeded or Failed"
    Aug 30 07:38:11.492: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.697465ms
    Aug 30 07:38:13.500: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017361226s
    Aug 30 07:38:15.504: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020684294s
    STEP: Saw pod success 08/30/23 07:38:15.504
    Aug 30 07:38:15.504: INFO: Pod "client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2" satisfied condition "Succeeded or Failed"
    Aug 30 07:38:15.522: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:38:15.55
    Aug 30 07:38:15.636: INFO: Waiting for pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 to disappear
    Aug 30 07:38:15.643: INFO: Pod client-containers-87ffb446-357a-4e2e-a8e1-c6e66e1540d2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:15.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4906" for this suite. 08/30/23 07:38:15.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:15.678
Aug 30 07:38:15.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:38:15.679
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:15.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:15.799
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/30/23 07:38:15.804
Aug 30 07:38:15.834: INFO: created test-pod-1
Aug 30 07:38:15.878: INFO: created test-pod-2
Aug 30 07:38:15.895: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/30/23 07:38:15.895
Aug 30 07:38:15.895: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7340' to be running and ready
Aug 30 07:38:16.013: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:16.013: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:16.013: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:16.013: INFO: 0 / 3 pods in namespace 'pods-7340' are running and ready (0 seconds elapsed)
Aug 30 07:38:16.013: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
Aug 30 07:38:16.013: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Aug 30 07:38:16.013: INFO: test-pod-1  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:16.013: INFO: test-pod-2  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:16.013: INFO: test-pod-3  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:16.013: INFO: 
Aug 30 07:38:18.036: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:18.036: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:18.036: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 30 07:38:18.036: INFO: 0 / 3 pods in namespace 'pods-7340' are running and ready (2 seconds elapsed)
Aug 30 07:38:18.036: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
Aug 30 07:38:18.036: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
Aug 30 07:38:18.036: INFO: test-pod-1  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:18.036: INFO: test-pod-2  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:18.036: INFO: test-pod-3  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
Aug 30 07:38:18.036: INFO: 
Aug 30 07:38:20.036: INFO: 3 / 3 pods in namespace 'pods-7340' are running and ready (4 seconds elapsed)
Aug 30 07:38:20.036: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/30/23 07:38:20.077
Aug 30 07:38:20.088: INFO: Pod quantity 3 is different from expected quantity 0
Aug 30 07:38:21.100: INFO: Pod quantity 3 is different from expected quantity 0
Aug 30 07:38:22.097: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7340" for this suite. 08/30/23 07:38:23.106
------------------------------
• [SLOW TEST] [7.447 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:15.678
    Aug 30 07:38:15.678: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:38:15.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:15.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:15.799
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/30/23 07:38:15.804
    Aug 30 07:38:15.834: INFO: created test-pod-1
    Aug 30 07:38:15.878: INFO: created test-pod-2
    Aug 30 07:38:15.895: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/30/23 07:38:15.895
    Aug 30 07:38:15.895: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-7340' to be running and ready
    Aug 30 07:38:16.013: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:16.013: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:16.013: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:16.013: INFO: 0 / 3 pods in namespace 'pods-7340' are running and ready (0 seconds elapsed)
    Aug 30 07:38:16.013: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
    Aug 30 07:38:16.013: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Aug 30 07:38:16.013: INFO: test-pod-1  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:16.013: INFO: test-pod-2  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:16.013: INFO: test-pod-3  10.135.139.190  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:16.013: INFO: 
    Aug 30 07:38:18.036: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:18.036: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:18.036: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 30 07:38:18.036: INFO: 0 / 3 pods in namespace 'pods-7340' are running and ready (2 seconds elapsed)
    Aug 30 07:38:18.036: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
    Aug 30 07:38:18.036: INFO: POD         NODE            PHASE    GRACE  CONDITIONS
    Aug 30 07:38:18.036: INFO: test-pod-1  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:18.036: INFO: test-pod-2  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:18.036: INFO: test-pod-3  10.135.139.190  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:38:15 +0000 UTC  }]
    Aug 30 07:38:18.036: INFO: 
    Aug 30 07:38:20.036: INFO: 3 / 3 pods in namespace 'pods-7340' are running and ready (4 seconds elapsed)
    Aug 30 07:38:20.036: INFO: expected 0 pod replicas in namespace 'pods-7340', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/30/23 07:38:20.077
    Aug 30 07:38:20.088: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 30 07:38:21.100: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 30 07:38:22.097: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:23.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7340" for this suite. 08/30/23 07:38:23.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:23.126
Aug 30 07:38:23.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:38:23.127
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:23.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:23.199
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/30/23 07:38:23.205
W0830 07:38:23.269714      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:38:23.269: INFO: Waiting up to 5m0s for pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0" in namespace "downward-api-734" to be "Succeeded or Failed"
Aug 30 07:38:23.302: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 33.146485ms
Aug 30 07:38:25.310: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040400639s
Aug 30 07:38:27.316: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047105217s
STEP: Saw pod success 08/30/23 07:38:27.317
Aug 30 07:38:27.317: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0" satisfied condition "Succeeded or Failed"
Aug 30 07:38:27.323: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:38:27.357
Aug 30 07:38:27.399: INFO: Waiting for pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 to disappear
Aug 30 07:38:27.411: INFO: Pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:27.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-734" for this suite. 08/30/23 07:38:27.422
------------------------------
• [4.315 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:23.126
    Aug 30 07:38:23.126: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:38:23.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:23.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:23.199
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/30/23 07:38:23.205
    W0830 07:38:23.269714      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:38:23.269: INFO: Waiting up to 5m0s for pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0" in namespace "downward-api-734" to be "Succeeded or Failed"
    Aug 30 07:38:23.302: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 33.146485ms
    Aug 30 07:38:25.310: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040400639s
    Aug 30 07:38:27.316: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047105217s
    STEP: Saw pod success 08/30/23 07:38:27.317
    Aug 30 07:38:27.317: INFO: Pod "downward-api-68054b28-4e1e-4640-8216-c98b77384ff0" satisfied condition "Succeeded or Failed"
    Aug 30 07:38:27.323: INFO: Trying to get logs from node 10.135.139.190 pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:38:27.357
    Aug 30 07:38:27.399: INFO: Waiting for pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 to disappear
    Aug 30 07:38:27.411: INFO: Pod downward-api-68054b28-4e1e-4640-8216-c98b77384ff0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:27.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-734" for this suite. 08/30/23 07:38:27.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:27.441
Aug 30 07:38:27.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:38:27.442
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:27.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:27.518
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/30/23 07:38:27.525
Aug 30 07:38:27.546: INFO: Waiting up to 5m0s for pod "pod-hgpdb" in namespace "pods-7956" to be "running"
Aug 30 07:38:27.555: INFO: Pod "pod-hgpdb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.531554ms
Aug 30 07:38:29.564: INFO: Pod "pod-hgpdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.017786333s
Aug 30 07:38:29.564: INFO: Pod "pod-hgpdb" satisfied condition "running"
STEP: patching /status 08/30/23 07:38:29.564
Aug 30 07:38:29.577: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:29.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7956" for this suite. 08/30/23 07:38:29.59
------------------------------
• [2.167 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:27.441
    Aug 30 07:38:27.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:38:27.442
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:27.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:27.518
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/30/23 07:38:27.525
    Aug 30 07:38:27.546: INFO: Waiting up to 5m0s for pod "pod-hgpdb" in namespace "pods-7956" to be "running"
    Aug 30 07:38:27.555: INFO: Pod "pod-hgpdb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.531554ms
    Aug 30 07:38:29.564: INFO: Pod "pod-hgpdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.017786333s
    Aug 30 07:38:29.564: INFO: Pod "pod-hgpdb" satisfied condition "running"
    STEP: patching /status 08/30/23 07:38:29.564
    Aug 30 07:38:29.577: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:29.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7956" for this suite. 08/30/23 07:38:29.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:29.622
Aug 30 07:38:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 07:38:29.623
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:29.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:29.758
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/30/23 07:38:29.783
STEP: delete the rc 08/30/23 07:38:34.837
STEP: wait for all pods to be garbage collected 08/30/23 07:38:34.858
STEP: Gathering metrics 08/30/23 07:38:39.879
W0830 07:38:39.948336      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 07:38:39.948: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:39.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2171" for this suite. 08/30/23 07:38:39.958
------------------------------
• [SLOW TEST] [10.356 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:29.622
    Aug 30 07:38:29.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 07:38:29.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:29.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:29.758
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/30/23 07:38:29.783
    STEP: delete the rc 08/30/23 07:38:34.837
    STEP: wait for all pods to be garbage collected 08/30/23 07:38:34.858
    STEP: Gathering metrics 08/30/23 07:38:39.879
    W0830 07:38:39.948336      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 07:38:39.948: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:39.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2171" for this suite. 08/30/23 07:38:39.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:39.978
Aug 30 07:38:39.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:38:39.98
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:40.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:40.038
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:38:40.044
Aug 30 07:38:40.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5066 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 30 07:38:40.242: INFO: stderr: ""
Aug 30 07:38:40.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/30/23 07:38:40.242
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 30 07:38:40.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5066 delete pods e2e-test-httpd-pod'
Aug 30 07:38:43.651: INFO: stderr: ""
Aug 30 07:38:43.651: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:43.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5066" for this suite. 08/30/23 07:38:43.661
------------------------------
• [3.700 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:39.978
    Aug 30 07:38:39.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:38:39.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:40.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:40.038
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:38:40.044
    Aug 30 07:38:40.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5066 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 30 07:38:40.242: INFO: stderr: ""
    Aug 30 07:38:40.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/30/23 07:38:40.242
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 30 07:38:40.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5066 delete pods e2e-test-httpd-pod'
    Aug 30 07:38:43.651: INFO: stderr: ""
    Aug 30 07:38:43.651: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:43.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5066" for this suite. 08/30/23 07:38:43.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:43.68
Aug 30 07:38:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:38:43.681
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:43.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:43.739
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6182 08/30/23 07:38:43.745
STEP: creating service affinity-clusterip in namespace services-6182 08/30/23 07:38:43.746
STEP: creating replication controller affinity-clusterip in namespace services-6182 08/30/23 07:38:43.773
I0830 07:38:43.785534      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6182, replica count: 3
I0830 07:38:46.837896      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:38:46.864: INFO: Creating new exec pod
Aug 30 07:38:46.883: INFO: Waiting up to 5m0s for pod "execpod-affinitym7mc9" in namespace "services-6182" to be "running"
Aug 30 07:38:46.890: INFO: Pod "execpod-affinitym7mc9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.147681ms
Aug 30 07:38:48.900: INFO: Pod "execpod-affinitym7mc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016416858s
Aug 30 07:38:48.900: INFO: Pod "execpod-affinitym7mc9" satisfied condition "running"
Aug 30 07:38:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 30 07:38:50.192: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 30 07:38:50.192: INFO: stdout: ""
Aug 30 07:38:50.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c nc -v -z -w 2 172.21.126.191 80'
Aug 30 07:38:50.479: INFO: stderr: "+ nc -v -z -w 2 172.21.126.191 80\nConnection to 172.21.126.191 80 port [tcp/http] succeeded!\n"
Aug 30 07:38:50.479: INFO: stdout: ""
Aug 30 07:38:50.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.126.191:80/ ; done'
Aug 30 07:38:50.819: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n"
Aug 30 07:38:50.819: INFO: stdout: "\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs"
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
Aug 30 07:38:50.819: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6182, will wait for the garbage collector to delete the pods 08/30/23 07:38:50.869
Aug 30 07:38:50.949: INFO: Deleting ReplicationController affinity-clusterip took: 21.423959ms
Aug 30 07:38:51.050: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.166332ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:54.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6182" for this suite. 08/30/23 07:38:54.104
------------------------------
• [SLOW TEST] [10.445 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:43.68
    Aug 30 07:38:43.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:38:43.681
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:43.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:43.739
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6182 08/30/23 07:38:43.745
    STEP: creating service affinity-clusterip in namespace services-6182 08/30/23 07:38:43.746
    STEP: creating replication controller affinity-clusterip in namespace services-6182 08/30/23 07:38:43.773
    I0830 07:38:43.785534      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6182, replica count: 3
    I0830 07:38:46.837896      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:38:46.864: INFO: Creating new exec pod
    Aug 30 07:38:46.883: INFO: Waiting up to 5m0s for pod "execpod-affinitym7mc9" in namespace "services-6182" to be "running"
    Aug 30 07:38:46.890: INFO: Pod "execpod-affinitym7mc9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.147681ms
    Aug 30 07:38:48.900: INFO: Pod "execpod-affinitym7mc9": Phase="Running", Reason="", readiness=true. Elapsed: 2.016416858s
    Aug 30 07:38:48.900: INFO: Pod "execpod-affinitym7mc9" satisfied condition "running"
    Aug 30 07:38:49.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 30 07:38:50.192: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 30 07:38:50.192: INFO: stdout: ""
    Aug 30 07:38:50.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c nc -v -z -w 2 172.21.126.191 80'
    Aug 30 07:38:50.479: INFO: stderr: "+ nc -v -z -w 2 172.21.126.191 80\nConnection to 172.21.126.191 80 port [tcp/http] succeeded!\n"
    Aug 30 07:38:50.479: INFO: stdout: ""
    Aug 30 07:38:50.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-6182 exec execpod-affinitym7mc9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.126.191:80/ ; done'
    Aug 30 07:38:50.819: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.126.191:80/\n"
    Aug 30 07:38:50.819: INFO: stdout: "\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs\naffinity-clusterip-9n4rs"
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Received response from host: affinity-clusterip-9n4rs
    Aug 30 07:38:50.819: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6182, will wait for the garbage collector to delete the pods 08/30/23 07:38:50.869
    Aug 30 07:38:50.949: INFO: Deleting ReplicationController affinity-clusterip took: 21.423959ms
    Aug 30 07:38:51.050: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.166332ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:54.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6182" for this suite. 08/30/23 07:38:54.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:54.127
Aug 30 07:38:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:38:54.128
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:54.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:54.215
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:38:54.285
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:38:54.714
STEP: Deploying the webhook pod 08/30/23 07:38:54.737
STEP: Wait for the deployment to be ready 08/30/23 07:38:54.775
Aug 30 07:38:54.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 07:38:56.849
STEP: Verifying the service has paired with the endpoint 08/30/23 07:38:56.87
Aug 30 07:38:57.871: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/30/23 07:38:57.882
STEP: create a configmap that should be updated by the webhook 08/30/23 07:38:58.117
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:38:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2262" for this suite. 08/30/23 07:38:58.469
STEP: Destroying namespace "webhook-2262-markers" for this suite. 08/30/23 07:38:58.49
------------------------------
• [4.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:54.127
    Aug 30 07:38:54.127: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:38:54.128
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:54.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:54.215
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:38:54.285
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:38:54.714
    STEP: Deploying the webhook pod 08/30/23 07:38:54.737
    STEP: Wait for the deployment to be ready 08/30/23 07:38:54.775
    Aug 30 07:38:54.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 07:38:56.849
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:38:56.87
    Aug 30 07:38:57.871: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/30/23 07:38:57.882
    STEP: create a configmap that should be updated by the webhook 08/30/23 07:38:58.117
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:38:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2262" for this suite. 08/30/23 07:38:58.469
    STEP: Destroying namespace "webhook-2262-markers" for this suite. 08/30/23 07:38:58.49
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:38:58.514
Aug 30 07:38:58.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 07:38:58.516
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:58.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:58.585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4222 08/30/23 07:38:58.59
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/30/23 07:38:58.611
W0830 07:38:58.673626      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:38:58.709: INFO: Found 0 stateful pods, waiting for 3
Aug 30 07:39:08.718: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 07:39:08.718: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 07:39:08.718: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 30 07:39:08.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 07:39:09.495: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 07:39:09.495: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 07:39:09.495: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/30/23 07:39:19.569
Aug 30 07:39:19.605: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/30/23 07:39:19.605
STEP: Updating Pods in reverse ordinal order 08/30/23 07:39:29.649
Aug 30 07:39:29.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 07:39:30.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 07:39:30.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 07:39:30.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 07:39:40.127: INFO: Waiting for StatefulSet statefulset-4222/ss2 to complete update
STEP: Rolling back to a previous revision 08/30/23 07:39:50.152
Aug 30 07:39:50.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 30 07:39:50.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 30 07:39:50.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 30 07:39:50.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 30 07:40:00.595: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/30/23 07:40:10.648
Aug 30 07:40:10.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 30 07:40:10.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 30 07:40:10.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 30 07:40:10.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 30 07:40:20.997: INFO: Waiting for StatefulSet statefulset-4222/ss2 to complete update
Aug 30 07:40:20.997: INFO: Waiting for Pod statefulset-4222/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 07:40:31.016: INFO: Deleting all statefulset in ns statefulset-4222
Aug 30 07:40:31.026: INFO: Scaling statefulset ss2 to 0
Aug 30 07:40:41.062: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 07:40:41.071: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:40:41.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4222" for this suite. 08/30/23 07:40:41.111
------------------------------
• [SLOW TEST] [102.630 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:38:58.514
    Aug 30 07:38:58.514: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 07:38:58.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:38:58.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:38:58.585
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4222 08/30/23 07:38:58.59
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/30/23 07:38:58.611
    W0830 07:38:58.673626      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:38:58.709: INFO: Found 0 stateful pods, waiting for 3
    Aug 30 07:39:08.718: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 07:39:08.718: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 07:39:08.718: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 30 07:39:08.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 07:39:09.495: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 07:39:09.495: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 07:39:09.495: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/30/23 07:39:19.569
    Aug 30 07:39:19.605: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/30/23 07:39:19.605
    STEP: Updating Pods in reverse ordinal order 08/30/23 07:39:29.649
    Aug 30 07:39:29.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 07:39:30.040: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 07:39:30.040: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 07:39:30.040: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 07:39:40.127: INFO: Waiting for StatefulSet statefulset-4222/ss2 to complete update
    STEP: Rolling back to a previous revision 08/30/23 07:39:50.152
    Aug 30 07:39:50.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 30 07:39:50.444: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 30 07:39:50.444: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 30 07:39:50.444: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 30 07:40:00.595: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/30/23 07:40:10.648
    Aug 30 07:40:10.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=statefulset-4222 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 30 07:40:10.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 30 07:40:10.940: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 30 07:40:10.940: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 30 07:40:20.997: INFO: Waiting for StatefulSet statefulset-4222/ss2 to complete update
    Aug 30 07:40:20.997: INFO: Waiting for Pod statefulset-4222/ss2-0 to have revision ss2-7b6c9599d5 update revision ss2-5459d8585b
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 07:40:31.016: INFO: Deleting all statefulset in ns statefulset-4222
    Aug 30 07:40:31.026: INFO: Scaling statefulset ss2 to 0
    Aug 30 07:40:41.062: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 07:40:41.071: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:40:41.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4222" for this suite. 08/30/23 07:40:41.111
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:40:41.145
Aug 30 07:40:41.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename endpointslice 08/30/23 07:40:41.146
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:41.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:41.25
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 30 07:40:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1742" for this suite. 08/30/23 07:40:43.426
------------------------------
• [2.297 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:40:41.145
    Aug 30 07:40:41.146: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename endpointslice 08/30/23 07:40:41.146
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:41.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:41.25
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:40:43.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1742" for this suite. 08/30/23 07:40:43.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:40:43.443
Aug 30 07:40:43.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:40:43.446
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:43.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:43.512
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/30/23 07:40:43.519
Aug 30 07:40:43.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 create -f -'
Aug 30 07:40:45.713: INFO: stderr: ""
Aug 30 07:40:45.714: INFO: stdout: "pod/pause created\n"
Aug 30 07:40:45.714: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 30 07:40:45.714: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8673" to be "running and ready"
Aug 30 07:40:45.721: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.196638ms
Aug 30 07:40:45.721: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.135.139.190' to be 'Running' but was 'Pending'
Aug 30 07:40:47.767: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053056342s
Aug 30 07:40:47.767: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.135.139.190' to be 'Running' but was 'Pending'
Aug 30 07:40:49.749: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.034951552s
Aug 30 07:40:49.749: INFO: Pod "pause" satisfied condition "running and ready"
Aug 30 07:40:49.749: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/30/23 07:40:49.749
Aug 30 07:40:49.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 label pods pause testing-label=testing-label-value'
Aug 30 07:40:50.021: INFO: stderr: ""
Aug 30 07:40:50.022: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/30/23 07:40:50.022
Aug 30 07:40:50.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pod pause -L testing-label'
Aug 30 07:40:50.163: INFO: stderr: ""
Aug 30 07:40:50.163: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/30/23 07:40:50.163
Aug 30 07:40:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 label pods pause testing-label-'
Aug 30 07:40:50.273: INFO: stderr: ""
Aug 30 07:40:50.273: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/30/23 07:40:50.273
Aug 30 07:40:50.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pod pause -L testing-label'
Aug 30 07:40:50.365: INFO: stderr: ""
Aug 30 07:40:50.365: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/30/23 07:40:50.365
Aug 30 07:40:50.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 delete --grace-period=0 --force -f -'
Aug 30 07:40:50.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:40:50.495: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 30 07:40:50.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get rc,svc -l name=pause --no-headers'
Aug 30 07:40:50.611: INFO: stderr: "No resources found in kubectl-8673 namespace.\n"
Aug 30 07:40:50.612: INFO: stdout: ""
Aug 30 07:40:50.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 30 07:40:50.733: INFO: stderr: ""
Aug 30 07:40:50.733: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:40:50.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8673" for this suite. 08/30/23 07:40:50.747
------------------------------
• [SLOW TEST] [7.329 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:40:43.443
    Aug 30 07:40:43.443: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:40:43.446
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:43.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:43.512
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/30/23 07:40:43.519
    Aug 30 07:40:43.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 create -f -'
    Aug 30 07:40:45.713: INFO: stderr: ""
    Aug 30 07:40:45.714: INFO: stdout: "pod/pause created\n"
    Aug 30 07:40:45.714: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 30 07:40:45.714: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8673" to be "running and ready"
    Aug 30 07:40:45.721: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 7.196638ms
    Aug 30 07:40:45.721: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.135.139.190' to be 'Running' but was 'Pending'
    Aug 30 07:40:47.767: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053056342s
    Aug 30 07:40:47.767: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.135.139.190' to be 'Running' but was 'Pending'
    Aug 30 07:40:49.749: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.034951552s
    Aug 30 07:40:49.749: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 30 07:40:49.749: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/30/23 07:40:49.749
    Aug 30 07:40:49.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 label pods pause testing-label=testing-label-value'
    Aug 30 07:40:50.021: INFO: stderr: ""
    Aug 30 07:40:50.022: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/30/23 07:40:50.022
    Aug 30 07:40:50.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pod pause -L testing-label'
    Aug 30 07:40:50.163: INFO: stderr: ""
    Aug 30 07:40:50.163: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/30/23 07:40:50.163
    Aug 30 07:40:50.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 label pods pause testing-label-'
    Aug 30 07:40:50.273: INFO: stderr: ""
    Aug 30 07:40:50.273: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/30/23 07:40:50.273
    Aug 30 07:40:50.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pod pause -L testing-label'
    Aug 30 07:40:50.365: INFO: stderr: ""
    Aug 30 07:40:50.365: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/30/23 07:40:50.365
    Aug 30 07:40:50.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 delete --grace-period=0 --force -f -'
    Aug 30 07:40:50.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:40:50.495: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 30 07:40:50.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get rc,svc -l name=pause --no-headers'
    Aug 30 07:40:50.611: INFO: stderr: "No resources found in kubectl-8673 namespace.\n"
    Aug 30 07:40:50.612: INFO: stdout: ""
    Aug 30 07:40:50.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-8673 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 30 07:40:50.733: INFO: stderr: ""
    Aug 30 07:40:50.733: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:40:50.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8673" for this suite. 08/30/23 07:40:50.747
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:40:50.774
Aug 30 07:40:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:40:50.774
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:50.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:50.97
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:40:50.993
Aug 30 07:40:52.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f" in namespace "downward-api-2034" to be "Succeeded or Failed"
Aug 30 07:40:52.023: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598042ms
Aug 30 07:40:54.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016131121s
Aug 30 07:40:56.033: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016772096s
Aug 30 07:40:58.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016315986s
STEP: Saw pod success 08/30/23 07:40:58.032
Aug 30 07:40:58.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f" satisfied condition "Succeeded or Failed"
Aug 30 07:40:58.041: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f container client-container: <nil>
STEP: delete the pod 08/30/23 07:40:58.107
Aug 30 07:40:58.197: INFO: Waiting for pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f to disappear
Aug 30 07:40:58.205: INFO: Pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:40:58.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2034" for this suite. 08/30/23 07:40:58.224
------------------------------
• [SLOW TEST] [7.475 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:40:50.774
    Aug 30 07:40:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:40:50.774
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:50.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:50.97
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:40:50.993
    Aug 30 07:40:52.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f" in namespace "downward-api-2034" to be "Succeeded or Failed"
    Aug 30 07:40:52.023: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598042ms
    Aug 30 07:40:54.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016131121s
    Aug 30 07:40:56.033: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016772096s
    Aug 30 07:40:58.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016315986s
    STEP: Saw pod success 08/30/23 07:40:58.032
    Aug 30 07:40:58.032: INFO: Pod "downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f" satisfied condition "Succeeded or Failed"
    Aug 30 07:40:58.041: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f container client-container: <nil>
    STEP: delete the pod 08/30/23 07:40:58.107
    Aug 30 07:40:58.197: INFO: Waiting for pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f to disappear
    Aug 30 07:40:58.205: INFO: Pod downwardapi-volume-3c8f4f72-0c49-4aab-bff2-9f7c8151f61f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:40:58.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2034" for this suite. 08/30/23 07:40:58.224
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:40:58.249
Aug 30 07:40:58.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 07:40:58.25
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:58.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:58.402
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-26rn5" 08/30/23 07:40:58.407
Aug 30 07:40:58.533: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-26rn5-3071" 08/30/23 07:40:58.533
Aug 30 07:40:58.659: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-26rn5-3071" 08/30/23 07:40:58.659
Aug 30 07:40:58.713: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:40:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9830" for this suite. 08/30/23 07:40:58.724
STEP: Destroying namespace "e2e-ns-26rn5-3071" for this suite. 08/30/23 07:40:58.771
------------------------------
• [0.558 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:40:58.249
    Aug 30 07:40:58.249: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 07:40:58.25
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:58.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:58.402
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-26rn5" 08/30/23 07:40:58.407
    Aug 30 07:40:58.533: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-26rn5-3071" 08/30/23 07:40:58.533
    Aug 30 07:40:58.659: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-26rn5-3071" 08/30/23 07:40:58.659
    Aug 30 07:40:58.713: INFO: Namespace "e2e-ns-26rn5-3071" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:40:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9830" for this suite. 08/30/23 07:40:58.724
    STEP: Destroying namespace "e2e-ns-26rn5-3071" for this suite. 08/30/23 07:40:58.771
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:40:58.809
Aug 30 07:40:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption 08/30/23 07:40:58.81
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:58.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:58.877
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 30 07:40:58.952: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 30 07:41:59.068: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:41:59.083
Aug 30 07:41:59.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename sched-preemption-path 08/30/23 07:41:59.084
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:41:59.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:41:59.137
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 30 07:41:59.187: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 30 07:41:59.199: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 30 07:41:59.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:41:59.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8320" for this suite. 08/30/23 07:41:59.421
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6393" for this suite. 08/30/23 07:41:59.442
------------------------------
• [SLOW TEST] [60.655 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:40:58.809
    Aug 30 07:40:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption 08/30/23 07:40:58.81
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:40:58.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:40:58.877
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 30 07:40:58.952: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 30 07:41:59.068: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:41:59.083
    Aug 30 07:41:59.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename sched-preemption-path 08/30/23 07:41:59.084
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:41:59.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:41:59.137
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 30 07:41:59.187: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 30 07:41:59.199: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:41:59.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:41:59.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8320" for this suite. 08/30/23 07:41:59.421
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6393" for this suite. 08/30/23 07:41:59.442
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:41:59.465
Aug 30 07:41:59.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:41:59.466
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:41:59.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:41:59.525
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:41:59.542
Aug 30 07:41:59.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14" in namespace "projected-5850" to be "Succeeded or Failed"
Aug 30 07:41:59.577: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900697ms
Aug 30 07:42:01.589: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024739083s
Aug 30 07:42:03.597: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03312347s
Aug 30 07:42:05.591: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027139687s
STEP: Saw pod success 08/30/23 07:42:05.591
Aug 30 07:42:05.591: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14" satisfied condition "Succeeded or Failed"
Aug 30 07:42:05.601: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 container client-container: <nil>
STEP: delete the pod 08/30/23 07:42:05.635
Aug 30 07:42:05.695: INFO: Waiting for pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 to disappear
Aug 30 07:42:05.704: INFO: Pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:05.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5850" for this suite. 08/30/23 07:42:05.721
------------------------------
• [SLOW TEST] [6.297 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:41:59.465
    Aug 30 07:41:59.465: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:41:59.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:41:59.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:41:59.525
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:41:59.542
    Aug 30 07:41:59.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14" in namespace "projected-5850" to be "Succeeded or Failed"
    Aug 30 07:41:59.577: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 12.900697ms
    Aug 30 07:42:01.589: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024739083s
    Aug 30 07:42:03.597: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03312347s
    Aug 30 07:42:05.591: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027139687s
    STEP: Saw pod success 08/30/23 07:42:05.591
    Aug 30 07:42:05.591: INFO: Pod "downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14" satisfied condition "Succeeded or Failed"
    Aug 30 07:42:05.601: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:42:05.635
    Aug 30 07:42:05.695: INFO: Waiting for pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 to disappear
    Aug 30 07:42:05.704: INFO: Pod downwardapi-volume-7505d34a-a1cd-4d1c-996e-522a06ee6e14 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:05.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5850" for this suite. 08/30/23 07:42:05.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:05.764
Aug 30 07:42:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:42:05.765
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:05.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:05.89
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/30/23 07:42:05.897
Aug 30 07:42:05.897: INFO: namespace kubectl-3661
Aug 30 07:42:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 create -f -'
Aug 30 07:42:06.404: INFO: stderr: ""
Aug 30 07:42:06.404: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/30/23 07:42:06.404
Aug 30 07:42:07.413: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:42:07.413: INFO: Found 0 / 1
Aug 30 07:42:08.413: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:42:08.413: INFO: Found 1 / 1
Aug 30 07:42:08.413: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 07:42:08.421: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:42:08.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 07:42:08.421: INFO: wait on agnhost-primary startup in kubectl-3661 
Aug 30 07:42:08.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 logs agnhost-primary-8sgzh agnhost-primary'
Aug 30 07:42:08.571: INFO: stderr: ""
Aug 30 07:42:08.571: INFO: stdout: "Paused\n"
STEP: exposing RC 08/30/23 07:42:08.571
Aug 30 07:42:08.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 30 07:42:08.777: INFO: stderr: ""
Aug 30 07:42:08.778: INFO: stdout: "service/rm2 exposed\n"
Aug 30 07:42:08.786: INFO: Service rm2 in namespace kubectl-3661 found.
STEP: exposing service 08/30/23 07:42:10.832
Aug 30 07:42:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 30 07:42:11.009: INFO: stderr: ""
Aug 30 07:42:11.009: INFO: stdout: "service/rm3 exposed\n"
Aug 30 07:42:11.110: INFO: Service rm3 in namespace kubectl-3661 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:13.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3661" for this suite. 08/30/23 07:42:13.155
------------------------------
• [SLOW TEST] [7.412 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:05.764
    Aug 30 07:42:05.765: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:42:05.765
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:05.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:05.89
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/30/23 07:42:05.897
    Aug 30 07:42:05.897: INFO: namespace kubectl-3661
    Aug 30 07:42:05.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 create -f -'
    Aug 30 07:42:06.404: INFO: stderr: ""
    Aug 30 07:42:06.404: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/30/23 07:42:06.404
    Aug 30 07:42:07.413: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:42:07.413: INFO: Found 0 / 1
    Aug 30 07:42:08.413: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:42:08.413: INFO: Found 1 / 1
    Aug 30 07:42:08.413: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 30 07:42:08.421: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:42:08.421: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 30 07:42:08.421: INFO: wait on agnhost-primary startup in kubectl-3661 
    Aug 30 07:42:08.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 logs agnhost-primary-8sgzh agnhost-primary'
    Aug 30 07:42:08.571: INFO: stderr: ""
    Aug 30 07:42:08.571: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/30/23 07:42:08.571
    Aug 30 07:42:08.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 30 07:42:08.777: INFO: stderr: ""
    Aug 30 07:42:08.778: INFO: stdout: "service/rm2 exposed\n"
    Aug 30 07:42:08.786: INFO: Service rm2 in namespace kubectl-3661 found.
    STEP: exposing service 08/30/23 07:42:10.832
    Aug 30 07:42:10.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-3661 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 30 07:42:11.009: INFO: stderr: ""
    Aug 30 07:42:11.009: INFO: stdout: "service/rm3 exposed\n"
    Aug 30 07:42:11.110: INFO: Service rm3 in namespace kubectl-3661 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:13.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3661" for this suite. 08/30/23 07:42:13.155
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:13.184
Aug 30 07:42:13.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:42:13.186
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:13.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:13.292
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:42:13.341
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:42:14.057
STEP: Deploying the webhook pod 08/30/23 07:42:14.084
STEP: Wait for the deployment to be ready 08/30/23 07:42:14.129
Aug 30 07:42:14.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 07:42:16.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 07:42:18.219
STEP: Verifying the service has paired with the endpoint 08/30/23 07:42:18.27
Aug 30 07:42:19.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 30 07:42:19.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5801-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 07:42:19.826
STEP: Creating a custom resource while v1 is storage version 08/30/23 07:42:19.906
STEP: Patching Custom Resource Definition to set v2 as storage 08/30/23 07:42:22.037
STEP: Patching the custom resource while v2 is storage version 08/30/23 07:42:22.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1775" for this suite. 08/30/23 07:42:22.866
STEP: Destroying namespace "webhook-1775-markers" for this suite. 08/30/23 07:42:22.924
------------------------------
• [SLOW TEST] [9.800 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:13.184
    Aug 30 07:42:13.184: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:42:13.186
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:13.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:13.292
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:42:13.341
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:42:14.057
    STEP: Deploying the webhook pod 08/30/23 07:42:14.084
    STEP: Wait for the deployment to be ready 08/30/23 07:42:14.129
    Aug 30 07:42:14.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 07:42:16.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 42, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 07:42:18.219
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:42:18.27
    Aug 30 07:42:19.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 30 07:42:19.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5801-crds.webhook.example.com via the AdmissionRegistration API 08/30/23 07:42:19.826
    STEP: Creating a custom resource while v1 is storage version 08/30/23 07:42:19.906
    STEP: Patching Custom Resource Definition to set v2 as storage 08/30/23 07:42:22.037
    STEP: Patching the custom resource while v2 is storage version 08/30/23 07:42:22.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1775" for this suite. 08/30/23 07:42:22.866
    STEP: Destroying namespace "webhook-1775-markers" for this suite. 08/30/23 07:42:22.924
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:22.986
Aug 30 07:42:22.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename containers 08/30/23 07:42:22.987
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:23.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:23.102
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/30/23 07:42:23.108
Aug 30 07:42:24.170: INFO: Waiting up to 5m0s for pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2" in namespace "containers-9005" to be "Succeeded or Failed"
Aug 30 07:42:24.178: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261704ms
Aug 30 07:42:26.187: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01729353s
Aug 30 07:42:28.194: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024071542s
STEP: Saw pod success 08/30/23 07:42:28.194
Aug 30 07:42:28.194: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2" satisfied condition "Succeeded or Failed"
Aug 30 07:42:28.217: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:42:28.293
Aug 30 07:42:28.337: INFO: Waiting for pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 to disappear
Aug 30 07:42:28.348: INFO: Pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:28.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9005" for this suite. 08/30/23 07:42:28.36
------------------------------
• [SLOW TEST] [5.397 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:22.986
    Aug 30 07:42:22.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename containers 08/30/23 07:42:22.987
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:23.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:23.102
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/30/23 07:42:23.108
    Aug 30 07:42:24.170: INFO: Waiting up to 5m0s for pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2" in namespace "containers-9005" to be "Succeeded or Failed"
    Aug 30 07:42:24.178: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.261704ms
    Aug 30 07:42:26.187: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01729353s
    Aug 30 07:42:28.194: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024071542s
    STEP: Saw pod success 08/30/23 07:42:28.194
    Aug 30 07:42:28.194: INFO: Pod "client-containers-56df1e72-b229-432a-8234-a53f97c33ff2" satisfied condition "Succeeded or Failed"
    Aug 30 07:42:28.217: INFO: Trying to get logs from node 10.135.139.190 pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:42:28.293
    Aug 30 07:42:28.337: INFO: Waiting for pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 to disappear
    Aug 30 07:42:28.348: INFO: Pod client-containers-56df1e72-b229-432a-8234-a53f97c33ff2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:28.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9005" for this suite. 08/30/23 07:42:28.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:28.387
Aug 30 07:42:28.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename subpath 08/30/23 07:42:28.389
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:28.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:28.495
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/30/23 07:42:28.5
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-6ftk 08/30/23 07:42:28.546
STEP: Creating a pod to test atomic-volume-subpath 08/30/23 07:42:28.546
Aug 30 07:42:28.575: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6ftk" in namespace "subpath-961" to be "Succeeded or Failed"
Aug 30 07:42:28.585: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.520683ms
Aug 30 07:42:30.594: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01913373s
Aug 30 07:42:32.596: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 4.021274585s
Aug 30 07:42:34.595: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 6.020190761s
Aug 30 07:42:36.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 8.018832493s
Aug 30 07:42:38.594: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 10.019327035s
Aug 30 07:42:40.601: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 12.026355725s
Aug 30 07:42:42.600: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 14.025170517s
Aug 30 07:42:44.598: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 16.023779558s
Aug 30 07:42:46.609: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 18.034255983s
Aug 30 07:42:48.599: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 20.024685921s
Aug 30 07:42:50.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=false. Elapsed: 22.018649151s
Aug 30 07:42:52.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018478387s
STEP: Saw pod success 08/30/23 07:42:52.593
Aug 30 07:42:52.593: INFO: Pod "pod-subpath-test-secret-6ftk" satisfied condition "Succeeded or Failed"
Aug 30 07:42:52.601: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-secret-6ftk container test-container-subpath-secret-6ftk: <nil>
STEP: delete the pod 08/30/23 07:42:52.628
Aug 30 07:42:52.707: INFO: Waiting for pod pod-subpath-test-secret-6ftk to disappear
Aug 30 07:42:52.722: INFO: Pod pod-subpath-test-secret-6ftk no longer exists
STEP: Deleting pod pod-subpath-test-secret-6ftk 08/30/23 07:42:52.722
Aug 30 07:42:52.722: INFO: Deleting pod "pod-subpath-test-secret-6ftk" in namespace "subpath-961"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:52.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-961" for this suite. 08/30/23 07:42:52.749
------------------------------
• [SLOW TEST] [24.381 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:28.387
    Aug 30 07:42:28.388: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename subpath 08/30/23 07:42:28.389
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:28.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:28.495
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/30/23 07:42:28.5
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-6ftk 08/30/23 07:42:28.546
    STEP: Creating a pod to test atomic-volume-subpath 08/30/23 07:42:28.546
    Aug 30 07:42:28.575: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-6ftk" in namespace "subpath-961" to be "Succeeded or Failed"
    Aug 30 07:42:28.585: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Pending", Reason="", readiness=false. Elapsed: 10.520683ms
    Aug 30 07:42:30.594: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01913373s
    Aug 30 07:42:32.596: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 4.021274585s
    Aug 30 07:42:34.595: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 6.020190761s
    Aug 30 07:42:36.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 8.018832493s
    Aug 30 07:42:38.594: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 10.019327035s
    Aug 30 07:42:40.601: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 12.026355725s
    Aug 30 07:42:42.600: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 14.025170517s
    Aug 30 07:42:44.598: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 16.023779558s
    Aug 30 07:42:46.609: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 18.034255983s
    Aug 30 07:42:48.599: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=true. Elapsed: 20.024685921s
    Aug 30 07:42:50.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Running", Reason="", readiness=false. Elapsed: 22.018649151s
    Aug 30 07:42:52.593: INFO: Pod "pod-subpath-test-secret-6ftk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.018478387s
    STEP: Saw pod success 08/30/23 07:42:52.593
    Aug 30 07:42:52.593: INFO: Pod "pod-subpath-test-secret-6ftk" satisfied condition "Succeeded or Failed"
    Aug 30 07:42:52.601: INFO: Trying to get logs from node 10.135.139.190 pod pod-subpath-test-secret-6ftk container test-container-subpath-secret-6ftk: <nil>
    STEP: delete the pod 08/30/23 07:42:52.628
    Aug 30 07:42:52.707: INFO: Waiting for pod pod-subpath-test-secret-6ftk to disappear
    Aug 30 07:42:52.722: INFO: Pod pod-subpath-test-secret-6ftk no longer exists
    STEP: Deleting pod pod-subpath-test-secret-6ftk 08/30/23 07:42:52.722
    Aug 30 07:42:52.722: INFO: Deleting pod "pod-subpath-test-secret-6ftk" in namespace "subpath-961"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:52.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-961" for this suite. 08/30/23 07:42:52.749
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:52.769
Aug 30 07:42:52.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:42:52.77
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:52.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:52.87
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-fe267518-902f-45bd-a3ef-abb9864bea20 08/30/23 07:42:52.935
STEP: Creating a pod to test consume configMaps 08/30/23 07:42:52.991
W0830 07:42:53.045234      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:42:53.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec" in namespace "projected-1110" to be "Succeeded or Failed"
Aug 30 07:42:53.073: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 27.782144ms
Aug 30 07:42:55.083: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038560693s
Aug 30 07:42:57.099: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054327774s
Aug 30 07:42:59.123: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077999575s
STEP: Saw pod success 08/30/23 07:42:59.123
Aug 30 07:42:59.123: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec" satisfied condition "Succeeded or Failed"
Aug 30 07:42:59.144: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:42:59.182
Aug 30 07:42:59.219: INFO: Waiting for pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec to disappear
Aug 30 07:42:59.232: INFO: Pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:42:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1110" for this suite. 08/30/23 07:42:59.243
------------------------------
• [SLOW TEST] [6.539 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:52.769
    Aug 30 07:42:52.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:42:52.77
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:52.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:52.87
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-fe267518-902f-45bd-a3ef-abb9864bea20 08/30/23 07:42:52.935
    STEP: Creating a pod to test consume configMaps 08/30/23 07:42:52.991
    W0830 07:42:53.045234      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:42:53.045: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec" in namespace "projected-1110" to be "Succeeded or Failed"
    Aug 30 07:42:53.073: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 27.782144ms
    Aug 30 07:42:55.083: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038560693s
    Aug 30 07:42:57.099: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054327774s
    Aug 30 07:42:59.123: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077999575s
    STEP: Saw pod success 08/30/23 07:42:59.123
    Aug 30 07:42:59.123: INFO: Pod "pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec" satisfied condition "Succeeded or Failed"
    Aug 30 07:42:59.144: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:42:59.182
    Aug 30 07:42:59.219: INFO: Waiting for pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec to disappear
    Aug 30 07:42:59.232: INFO: Pod pod-projected-configmaps-bc1309ef-d235-4573-b702-4966215c18ec no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:42:59.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1110" for this suite. 08/30/23 07:42:59.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:42:59.309
Aug 30 07:42:59.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context-test 08/30/23 07:42:59.311
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:59.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:59.45
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 30 07:42:59.545: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515" in namespace "security-context-test-2040" to be "Succeeded or Failed"
Aug 30 07:42:59.569: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Pending", Reason="", readiness=false. Elapsed: 23.395919ms
Aug 30 07:43:01.604: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058433663s
Aug 30 07:43:03.580: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035151476s
Aug 30 07:43:03.581: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515" satisfied condition "Succeeded or Failed"
Aug 30 07:43:03.714: INFO: Got logs for pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2040" for this suite. 08/30/23 07:43:03.726
------------------------------
• [4.449 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:42:59.309
    Aug 30 07:42:59.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context-test 08/30/23 07:42:59.311
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:42:59.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:42:59.45
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 30 07:42:59.545: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515" in namespace "security-context-test-2040" to be "Succeeded or Failed"
    Aug 30 07:42:59.569: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Pending", Reason="", readiness=false. Elapsed: 23.395919ms
    Aug 30 07:43:01.604: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058433663s
    Aug 30 07:43:03.580: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035151476s
    Aug 30 07:43:03.581: INFO: Pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515" satisfied condition "Succeeded or Failed"
    Aug 30 07:43:03.714: INFO: Got logs for pod "busybox-privileged-false-42028c61-7a6c-48b3-b8cb-c50c88ce0515": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:03.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2040" for this suite. 08/30/23 07:43:03.726
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:03.759
Aug 30 07:43:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename hostport 08/30/23 07:43:03.76
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:03.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:03.847
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/30/23 07:43:03.898
Aug 30 07:43:03.971: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9791" to be "running and ready"
Aug 30 07:43:03.994: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.844518ms
Aug 30 07:43:03.994: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:06.008: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037103378s
Aug 30 07:43:06.008: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:08.014: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.043183844s
Aug 30 07:43:08.014: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 30 07:43:08.014: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.135.139.185 on the node which pod1 resides and expect scheduled 08/30/23 07:43:08.014
Aug 30 07:43:08.043: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9791" to be "running and ready"
Aug 30 07:43:08.107: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 63.70844ms
Aug 30 07:43:08.107: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:10.123: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079674761s
Aug 30 07:43:10.123: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:12.116: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.072854202s
Aug 30 07:43:12.116: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 30 07:43:12.116: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.135.139.185 but use UDP protocol on the node which pod2 resides 08/30/23 07:43:12.116
Aug 30 07:43:12.156: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9791" to be "running and ready"
Aug 30 07:43:12.165: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.420276ms
Aug 30 07:43:12.165: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:14.174: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017321747s
Aug 30 07:43:14.174: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:16.173: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.017040714s
Aug 30 07:43:16.174: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 30 07:43:16.174: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 30 07:43:16.196: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9791" to be "running and ready"
Aug 30 07:43:16.203: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506633ms
Aug 30 07:43:16.203: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:18.213: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.017346394s
Aug 30 07:43:18.213: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 30 07:43:18.213: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/30/23 07:43:18.221
Aug 30 07:43:18.221: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.135.139.185 http://127.0.0.1:54323/hostname] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:43:18.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:43:18.221: INFO: ExecWithOptions: Clientset creation
Aug 30 07:43:18.222: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.135.139.185+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.135.139.185, port: 54323 08/30/23 07:43:18.493
Aug 30 07:43:18.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.135.139.185:54323/hostname] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:43:18.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:43:18.494: INFO: ExecWithOptions: Clientset creation
Aug 30 07:43:18.494: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.135.139.185%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.135.139.185, port: 54323 UDP 08/30/23 07:43:18.695
Aug 30 07:43:18.695: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.135.139.185 54323] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 30 07:43:18.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:43:18.696: INFO: ExecWithOptions: Clientset creation
Aug 30 07:43:18.696: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.135.139.185+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9791" for this suite. 08/30/23 07:43:23.881
------------------------------
• [SLOW TEST] [20.141 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:03.759
    Aug 30 07:43:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename hostport 08/30/23 07:43:03.76
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:03.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:03.847
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/30/23 07:43:03.898
    Aug 30 07:43:03.971: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9791" to be "running and ready"
    Aug 30 07:43:03.994: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.844518ms
    Aug 30 07:43:03.994: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:06.008: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037103378s
    Aug 30 07:43:06.008: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:08.014: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.043183844s
    Aug 30 07:43:08.014: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 30 07:43:08.014: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.135.139.185 on the node which pod1 resides and expect scheduled 08/30/23 07:43:08.014
    Aug 30 07:43:08.043: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9791" to be "running and ready"
    Aug 30 07:43:08.107: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 63.70844ms
    Aug 30 07:43:08.107: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:10.123: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079674761s
    Aug 30 07:43:10.123: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:12.116: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.072854202s
    Aug 30 07:43:12.116: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 30 07:43:12.116: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.135.139.185 but use UDP protocol on the node which pod2 resides 08/30/23 07:43:12.116
    Aug 30 07:43:12.156: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9791" to be "running and ready"
    Aug 30 07:43:12.165: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.420276ms
    Aug 30 07:43:12.165: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:14.174: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017321747s
    Aug 30 07:43:14.174: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:16.173: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.017040714s
    Aug 30 07:43:16.174: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 30 07:43:16.174: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 30 07:43:16.196: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9791" to be "running and ready"
    Aug 30 07:43:16.203: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.506633ms
    Aug 30 07:43:16.203: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:18.213: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.017346394s
    Aug 30 07:43:18.213: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 30 07:43:18.213: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/30/23 07:43:18.221
    Aug 30 07:43:18.221: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.135.139.185 http://127.0.0.1:54323/hostname] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:43:18.221: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:43:18.221: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:43:18.222: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.135.139.185+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.135.139.185, port: 54323 08/30/23 07:43:18.493
    Aug 30 07:43:18.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.135.139.185:54323/hostname] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:43:18.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:43:18.494: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:43:18.494: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.135.139.185%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.135.139.185, port: 54323 UDP 08/30/23 07:43:18.695
    Aug 30 07:43:18.695: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.135.139.185 54323] Namespace:hostport-9791 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 30 07:43:18.695: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:43:18.696: INFO: ExecWithOptions: Clientset creation
    Aug 30 07:43:18.696: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-9791/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.135.139.185+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:23.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9791" for this suite. 08/30/23 07:43:23.881
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:23.901
Aug 30 07:43:23.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:43:23.902
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:23.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:23.966
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/30/23 07:43:24.007
STEP: watching for Pod to be ready 08/30/23 07:43:24.027
Aug 30 07:43:24.030: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 30 07:43:24.071: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
Aug 30 07:43:24.149: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
Aug 30 07:43:24.841: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
Aug 30 07:43:24.920: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
Aug 30 07:43:25.342: INFO: Found Pod pod-test in namespace pods-3397 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/30/23 07:43:25.351
STEP: getting the Pod and ensuring that it's patched 08/30/23 07:43:25.368
STEP: replacing the Pod's status Ready condition to False 08/30/23 07:43:25.375
STEP: check the Pod again to ensure its Ready conditions are False 08/30/23 07:43:25.399
STEP: deleting the Pod via a Collection with a LabelSelector 08/30/23 07:43:25.399
STEP: watching for the Pod to be deleted 08/30/23 07:43:25.419
Aug 30 07:43:25.422: INFO: observed event type MODIFIED
Aug 30 07:43:27.329: INFO: observed event type MODIFIED
Aug 30 07:43:27.611: INFO: observed event type MODIFIED
Aug 30 07:43:28.360: INFO: observed event type MODIFIED
Aug 30 07:43:28.378: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:28.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3397" for this suite. 08/30/23 07:43:28.432
------------------------------
• [4.558 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:23.901
    Aug 30 07:43:23.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:43:23.902
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:23.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:23.966
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/30/23 07:43:24.007
    STEP: watching for Pod to be ready 08/30/23 07:43:24.027
    Aug 30 07:43:24.030: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 30 07:43:24.071: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
    Aug 30 07:43:24.149: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
    Aug 30 07:43:24.841: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
    Aug 30 07:43:24.920: INFO: observed Pod pod-test in namespace pods-3397 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
    Aug 30 07:43:25.342: INFO: Found Pod pod-test in namespace pods-3397 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-30 07:43:24 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/30/23 07:43:25.351
    STEP: getting the Pod and ensuring that it's patched 08/30/23 07:43:25.368
    STEP: replacing the Pod's status Ready condition to False 08/30/23 07:43:25.375
    STEP: check the Pod again to ensure its Ready conditions are False 08/30/23 07:43:25.399
    STEP: deleting the Pod via a Collection with a LabelSelector 08/30/23 07:43:25.399
    STEP: watching for the Pod to be deleted 08/30/23 07:43:25.419
    Aug 30 07:43:25.422: INFO: observed event type MODIFIED
    Aug 30 07:43:27.329: INFO: observed event type MODIFIED
    Aug 30 07:43:27.611: INFO: observed event type MODIFIED
    Aug 30 07:43:28.360: INFO: observed event type MODIFIED
    Aug 30 07:43:28.378: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:28.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3397" for this suite. 08/30/23 07:43:28.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:28.46
Aug 30 07:43:28.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir-wrapper 08/30/23 07:43:28.464
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:28.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:28.538
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 30 07:43:28.616: INFO: Waiting up to 5m0s for pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f" in namespace "emptydir-wrapper-4588" to be "running and ready"
Aug 30 07:43:28.624: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010954ms
Aug 30 07:43:28.624: INFO: The phase of Pod pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:43:30.644: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.028240758s
Aug 30 07:43:30.644: INFO: The phase of Pod pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f is Running (Ready = true)
Aug 30 07:43:30.644: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/30/23 07:43:30.653
STEP: Cleaning up the configmap 08/30/23 07:43:30.677
STEP: Cleaning up the pod 08/30/23 07:43:30.698
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:30.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-4588" for this suite. 08/30/23 07:43:30.733
------------------------------
• [2.293 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:28.46
    Aug 30 07:43:28.461: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir-wrapper 08/30/23 07:43:28.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:28.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:28.538
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 30 07:43:28.616: INFO: Waiting up to 5m0s for pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f" in namespace "emptydir-wrapper-4588" to be "running and ready"
    Aug 30 07:43:28.624: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010954ms
    Aug 30 07:43:28.624: INFO: The phase of Pod pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:43:30.644: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.028240758s
    Aug 30 07:43:30.644: INFO: The phase of Pod pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f is Running (Ready = true)
    Aug 30 07:43:30.644: INFO: Pod "pod-secrets-0e69368f-e9d8-495d-8ae9-5890cd76bc7f" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/30/23 07:43:30.653
    STEP: Cleaning up the configmap 08/30/23 07:43:30.677
    STEP: Cleaning up the pod 08/30/23 07:43:30.698
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:30.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-4588" for this suite. 08/30/23 07:43:30.733
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:30.753
Aug 30 07:43:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:43:30.754
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:30.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:30.818
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:43:30.823
Aug 30 07:43:31.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17" in namespace "downward-api-8758" to be "Succeeded or Failed"
Aug 30 07:43:31.882: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Pending", Reason="", readiness=false. Elapsed: 10.624508ms
Aug 30 07:43:33.901: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029651132s
Aug 30 07:43:35.934: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062890411s
STEP: Saw pod success 08/30/23 07:43:35.957
Aug 30 07:43:35.957: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17" satisfied condition "Succeeded or Failed"
Aug 30 07:43:35.975: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 container client-container: <nil>
STEP: delete the pod 08/30/23 07:43:36.081
Aug 30 07:43:36.120: INFO: Waiting for pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 to disappear
Aug 30 07:43:36.133: INFO: Pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:36.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8758" for this suite. 08/30/23 07:43:36.159
------------------------------
• [SLOW TEST] [5.432 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:30.753
    Aug 30 07:43:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:43:30.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:30.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:30.818
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:43:30.823
    Aug 30 07:43:31.871: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17" in namespace "downward-api-8758" to be "Succeeded or Failed"
    Aug 30 07:43:31.882: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Pending", Reason="", readiness=false. Elapsed: 10.624508ms
    Aug 30 07:43:33.901: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029651132s
    Aug 30 07:43:35.934: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.062890411s
    STEP: Saw pod success 08/30/23 07:43:35.957
    Aug 30 07:43:35.957: INFO: Pod "downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17" satisfied condition "Succeeded or Failed"
    Aug 30 07:43:35.975: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:43:36.081
    Aug 30 07:43:36.120: INFO: Waiting for pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 to disappear
    Aug 30 07:43:36.133: INFO: Pod downwardapi-volume-b22f4f92-647b-4879-8094-086a9d068f17 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:36.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8758" for this suite. 08/30/23 07:43:36.159
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:36.21
Aug 30 07:43:36.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:43:36.211
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:36.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:36.3
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:36.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8561" for this suite. 08/30/23 07:43:36.566
------------------------------
• [0.389 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:36.21
    Aug 30 07:43:36.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:43:36.211
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:36.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:36.3
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:36.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8561" for this suite. 08/30/23 07:43:36.566
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:36.603
Aug 30 07:43:36.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename containers 08/30/23 07:43:36.604
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:36.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:36.701
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 30 07:43:36.760: INFO: Waiting up to 5m0s for pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf" in namespace "containers-3795" to be "running"
Aug 30 07:43:36.803: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf": Phase="Pending", Reason="", readiness=false. Elapsed: 42.920887ms
Aug 30 07:43:38.832: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf": Phase="Running", Reason="", readiness=true. Elapsed: 2.071303979s
Aug 30 07:43:38.837: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:38.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3795" for this suite. 08/30/23 07:43:38.872
------------------------------
• [2.293 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:36.603
    Aug 30 07:43:36.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename containers 08/30/23 07:43:36.604
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:36.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:36.701
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 30 07:43:36.760: INFO: Waiting up to 5m0s for pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf" in namespace "containers-3795" to be "running"
    Aug 30 07:43:36.803: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf": Phase="Pending", Reason="", readiness=false. Elapsed: 42.920887ms
    Aug 30 07:43:38.832: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf": Phase="Running", Reason="", readiness=true. Elapsed: 2.071303979s
    Aug 30 07:43:38.837: INFO: Pod "client-containers-c6270391-fc64-4020-a19e-b127d6cb4bcf" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:38.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3795" for this suite. 08/30/23 07:43:38.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:38.898
Aug 30 07:43:38.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename daemonsets 08/30/23 07:43:38.899
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:38.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:39.004
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Aug 30 07:43:39.169: INFO: Create a RollingUpdate DaemonSet
Aug 30 07:43:39.205: INFO: Check that daemon pods launch on every node of the cluster
Aug 30 07:43:39.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:43:39.240: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:43:40.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:43:40.323: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:43:41.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:43:41.267: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
Aug 30 07:43:42.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 30 07:43:42.266: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Aug 30 07:43:42.266: INFO: Update the DaemonSet to trigger a rollout
Aug 30 07:43:42.341: INFO: Updating DaemonSet daemon-set
Aug 30 07:43:45.376: INFO: Roll back the DaemonSet before rollout is complete
Aug 30 07:43:45.413: INFO: Updating DaemonSet daemon-set
Aug 30 07:43:45.413: INFO: Make sure DaemonSet rollback is complete
Aug 30 07:43:45.448: INFO: Wrong image for pod: daemon-set-skt2z. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 30 07:43:45.448: INFO: Pod daemon-set-skt2z is not available
Aug 30 07:43:49.474: INFO: Pod daemon-set-6559t is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:43:49.534
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8556, will wait for the garbage collector to delete the pods 08/30/23 07:43:49.534
Aug 30 07:43:49.698: INFO: Deleting DaemonSet.extensions daemon-set took: 40.695075ms
Aug 30 07:43:49.800: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.737584ms
Aug 30 07:43:52.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 30 07:43:52.610: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 30 07:43:52.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"122187"},"items":null}

Aug 30 07:43:52.628: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"122187"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8556" for this suite. 08/30/23 07:43:52.677
------------------------------
• [SLOW TEST] [13.797 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:38.898
    Aug 30 07:43:38.898: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename daemonsets 08/30/23 07:43:38.899
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:38.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:39.004
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Aug 30 07:43:39.169: INFO: Create a RollingUpdate DaemonSet
    Aug 30 07:43:39.205: INFO: Check that daemon pods launch on every node of the cluster
    Aug 30 07:43:39.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:43:39.240: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:43:40.321: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:43:40.323: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:43:41.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:43:41.267: INFO: Node 10.135.139.183 is running 0 daemon pod, expected 1
    Aug 30 07:43:42.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 30 07:43:42.266: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Aug 30 07:43:42.266: INFO: Update the DaemonSet to trigger a rollout
    Aug 30 07:43:42.341: INFO: Updating DaemonSet daemon-set
    Aug 30 07:43:45.376: INFO: Roll back the DaemonSet before rollout is complete
    Aug 30 07:43:45.413: INFO: Updating DaemonSet daemon-set
    Aug 30 07:43:45.413: INFO: Make sure DaemonSet rollback is complete
    Aug 30 07:43:45.448: INFO: Wrong image for pod: daemon-set-skt2z. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 30 07:43:45.448: INFO: Pod daemon-set-skt2z is not available
    Aug 30 07:43:49.474: INFO: Pod daemon-set-6559t is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 08/30/23 07:43:49.534
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8556, will wait for the garbage collector to delete the pods 08/30/23 07:43:49.534
    Aug 30 07:43:49.698: INFO: Deleting DaemonSet.extensions daemon-set took: 40.695075ms
    Aug 30 07:43:49.800: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.737584ms
    Aug 30 07:43:52.610: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 30 07:43:52.610: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 30 07:43:52.619: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"122187"},"items":null}

    Aug 30 07:43:52.628: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"122187"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:52.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8556" for this suite. 08/30/23 07:43:52.677
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:52.696
Aug 30 07:43:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:43:52.698
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:52.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:52.766
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/30/23 07:43:52.772
STEP: fetching the ConfigMap 08/30/23 07:43:52.792
STEP: patching the ConfigMap 08/30/23 07:43:52.805
STEP: listing all ConfigMaps in all namespaces with a label selector 08/30/23 07:43:52.818
STEP: deleting the ConfigMap by collection with a label selector 08/30/23 07:43:53.056
STEP: listing all ConfigMaps in test namespace 08/30/23 07:43:53.086
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5138" for this suite. 08/30/23 07:43:53.108
------------------------------
• [0.436 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:52.696
    Aug 30 07:43:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:43:52.698
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:52.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:52.766
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/30/23 07:43:52.772
    STEP: fetching the ConfigMap 08/30/23 07:43:52.792
    STEP: patching the ConfigMap 08/30/23 07:43:52.805
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/30/23 07:43:52.818
    STEP: deleting the ConfigMap by collection with a label selector 08/30/23 07:43:53.056
    STEP: listing all ConfigMaps in test namespace 08/30/23 07:43:53.086
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:53.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5138" for this suite. 08/30/23 07:43:53.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:53.135
Aug 30 07:43:53.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:43:53.135
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:53.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:53.211
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:43:53.276
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:43:53.524
STEP: Deploying the webhook pod 08/30/23 07:43:53.566
STEP: Wait for the deployment to be ready 08/30/23 07:43:53.598
Aug 30 07:43:53.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 30 07:43:55.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/30/23 07:43:57.66
STEP: Verifying the service has paired with the endpoint 08/30/23 07:43:57.687
Aug 30 07:43:58.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/30/23 07:43:58.988
STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 07:43:59.044
STEP: Deleting the collection of validation webhooks 08/30/23 07:43:59.086
STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 07:43:59.244
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:43:59.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5285" for this suite. 08/30/23 07:43:59.442
STEP: Destroying namespace "webhook-5285-markers" for this suite. 08/30/23 07:43:59.484
------------------------------
• [SLOW TEST] [6.370 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:53.135
    Aug 30 07:43:53.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:43:53.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:53.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:53.211
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:43:53.276
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:43:53.524
    STEP: Deploying the webhook pod 08/30/23 07:43:53.566
    STEP: Wait for the deployment to be ready 08/30/23 07:43:53.598
    Aug 30 07:43:53.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 30 07:43:55.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 43, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/30/23 07:43:57.66
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:43:57.687
    Aug 30 07:43:58.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/30/23 07:43:58.988
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 07:43:59.044
    STEP: Deleting the collection of validation webhooks 08/30/23 07:43:59.086
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/30/23 07:43:59.244
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:43:59.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5285" for this suite. 08/30/23 07:43:59.442
    STEP: Destroying namespace "webhook-5285-markers" for this suite. 08/30/23 07:43:59.484
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:43:59.505
Aug 30 07:43:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:43:59.506
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:59.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:59.664
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/30/23 07:43:59.669
Aug 30 07:43:59.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 create -f -'
Aug 30 07:44:00.387: INFO: stderr: ""
Aug 30 07:44:00.387: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/30/23 07:44:00.387
Aug 30 07:44:00.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 diff -f -'
Aug 30 07:44:02.661: INFO: rc: 1
Aug 30 07:44:02.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 delete -f -'
Aug 30 07:44:02.772: INFO: stderr: ""
Aug 30 07:44:02.772: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:44:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5266" for this suite. 08/30/23 07:44:02.783
------------------------------
• [3.326 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:43:59.505
    Aug 30 07:43:59.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:43:59.506
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:43:59.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:43:59.664
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/30/23 07:43:59.669
    Aug 30 07:43:59.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 create -f -'
    Aug 30 07:44:00.387: INFO: stderr: ""
    Aug 30 07:44:00.387: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/30/23 07:44:00.387
    Aug 30 07:44:00.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 diff -f -'
    Aug 30 07:44:02.661: INFO: rc: 1
    Aug 30 07:44:02.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-5266 delete -f -'
    Aug 30 07:44:02.772: INFO: stderr: ""
    Aug 30 07:44:02.772: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:44:02.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5266" for this suite. 08/30/23 07:44:02.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:44:02.835
Aug 30 07:44:02.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:44:02.836
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:44:02.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:44:02.899
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 30 07:44:02.967: INFO: created pod pod-service-account-defaultsa
Aug 30 07:44:02.967: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 30 07:44:02.988: INFO: created pod pod-service-account-mountsa
Aug 30 07:44:02.988: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 30 07:44:03.043: INFO: created pod pod-service-account-nomountsa
Aug 30 07:44:03.043: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 30 07:44:03.067: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 30 07:44:03.067: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 30 07:44:03.144: INFO: created pod pod-service-account-mountsa-mountspec
Aug 30 07:44:03.144: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 30 07:44:03.170: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 30 07:44:03.170: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 30 07:44:03.192: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 30 07:44:03.192: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 30 07:44:03.205: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 30 07:44:03.206: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 30 07:44:03.222: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 30 07:44:03.222: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 07:44:03.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5126" for this suite. 08/30/23 07:44:03.233
------------------------------
• [0.425 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:44:02.835
    Aug 30 07:44:02.835: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:44:02.836
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:44:02.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:44:02.899
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 30 07:44:02.967: INFO: created pod pod-service-account-defaultsa
    Aug 30 07:44:02.967: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 30 07:44:02.988: INFO: created pod pod-service-account-mountsa
    Aug 30 07:44:02.988: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 30 07:44:03.043: INFO: created pod pod-service-account-nomountsa
    Aug 30 07:44:03.043: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 30 07:44:03.067: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 30 07:44:03.067: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 30 07:44:03.144: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 30 07:44:03.144: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 30 07:44:03.170: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 30 07:44:03.170: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 30 07:44:03.192: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 30 07:44:03.192: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 30 07:44:03.205: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 30 07:44:03.206: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 30 07:44:03.222: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 30 07:44:03.222: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:44:03.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5126" for this suite. 08/30/23 07:44:03.233
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:44:03.261
Aug 30 07:44:03.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename cronjob 08/30/23 07:44:03.263
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:44:03.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:44:03.346
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/30/23 07:44:03.351
W0830 07:44:03.370598      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 08/30/23 07:44:03.37
STEP: Ensuring exactly one is scheduled 08/30/23 07:45:01.385
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/30/23 07:45:01.394
STEP: Ensuring the job is replaced with a new one 08/30/23 07:45:01.404
STEP: Removing cronjob 08/30/23 07:46:01.417
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:01.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5645" for this suite. 08/30/23 07:46:01.481
------------------------------
• [SLOW TEST] [118.245 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:44:03.261
    Aug 30 07:44:03.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename cronjob 08/30/23 07:44:03.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:44:03.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:44:03.346
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/30/23 07:44:03.351
    W0830 07:44:03.370598      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 08/30/23 07:44:03.37
    STEP: Ensuring exactly one is scheduled 08/30/23 07:45:01.385
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/30/23 07:45:01.394
    STEP: Ensuring the job is replaced with a new one 08/30/23 07:45:01.404
    STEP: Removing cronjob 08/30/23 07:46:01.417
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:01.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5645" for this suite. 08/30/23 07:46:01.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:01.508
Aug 30 07:46:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename ingress 08/30/23 07:46:01.509
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:01.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:01.593
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/30/23 07:46:01.599
STEP: getting /apis/networking.k8s.io 08/30/23 07:46:01.604
STEP: getting /apis/networking.k8s.iov1 08/30/23 07:46:01.607
STEP: creating 08/30/23 07:46:01.61
STEP: getting 08/30/23 07:46:01.653
STEP: listing 08/30/23 07:46:01.66
STEP: watching 08/30/23 07:46:01.69
Aug 30 07:46:01.691: INFO: starting watch
STEP: cluster-wide listing 08/30/23 07:46:01.693
STEP: cluster-wide watching 08/30/23 07:46:01.707
Aug 30 07:46:01.707: INFO: starting watch
STEP: patching 08/30/23 07:46:01.71
STEP: updating 08/30/23 07:46:01.746
Aug 30 07:46:01.783: INFO: waiting for watch events with expected annotations
Aug 30 07:46:01.784: INFO: saw patched and updated annotations
STEP: patching /status 08/30/23 07:46:01.784
STEP: updating /status 08/30/23 07:46:01.795
STEP: get /status 08/30/23 07:46:01.815
STEP: deleting 08/30/23 07:46:01.822
STEP: deleting a collection 08/30/23 07:46:01.847
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:01.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-2963" for this suite. 08/30/23 07:46:01.889
------------------------------
• [0.410 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:01.508
    Aug 30 07:46:01.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename ingress 08/30/23 07:46:01.509
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:01.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:01.593
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/30/23 07:46:01.599
    STEP: getting /apis/networking.k8s.io 08/30/23 07:46:01.604
    STEP: getting /apis/networking.k8s.iov1 08/30/23 07:46:01.607
    STEP: creating 08/30/23 07:46:01.61
    STEP: getting 08/30/23 07:46:01.653
    STEP: listing 08/30/23 07:46:01.66
    STEP: watching 08/30/23 07:46:01.69
    Aug 30 07:46:01.691: INFO: starting watch
    STEP: cluster-wide listing 08/30/23 07:46:01.693
    STEP: cluster-wide watching 08/30/23 07:46:01.707
    Aug 30 07:46:01.707: INFO: starting watch
    STEP: patching 08/30/23 07:46:01.71
    STEP: updating 08/30/23 07:46:01.746
    Aug 30 07:46:01.783: INFO: waiting for watch events with expected annotations
    Aug 30 07:46:01.784: INFO: saw patched and updated annotations
    STEP: patching /status 08/30/23 07:46:01.784
    STEP: updating /status 08/30/23 07:46:01.795
    STEP: get /status 08/30/23 07:46:01.815
    STEP: deleting 08/30/23 07:46:01.822
    STEP: deleting a collection 08/30/23 07:46:01.847
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:01.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-2963" for this suite. 08/30/23 07:46:01.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:01.923
Aug 30 07:46:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename init-container 08/30/23 07:46:01.924
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:01.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:01.992
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/30/23 07:46:02
Aug 30 07:46:02.006: INFO: PodSpec: initContainers in spec.initContainers
W0830 07:46:02.063898      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:06.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9885" for this suite. 08/30/23 07:46:06.015
------------------------------
• [4.111 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:01.923
    Aug 30 07:46:01.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename init-container 08/30/23 07:46:01.924
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:01.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:01.992
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/30/23 07:46:02
    Aug 30 07:46:02.006: INFO: PodSpec: initContainers in spec.initContainers
    W0830 07:46:02.063898      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "init1", "init2", "run1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "init1", "init2", "run1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "init1", "init2", "run1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "init1", "init2", "run1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:06.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9885" for this suite. 08/30/23 07:46:06.015
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:06.034
Aug 30 07:46:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename pods 08/30/23 07:46:06.035
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:06.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:06.096
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 30 07:46:06.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: creating the pod 08/30/23 07:46:06.103
STEP: submitting the pod to kubernetes 08/30/23 07:46:06.103
Aug 30 07:46:06.123: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051" in namespace "pods-118" to be "running and ready"
Aug 30 07:46:06.129: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051": Phase="Pending", Reason="", readiness=false. Elapsed: 5.767349ms
Aug 30 07:46:06.129: INFO: The phase of Pod pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:46:08.139: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051": Phase="Running", Reason="", readiness=true. Elapsed: 2.015448759s
Aug 30 07:46:08.139: INFO: The phase of Pod pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051 is Running (Ready = true)
Aug 30 07:46:08.139: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:08.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-118" for this suite. 08/30/23 07:46:08.313
------------------------------
• [2.295 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:06.034
    Aug 30 07:46:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename pods 08/30/23 07:46:06.035
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:06.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:06.096
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 30 07:46:06.101: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: creating the pod 08/30/23 07:46:06.103
    STEP: submitting the pod to kubernetes 08/30/23 07:46:06.103
    Aug 30 07:46:06.123: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051" in namespace "pods-118" to be "running and ready"
    Aug 30 07:46:06.129: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051": Phase="Pending", Reason="", readiness=false. Elapsed: 5.767349ms
    Aug 30 07:46:06.129: INFO: The phase of Pod pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:46:08.139: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051": Phase="Running", Reason="", readiness=true. Elapsed: 2.015448759s
    Aug 30 07:46:08.139: INFO: The phase of Pod pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051 is Running (Ready = true)
    Aug 30 07:46:08.139: INFO: Pod "pod-exec-websocket-dc68e8f1-3e92-4aa7-9587-f74168efb051" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:08.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-118" for this suite. 08/30/23 07:46:08.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:08.332
Aug 30 07:46:08.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 07:46:08.333
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:08.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:08.398
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/30/23 07:46:08.406
W0830 07:46:08.429116      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 08/30/23 07:46:08.429
STEP: delete a job 08/30/23 07:46:10.437
STEP: deleting Job.batch foo in namespace job-5952, will wait for the garbage collector to delete the pods 08/30/23 07:46:10.438
Aug 30 07:46:10.523: INFO: Deleting Job.batch foo took: 23.395108ms
Aug 30 07:46:10.624: INFO: Terminating Job.batch foo pods took: 100.953878ms
STEP: Ensuring job was deleted 08/30/23 07:46:43.524
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5952" for this suite. 08/30/23 07:46:43.542
------------------------------
• [SLOW TEST] [35.228 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:08.332
    Aug 30 07:46:08.332: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 07:46:08.333
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:08.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:08.398
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/30/23 07:46:08.406
    W0830 07:46:08.429116      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 08/30/23 07:46:08.429
    STEP: delete a job 08/30/23 07:46:10.437
    STEP: deleting Job.batch foo in namespace job-5952, will wait for the garbage collector to delete the pods 08/30/23 07:46:10.438
    Aug 30 07:46:10.523: INFO: Deleting Job.batch foo took: 23.395108ms
    Aug 30 07:46:10.624: INFO: Terminating Job.batch foo pods took: 100.953878ms
    STEP: Ensuring job was deleted 08/30/23 07:46:43.524
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5952" for this suite. 08/30/23 07:46:43.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:43.561
Aug 30 07:46:43.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:46:43.562
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:43.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:43.617
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-f70a019a-a3c2-4b54-9941-8d7e62ec6ffd 08/30/23 07:46:43.621
STEP: Creating a pod to test consume configMaps 08/30/23 07:46:43.642
Aug 30 07:46:43.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e" in namespace "configmap-6937" to be "Succeeded or Failed"
Aug 30 07:46:43.670: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62784ms
Aug 30 07:46:45.685: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021728032s
Aug 30 07:46:47.684: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02018832s
STEP: Saw pod success 08/30/23 07:46:47.684
Aug 30 07:46:47.684: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e" satisfied condition "Succeeded or Failed"
Aug 30 07:46:47.694: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:46:47.953
Aug 30 07:46:47.994: INFO: Waiting for pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e to disappear
Aug 30 07:46:48.003: INFO: Pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6937" for this suite. 08/30/23 07:46:48.042
------------------------------
• [4.514 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:43.561
    Aug 30 07:46:43.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:46:43.562
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:43.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:43.617
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-f70a019a-a3c2-4b54-9941-8d7e62ec6ffd 08/30/23 07:46:43.621
    STEP: Creating a pod to test consume configMaps 08/30/23 07:46:43.642
    Aug 30 07:46:43.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e" in namespace "configmap-6937" to be "Succeeded or Failed"
    Aug 30 07:46:43.670: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62784ms
    Aug 30 07:46:45.685: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021728032s
    Aug 30 07:46:47.684: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02018832s
    STEP: Saw pod success 08/30/23 07:46:47.684
    Aug 30 07:46:47.684: INFO: Pod "pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e" satisfied condition "Succeeded or Failed"
    Aug 30 07:46:47.694: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:46:47.953
    Aug 30 07:46:47.994: INFO: Waiting for pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e to disappear
    Aug 30 07:46:48.003: INFO: Pod pod-configmaps-0d1d86d4-284b-4e1a-9db8-5a9d9c56fe1e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6937" for this suite. 08/30/23 07:46:48.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:48.077
Aug 30 07:46:48.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:46:48.078
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:48.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:48.165
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
W0830 07:46:48.197323      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:52.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5519" for this suite. 08/30/23 07:46:52.243
------------------------------
• [4.186 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:48.077
    Aug 30 07:46:48.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:46:48.078
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:48.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:48.165
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    W0830 07:46:48.197323      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "bin-false8aba72de-2c2e-446b-8f72-086de92d9067" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:52.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5519" for this suite. 08/30/23 07:46:52.243
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:52.263
Aug 30 07:46:52.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:46:52.264
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:52.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:52.327
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/30/23 07:46:52.363
Aug 30 07:46:52.383: INFO: Waiting up to 5m0s for pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170" in namespace "emptydir-4168" to be "Succeeded or Failed"
Aug 30 07:46:52.393: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Pending", Reason="", readiness=false. Elapsed: 9.983305ms
Aug 30 07:46:54.409: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026192997s
Aug 30 07:46:56.404: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020894727s
STEP: Saw pod success 08/30/23 07:46:56.404
Aug 30 07:46:56.404: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170" satisfied condition "Succeeded or Failed"
Aug 30 07:46:56.413: INFO: Trying to get logs from node 10.135.139.190 pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 container test-container: <nil>
STEP: delete the pod 08/30/23 07:46:56.439
Aug 30 07:46:56.469: INFO: Waiting for pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 to disappear
Aug 30 07:46:56.478: INFO: Pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:46:56.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4168" for this suite. 08/30/23 07:46:56.492
------------------------------
• [4.252 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:52.263
    Aug 30 07:46:52.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:46:52.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:52.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:52.327
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/30/23 07:46:52.363
    Aug 30 07:46:52.383: INFO: Waiting up to 5m0s for pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170" in namespace "emptydir-4168" to be "Succeeded or Failed"
    Aug 30 07:46:52.393: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Pending", Reason="", readiness=false. Elapsed: 9.983305ms
    Aug 30 07:46:54.409: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026192997s
    Aug 30 07:46:56.404: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020894727s
    STEP: Saw pod success 08/30/23 07:46:56.404
    Aug 30 07:46:56.404: INFO: Pod "pod-536ae3a7-8d3a-4194-ba79-65c2019a1170" satisfied condition "Succeeded or Failed"
    Aug 30 07:46:56.413: INFO: Trying to get logs from node 10.135.139.190 pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 container test-container: <nil>
    STEP: delete the pod 08/30/23 07:46:56.439
    Aug 30 07:46:56.469: INFO: Waiting for pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 to disappear
    Aug 30 07:46:56.478: INFO: Pod pod-536ae3a7-8d3a-4194-ba79-65c2019a1170 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:46:56.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4168" for this suite. 08/30/23 07:46:56.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:46:56.516
Aug 30 07:46:56.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename security-context-test 08/30/23 07:46:56.517
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:56.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:56.578
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
W0830 07:46:56.608796      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:46:56.608: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" in namespace "security-context-test-3021" to be "Succeeded or Failed"
Aug 30 07:46:56.643: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.724393ms
Aug 30 07:46:58.653: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044571426s
Aug 30 07:47:00.652: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043180473s
Aug 30 07:47:00.652: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3021" for this suite. 08/30/23 07:47:00.662
------------------------------
• [4.163 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:46:56.516
    Aug 30 07:46:56.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename security-context-test 08/30/23 07:46:56.517
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:46:56.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:46:56.578
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    W0830 07:46:56.608796      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:46:56.608: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" in namespace "security-context-test-3021" to be "Succeeded or Failed"
    Aug 30 07:46:56.643: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Pending", Reason="", readiness=false. Elapsed: 34.724393ms
    Aug 30 07:46:58.653: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044571426s
    Aug 30 07:47:00.652: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043180473s
    Aug 30 07:47:00.652: INFO: Pod "busybox-readonly-false-1948c427-bd39-4bd6-b6e6-4a19cc1b71fe" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3021" for this suite. 08/30/23 07:47:00.662
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:00.68
Aug 30 07:47:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:47:00.681
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:00.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:00.754
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/30/23 07:47:00.759
W0830 07:47:00.777062      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:47:00.777: INFO: Waiting up to 5m0s for pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b" in namespace "emptydir-9291" to be "Succeeded or Failed"
Aug 30 07:47:00.788: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.095714ms
Aug 30 07:47:02.797: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019794877s
Aug 30 07:47:04.798: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021245986s
STEP: Saw pod success 08/30/23 07:47:04.798
Aug 30 07:47:04.798: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b" satisfied condition "Succeeded or Failed"
Aug 30 07:47:04.806: INFO: Trying to get logs from node 10.135.139.190 pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b container test-container: <nil>
STEP: delete the pod 08/30/23 07:47:04.822
Aug 30 07:47:04.873: INFO: Waiting for pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b to disappear
Aug 30 07:47:04.880: INFO: Pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:04.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9291" for this suite. 08/30/23 07:47:04.89
------------------------------
• [4.230 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:00.68
    Aug 30 07:47:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:47:00.681
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:00.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:00.754
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/30/23 07:47:00.759
    W0830 07:47:00.777062      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:47:00.777: INFO: Waiting up to 5m0s for pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b" in namespace "emptydir-9291" to be "Succeeded or Failed"
    Aug 30 07:47:00.788: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.095714ms
    Aug 30 07:47:02.797: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019794877s
    Aug 30 07:47:04.798: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021245986s
    STEP: Saw pod success 08/30/23 07:47:04.798
    Aug 30 07:47:04.798: INFO: Pod "pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b" satisfied condition "Succeeded or Failed"
    Aug 30 07:47:04.806: INFO: Trying to get logs from node 10.135.139.190 pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b container test-container: <nil>
    STEP: delete the pod 08/30/23 07:47:04.822
    Aug 30 07:47:04.873: INFO: Waiting for pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b to disappear
    Aug 30 07:47:04.880: INFO: Pod pod-2a9fd243-23fc-4fd8-a897-1ab8cd45e36b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:04.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9291" for this suite. 08/30/23 07:47:04.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:04.912
Aug 30 07:47:04.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:47:04.913
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:04.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:04.997
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-56926867-f3b5-4d07-9e35-812aaf5de18a 08/30/23 07:47:05.003
STEP: Creating a pod to test consume secrets 08/30/23 07:47:05.017
Aug 30 07:47:05.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094" in namespace "projected-5061" to be "Succeeded or Failed"
Aug 30 07:47:05.072: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Pending", Reason="", readiness=false. Elapsed: 23.677061ms
Aug 30 07:47:07.084: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03572058s
Aug 30 07:47:09.081: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03255041s
STEP: Saw pod success 08/30/23 07:47:09.081
Aug 30 07:47:09.081: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094" satisfied condition "Succeeded or Failed"
Aug 30 07:47:09.089: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:47:09.136
Aug 30 07:47:09.194: INFO: Waiting for pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 to disappear
Aug 30 07:47:09.201: INFO: Pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:09.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5061" for this suite. 08/30/23 07:47:09.212
------------------------------
• [4.320 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:04.912
    Aug 30 07:47:04.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:47:04.913
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:04.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:04.997
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-56926867-f3b5-4d07-9e35-812aaf5de18a 08/30/23 07:47:05.003
    STEP: Creating a pod to test consume secrets 08/30/23 07:47:05.017
    Aug 30 07:47:05.048: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094" in namespace "projected-5061" to be "Succeeded or Failed"
    Aug 30 07:47:05.072: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Pending", Reason="", readiness=false. Elapsed: 23.677061ms
    Aug 30 07:47:07.084: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03572058s
    Aug 30 07:47:09.081: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03255041s
    STEP: Saw pod success 08/30/23 07:47:09.081
    Aug 30 07:47:09.081: INFO: Pod "pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094" satisfied condition "Succeeded or Failed"
    Aug 30 07:47:09.089: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:47:09.136
    Aug 30 07:47:09.194: INFO: Waiting for pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 to disappear
    Aug 30 07:47:09.201: INFO: Pod pod-projected-secrets-d467462c-8288-4dde-b30d-3a11e61a9094 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:09.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5061" for this suite. 08/30/23 07:47:09.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:09.233
Aug 30 07:47:09.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:47:09.234
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:09.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:09.292
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-83a8f4a0-7fd6-41d2-aaff-0f656dbffff3 08/30/23 07:47:09.296
STEP: Creating a pod to test consume configMaps 08/30/23 07:47:09.31
W0830 07:47:09.333379      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:47:09.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8" in namespace "projected-295" to be "Succeeded or Failed"
Aug 30 07:47:09.342: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.434425ms
Aug 30 07:47:11.350: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01744124s
Aug 30 07:47:13.351: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017498355s
Aug 30 07:47:15.351: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01835332s
STEP: Saw pod success 08/30/23 07:47:15.351
Aug 30 07:47:15.352: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8" satisfied condition "Succeeded or Failed"
Aug 30 07:47:15.359: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:47:15.376
Aug 30 07:47:15.403: INFO: Waiting for pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 to disappear
Aug 30 07:47:15.410: INFO: Pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:15.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-295" for this suite. 08/30/23 07:47:15.419
------------------------------
• [SLOW TEST] [6.204 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:09.233
    Aug 30 07:47:09.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:47:09.234
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:09.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:09.292
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-83a8f4a0-7fd6-41d2-aaff-0f656dbffff3 08/30/23 07:47:09.296
    STEP: Creating a pod to test consume configMaps 08/30/23 07:47:09.31
    W0830 07:47:09.333379      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:47:09.333: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8" in namespace "projected-295" to be "Succeeded or Failed"
    Aug 30 07:47:09.342: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.434425ms
    Aug 30 07:47:11.350: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01744124s
    Aug 30 07:47:13.351: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017498355s
    Aug 30 07:47:15.351: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01835332s
    STEP: Saw pod success 08/30/23 07:47:15.351
    Aug 30 07:47:15.352: INFO: Pod "pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8" satisfied condition "Succeeded or Failed"
    Aug 30 07:47:15.359: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:47:15.376
    Aug 30 07:47:15.403: INFO: Waiting for pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 to disappear
    Aug 30 07:47:15.410: INFO: Pod pod-projected-configmaps-48c69a32-5c2f-4673-88f5-cca2db07ade8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:15.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-295" for this suite. 08/30/23 07:47:15.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:15.44
Aug 30 07:47:15.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:47:15.441
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:15.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:15.5
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/30/23 07:47:15.507
Aug 30 07:47:15.507: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 30 07:47:15.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:16.067: INFO: stderr: ""
Aug 30 07:47:16.067: INFO: stdout: "service/agnhost-replica created\n"
Aug 30 07:47:16.067: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 30 07:47:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:16.728: INFO: stderr: ""
Aug 30 07:47:16.728: INFO: stdout: "service/agnhost-primary created\n"
Aug 30 07:47:16.728: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 30 07:47:16.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:17.330: INFO: stderr: ""
Aug 30 07:47:17.331: INFO: stdout: "service/frontend created\n"
Aug 30 07:47:17.331: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 30 07:47:17.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:17.901: INFO: stderr: ""
Aug 30 07:47:17.901: INFO: stdout: "deployment.apps/frontend created\n"
Aug 30 07:47:17.902: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 07:47:17.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:18.481: INFO: stderr: ""
Aug 30 07:47:18.481: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 30 07:47:18.481: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 30 07:47:18.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
Aug 30 07:47:19.083: INFO: stderr: ""
Aug 30 07:47:19.083: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/30/23 07:47:19.083
Aug 30 07:47:19.083: INFO: Waiting for all frontend pods to be Running.
Aug 30 07:47:24.136: INFO: Waiting for frontend to serve content.
Aug 30 07:47:24.161: INFO: Trying to add a new entry to the guestbook.
Aug 30 07:47:24.183: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/30/23 07:47:24.207
Aug 30 07:47:24.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:24.432: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:24.433: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/30/23 07:47:24.433
Aug 30 07:47:24.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:24.647: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:24.647: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/30/23 07:47:24.647
Aug 30 07:47:24.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:24.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:24.771: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/30/23 07:47:24.771
Aug 30 07:47:24.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:24.880: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:24.880: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/30/23 07:47:24.88
Aug 30 07:47:24.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:25.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:25.009: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/30/23 07:47:25.009
Aug 30 07:47:25.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
Aug 30 07:47:25.129: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 30 07:47:25.129: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1016" for this suite. 08/30/23 07:47:25.138
------------------------------
• [SLOW TEST] [9.717 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:15.44
    Aug 30 07:47:15.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:47:15.441
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:15.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:15.5
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/30/23 07:47:15.507
    Aug 30 07:47:15.507: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 30 07:47:15.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:16.067: INFO: stderr: ""
    Aug 30 07:47:16.067: INFO: stdout: "service/agnhost-replica created\n"
    Aug 30 07:47:16.067: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 30 07:47:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:16.728: INFO: stderr: ""
    Aug 30 07:47:16.728: INFO: stdout: "service/agnhost-primary created\n"
    Aug 30 07:47:16.728: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 30 07:47:16.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:17.330: INFO: stderr: ""
    Aug 30 07:47:17.331: INFO: stdout: "service/frontend created\n"
    Aug 30 07:47:17.331: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 30 07:47:17.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:17.901: INFO: stderr: ""
    Aug 30 07:47:17.901: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 30 07:47:17.902: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 30 07:47:17.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:18.481: INFO: stderr: ""
    Aug 30 07:47:18.481: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 30 07:47:18.481: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 30 07:47:18.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 create -f -'
    Aug 30 07:47:19.083: INFO: stderr: ""
    Aug 30 07:47:19.083: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/30/23 07:47:19.083
    Aug 30 07:47:19.083: INFO: Waiting for all frontend pods to be Running.
    Aug 30 07:47:24.136: INFO: Waiting for frontend to serve content.
    Aug 30 07:47:24.161: INFO: Trying to add a new entry to the guestbook.
    Aug 30 07:47:24.183: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/30/23 07:47:24.207
    Aug 30 07:47:24.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:24.432: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:24.433: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/30/23 07:47:24.433
    Aug 30 07:47:24.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:24.647: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:24.647: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/30/23 07:47:24.647
    Aug 30 07:47:24.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:24.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:24.771: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/30/23 07:47:24.771
    Aug 30 07:47:24.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:24.880: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:24.880: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/30/23 07:47:24.88
    Aug 30 07:47:24.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:25.009: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:25.009: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/30/23 07:47:25.009
    Aug 30 07:47:25.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-1016 delete --grace-period=0 --force -f -'
    Aug 30 07:47:25.129: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 30 07:47:25.129: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:25.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1016" for this suite. 08/30/23 07:47:25.138
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:25.158
Aug 30 07:47:25.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:47:25.159
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:25.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:25.245
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 30 07:47:25.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 07:47:32.539
Aug 30 07:47:32.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 create -f -'
Aug 30 07:47:34.129: INFO: stderr: ""
Aug 30 07:47:34.130: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 07:47:34.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 delete e2e-test-crd-publish-openapi-4166-crds test-cr'
Aug 30 07:47:34.288: INFO: stderr: ""
Aug 30 07:47:34.288: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 30 07:47:34.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 apply -f -'
Aug 30 07:47:35.709: INFO: stderr: ""
Aug 30 07:47:35.709: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 30 07:47:35.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 delete e2e-test-crd-publish-openapi-4166-crds test-cr'
Aug 30 07:47:35.823: INFO: stderr: ""
Aug 30 07:47:35.823: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/30/23 07:47:35.823
Aug 30 07:47:35.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 explain e2e-test-crd-publish-openapi-4166-crds'
Aug 30 07:47:38.745: INFO: stderr: ""
Aug 30 07:47:38.746: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4166-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:47:42.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1135" for this suite. 08/30/23 07:47:42.898
------------------------------
• [SLOW TEST] [17.780 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:25.158
    Aug 30 07:47:25.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:47:25.159
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:25.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:25.245
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 30 07:47:25.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/30/23 07:47:32.539
    Aug 30 07:47:32.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 create -f -'
    Aug 30 07:47:34.129: INFO: stderr: ""
    Aug 30 07:47:34.130: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 30 07:47:34.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 delete e2e-test-crd-publish-openapi-4166-crds test-cr'
    Aug 30 07:47:34.288: INFO: stderr: ""
    Aug 30 07:47:34.288: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 30 07:47:34.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 apply -f -'
    Aug 30 07:47:35.709: INFO: stderr: ""
    Aug 30 07:47:35.709: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 30 07:47:35.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 --namespace=crd-publish-openapi-1135 delete e2e-test-crd-publish-openapi-4166-crds test-cr'
    Aug 30 07:47:35.823: INFO: stderr: ""
    Aug 30 07:47:35.823: INFO: stdout: "e2e-test-crd-publish-openapi-4166-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/30/23 07:47:35.823
    Aug 30 07:47:35.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=crd-publish-openapi-1135 explain e2e-test-crd-publish-openapi-4166-crds'
    Aug 30 07:47:38.745: INFO: stderr: ""
    Aug 30 07:47:38.746: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4166-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:47:42.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1135" for this suite. 08/30/23 07:47:42.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:47:42.939
Aug 30 07:47:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 07:47:42.941
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:43.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:43.091
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 in namespace container-probe-6341 08/30/23 07:47:43.141
Aug 30 07:47:43.242: INFO: Waiting up to 5m0s for pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25" in namespace "container-probe-6341" to be "not pending"
Aug 30 07:47:43.315: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25": Phase="Pending", Reason="", readiness=false. Elapsed: 73.281779ms
Aug 30 07:47:45.359: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25": Phase="Running", Reason="", readiness=true. Elapsed: 2.116445416s
Aug 30 07:47:45.359: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25" satisfied condition "not pending"
Aug 30 07:47:45.359: INFO: Started pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 in namespace container-probe-6341
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 07:47:45.359
Aug 30 07:47:45.375: INFO: Initial restart count of pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 is 0
Aug 30 07:48:05.755: INFO: Restart count of pod container-probe-6341/liveness-240d297a-5fc5-4c67-9df7-202052d05d25 is now 1 (20.380389201s elapsed)
STEP: deleting the pod 08/30/23 07:48:05.755
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 07:48:05.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6341" for this suite. 08/30/23 07:48:05.851
------------------------------
• [SLOW TEST] [22.932 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:47:42.939
    Aug 30 07:47:42.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 07:47:42.941
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:47:43.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:47:43.091
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 in namespace container-probe-6341 08/30/23 07:47:43.141
    Aug 30 07:47:43.242: INFO: Waiting up to 5m0s for pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25" in namespace "container-probe-6341" to be "not pending"
    Aug 30 07:47:43.315: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25": Phase="Pending", Reason="", readiness=false. Elapsed: 73.281779ms
    Aug 30 07:47:45.359: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25": Phase="Running", Reason="", readiness=true. Elapsed: 2.116445416s
    Aug 30 07:47:45.359: INFO: Pod "liveness-240d297a-5fc5-4c67-9df7-202052d05d25" satisfied condition "not pending"
    Aug 30 07:47:45.359: INFO: Started pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 in namespace container-probe-6341
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 07:47:45.359
    Aug 30 07:47:45.375: INFO: Initial restart count of pod liveness-240d297a-5fc5-4c67-9df7-202052d05d25 is 0
    Aug 30 07:48:05.755: INFO: Restart count of pod container-probe-6341/liveness-240d297a-5fc5-4c67-9df7-202052d05d25 is now 1 (20.380389201s elapsed)
    STEP: deleting the pod 08/30/23 07:48:05.755
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:48:05.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6341" for this suite. 08/30/23 07:48:05.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:48:05.872
Aug 30 07:48:05.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:48:05.873
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:05.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:05.959
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/30/23 07:48:05.972
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/30/23 07:48:06.003
STEP: patching the secret 08/30/23 07:48:06.294
STEP: deleting the secret using a LabelSelector 08/30/23 07:48:06.347
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/30/23 07:48:06.402
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:48:06.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6913" for this suite. 08/30/23 07:48:06.729
------------------------------
• [0.932 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:48:05.872
    Aug 30 07:48:05.872: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:48:05.873
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:05.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:05.959
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/30/23 07:48:05.972
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/30/23 07:48:06.003
    STEP: patching the secret 08/30/23 07:48:06.294
    STEP: deleting the secret using a LabelSelector 08/30/23 07:48:06.347
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/30/23 07:48:06.402
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:48:06.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6913" for this suite. 08/30/23 07:48:06.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:48:06.807
Aug 30 07:48:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:48:06.808
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:06.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:06.946
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:48:06.962
Aug 30 07:48:07.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343" in namespace "downward-api-8263" to be "Succeeded or Failed"
Aug 30 07:48:07.030: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Pending", Reason="", readiness=false. Elapsed: 18.483214ms
Aug 30 07:48:09.044: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032241792s
Aug 30 07:48:11.046: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034727589s
STEP: Saw pod success 08/30/23 07:48:11.046
Aug 30 07:48:11.046: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343" satisfied condition "Succeeded or Failed"
Aug 30 07:48:11.057: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 container client-container: <nil>
STEP: delete the pod 08/30/23 07:48:11.115
Aug 30 07:48:11.150: INFO: Waiting for pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 to disappear
Aug 30 07:48:11.163: INFO: Pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:48:11.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8263" for this suite. 08/30/23 07:48:11.178
------------------------------
• [4.393 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:48:06.807
    Aug 30 07:48:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:48:06.808
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:06.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:06.946
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:48:06.962
    Aug 30 07:48:07.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343" in namespace "downward-api-8263" to be "Succeeded or Failed"
    Aug 30 07:48:07.030: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Pending", Reason="", readiness=false. Elapsed: 18.483214ms
    Aug 30 07:48:09.044: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032241792s
    Aug 30 07:48:11.046: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034727589s
    STEP: Saw pod success 08/30/23 07:48:11.046
    Aug 30 07:48:11.046: INFO: Pod "downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343" satisfied condition "Succeeded or Failed"
    Aug 30 07:48:11.057: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:48:11.115
    Aug 30 07:48:11.150: INFO: Waiting for pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 to disappear
    Aug 30 07:48:11.163: INFO: Pod downwardapi-volume-757f891c-f7dc-4000-bcc2-9da03da19343 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:48:11.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8263" for this suite. 08/30/23 07:48:11.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:48:11.201
Aug 30 07:48:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:48:11.201
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:11.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:11.277
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
Aug 30 07:48:11.304: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-e76ee1cb-8691-4289-887b-17c4a0d6116a 08/30/23 07:48:11.304
STEP: Creating configMap with name cm-test-opt-upd-07290019-7156-4c45-a355-cdc80f5038dd 08/30/23 07:48:11.322
STEP: Creating the pod 08/30/23 07:48:11.34
Aug 30 07:48:11.385: INFO: Waiting up to 5m0s for pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9" in namespace "configmap-1406" to be "running and ready"
Aug 30 07:48:11.413: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.264189ms
Aug 30 07:48:11.414: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:48:13.433: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047627578s
Aug 30 07:48:13.433: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:48:15.428: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Running", Reason="", readiness=true. Elapsed: 4.043028428s
Aug 30 07:48:15.428: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Running (Ready = true)
Aug 30 07:48:15.428: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-e76ee1cb-8691-4289-887b-17c4a0d6116a 08/30/23 07:48:15.514
STEP: Updating configmap cm-test-opt-upd-07290019-7156-4c45-a355-cdc80f5038dd 08/30/23 07:48:15.534
STEP: Creating configMap with name cm-test-opt-create-2d2417ec-8e1c-46b1-ab84-e64f21a9cf05 08/30/23 07:48:15.548
STEP: waiting to observe update in volume 08/30/23 07:48:15.564
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:49:32.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1406" for this suite. 08/30/23 07:49:32.988
------------------------------
• [SLOW TEST] [81.820 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:48:11.201
    Aug 30 07:48:11.201: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:48:11.201
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:48:11.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:48:11.277
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    Aug 30 07:48:11.304: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-e76ee1cb-8691-4289-887b-17c4a0d6116a 08/30/23 07:48:11.304
    STEP: Creating configMap with name cm-test-opt-upd-07290019-7156-4c45-a355-cdc80f5038dd 08/30/23 07:48:11.322
    STEP: Creating the pod 08/30/23 07:48:11.34
    Aug 30 07:48:11.385: INFO: Waiting up to 5m0s for pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9" in namespace "configmap-1406" to be "running and ready"
    Aug 30 07:48:11.413: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.264189ms
    Aug 30 07:48:11.414: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:48:13.433: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047627578s
    Aug 30 07:48:13.433: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:48:15.428: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9": Phase="Running", Reason="", readiness=true. Elapsed: 4.043028428s
    Aug 30 07:48:15.428: INFO: The phase of Pod pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9 is Running (Ready = true)
    Aug 30 07:48:15.428: INFO: Pod "pod-configmaps-97944483-1b90-421a-9366-5d6f6e3c47b9" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-e76ee1cb-8691-4289-887b-17c4a0d6116a 08/30/23 07:48:15.514
    STEP: Updating configmap cm-test-opt-upd-07290019-7156-4c45-a355-cdc80f5038dd 08/30/23 07:48:15.534
    STEP: Creating configMap with name cm-test-opt-create-2d2417ec-8e1c-46b1-ab84-e64f21a9cf05 08/30/23 07:48:15.548
    STEP: waiting to observe update in volume 08/30/23 07:48:15.564
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:49:32.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1406" for this suite. 08/30/23 07:49:32.988
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:49:33.022
Aug 30 07:49:33.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:49:33.023
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:33.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:33.123
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:49:33.134
Aug 30 07:49:33.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 30 07:49:33.283: INFO: stderr: ""
Aug 30 07:49:33.283: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/30/23 07:49:33.283
Aug 30 07:49:33.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 30 07:49:34.272: INFO: stderr: ""
Aug 30 07:49:34.272: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:49:34.272
Aug 30 07:49:34.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 delete pods e2e-test-httpd-pod'
Aug 30 07:49:36.289: INFO: stderr: ""
Aug 30 07:49:36.289: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:49:36.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7534" for this suite. 08/30/23 07:49:36.331
------------------------------
• [3.333 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:49:33.022
    Aug 30 07:49:33.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:49:33.023
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:33.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:33.123
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:49:33.134
    Aug 30 07:49:33.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 30 07:49:33.283: INFO: stderr: ""
    Aug 30 07:49:33.283: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/30/23 07:49:33.283
    Aug 30 07:49:33.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 30 07:49:34.272: INFO: stderr: ""
    Aug 30 07:49:34.272: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/30/23 07:49:34.272
    Aug 30 07:49:34.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7534 delete pods e2e-test-httpd-pod'
    Aug 30 07:49:36.289: INFO: stderr: ""
    Aug 30 07:49:36.289: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:49:36.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7534" for this suite. 08/30/23 07:49:36.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:49:36.356
Aug 30 07:49:36.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename watch 08/30/23 07:49:36.357
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:36.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:36.599
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/30/23 07:49:36.64
STEP: modifying the configmap once 08/30/23 07:49:36.726
STEP: modifying the configmap a second time 08/30/23 07:49:36.797
STEP: deleting the configmap 08/30/23 07:49:36.864
STEP: creating a watch on configmaps from the resource version returned by the first update 08/30/23 07:49:36.945
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/30/23 07:49:36.95
Aug 30 07:49:36.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1714  c9054d73-a2db-48fa-b848-8960e1378f9c 126050 0 2023-08-30 07:49:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-30 07:49:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 30 07:49:36.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1714  c9054d73-a2db-48fa-b848-8960e1378f9c 126054 0 2023-08-30 07:49:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-30 07:49:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 30 07:49:36.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1714" for this suite. 08/30/23 07:49:36.972
------------------------------
• [0.651 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:49:36.356
    Aug 30 07:49:36.356: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename watch 08/30/23 07:49:36.357
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:36.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:36.599
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/30/23 07:49:36.64
    STEP: modifying the configmap once 08/30/23 07:49:36.726
    STEP: modifying the configmap a second time 08/30/23 07:49:36.797
    STEP: deleting the configmap 08/30/23 07:49:36.864
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/30/23 07:49:36.945
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/30/23 07:49:36.95
    Aug 30 07:49:36.950: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1714  c9054d73-a2db-48fa-b848-8960e1378f9c 126050 0 2023-08-30 07:49:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-30 07:49:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 30 07:49:36.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1714  c9054d73-a2db-48fa-b848-8960e1378f9c 126054 0 2023-08-30 07:49:36 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-30 07:49:36 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:49:36.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1714" for this suite. 08/30/23 07:49:36.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:49:37.007
Aug 30 07:49:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:49:37.008
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:37.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:37.09
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 30 07:49:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:49:37.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5073" for this suite. 08/30/23 07:49:37.751
------------------------------
• [0.787 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:49:37.007
    Aug 30 07:49:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:49:37.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:37.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:37.09
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 30 07:49:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:49:37.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5073" for this suite. 08/30/23 07:49:37.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:49:37.795
Aug 30 07:49:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 07:49:37.796
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:37.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:37.868
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/30/23 07:49:37.893
W0830 07:49:37.923946      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 08/30/23 07:49:43.002
STEP: wait for the rc to be deleted 08/30/23 07:49:43.085
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/30/23 07:49:48.111
STEP: Gathering metrics 08/30/23 07:50:18.159
W0830 07:50:18.177335      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 30 07:50:18.177: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 30 07:50:18.177: INFO: Deleting pod "simpletest.rc-2fbv7" in namespace "gc-415"
Aug 30 07:50:18.215: INFO: Deleting pod "simpletest.rc-2v4wl" in namespace "gc-415"
Aug 30 07:50:18.266: INFO: Deleting pod "simpletest.rc-44p99" in namespace "gc-415"
Aug 30 07:50:18.331: INFO: Deleting pod "simpletest.rc-46mp4" in namespace "gc-415"
Aug 30 07:50:18.363: INFO: Deleting pod "simpletest.rc-47n9v" in namespace "gc-415"
Aug 30 07:50:18.406: INFO: Deleting pod "simpletest.rc-4rvq7" in namespace "gc-415"
Aug 30 07:50:18.442: INFO: Deleting pod "simpletest.rc-4wqv6" in namespace "gc-415"
Aug 30 07:50:18.486: INFO: Deleting pod "simpletest.rc-5b7dq" in namespace "gc-415"
Aug 30 07:50:18.555: INFO: Deleting pod "simpletest.rc-5kxd7" in namespace "gc-415"
Aug 30 07:50:18.630: INFO: Deleting pod "simpletest.rc-5n2qw" in namespace "gc-415"
Aug 30 07:50:18.676: INFO: Deleting pod "simpletest.rc-5tbmj" in namespace "gc-415"
Aug 30 07:50:18.707: INFO: Deleting pod "simpletest.rc-6lwvt" in namespace "gc-415"
Aug 30 07:50:18.757: INFO: Deleting pod "simpletest.rc-6rv6c" in namespace "gc-415"
Aug 30 07:50:18.810: INFO: Deleting pod "simpletest.rc-756t2" in namespace "gc-415"
Aug 30 07:50:18.847: INFO: Deleting pod "simpletest.rc-789fv" in namespace "gc-415"
Aug 30 07:50:18.907: INFO: Deleting pod "simpletest.rc-7ksx5" in namespace "gc-415"
Aug 30 07:50:18.977: INFO: Deleting pod "simpletest.rc-8gf2p" in namespace "gc-415"
Aug 30 07:50:19.095: INFO: Deleting pod "simpletest.rc-8kggl" in namespace "gc-415"
Aug 30 07:50:19.127: INFO: Deleting pod "simpletest.rc-8kksj" in namespace "gc-415"
Aug 30 07:50:19.162: INFO: Deleting pod "simpletest.rc-8nqkd" in namespace "gc-415"
Aug 30 07:50:19.220: INFO: Deleting pod "simpletest.rc-8vq5m" in namespace "gc-415"
Aug 30 07:50:19.255: INFO: Deleting pod "simpletest.rc-96fmk" in namespace "gc-415"
Aug 30 07:50:19.286: INFO: Deleting pod "simpletest.rc-9799q" in namespace "gc-415"
Aug 30 07:50:19.325: INFO: Deleting pod "simpletest.rc-97pgd" in namespace "gc-415"
Aug 30 07:50:19.375: INFO: Deleting pod "simpletest.rc-98qgn" in namespace "gc-415"
Aug 30 07:50:19.414: INFO: Deleting pod "simpletest.rc-98tvf" in namespace "gc-415"
Aug 30 07:50:19.445: INFO: Deleting pod "simpletest.rc-9drmp" in namespace "gc-415"
Aug 30 07:50:19.497: INFO: Deleting pod "simpletest.rc-9jcfb" in namespace "gc-415"
Aug 30 07:50:19.530: INFO: Deleting pod "simpletest.rc-9rzc7" in namespace "gc-415"
Aug 30 07:50:19.574: INFO: Deleting pod "simpletest.rc-9vr5j" in namespace "gc-415"
Aug 30 07:50:19.604: INFO: Deleting pod "simpletest.rc-9znfj" in namespace "gc-415"
Aug 30 07:50:19.654: INFO: Deleting pod "simpletest.rc-bvjsk" in namespace "gc-415"
Aug 30 07:50:19.711: INFO: Deleting pod "simpletest.rc-cmkk5" in namespace "gc-415"
Aug 30 07:50:19.756: INFO: Deleting pod "simpletest.rc-cnpdn" in namespace "gc-415"
Aug 30 07:50:19.786: INFO: Deleting pod "simpletest.rc-cpw9g" in namespace "gc-415"
Aug 30 07:50:19.824: INFO: Deleting pod "simpletest.rc-cr955" in namespace "gc-415"
Aug 30 07:50:19.859: INFO: Deleting pod "simpletest.rc-ct66l" in namespace "gc-415"
Aug 30 07:50:19.892: INFO: Deleting pod "simpletest.rc-dhcvz" in namespace "gc-415"
Aug 30 07:50:19.952: INFO: Deleting pod "simpletest.rc-dmr78" in namespace "gc-415"
Aug 30 07:50:20.009: INFO: Deleting pod "simpletest.rc-f9ns7" in namespace "gc-415"
Aug 30 07:50:20.081: INFO: Deleting pod "simpletest.rc-gnpq8" in namespace "gc-415"
Aug 30 07:50:20.177: INFO: Deleting pod "simpletest.rc-grlvk" in namespace "gc-415"
Aug 30 07:50:20.227: INFO: Deleting pod "simpletest.rc-gsvpr" in namespace "gc-415"
Aug 30 07:50:20.289: INFO: Deleting pod "simpletest.rc-gvdz5" in namespace "gc-415"
Aug 30 07:50:20.338: INFO: Deleting pod "simpletest.rc-h92h6" in namespace "gc-415"
Aug 30 07:50:20.387: INFO: Deleting pod "simpletest.rc-hgnll" in namespace "gc-415"
Aug 30 07:50:20.475: INFO: Deleting pod "simpletest.rc-hj8gr" in namespace "gc-415"
Aug 30 07:50:20.532: INFO: Deleting pod "simpletest.rc-hll7j" in namespace "gc-415"
Aug 30 07:50:20.609: INFO: Deleting pod "simpletest.rc-hntll" in namespace "gc-415"
Aug 30 07:50:20.658: INFO: Deleting pod "simpletest.rc-j64cn" in namespace "gc-415"
Aug 30 07:50:20.699: INFO: Deleting pod "simpletest.rc-j9lvw" in namespace "gc-415"
Aug 30 07:50:20.750: INFO: Deleting pod "simpletest.rc-jhn8d" in namespace "gc-415"
Aug 30 07:50:20.782: INFO: Deleting pod "simpletest.rc-k82cr" in namespace "gc-415"
Aug 30 07:50:20.821: INFO: Deleting pod "simpletest.rc-kjtm7" in namespace "gc-415"
Aug 30 07:50:20.897: INFO: Deleting pod "simpletest.rc-kt8ph" in namespace "gc-415"
Aug 30 07:50:20.943: INFO: Deleting pod "simpletest.rc-lklcr" in namespace "gc-415"
Aug 30 07:50:21.032: INFO: Deleting pod "simpletest.rc-mfhjh" in namespace "gc-415"
Aug 30 07:50:21.104: INFO: Deleting pod "simpletest.rc-mqfs7" in namespace "gc-415"
Aug 30 07:50:21.172: INFO: Deleting pod "simpletest.rc-mx65f" in namespace "gc-415"
Aug 30 07:50:21.207: INFO: Deleting pod "simpletest.rc-n6zqh" in namespace "gc-415"
Aug 30 07:50:21.347: INFO: Deleting pod "simpletest.rc-ncfjq" in namespace "gc-415"
Aug 30 07:50:21.413: INFO: Deleting pod "simpletest.rc-nfz5j" in namespace "gc-415"
Aug 30 07:50:21.488: INFO: Deleting pod "simpletest.rc-nt984" in namespace "gc-415"
Aug 30 07:50:21.545: INFO: Deleting pod "simpletest.rc-pdflp" in namespace "gc-415"
Aug 30 07:50:21.591: INFO: Deleting pod "simpletest.rc-px7p7" in namespace "gc-415"
Aug 30 07:50:21.638: INFO: Deleting pod "simpletest.rc-q28wt" in namespace "gc-415"
Aug 30 07:50:21.680: INFO: Deleting pod "simpletest.rc-qjc7d" in namespace "gc-415"
Aug 30 07:50:21.724: INFO: Deleting pod "simpletest.rc-qn4m4" in namespace "gc-415"
Aug 30 07:50:21.760: INFO: Deleting pod "simpletest.rc-qzzdn" in namespace "gc-415"
Aug 30 07:50:21.819: INFO: Deleting pod "simpletest.rc-rbs5x" in namespace "gc-415"
Aug 30 07:50:21.890: INFO: Deleting pod "simpletest.rc-rc6vz" in namespace "gc-415"
Aug 30 07:50:21.931: INFO: Deleting pod "simpletest.rc-rsg7f" in namespace "gc-415"
Aug 30 07:50:21.983: INFO: Deleting pod "simpletest.rc-rt7cb" in namespace "gc-415"
Aug 30 07:50:22.026: INFO: Deleting pod "simpletest.rc-sj7g4" in namespace "gc-415"
Aug 30 07:50:22.061: INFO: Deleting pod "simpletest.rc-sm9xr" in namespace "gc-415"
Aug 30 07:50:22.103: INFO: Deleting pod "simpletest.rc-sp82c" in namespace "gc-415"
Aug 30 07:50:22.198: INFO: Deleting pod "simpletest.rc-ttzrr" in namespace "gc-415"
Aug 30 07:50:22.254: INFO: Deleting pod "simpletest.rc-v2zv7" in namespace "gc-415"
Aug 30 07:50:22.294: INFO: Deleting pod "simpletest.rc-v988m" in namespace "gc-415"
Aug 30 07:50:22.327: INFO: Deleting pod "simpletest.rc-vc8lh" in namespace "gc-415"
Aug 30 07:50:22.364: INFO: Deleting pod "simpletest.rc-vpzmk" in namespace "gc-415"
Aug 30 07:50:22.416: INFO: Deleting pod "simpletest.rc-vsfx6" in namespace "gc-415"
Aug 30 07:50:22.460: INFO: Deleting pod "simpletest.rc-wjhcj" in namespace "gc-415"
Aug 30 07:50:22.512: INFO: Deleting pod "simpletest.rc-wjv6k" in namespace "gc-415"
Aug 30 07:50:22.553: INFO: Deleting pod "simpletest.rc-wx4ld" in namespace "gc-415"
Aug 30 07:50:22.669: INFO: Deleting pod "simpletest.rc-x2zrr" in namespace "gc-415"
Aug 30 07:50:22.739: INFO: Deleting pod "simpletest.rc-x5d8p" in namespace "gc-415"
Aug 30 07:50:22.840: INFO: Deleting pod "simpletest.rc-x5r7m" in namespace "gc-415"
Aug 30 07:50:22.911: INFO: Deleting pod "simpletest.rc-x9z9q" in namespace "gc-415"
Aug 30 07:50:22.970: INFO: Deleting pod "simpletest.rc-xftr4" in namespace "gc-415"
Aug 30 07:50:23.023: INFO: Deleting pod "simpletest.rc-xn9pm" in namespace "gc-415"
Aug 30 07:50:23.080: INFO: Deleting pod "simpletest.rc-xsjjv" in namespace "gc-415"
Aug 30 07:50:23.123: INFO: Deleting pod "simpletest.rc-xsmcx" in namespace "gc-415"
Aug 30 07:50:23.205: INFO: Deleting pod "simpletest.rc-xwshj" in namespace "gc-415"
Aug 30 07:50:23.250: INFO: Deleting pod "simpletest.rc-xz4k2" in namespace "gc-415"
Aug 30 07:50:23.319: INFO: Deleting pod "simpletest.rc-z6j4w" in namespace "gc-415"
Aug 30 07:50:23.363: INFO: Deleting pod "simpletest.rc-z6q6f" in namespace "gc-415"
Aug 30 07:50:23.402: INFO: Deleting pod "simpletest.rc-zd8m2" in namespace "gc-415"
Aug 30 07:50:23.447: INFO: Deleting pod "simpletest.rc-zdkhv" in namespace "gc-415"
Aug 30 07:50:23.503: INFO: Deleting pod "simpletest.rc-zrbqt" in namespace "gc-415"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 07:50:23.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-415" for this suite. 08/30/23 07:50:23.55
------------------------------
• [SLOW TEST] [45.778 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:49:37.795
    Aug 30 07:49:37.795: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 07:49:37.796
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:49:37.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:49:37.868
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/30/23 07:49:37.893
    W0830 07:49:37.923946      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 08/30/23 07:49:43.002
    STEP: wait for the rc to be deleted 08/30/23 07:49:43.085
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/30/23 07:49:48.111
    STEP: Gathering metrics 08/30/23 07:50:18.159
    W0830 07:50:18.177335      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 30 07:50:18.177: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 30 07:50:18.177: INFO: Deleting pod "simpletest.rc-2fbv7" in namespace "gc-415"
    Aug 30 07:50:18.215: INFO: Deleting pod "simpletest.rc-2v4wl" in namespace "gc-415"
    Aug 30 07:50:18.266: INFO: Deleting pod "simpletest.rc-44p99" in namespace "gc-415"
    Aug 30 07:50:18.331: INFO: Deleting pod "simpletest.rc-46mp4" in namespace "gc-415"
    Aug 30 07:50:18.363: INFO: Deleting pod "simpletest.rc-47n9v" in namespace "gc-415"
    Aug 30 07:50:18.406: INFO: Deleting pod "simpletest.rc-4rvq7" in namespace "gc-415"
    Aug 30 07:50:18.442: INFO: Deleting pod "simpletest.rc-4wqv6" in namespace "gc-415"
    Aug 30 07:50:18.486: INFO: Deleting pod "simpletest.rc-5b7dq" in namespace "gc-415"
    Aug 30 07:50:18.555: INFO: Deleting pod "simpletest.rc-5kxd7" in namespace "gc-415"
    Aug 30 07:50:18.630: INFO: Deleting pod "simpletest.rc-5n2qw" in namespace "gc-415"
    Aug 30 07:50:18.676: INFO: Deleting pod "simpletest.rc-5tbmj" in namespace "gc-415"
    Aug 30 07:50:18.707: INFO: Deleting pod "simpletest.rc-6lwvt" in namespace "gc-415"
    Aug 30 07:50:18.757: INFO: Deleting pod "simpletest.rc-6rv6c" in namespace "gc-415"
    Aug 30 07:50:18.810: INFO: Deleting pod "simpletest.rc-756t2" in namespace "gc-415"
    Aug 30 07:50:18.847: INFO: Deleting pod "simpletest.rc-789fv" in namespace "gc-415"
    Aug 30 07:50:18.907: INFO: Deleting pod "simpletest.rc-7ksx5" in namespace "gc-415"
    Aug 30 07:50:18.977: INFO: Deleting pod "simpletest.rc-8gf2p" in namespace "gc-415"
    Aug 30 07:50:19.095: INFO: Deleting pod "simpletest.rc-8kggl" in namespace "gc-415"
    Aug 30 07:50:19.127: INFO: Deleting pod "simpletest.rc-8kksj" in namespace "gc-415"
    Aug 30 07:50:19.162: INFO: Deleting pod "simpletest.rc-8nqkd" in namespace "gc-415"
    Aug 30 07:50:19.220: INFO: Deleting pod "simpletest.rc-8vq5m" in namespace "gc-415"
    Aug 30 07:50:19.255: INFO: Deleting pod "simpletest.rc-96fmk" in namespace "gc-415"
    Aug 30 07:50:19.286: INFO: Deleting pod "simpletest.rc-9799q" in namespace "gc-415"
    Aug 30 07:50:19.325: INFO: Deleting pod "simpletest.rc-97pgd" in namespace "gc-415"
    Aug 30 07:50:19.375: INFO: Deleting pod "simpletest.rc-98qgn" in namespace "gc-415"
    Aug 30 07:50:19.414: INFO: Deleting pod "simpletest.rc-98tvf" in namespace "gc-415"
    Aug 30 07:50:19.445: INFO: Deleting pod "simpletest.rc-9drmp" in namespace "gc-415"
    Aug 30 07:50:19.497: INFO: Deleting pod "simpletest.rc-9jcfb" in namespace "gc-415"
    Aug 30 07:50:19.530: INFO: Deleting pod "simpletest.rc-9rzc7" in namespace "gc-415"
    Aug 30 07:50:19.574: INFO: Deleting pod "simpletest.rc-9vr5j" in namespace "gc-415"
    Aug 30 07:50:19.604: INFO: Deleting pod "simpletest.rc-9znfj" in namespace "gc-415"
    Aug 30 07:50:19.654: INFO: Deleting pod "simpletest.rc-bvjsk" in namespace "gc-415"
    Aug 30 07:50:19.711: INFO: Deleting pod "simpletest.rc-cmkk5" in namespace "gc-415"
    Aug 30 07:50:19.756: INFO: Deleting pod "simpletest.rc-cnpdn" in namespace "gc-415"
    Aug 30 07:50:19.786: INFO: Deleting pod "simpletest.rc-cpw9g" in namespace "gc-415"
    Aug 30 07:50:19.824: INFO: Deleting pod "simpletest.rc-cr955" in namespace "gc-415"
    Aug 30 07:50:19.859: INFO: Deleting pod "simpletest.rc-ct66l" in namespace "gc-415"
    Aug 30 07:50:19.892: INFO: Deleting pod "simpletest.rc-dhcvz" in namespace "gc-415"
    Aug 30 07:50:19.952: INFO: Deleting pod "simpletest.rc-dmr78" in namespace "gc-415"
    Aug 30 07:50:20.009: INFO: Deleting pod "simpletest.rc-f9ns7" in namespace "gc-415"
    Aug 30 07:50:20.081: INFO: Deleting pod "simpletest.rc-gnpq8" in namespace "gc-415"
    Aug 30 07:50:20.177: INFO: Deleting pod "simpletest.rc-grlvk" in namespace "gc-415"
    Aug 30 07:50:20.227: INFO: Deleting pod "simpletest.rc-gsvpr" in namespace "gc-415"
    Aug 30 07:50:20.289: INFO: Deleting pod "simpletest.rc-gvdz5" in namespace "gc-415"
    Aug 30 07:50:20.338: INFO: Deleting pod "simpletest.rc-h92h6" in namespace "gc-415"
    Aug 30 07:50:20.387: INFO: Deleting pod "simpletest.rc-hgnll" in namespace "gc-415"
    Aug 30 07:50:20.475: INFO: Deleting pod "simpletest.rc-hj8gr" in namespace "gc-415"
    Aug 30 07:50:20.532: INFO: Deleting pod "simpletest.rc-hll7j" in namespace "gc-415"
    Aug 30 07:50:20.609: INFO: Deleting pod "simpletest.rc-hntll" in namespace "gc-415"
    Aug 30 07:50:20.658: INFO: Deleting pod "simpletest.rc-j64cn" in namespace "gc-415"
    Aug 30 07:50:20.699: INFO: Deleting pod "simpletest.rc-j9lvw" in namespace "gc-415"
    Aug 30 07:50:20.750: INFO: Deleting pod "simpletest.rc-jhn8d" in namespace "gc-415"
    Aug 30 07:50:20.782: INFO: Deleting pod "simpletest.rc-k82cr" in namespace "gc-415"
    Aug 30 07:50:20.821: INFO: Deleting pod "simpletest.rc-kjtm7" in namespace "gc-415"
    Aug 30 07:50:20.897: INFO: Deleting pod "simpletest.rc-kt8ph" in namespace "gc-415"
    Aug 30 07:50:20.943: INFO: Deleting pod "simpletest.rc-lklcr" in namespace "gc-415"
    Aug 30 07:50:21.032: INFO: Deleting pod "simpletest.rc-mfhjh" in namespace "gc-415"
    Aug 30 07:50:21.104: INFO: Deleting pod "simpletest.rc-mqfs7" in namespace "gc-415"
    Aug 30 07:50:21.172: INFO: Deleting pod "simpletest.rc-mx65f" in namespace "gc-415"
    Aug 30 07:50:21.207: INFO: Deleting pod "simpletest.rc-n6zqh" in namespace "gc-415"
    Aug 30 07:50:21.347: INFO: Deleting pod "simpletest.rc-ncfjq" in namespace "gc-415"
    Aug 30 07:50:21.413: INFO: Deleting pod "simpletest.rc-nfz5j" in namespace "gc-415"
    Aug 30 07:50:21.488: INFO: Deleting pod "simpletest.rc-nt984" in namespace "gc-415"
    Aug 30 07:50:21.545: INFO: Deleting pod "simpletest.rc-pdflp" in namespace "gc-415"
    Aug 30 07:50:21.591: INFO: Deleting pod "simpletest.rc-px7p7" in namespace "gc-415"
    Aug 30 07:50:21.638: INFO: Deleting pod "simpletest.rc-q28wt" in namespace "gc-415"
    Aug 30 07:50:21.680: INFO: Deleting pod "simpletest.rc-qjc7d" in namespace "gc-415"
    Aug 30 07:50:21.724: INFO: Deleting pod "simpletest.rc-qn4m4" in namespace "gc-415"
    Aug 30 07:50:21.760: INFO: Deleting pod "simpletest.rc-qzzdn" in namespace "gc-415"
    Aug 30 07:50:21.819: INFO: Deleting pod "simpletest.rc-rbs5x" in namespace "gc-415"
    Aug 30 07:50:21.890: INFO: Deleting pod "simpletest.rc-rc6vz" in namespace "gc-415"
    Aug 30 07:50:21.931: INFO: Deleting pod "simpletest.rc-rsg7f" in namespace "gc-415"
    Aug 30 07:50:21.983: INFO: Deleting pod "simpletest.rc-rt7cb" in namespace "gc-415"
    Aug 30 07:50:22.026: INFO: Deleting pod "simpletest.rc-sj7g4" in namespace "gc-415"
    Aug 30 07:50:22.061: INFO: Deleting pod "simpletest.rc-sm9xr" in namespace "gc-415"
    Aug 30 07:50:22.103: INFO: Deleting pod "simpletest.rc-sp82c" in namespace "gc-415"
    Aug 30 07:50:22.198: INFO: Deleting pod "simpletest.rc-ttzrr" in namespace "gc-415"
    Aug 30 07:50:22.254: INFO: Deleting pod "simpletest.rc-v2zv7" in namespace "gc-415"
    Aug 30 07:50:22.294: INFO: Deleting pod "simpletest.rc-v988m" in namespace "gc-415"
    Aug 30 07:50:22.327: INFO: Deleting pod "simpletest.rc-vc8lh" in namespace "gc-415"
    Aug 30 07:50:22.364: INFO: Deleting pod "simpletest.rc-vpzmk" in namespace "gc-415"
    Aug 30 07:50:22.416: INFO: Deleting pod "simpletest.rc-vsfx6" in namespace "gc-415"
    Aug 30 07:50:22.460: INFO: Deleting pod "simpletest.rc-wjhcj" in namespace "gc-415"
    Aug 30 07:50:22.512: INFO: Deleting pod "simpletest.rc-wjv6k" in namespace "gc-415"
    Aug 30 07:50:22.553: INFO: Deleting pod "simpletest.rc-wx4ld" in namespace "gc-415"
    Aug 30 07:50:22.669: INFO: Deleting pod "simpletest.rc-x2zrr" in namespace "gc-415"
    Aug 30 07:50:22.739: INFO: Deleting pod "simpletest.rc-x5d8p" in namespace "gc-415"
    Aug 30 07:50:22.840: INFO: Deleting pod "simpletest.rc-x5r7m" in namespace "gc-415"
    Aug 30 07:50:22.911: INFO: Deleting pod "simpletest.rc-x9z9q" in namespace "gc-415"
    Aug 30 07:50:22.970: INFO: Deleting pod "simpletest.rc-xftr4" in namespace "gc-415"
    Aug 30 07:50:23.023: INFO: Deleting pod "simpletest.rc-xn9pm" in namespace "gc-415"
    Aug 30 07:50:23.080: INFO: Deleting pod "simpletest.rc-xsjjv" in namespace "gc-415"
    Aug 30 07:50:23.123: INFO: Deleting pod "simpletest.rc-xsmcx" in namespace "gc-415"
    Aug 30 07:50:23.205: INFO: Deleting pod "simpletest.rc-xwshj" in namespace "gc-415"
    Aug 30 07:50:23.250: INFO: Deleting pod "simpletest.rc-xz4k2" in namespace "gc-415"
    Aug 30 07:50:23.319: INFO: Deleting pod "simpletest.rc-z6j4w" in namespace "gc-415"
    Aug 30 07:50:23.363: INFO: Deleting pod "simpletest.rc-z6q6f" in namespace "gc-415"
    Aug 30 07:50:23.402: INFO: Deleting pod "simpletest.rc-zd8m2" in namespace "gc-415"
    Aug 30 07:50:23.447: INFO: Deleting pod "simpletest.rc-zdkhv" in namespace "gc-415"
    Aug 30 07:50:23.503: INFO: Deleting pod "simpletest.rc-zrbqt" in namespace "gc-415"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:50:23.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-415" for this suite. 08/30/23 07:50:23.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:50:23.575
Aug 30 07:50:23.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 07:50:23.577
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:50:23.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:50:23.676
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W0830 07:50:23.713310      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:50:23.734: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 30 07:50:28.849: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/30/23 07:50:28.849
Aug 30 07:50:28.850: INFO: Waiting up to 5m0s for pod "test-rollover-controller-hk72z" in namespace "deployment-797" to be "running"
Aug 30 07:50:28.931: INFO: Pod "test-rollover-controller-hk72z": Phase="Pending", Reason="", readiness=false. Elapsed: 81.691532ms
Aug 30 07:50:30.955: INFO: Pod "test-rollover-controller-hk72z": Phase="Running", Reason="", readiness=true. Elapsed: 2.104986839s
Aug 30 07:50:30.955: INFO: Pod "test-rollover-controller-hk72z" satisfied condition "running"
Aug 30 07:50:30.955: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 30 07:50:32.969: INFO: Creating deployment "test-rollover-deployment"
Aug 30 07:50:33.007: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 30 07:50:35.048: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 30 07:50:35.068: INFO: Ensure that both replica sets have 1 created replica
Aug 30 07:50:35.089: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 30 07:50:35.123: INFO: Updating deployment test-rollover-deployment
Aug 30 07:50:35.123: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 30 07:50:37.147: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 30 07:50:37.198: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 30 07:50:37.220: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 07:50:37.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:50:39.302: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 07:50:39.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:50:41.246: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 07:50:41.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:50:43.247: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 07:50:43.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:50:45.247: INFO: all replica sets need to contain the pod-template-hash label
Aug 30 07:50:45.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 30 07:50:47.247: INFO: 
Aug 30 07:50:47.247: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 07:50:47.276: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-797  5224c946-2b47-4a41-b963-8cd31debfdb8 129340 2 2023-08-30 07:50:32 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a21be18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 07:50:33 +0000 UTC,LastTransitionTime:2023-08-30 07:50:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-30 07:50:46 +0000 UTC,LastTransitionTime:2023-08-30 07:50:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 07:50:47.289: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-797  6a5f50c4-7cee-4310-9c07-59a650736df5 129330 2 2023-08-30 07:50:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcea7 0xc00abbcea8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00abbcf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:50:47.289: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 30 07:50:47.289: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-797  bf80600f-2787-4e72-a633-6dfa7bb06d33 129339 2 2023-08-30 07:50:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcd77 0xc00abbcd78}] [] [{e2e.test Update apps/v1 2023-08-30 07:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00abbce38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:50:47.289: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-797  c7f6c6ed-3a81-464d-9513-94b9daf9b6b6 128914 2 2023-08-30 07:50:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcfe7 0xc00abbcfe8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00abbd0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:50:47.300: INFO: Pod "test-rollover-deployment-6c6df9974f-jjfv9" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-jjfv9 test-rollover-deployment-6c6df9974f- deployment-797  651e2da7-64d6-42a9-880d-1f860fca3eb0 129015 0 2023-08-30 07:50:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:bb11d221f819146bd9373056b6338a4f168d0a5797f4db5b8aeeff739e30769a cni.projectcalico.org/podIP:172.30.58.95/32 cni.projectcalico.org/podIPs:172.30.58.95/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.95"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 6a5f50c4-7cee-4310-9c07-59a650736df5 0xc006b61107 0xc006b61108}] [] [{kube-controller-manager Update v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a5f50c4-7cee-4310-9c07-59a650736df5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7m988,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.95,StartTime:2023-08-30 07:50:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:50:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://8b78995769c94cf47b8eecfdaf271bd0905a8be82a1199cdc285db07cb0803ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 07:50:47.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-797" for this suite. 08/30/23 07:50:47.312
------------------------------
• [SLOW TEST] [23.758 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:50:23.575
    Aug 30 07:50:23.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 07:50:23.577
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:50:23.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:50:23.676
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W0830 07:50:23.713310      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:50:23.734: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 30 07:50:28.849: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/30/23 07:50:28.849
    Aug 30 07:50:28.850: INFO: Waiting up to 5m0s for pod "test-rollover-controller-hk72z" in namespace "deployment-797" to be "running"
    Aug 30 07:50:28.931: INFO: Pod "test-rollover-controller-hk72z": Phase="Pending", Reason="", readiness=false. Elapsed: 81.691532ms
    Aug 30 07:50:30.955: INFO: Pod "test-rollover-controller-hk72z": Phase="Running", Reason="", readiness=true. Elapsed: 2.104986839s
    Aug 30 07:50:30.955: INFO: Pod "test-rollover-controller-hk72z" satisfied condition "running"
    Aug 30 07:50:30.955: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 30 07:50:32.969: INFO: Creating deployment "test-rollover-deployment"
    Aug 30 07:50:33.007: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 30 07:50:35.048: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 30 07:50:35.068: INFO: Ensure that both replica sets have 1 created replica
    Aug 30 07:50:35.089: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 30 07:50:35.123: INFO: Updating deployment test-rollover-deployment
    Aug 30 07:50:35.123: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 30 07:50:37.147: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 30 07:50:37.198: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 30 07:50:37.220: INFO: all replica sets need to contain the pod-template-hash label
    Aug 30 07:50:37.220: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:50:39.302: INFO: all replica sets need to contain the pod-template-hash label
    Aug 30 07:50:39.302: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:50:41.246: INFO: all replica sets need to contain the pod-template-hash label
    Aug 30 07:50:41.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:50:43.247: INFO: all replica sets need to contain the pod-template-hash label
    Aug 30 07:50:43.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:50:45.247: INFO: all replica sets need to contain the pod-template-hash label
    Aug 30 07:50:45.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 50, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 50, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 30 07:50:47.247: INFO: 
    Aug 30 07:50:47.247: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 07:50:47.276: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-797  5224c946-2b47-4a41-b963-8cd31debfdb8 129340 2 2023-08-30 07:50:32 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a21be18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 07:50:33 +0000 UTC,LastTransitionTime:2023-08-30 07:50:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-30 07:50:46 +0000 UTC,LastTransitionTime:2023-08-30 07:50:33 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 30 07:50:47.289: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-797  6a5f50c4-7cee-4310-9c07-59a650736df5 129330 2 2023-08-30 07:50:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcea7 0xc00abbcea8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00abbcf68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:50:47.289: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 30 07:50:47.289: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-797  bf80600f-2787-4e72-a633-6dfa7bb06d33 129339 2 2023-08-30 07:50:23 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcd77 0xc00abbcd78}] [] [{e2e.test Update apps/v1 2023-08-30 07:50:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00abbce38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:50:47.289: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-797  c7f6c6ed-3a81-464d-9513-94b9daf9b6b6 128914 2 2023-08-30 07:50:33 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5224c946-2b47-4a41-b963-8cd31debfdb8 0xc00abbcfe7 0xc00abbcfe8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5224c946-2b47-4a41-b963-8cd31debfdb8\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00abbd0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:50:47.300: INFO: Pod "test-rollover-deployment-6c6df9974f-jjfv9" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-jjfv9 test-rollover-deployment-6c6df9974f- deployment-797  651e2da7-64d6-42a9-880d-1f860fca3eb0 129015 0 2023-08-30 07:50:35 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:bb11d221f819146bd9373056b6338a4f168d0a5797f4db5b8aeeff739e30769a cni.projectcalico.org/podIP:172.30.58.95/32 cni.projectcalico.org/podIPs:172.30.58.95/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.95"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 6a5f50c4-7cee-4310-9c07-59a650736df5 0xc006b61107 0xc006b61108}] [] [{kube-controller-manager Update v1 2023-08-30 07:50:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6a5f50c4-7cee-4310-9c07-59a650736df5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-08-30 07:50:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lg9g7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lg9g7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c66,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7m988,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:50:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.95,StartTime:2023-08-30 07:50:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:50:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://8b78995769c94cf47b8eecfdaf271bd0905a8be82a1199cdc285db07cb0803ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:50:47.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-797" for this suite. 08/30/23 07:50:47.312
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:50:47.334
Aug 30 07:50:47.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 07:50:47.335
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:50:47.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:50:47.394
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-325 08/30/23 07:50:47.403
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-325 08/30/23 07:50:47.475
Aug 30 07:50:47.525: INFO: Found 0 stateful pods, waiting for 1
Aug 30 07:50:57.539: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/30/23 07:50:57.562
STEP: Getting /status 08/30/23 07:50:57.592
Aug 30 07:50:57.606: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/30/23 07:50:57.606
Aug 30 07:50:57.632: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/30/23 07:50:57.632
Aug 30 07:50:57.637: INFO: Observed &StatefulSet event: ADDED
Aug 30 07:50:57.637: INFO: Found Statefulset ss in namespace statefulset-325 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 30 07:50:57.637: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/30/23 07:50:57.637
Aug 30 07:50:57.637: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 30 07:50:57.657: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/30/23 07:50:57.657
Aug 30 07:50:57.663: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 07:50:57.663: INFO: Deleting all statefulset in ns statefulset-325
Aug 30 07:50:57.677: INFO: Scaling statefulset ss to 0
Aug 30 07:51:07.731: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 07:51:07.743: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:51:07.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-325" for this suite. 08/30/23 07:51:07.798
------------------------------
• [SLOW TEST] [20.496 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:50:47.334
    Aug 30 07:50:47.334: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 07:50:47.335
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:50:47.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:50:47.394
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-325 08/30/23 07:50:47.403
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-325 08/30/23 07:50:47.475
    Aug 30 07:50:47.525: INFO: Found 0 stateful pods, waiting for 1
    Aug 30 07:50:57.539: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/30/23 07:50:57.562
    STEP: Getting /status 08/30/23 07:50:57.592
    Aug 30 07:50:57.606: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/30/23 07:50:57.606
    Aug 30 07:50:57.632: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/30/23 07:50:57.632
    Aug 30 07:50:57.637: INFO: Observed &StatefulSet event: ADDED
    Aug 30 07:50:57.637: INFO: Found Statefulset ss in namespace statefulset-325 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 30 07:50:57.637: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/30/23 07:50:57.637
    Aug 30 07:50:57.637: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 30 07:50:57.657: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/30/23 07:50:57.657
    Aug 30 07:50:57.663: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 07:50:57.663: INFO: Deleting all statefulset in ns statefulset-325
    Aug 30 07:50:57.677: INFO: Scaling statefulset ss to 0
    Aug 30 07:51:07.731: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 07:51:07.743: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:51:07.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-325" for this suite. 08/30/23 07:51:07.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:51:07.831
Aug 30 07:51:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename cronjob 08/30/23 07:51:07.832
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:51:07.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:51:07.902
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/30/23 07:51:07.913
W0830 07:51:07.934122      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 08/30/23 07:51:07.934
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/30/23 07:53:01.953
STEP: Removing cronjob 08/30/23 07:53:01.969
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1630" for this suite. 08/30/23 07:53:02.037
------------------------------
• [SLOW TEST] [114.241 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:51:07.831
    Aug 30 07:51:07.831: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename cronjob 08/30/23 07:51:07.832
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:51:07.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:51:07.902
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/30/23 07:51:07.913
    W0830 07:51:07.934122      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 08/30/23 07:51:07.934
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/30/23 07:53:01.953
    STEP: Removing cronjob 08/30/23 07:53:01.969
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:02.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1630" for this suite. 08/30/23 07:53:02.037
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:02.092
Aug 30 07:53:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename downward-api 08/30/23 07:53:02.093
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:02.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:02.191
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:53:02.23
Aug 30 07:53:02.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06" in namespace "downward-api-7268" to be "Succeeded or Failed"
Aug 30 07:53:02.366: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Pending", Reason="", readiness=false. Elapsed: 84.036554ms
Aug 30 07:53:04.385: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103381604s
Aug 30 07:53:06.380: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097845802s
STEP: Saw pod success 08/30/23 07:53:06.38
Aug 30 07:53:06.380: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06" satisfied condition "Succeeded or Failed"
Aug 30 07:53:06.390: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 container client-container: <nil>
STEP: delete the pod 08/30/23 07:53:06.428
Aug 30 07:53:06.470: INFO: Waiting for pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 to disappear
Aug 30 07:53:06.481: INFO: Pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:06.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7268" for this suite. 08/30/23 07:53:06.492
------------------------------
• [4.422 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:02.092
    Aug 30 07:53:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename downward-api 08/30/23 07:53:02.093
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:02.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:02.191
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:53:02.23
    Aug 30 07:53:02.282: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06" in namespace "downward-api-7268" to be "Succeeded or Failed"
    Aug 30 07:53:02.366: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Pending", Reason="", readiness=false. Elapsed: 84.036554ms
    Aug 30 07:53:04.385: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103381604s
    Aug 30 07:53:06.380: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097845802s
    STEP: Saw pod success 08/30/23 07:53:06.38
    Aug 30 07:53:06.380: INFO: Pod "downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06" satisfied condition "Succeeded or Failed"
    Aug 30 07:53:06.390: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:53:06.428
    Aug 30 07:53:06.470: INFO: Waiting for pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 to disappear
    Aug 30 07:53:06.481: INFO: Pod downwardapi-volume-d9d0b501-df88-4ba8-ad6a-a954628fbe06 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:06.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7268" for this suite. 08/30/23 07:53:06.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:06.515
Aug 30 07:53:06.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:53:06.516
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:06.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:06.575
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-65906b9e-24cb-4e78-a8f5-07d2b289d406 08/30/23 07:53:06.586
STEP: Creating a pod to test consume secrets 08/30/23 07:53:06.612
Aug 30 07:53:06.644: INFO: Waiting up to 5m0s for pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee" in namespace "secrets-910" to be "Succeeded or Failed"
Aug 30 07:53:06.666: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Pending", Reason="", readiness=false. Elapsed: 21.567492ms
Aug 30 07:53:08.679: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034945571s
Aug 30 07:53:10.685: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040065023s
STEP: Saw pod success 08/30/23 07:53:10.685
Aug 30 07:53:10.685: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee" satisfied condition "Succeeded or Failed"
Aug 30 07:53:10.697: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee container secret-env-test: <nil>
STEP: delete the pod 08/30/23 07:53:10.719
Aug 30 07:53:10.761: INFO: Waiting for pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee to disappear
Aug 30 07:53:10.771: INFO: Pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:10.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-910" for this suite. 08/30/23 07:53:10.783
------------------------------
• [4.290 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:06.515
    Aug 30 07:53:06.515: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:53:06.516
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:06.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:06.575
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-65906b9e-24cb-4e78-a8f5-07d2b289d406 08/30/23 07:53:06.586
    STEP: Creating a pod to test consume secrets 08/30/23 07:53:06.612
    Aug 30 07:53:06.644: INFO: Waiting up to 5m0s for pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee" in namespace "secrets-910" to be "Succeeded or Failed"
    Aug 30 07:53:06.666: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Pending", Reason="", readiness=false. Elapsed: 21.567492ms
    Aug 30 07:53:08.679: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034945571s
    Aug 30 07:53:10.685: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040065023s
    STEP: Saw pod success 08/30/23 07:53:10.685
    Aug 30 07:53:10.685: INFO: Pod "pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee" satisfied condition "Succeeded or Failed"
    Aug 30 07:53:10.697: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee container secret-env-test: <nil>
    STEP: delete the pod 08/30/23 07:53:10.719
    Aug 30 07:53:10.761: INFO: Waiting for pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee to disappear
    Aug 30 07:53:10.771: INFO: Pod pod-secrets-02be9cc6-3f4c-4e6b-b9cb-85a9560009ee no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:10.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-910" for this suite. 08/30/23 07:53:10.783
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:10.807
Aug 30 07:53:10.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-runtime 08/30/23 07:53:10.808
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:10.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:10.869
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/30/23 07:53:10.882
STEP: wait for the container to reach Succeeded 08/30/23 07:53:10.925
STEP: get the container status 08/30/23 07:53:15.031
STEP: the container should be terminated 08/30/23 07:53:15.043
STEP: the termination message should be set 08/30/23 07:53:15.044
Aug 30 07:53:15.044: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/30/23 07:53:15.044
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:15.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8719" for this suite. 08/30/23 07:53:15.112
------------------------------
• [4.328 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:10.807
    Aug 30 07:53:10.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-runtime 08/30/23 07:53:10.808
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:10.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:10.869
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/30/23 07:53:10.882
    STEP: wait for the container to reach Succeeded 08/30/23 07:53:10.925
    STEP: get the container status 08/30/23 07:53:15.031
    STEP: the container should be terminated 08/30/23 07:53:15.043
    STEP: the termination message should be set 08/30/23 07:53:15.044
    Aug 30 07:53:15.044: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/30/23 07:53:15.044
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:15.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8719" for this suite. 08/30/23 07:53:15.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:15.137
Aug 30 07:53:15.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename init-container 08/30/23 07:53:15.138
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:15.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:15.197
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/30/23 07:53:15.21
Aug 30 07:53:15.210: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:21.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4521" for this suite. 08/30/23 07:53:21.426
------------------------------
• [SLOW TEST] [6.341 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:15.137
    Aug 30 07:53:15.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename init-container 08/30/23 07:53:15.138
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:15.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:15.197
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/30/23 07:53:15.21
    Aug 30 07:53:15.210: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:21.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4521" for this suite. 08/30/23 07:53:21.426
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:21.479
Aug 30 07:53:21.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:53:21.48
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:21.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:21.595
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/30/23 07:53:21.605
Aug 30 07:53:21.704: INFO: Waiting up to 5m0s for pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6" in namespace "emptydir-5114" to be "Succeeded or Failed"
Aug 30 07:53:21.760: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Pending", Reason="", readiness=false. Elapsed: 55.739229ms
Aug 30 07:53:23.825: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120863002s
Aug 30 07:53:25.771: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066588981s
STEP: Saw pod success 08/30/23 07:53:25.771
Aug 30 07:53:25.771: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6" satisfied condition "Succeeded or Failed"
Aug 30 07:53:25.780: INFO: Trying to get logs from node 10.135.139.190 pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 container test-container: <nil>
STEP: delete the pod 08/30/23 07:53:25.807
Aug 30 07:53:25.837: INFO: Waiting for pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 to disappear
Aug 30 07:53:25.848: INFO: Pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:25.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5114" for this suite. 08/30/23 07:53:25.859
------------------------------
• [4.400 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:21.479
    Aug 30 07:53:21.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:53:21.48
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:21.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:21.595
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/30/23 07:53:21.605
    Aug 30 07:53:21.704: INFO: Waiting up to 5m0s for pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6" in namespace "emptydir-5114" to be "Succeeded or Failed"
    Aug 30 07:53:21.760: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Pending", Reason="", readiness=false. Elapsed: 55.739229ms
    Aug 30 07:53:23.825: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120863002s
    Aug 30 07:53:25.771: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066588981s
    STEP: Saw pod success 08/30/23 07:53:25.771
    Aug 30 07:53:25.771: INFO: Pod "pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6" satisfied condition "Succeeded or Failed"
    Aug 30 07:53:25.780: INFO: Trying to get logs from node 10.135.139.190 pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 container test-container: <nil>
    STEP: delete the pod 08/30/23 07:53:25.807
    Aug 30 07:53:25.837: INFO: Waiting for pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 to disappear
    Aug 30 07:53:25.848: INFO: Pod pod-5ba1b3a7-06a6-4a4a-a695-5c03222384a6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:25.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5114" for this suite. 08/30/23 07:53:25.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:25.88
Aug 30 07:53:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:53:25.88
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:25.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:25.938
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/30/23 07:53:25.947
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/30/23 07:53:25.952
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/30/23 07:53:25.952
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/30/23 07:53:25.953
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/30/23 07:53:25.955
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/30/23 07:53:25.956
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/30/23 07:53:25.959
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-4043" for this suite. 08/30/23 07:53:25.989
------------------------------
• [0.134 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:25.88
    Aug 30 07:53:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename custom-resource-definition 08/30/23 07:53:25.88
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:25.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:25.938
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/30/23 07:53:25.947
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/30/23 07:53:25.952
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/30/23 07:53:25.952
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/30/23 07:53:25.953
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/30/23 07:53:25.955
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/30/23 07:53:25.956
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/30/23 07:53:25.959
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-4043" for this suite. 08/30/23 07:53:25.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:26.015
Aug 30 07:53:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename webhook 08/30/23 07:53:26.015
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:26.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:26.093
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/30/23 07:53:26.158
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:53:26.466
STEP: Deploying the webhook pod 08/30/23 07:53:26.501
STEP: Wait for the deployment to be ready 08/30/23 07:53:26.542
Aug 30 07:53:26.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/30/23 07:53:28.602
STEP: Verifying the service has paired with the endpoint 08/30/23 07:53:28.674
Aug 30 07:53:29.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/30/23 07:53:29.689
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/30/23 07:53:29.695
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/30/23 07:53:29.696
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/30/23 07:53:29.696
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/30/23 07:53:29.699
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/30/23 07:53:29.699
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/30/23 07:53:29.705
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8244" for this suite. 08/30/23 07:53:29.895
STEP: Destroying namespace "webhook-8244-markers" for this suite. 08/30/23 07:53:29.918
------------------------------
• [3.926 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:26.015
    Aug 30 07:53:26.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename webhook 08/30/23 07:53:26.015
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:26.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:26.093
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/30/23 07:53:26.158
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/30/23 07:53:26.466
    STEP: Deploying the webhook pod 08/30/23 07:53:26.501
    STEP: Wait for the deployment to be ready 08/30/23 07:53:26.542
    Aug 30 07:53:26.565: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/30/23 07:53:28.602
    STEP: Verifying the service has paired with the endpoint 08/30/23 07:53:28.674
    Aug 30 07:53:29.676: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/30/23 07:53:29.689
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/30/23 07:53:29.695
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/30/23 07:53:29.696
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/30/23 07:53:29.696
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/30/23 07:53:29.699
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/30/23 07:53:29.699
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/30/23 07:53:29.705
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8244" for this suite. 08/30/23 07:53:29.895
    STEP: Destroying namespace "webhook-8244-markers" for this suite. 08/30/23 07:53:29.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:29.942
Aug 30 07:53:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:53:29.942
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:29.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:30.002
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-0ead96d5-c513-45b1-b1ec-29c8c6bbf7c9 08/30/23 07:53:30.012
STEP: Creating a pod to test consume secrets 08/30/23 07:53:30.037
Aug 30 07:53:30.065: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086" in namespace "projected-1802" to be "Succeeded or Failed"
Aug 30 07:53:30.083: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Pending", Reason="", readiness=false. Elapsed: 17.862407ms
Aug 30 07:53:32.100: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034890179s
Aug 30 07:53:34.094: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029478534s
STEP: Saw pod success 08/30/23 07:53:34.094
Aug 30 07:53:34.095: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086" satisfied condition "Succeeded or Failed"
Aug 30 07:53:34.111: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:53:34.131
Aug 30 07:53:34.178: INFO: Waiting for pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 to disappear
Aug 30 07:53:34.190: INFO: Pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:34.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1802" for this suite. 08/30/23 07:53:34.205
------------------------------
• [4.328 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:29.942
    Aug 30 07:53:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:53:29.942
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:29.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:30.002
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-0ead96d5-c513-45b1-b1ec-29c8c6bbf7c9 08/30/23 07:53:30.012
    STEP: Creating a pod to test consume secrets 08/30/23 07:53:30.037
    Aug 30 07:53:30.065: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086" in namespace "projected-1802" to be "Succeeded or Failed"
    Aug 30 07:53:30.083: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Pending", Reason="", readiness=false. Elapsed: 17.862407ms
    Aug 30 07:53:32.100: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034890179s
    Aug 30 07:53:34.094: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029478534s
    STEP: Saw pod success 08/30/23 07:53:34.094
    Aug 30 07:53:34.095: INFO: Pod "pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086" satisfied condition "Succeeded or Failed"
    Aug 30 07:53:34.111: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:53:34.131
    Aug 30 07:53:34.178: INFO: Waiting for pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 to disappear
    Aug 30 07:53:34.190: INFO: Pod pod-projected-secrets-f3da63a2-af18-4f1d-bf6b-e4be7bb0b086 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:34.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1802" for this suite. 08/30/23 07:53:34.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:34.273
Aug 30 07:53:34.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename var-expansion 08/30/23 07:53:34.274
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:34.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:34.328
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/30/23 07:53:34.342
Aug 30 07:53:34.376: INFO: Waiting up to 5m0s for pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc" in namespace "var-expansion-313" to be "Succeeded or Failed"
Aug 30 07:53:34.395: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.211785ms
Aug 30 07:53:36.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033232528s
Aug 30 07:53:38.428: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051568557s
Aug 30 07:53:40.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033020243s
STEP: Saw pod success 08/30/23 07:53:40.409
Aug 30 07:53:40.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc" satisfied condition "Succeeded or Failed"
Aug 30 07:53:40.431: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc container dapi-container: <nil>
STEP: delete the pod 08/30/23 07:53:40.453
Aug 30 07:53:40.497: INFO: Waiting for pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc to disappear
Aug 30 07:53:40.508: INFO: Pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-313" for this suite. 08/30/23 07:53:40.522
------------------------------
• [SLOW TEST] [6.275 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:34.273
    Aug 30 07:53:34.273: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename var-expansion 08/30/23 07:53:34.274
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:34.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:34.328
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/30/23 07:53:34.342
    Aug 30 07:53:34.376: INFO: Waiting up to 5m0s for pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc" in namespace "var-expansion-313" to be "Succeeded or Failed"
    Aug 30 07:53:34.395: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.211785ms
    Aug 30 07:53:36.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033232528s
    Aug 30 07:53:38.428: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051568557s
    Aug 30 07:53:40.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033020243s
    STEP: Saw pod success 08/30/23 07:53:40.409
    Aug 30 07:53:40.409: INFO: Pod "var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc" satisfied condition "Succeeded or Failed"
    Aug 30 07:53:40.431: INFO: Trying to get logs from node 10.135.139.190 pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc container dapi-container: <nil>
    STEP: delete the pod 08/30/23 07:53:40.453
    Aug 30 07:53:40.497: INFO: Waiting for pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc to disappear
    Aug 30 07:53:40.508: INFO: Pod var-expansion-759ba1ab-9af2-4ae2-b046-ccbedcb2eedc no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:40.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-313" for this suite. 08/30/23 07:53:40.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:40.552
Aug 30 07:53:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 07:53:40.554
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:40.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:40.611
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/30/23 07:53:40.648
Aug 30 07:53:40.684: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7433" to be "running and ready"
Aug 30 07:53:40.704: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.541338ms
Aug 30 07:53:40.704: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:53:42.739: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054344977s
Aug 30 07:53:42.739: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:53:44.718: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033333555s
Aug 30 07:53:44.718: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 30 07:53:44.718: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/30/23 07:53:44.731
Aug 30 07:53:44.770: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7433" to be "running and ready"
Aug 30 07:53:44.783: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.803994ms
Aug 30 07:53:44.783: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:53:46.858: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088100015s
Aug 30 07:53:46.858: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:53:48.872: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.102343571s
Aug 30 07:53:48.872: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 30 07:53:48.872: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/30/23 07:53:48.887
Aug 30 07:53:48.976: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 07:53:49.100: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 07:53:51.103: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 07:53:51.121: INFO: Pod pod-with-prestop-http-hook still exists
Aug 30 07:53:53.106: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 30 07:53:53.120: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/30/23 07:53:53.12
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-7433" for this suite. 08/30/23 07:53:53.241
------------------------------
• [SLOW TEST] [12.718 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:40.552
    Aug 30 07:53:40.552: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/30/23 07:53:40.554
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:40.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:40.611
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/30/23 07:53:40.648
    Aug 30 07:53:40.684: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7433" to be "running and ready"
    Aug 30 07:53:40.704: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 19.541338ms
    Aug 30 07:53:40.704: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:53:42.739: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054344977s
    Aug 30 07:53:42.739: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:53:44.718: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.033333555s
    Aug 30 07:53:44.718: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 30 07:53:44.718: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/30/23 07:53:44.731
    Aug 30 07:53:44.770: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-7433" to be "running and ready"
    Aug 30 07:53:44.783: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.803994ms
    Aug 30 07:53:44.783: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:53:46.858: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088100015s
    Aug 30 07:53:46.858: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:53:48.872: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.102343571s
    Aug 30 07:53:48.872: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 30 07:53:48.872: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/30/23 07:53:48.887
    Aug 30 07:53:48.976: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 30 07:53:49.100: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 30 07:53:51.103: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 30 07:53:51.121: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 30 07:53:53.106: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 30 07:53:53.120: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/30/23 07:53:53.12
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-7433" for this suite. 08/30/23 07:53:53.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:53.282
Aug 30 07:53:53.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename proxy 08/30/23 07:53:53.284
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:53.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:53.448
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 30 07:53:53.534: INFO: Creating pod...
Aug 30 07:53:53.573: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-993" to be "running"
Aug 30 07:53:53.587: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.739208ms
Aug 30 07:53:55.604: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.031048297s
Aug 30 07:53:55.604: INFO: Pod "agnhost" satisfied condition "running"
Aug 30 07:53:55.604: INFO: Creating service...
Aug 30 07:53:55.653: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/DELETE
Aug 30 07:53:55.744: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 07:53:55.744: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/GET
Aug 30 07:53:55.822: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 30 07:53:55.822: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/HEAD
Aug 30 07:53:55.878: INFO: http.Client request:HEAD | StatusCode:200
Aug 30 07:53:55.878: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 30 07:53:55.948: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 07:53:55.948: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/PATCH
Aug 30 07:53:56.017: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 07:53:56.017: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/POST
Aug 30 07:53:56.096: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 07:53:56.096: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/PUT
Aug 30 07:53:56.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 30 07:53:56.143: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/DELETE
Aug 30 07:53:56.246: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 30 07:53:56.246: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/GET
Aug 30 07:53:56.336: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 30 07:53:56.337: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/HEAD
Aug 30 07:53:56.461: INFO: http.Client request:HEAD | StatusCode:200
Aug 30 07:53:56.461: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/OPTIONS
Aug 30 07:53:56.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 30 07:53:56.558: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/PATCH
Aug 30 07:53:56.646: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 30 07:53:56.646: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/POST
Aug 30 07:53:56.815: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 30 07:53:56.816: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/PUT
Aug 30 07:53:56.924: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:56.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-993" for this suite. 08/30/23 07:53:56.943
------------------------------
• [3.689 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:53.282
    Aug 30 07:53:53.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename proxy 08/30/23 07:53:53.284
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:53.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:53.448
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 30 07:53:53.534: INFO: Creating pod...
    Aug 30 07:53:53.573: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-993" to be "running"
    Aug 30 07:53:53.587: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 13.739208ms
    Aug 30 07:53:55.604: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.031048297s
    Aug 30 07:53:55.604: INFO: Pod "agnhost" satisfied condition "running"
    Aug 30 07:53:55.604: INFO: Creating service...
    Aug 30 07:53:55.653: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/DELETE
    Aug 30 07:53:55.744: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 30 07:53:55.744: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/GET
    Aug 30 07:53:55.822: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 30 07:53:55.822: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/HEAD
    Aug 30 07:53:55.878: INFO: http.Client request:HEAD | StatusCode:200
    Aug 30 07:53:55.878: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 30 07:53:55.948: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 30 07:53:55.948: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/PATCH
    Aug 30 07:53:56.017: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 30 07:53:56.017: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/POST
    Aug 30 07:53:56.096: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 30 07:53:56.096: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/pods/agnhost/proxy/some/path/with/PUT
    Aug 30 07:53:56.143: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 30 07:53:56.143: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/DELETE
    Aug 30 07:53:56.246: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 30 07:53:56.246: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/GET
    Aug 30 07:53:56.336: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 30 07:53:56.337: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/HEAD
    Aug 30 07:53:56.461: INFO: http.Client request:HEAD | StatusCode:200
    Aug 30 07:53:56.461: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/OPTIONS
    Aug 30 07:53:56.558: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 30 07:53:56.558: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/PATCH
    Aug 30 07:53:56.646: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 30 07:53:56.646: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/POST
    Aug 30 07:53:56.815: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 30 07:53:56.816: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-993/services/test-service/proxy/some/path/with/PUT
    Aug 30 07:53:56.924: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:56.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-993" for this suite. 08/30/23 07:53:56.943
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:56.971
Aug 30 07:53:56.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename events 08/30/23 07:53:56.972
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:57.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:57.056
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/30/23 07:53:57.066
STEP: listing events in all namespaces 08/30/23 07:53:57.095
STEP: listing events in test namespace 08/30/23 07:53:57.166
STEP: listing events with field selection filtering on source 08/30/23 07:53:57.178
STEP: listing events with field selection filtering on reportingController 08/30/23 07:53:57.193
STEP: getting the test event 08/30/23 07:53:57.202
STEP: patching the test event 08/30/23 07:53:57.216
STEP: getting the test event 08/30/23 07:53:57.247
STEP: updating the test event 08/30/23 07:53:57.257
STEP: getting the test event 08/30/23 07:53:57.281
STEP: deleting the test event 08/30/23 07:53:57.292
STEP: listing events in all namespaces 08/30/23 07:53:57.315
STEP: listing events in test namespace 08/30/23 07:53:57.367
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:53:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1670" for this suite. 08/30/23 07:53:57.396
------------------------------
• [0.458 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:56.971
    Aug 30 07:53:56.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename events 08/30/23 07:53:56.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:57.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:57.056
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/30/23 07:53:57.066
    STEP: listing events in all namespaces 08/30/23 07:53:57.095
    STEP: listing events in test namespace 08/30/23 07:53:57.166
    STEP: listing events with field selection filtering on source 08/30/23 07:53:57.178
    STEP: listing events with field selection filtering on reportingController 08/30/23 07:53:57.193
    STEP: getting the test event 08/30/23 07:53:57.202
    STEP: patching the test event 08/30/23 07:53:57.216
    STEP: getting the test event 08/30/23 07:53:57.247
    STEP: updating the test event 08/30/23 07:53:57.257
    STEP: getting the test event 08/30/23 07:53:57.281
    STEP: deleting the test event 08/30/23 07:53:57.292
    STEP: listing events in all namespaces 08/30/23 07:53:57.315
    STEP: listing events in test namespace 08/30/23 07:53:57.367
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:53:57.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1670" for this suite. 08/30/23 07:53:57.396
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:53:57.429
Aug 30 07:53:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename emptydir 08/30/23 07:53:57.431
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:57.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:57.516
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/30/23 07:53:57.527
Aug 30 07:53:57.574: INFO: Waiting up to 5m0s for pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d" in namespace "emptydir-6737" to be "Succeeded or Failed"
Aug 30 07:53:57.594: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.682636ms
Aug 30 07:53:59.644: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069666217s
Aug 30 07:54:01.606: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031366395s
STEP: Saw pod success 08/30/23 07:54:01.606
Aug 30 07:54:01.606: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d" satisfied condition "Succeeded or Failed"
Aug 30 07:54:01.617: INFO: Trying to get logs from node 10.135.139.190 pod pod-ecd03f17-f290-479f-bf00-b394d91af45d container test-container: <nil>
STEP: delete the pod 08/30/23 07:54:01.663
Aug 30 07:54:01.703: INFO: Waiting for pod pod-ecd03f17-f290-479f-bf00-b394d91af45d to disappear
Aug 30 07:54:01.714: INFO: Pod pod-ecd03f17-f290-479f-bf00-b394d91af45d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:01.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6737" for this suite. 08/30/23 07:54:01.725
------------------------------
• [4.318 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:53:57.429
    Aug 30 07:53:57.429: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename emptydir 08/30/23 07:53:57.431
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:53:57.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:53:57.516
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/30/23 07:53:57.527
    Aug 30 07:53:57.574: INFO: Waiting up to 5m0s for pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d" in namespace "emptydir-6737" to be "Succeeded or Failed"
    Aug 30 07:53:57.594: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.682636ms
    Aug 30 07:53:59.644: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069666217s
    Aug 30 07:54:01.606: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031366395s
    STEP: Saw pod success 08/30/23 07:54:01.606
    Aug 30 07:54:01.606: INFO: Pod "pod-ecd03f17-f290-479f-bf00-b394d91af45d" satisfied condition "Succeeded or Failed"
    Aug 30 07:54:01.617: INFO: Trying to get logs from node 10.135.139.190 pod pod-ecd03f17-f290-479f-bf00-b394d91af45d container test-container: <nil>
    STEP: delete the pod 08/30/23 07:54:01.663
    Aug 30 07:54:01.703: INFO: Waiting for pod pod-ecd03f17-f290-479f-bf00-b394d91af45d to disappear
    Aug 30 07:54:01.714: INFO: Pod pod-ecd03f17-f290-479f-bf00-b394d91af45d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:01.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6737" for this suite. 08/30/23 07:54:01.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:01.748
Aug 30 07:54:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:54:01.749
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:01.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:01.808
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-544e8a8d-1b32-453e-be1f-c05d5c6d53a4 08/30/23 07:54:01.834
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2803" for this suite. 08/30/23 07:54:01.857
------------------------------
• [0.137 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:01.748
    Aug 30 07:54:01.749: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:54:01.749
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:01.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:01.808
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-544e8a8d-1b32-453e-be1f-c05d5c6d53a4 08/30/23 07:54:01.834
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:01.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2803" for this suite. 08/30/23 07:54:01.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:01.888
Aug 30 07:54:01.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename ingressclass 08/30/23 07:54:01.889
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:01.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:01.958
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/30/23 07:54:01.97
STEP: getting /apis/networking.k8s.io 08/30/23 07:54:01.979
STEP: getting /apis/networking.k8s.iov1 08/30/23 07:54:01.983
STEP: creating 08/30/23 07:54:01.997
STEP: getting 08/30/23 07:54:02.098
STEP: listing 08/30/23 07:54:02.127
STEP: watching 08/30/23 07:54:02.136
Aug 30 07:54:02.136: INFO: starting watch
STEP: patching 08/30/23 07:54:02.14
STEP: updating 08/30/23 07:54:02.155
Aug 30 07:54:02.177: INFO: waiting for watch events with expected annotations
Aug 30 07:54:02.177: INFO: saw patched and updated annotations
STEP: deleting 08/30/23 07:54:02.177
STEP: deleting a collection 08/30/23 07:54:02.218
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-9129" for this suite. 08/30/23 07:54:02.265
------------------------------
• [0.401 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:01.888
    Aug 30 07:54:01.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename ingressclass 08/30/23 07:54:01.889
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:01.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:01.958
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/30/23 07:54:01.97
    STEP: getting /apis/networking.k8s.io 08/30/23 07:54:01.979
    STEP: getting /apis/networking.k8s.iov1 08/30/23 07:54:01.983
    STEP: creating 08/30/23 07:54:01.997
    STEP: getting 08/30/23 07:54:02.098
    STEP: listing 08/30/23 07:54:02.127
    STEP: watching 08/30/23 07:54:02.136
    Aug 30 07:54:02.136: INFO: starting watch
    STEP: patching 08/30/23 07:54:02.14
    STEP: updating 08/30/23 07:54:02.155
    Aug 30 07:54:02.177: INFO: waiting for watch events with expected annotations
    Aug 30 07:54:02.177: INFO: saw patched and updated annotations
    STEP: deleting 08/30/23 07:54:02.177
    STEP: deleting a collection 08/30/23 07:54:02.218
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-9129" for this suite. 08/30/23 07:54:02.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:02.29
Aug 30 07:54:02.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename deployment 08/30/23 07:54:02.292
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:02.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:02.347
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 30 07:54:02.358: INFO: Creating simple deployment test-new-deployment
W0830 07:54:02.395948      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:54:02.443: INFO: deployment "test-new-deployment" doesn't have the required revision set
Aug 30 07:54:04.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 08/30/23 07:54:06.503
STEP: updating a scale subresource 08/30/23 07:54:06.513
STEP: verifying the deployment Spec.Replicas was modified 08/30/23 07:54:06.532
STEP: Patch a scale subresource 08/30/23 07:54:06.544
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 30 07:54:06.606: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4927  3837324a-1f3f-49cb-8e52-7f628f908896 131754 3 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-30 07:54:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b77548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 07:54:04 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-30 07:54:04 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 30 07:54:06.618: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4927  0b5f5873-eec1-4dd2-9435-53553bd22605 131761 2 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3837324a-1f3f-49cb-8e52-7f628f908896 0xc004e56cd7 0xc004e56cd8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3837324a-1f3f-49cb-8e52-7f628f908896\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e56d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 30 07:54:06.631: INFO: Pod "test-new-deployment-7f5969cbc7-bp75n" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-bp75n test-new-deployment-7f5969cbc7- deployment-4927  8b00e9dc-2f88-4d1a-bc7d-7ce06a6e3727 131762 0 2023-08-30 07:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0b5f5873-eec1-4dd2-9435-53553bd22605 0xc004e57107 0xc004e57108}] [] [{kube-controller-manager Update v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b5f5873-eec1-4dd2-9435-53553bd22605\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9g8dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9g8dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dmdlb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:,StartTime:2023-08-30 07:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 30 07:54:06.631: INFO: Pod "test-new-deployment-7f5969cbc7-pp4wn" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-pp4wn test-new-deployment-7f5969cbc7- deployment-4927  66e60236-2c8b-48ba-b460-22438f169149 131731 0 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cee031b40ad1d7df2cff9f5390e19e7f40c7b27e4323c426f462208b55a0b259 cni.projectcalico.org/podIP:172.30.58.111/32 cni.projectcalico.org/podIPs:172.30.58.111/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.58.111"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0b5f5873-eec1-4dd2-9435-53553bd22605 0xc004e57337 0xc004e57338}] [] [{kube-controller-manager Update v1 2023-08-30 07:54:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b5f5873-eec1-4dd2-9435-53553bd22605\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 07:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 07:54:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2s84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2s84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.111,StartTime:2023-08-30 07:54:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:54:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7b5486d3cc83dc2e1362d6348df53f642e1e71a81a9ebcc4867e8732a0056b1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:06.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4927" for this suite. 08/30/23 07:54:06.643
------------------------------
• [4.419 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:02.29
    Aug 30 07:54:02.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename deployment 08/30/23 07:54:02.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:02.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:02.347
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 30 07:54:02.358: INFO: Creating simple deployment test-new-deployment
    W0830 07:54:02.395948      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:54:02.443: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Aug 30 07:54:04.479: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 30, 7, 54, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-7f5969cbc7\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 08/30/23 07:54:06.503
    STEP: updating a scale subresource 08/30/23 07:54:06.513
    STEP: verifying the deployment Spec.Replicas was modified 08/30/23 07:54:06.532
    STEP: Patch a scale subresource 08/30/23 07:54:06.544
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 30 07:54:06.606: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4927  3837324a-1f3f-49cb-8e52-7f628f908896 131754 3 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-30 07:54:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:54:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001b77548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-30 07:54:04 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-30 07:54:04 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 30 07:54:06.618: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4927  0b5f5873-eec1-4dd2-9435-53553bd22605 131761 2 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 3837324a-1f3f-49cb-8e52-7f628f908896 0xc004e56cd7 0xc004e56cd8}] [] [{kube-controller-manager Update apps/v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3837324a-1f3f-49cb-8e52-7f628f908896\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e56d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 30 07:54:06.631: INFO: Pod "test-new-deployment-7f5969cbc7-bp75n" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-bp75n test-new-deployment-7f5969cbc7- deployment-4927  8b00e9dc-2f88-4d1a-bc7d-7ce06a6e3727 131762 0 2023-08-30 07:54:06 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0b5f5873-eec1-4dd2-9435-53553bd22605 0xc004e57107 0xc004e57108}] [] [{kube-controller-manager Update v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b5f5873-eec1-4dd2-9435-53553bd22605\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-30 07:54:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9g8dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9g8dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.185,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dmdlb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.185,PodIP:,StartTime:2023-08-30 07:54:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 30 07:54:06.631: INFO: Pod "test-new-deployment-7f5969cbc7-pp4wn" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-pp4wn test-new-deployment-7f5969cbc7- deployment-4927  66e60236-2c8b-48ba-b460-22438f169149 131731 0 2023-08-30 07:54:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cee031b40ad1d7df2cff9f5390e19e7f40c7b27e4323c426f462208b55a0b259 cni.projectcalico.org/podIP:172.30.58.111/32 cni.projectcalico.org/podIPs:172.30.58.111/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.58.111"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0b5f5873-eec1-4dd2-9435-53553bd22605 0xc004e57337 0xc004e57338}] [] [{kube-controller-manager Update v1 2023-08-30 07:54:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0b5f5873-eec1-4dd2-9435-53553bd22605\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-30 07:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-08-30 07:54:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{}}}} status} {kubelet Update v1 2023-08-30 07:54:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.58.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2s84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2s84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.135.139.190,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c68,c7,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-30 07:54:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.135.139.190,PodIP:172.30.58.111,StartTime:2023-08-30 07:54:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-30 07:54:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://7b5486d3cc83dc2e1362d6348df53f642e1e71a81a9ebcc4867e8732a0056b1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.58.111,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:06.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4927" for this suite. 08/30/23 07:54:06.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:06.71
Aug 30 07:54:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:54:06.711
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:06.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:06.772
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-322 08/30/23 07:54:06.78
STEP: creating replication controller nodeport-test in namespace services-322 08/30/23 07:54:06.839
I0830 07:54:06.860179      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-322, replica count: 2
I0830 07:54:09.912777      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:54:09.912: INFO: Creating new exec pod
Aug 30 07:54:09.941: INFO: Waiting up to 5m0s for pod "execpod5s4t9" in namespace "services-322" to be "running"
Aug 30 07:54:09.956: INFO: Pod "execpod5s4t9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.775187ms
Aug 30 07:54:11.976: INFO: Pod "execpod5s4t9": Phase="Running", Reason="", readiness=true. Elapsed: 2.034365584s
Aug 30 07:54:11.976: INFO: Pod "execpod5s4t9" satisfied condition "running"
Aug 30 07:54:12.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 30 07:54:13.459: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 30 07:54:13.459: INFO: stdout: ""
Aug 30 07:54:13.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 172.21.70.63 80'
Aug 30 07:54:13.755: INFO: stderr: "+ nc -v -z -w 2 172.21.70.63 80\nConnection to 172.21.70.63 80 port [tcp/http] succeeded!\n"
Aug 30 07:54:13.755: INFO: stdout: ""
Aug 30 07:54:13.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 32159'
Aug 30 07:54:13.993: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 32159\nConnection to 10.135.139.183 32159 port [tcp/*] succeeded!\n"
Aug 30 07:54:13.993: INFO: stdout: ""
Aug 30 07:54:13.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 32159'
Aug 30 07:54:14.232: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 32159\nConnection to 10.135.139.190 32159 port [tcp/*] succeeded!\n"
Aug 30 07:54:14.232: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-322" for this suite. 08/30/23 07:54:14.247
------------------------------
• [SLOW TEST] [7.571 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:06.71
    Aug 30 07:54:06.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:54:06.711
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:06.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:06.772
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-322 08/30/23 07:54:06.78
    STEP: creating replication controller nodeport-test in namespace services-322 08/30/23 07:54:06.839
    I0830 07:54:06.860179      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-322, replica count: 2
    I0830 07:54:09.912777      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:54:09.912: INFO: Creating new exec pod
    Aug 30 07:54:09.941: INFO: Waiting up to 5m0s for pod "execpod5s4t9" in namespace "services-322" to be "running"
    Aug 30 07:54:09.956: INFO: Pod "execpod5s4t9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.775187ms
    Aug 30 07:54:11.976: INFO: Pod "execpod5s4t9": Phase="Running", Reason="", readiness=true. Elapsed: 2.034365584s
    Aug 30 07:54:11.976: INFO: Pod "execpod5s4t9" satisfied condition "running"
    Aug 30 07:54:12.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 30 07:54:13.459: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 30 07:54:13.459: INFO: stdout: ""
    Aug 30 07:54:13.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 172.21.70.63 80'
    Aug 30 07:54:13.755: INFO: stderr: "+ nc -v -z -w 2 172.21.70.63 80\nConnection to 172.21.70.63 80 port [tcp/http] succeeded!\n"
    Aug 30 07:54:13.755: INFO: stdout: ""
    Aug 30 07:54:13.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.183 32159'
    Aug 30 07:54:13.993: INFO: stderr: "+ nc -v -z -w 2 10.135.139.183 32159\nConnection to 10.135.139.183 32159 port [tcp/*] succeeded!\n"
    Aug 30 07:54:13.993: INFO: stdout: ""
    Aug 30 07:54:13.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-322 exec execpod5s4t9 -- /bin/sh -x -c nc -v -z -w 2 10.135.139.190 32159'
    Aug 30 07:54:14.232: INFO: stderr: "+ nc -v -z -w 2 10.135.139.190 32159\nConnection to 10.135.139.190 32159 port [tcp/*] succeeded!\n"
    Aug 30 07:54:14.232: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:14.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-322" for this suite. 08/30/23 07:54:14.247
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:14.282
Aug 30 07:54:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:54:14.283
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:14.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:14.348
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/30/23 07:54:14.361
STEP: watching for the ServiceAccount to be added 08/30/23 07:54:14.385
STEP: patching the ServiceAccount 08/30/23 07:54:14.389
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/30/23 07:54:14.42
STEP: deleting the ServiceAccount 08/30/23 07:54:14.445
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7684" for this suite. 08/30/23 07:54:14.55
------------------------------
• [0.319 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:14.282
    Aug 30 07:54:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svcaccounts 08/30/23 07:54:14.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:14.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:14.348
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/30/23 07:54:14.361
    STEP: watching for the ServiceAccount to be added 08/30/23 07:54:14.385
    STEP: patching the ServiceAccount 08/30/23 07:54:14.389
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/30/23 07:54:14.42
    STEP: deleting the ServiceAccount 08/30/23 07:54:14.445
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7684" for this suite. 08/30/23 07:54:14.55
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:14.601
Aug 30 07:54:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename cronjob 08/30/23 07:54:14.602
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:14.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:14.669
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/30/23 07:54:14.678
STEP: creating 08/30/23 07:54:14.678
W0830 07:54:14.696716      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 08/30/23 07:54:14.696
STEP: listing 08/30/23 07:54:14.708
STEP: watching 08/30/23 07:54:14.721
Aug 30 07:54:14.721: INFO: starting watch
STEP: cluster-wide listing 08/30/23 07:54:14.724
STEP: cluster-wide watching 08/30/23 07:54:14.735
Aug 30 07:54:14.735: INFO: starting watch
STEP: patching 08/30/23 07:54:14.739
STEP: updating 08/30/23 07:54:14.762
Aug 30 07:54:14.791: INFO: waiting for watch events with expected annotations
Aug 30 07:54:14.791: INFO: saw patched and updated annotations
STEP: patching /status 08/30/23 07:54:14.791
STEP: updating /status 08/30/23 07:54:14.809
STEP: get /status 08/30/23 07:54:14.834
STEP: deleting 08/30/23 07:54:14.845
STEP: deleting a collection 08/30/23 07:54:14.888
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:14.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5071" for this suite. 08/30/23 07:54:14.936
------------------------------
• [0.387 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:14.601
    Aug 30 07:54:14.601: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename cronjob 08/30/23 07:54:14.602
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:14.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:14.669
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/30/23 07:54:14.678
    STEP: creating 08/30/23 07:54:14.678
    W0830 07:54:14.696716      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 08/30/23 07:54:14.696
    STEP: listing 08/30/23 07:54:14.708
    STEP: watching 08/30/23 07:54:14.721
    Aug 30 07:54:14.721: INFO: starting watch
    STEP: cluster-wide listing 08/30/23 07:54:14.724
    STEP: cluster-wide watching 08/30/23 07:54:14.735
    Aug 30 07:54:14.735: INFO: starting watch
    STEP: patching 08/30/23 07:54:14.739
    STEP: updating 08/30/23 07:54:14.762
    Aug 30 07:54:14.791: INFO: waiting for watch events with expected annotations
    Aug 30 07:54:14.791: INFO: saw patched and updated annotations
    STEP: patching /status 08/30/23 07:54:14.791
    STEP: updating /status 08/30/23 07:54:14.809
    STEP: get /status 08/30/23 07:54:14.834
    STEP: deleting 08/30/23 07:54:14.845
    STEP: deleting a collection 08/30/23 07:54:14.888
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:14.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5071" for this suite. 08/30/23 07:54:14.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:14.99
Aug 30 07:54:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 07:54:14.991
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:15.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:15.071
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/30/23 07:54:15.081
W0830 07:54:15.106612      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 08/30/23 07:54:15.106
STEP: Orphaning one of the Job's Pods 08/30/23 07:54:19.119
Aug 30 07:54:19.710: INFO: Successfully updated pod "adopt-release-n4gww"
STEP: Checking that the Job readopts the Pod 08/30/23 07:54:19.71
Aug 30 07:54:19.710: INFO: Waiting up to 15m0s for pod "adopt-release-n4gww" in namespace "job-929" to be "adopted"
Aug 30 07:54:19.734: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 24.644282ms
Aug 30 07:54:21.775: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 2.065110155s
Aug 30 07:54:21.775: INFO: Pod "adopt-release-n4gww" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/30/23 07:54:21.775
Aug 30 07:54:22.318: INFO: Successfully updated pod "adopt-release-n4gww"
STEP: Checking that the Job releases the Pod 08/30/23 07:54:22.318
Aug 30 07:54:22.318: INFO: Waiting up to 15m0s for pod "adopt-release-n4gww" in namespace "job-929" to be "released"
Aug 30 07:54:22.331: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 13.231636ms
Aug 30 07:54:24.351: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 2.03296382s
Aug 30 07:54:24.351: INFO: Pod "adopt-release-n4gww" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:24.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-929" for this suite. 08/30/23 07:54:24.361
------------------------------
• [SLOW TEST] [9.391 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:14.99
    Aug 30 07:54:14.990: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 07:54:14.991
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:15.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:15.071
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/30/23 07:54:15.081
    W0830 07:54:15.106612      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 08/30/23 07:54:15.106
    STEP: Orphaning one of the Job's Pods 08/30/23 07:54:19.119
    Aug 30 07:54:19.710: INFO: Successfully updated pod "adopt-release-n4gww"
    STEP: Checking that the Job readopts the Pod 08/30/23 07:54:19.71
    Aug 30 07:54:19.710: INFO: Waiting up to 15m0s for pod "adopt-release-n4gww" in namespace "job-929" to be "adopted"
    Aug 30 07:54:19.734: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 24.644282ms
    Aug 30 07:54:21.775: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 2.065110155s
    Aug 30 07:54:21.775: INFO: Pod "adopt-release-n4gww" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/30/23 07:54:21.775
    Aug 30 07:54:22.318: INFO: Successfully updated pod "adopt-release-n4gww"
    STEP: Checking that the Job releases the Pod 08/30/23 07:54:22.318
    Aug 30 07:54:22.318: INFO: Waiting up to 15m0s for pod "adopt-release-n4gww" in namespace "job-929" to be "released"
    Aug 30 07:54:22.331: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 13.231636ms
    Aug 30 07:54:24.351: INFO: Pod "adopt-release-n4gww": Phase="Running", Reason="", readiness=true. Elapsed: 2.03296382s
    Aug 30 07:54:24.351: INFO: Pod "adopt-release-n4gww" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:24.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-929" for this suite. 08/30/23 07:54:24.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:24.383
Aug 30 07:54:24.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename gc 08/30/23 07:54:24.384
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:24.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:24.443
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 30 07:54:24.598: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d0be369a-bea4-40ad-9c2a-314296a75b8e", Controller:(*bool)(0xc0040488a2), BlockOwnerDeletion:(*bool)(0xc0040488a3)}}
Aug 30 07:54:24.634: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1ff788a0-ad33-4b33-97fa-4c608821cca2", Controller:(*bool)(0xc004048b66), BlockOwnerDeletion:(*bool)(0xc004048b67)}}
Aug 30 07:54:24.659: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9a90f628-e86d-4d78-89bc-6d365888a130", Controller:(*bool)(0xc0053271e6), BlockOwnerDeletion:(*bool)(0xc0053271e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6716" for this suite. 08/30/23 07:54:29.736
------------------------------
• [SLOW TEST] [5.392 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:24.383
    Aug 30 07:54:24.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename gc 08/30/23 07:54:24.384
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:24.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:24.443
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 30 07:54:24.598: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d0be369a-bea4-40ad-9c2a-314296a75b8e", Controller:(*bool)(0xc0040488a2), BlockOwnerDeletion:(*bool)(0xc0040488a3)}}
    Aug 30 07:54:24.634: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"1ff788a0-ad33-4b33-97fa-4c608821cca2", Controller:(*bool)(0xc004048b66), BlockOwnerDeletion:(*bool)(0xc004048b67)}}
    Aug 30 07:54:24.659: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9a90f628-e86d-4d78-89bc-6d365888a130", Controller:(*bool)(0xc0053271e6), BlockOwnerDeletion:(*bool)(0xc0053271e7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6716" for this suite. 08/30/23 07:54:29.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:29.81
Aug 30 07:54:29.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:54:29.811
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:29.896
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:29.925
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 30 07:54:29.988: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12" in namespace "kubelet-test-3573" to be "running and ready"
Aug 30 07:54:30.018: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12": Phase="Pending", Reason="", readiness=false. Elapsed: 30.304128ms
Aug 30 07:54:30.019: INFO: The phase of Pod busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12 is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:54:32.035: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12": Phase="Running", Reason="", readiness=true. Elapsed: 2.046507299s
Aug 30 07:54:32.035: INFO: The phase of Pod busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12 is Running (Ready = true)
Aug 30 07:54:32.035: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:32.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3573" for this suite. 08/30/23 07:54:32.109
------------------------------
• [2.321 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:29.81
    Aug 30 07:54:29.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubelet-test 08/30/23 07:54:29.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:29.896
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:29.925
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 30 07:54:29.988: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12" in namespace "kubelet-test-3573" to be "running and ready"
    Aug 30 07:54:30.018: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12": Phase="Pending", Reason="", readiness=false. Elapsed: 30.304128ms
    Aug 30 07:54:30.019: INFO: The phase of Pod busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12 is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:54:32.035: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12": Phase="Running", Reason="", readiness=true. Elapsed: 2.046507299s
    Aug 30 07:54:32.035: INFO: The phase of Pod busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12 is Running (Ready = true)
    Aug 30 07:54:32.035: INFO: Pod "busybox-scheduling-ddcea57f-fcd7-4563-aefc-686b76c10a12" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:32.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3573" for this suite. 08/30/23 07:54:32.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:32.133
Aug 30 07:54:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename lease-test 08/30/23 07:54:32.135
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:32.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:32.2
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:32.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6965" for this suite. 08/30/23 07:54:32.472
------------------------------
• [0.363 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:32.133
    Aug 30 07:54:32.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename lease-test 08/30/23 07:54:32.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:32.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:32.2
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:32.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6965" for this suite. 08/30/23 07:54:32.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:32.498
Aug 30 07:54:32.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:54:32.499
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:32.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:32.559
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-7663b9c8-880d-4cbb-9a0d-098659cd14a3 08/30/23 07:54:32.567
STEP: Creating a pod to test consume configMaps 08/30/23 07:54:32.586
Aug 30 07:54:32.616: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7" in namespace "projected-4698" to be "Succeeded or Failed"
Aug 30 07:54:32.649: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.498346ms
Aug 30 07:54:34.663: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046825288s
Aug 30 07:54:36.661: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044534748s
Aug 30 07:54:38.687: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070074668s
STEP: Saw pod success 08/30/23 07:54:38.687
Aug 30 07:54:38.687: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7" satisfied condition "Succeeded or Failed"
Aug 30 07:54:38.735: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:54:38.789
Aug 30 07:54:38.937: INFO: Waiting for pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 to disappear
Aug 30 07:54:38.955: INFO: Pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:38.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4698" for this suite. 08/30/23 07:54:38.976
------------------------------
• [SLOW TEST] [6.499 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:32.498
    Aug 30 07:54:32.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:54:32.499
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:32.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:32.559
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-7663b9c8-880d-4cbb-9a0d-098659cd14a3 08/30/23 07:54:32.567
    STEP: Creating a pod to test consume configMaps 08/30/23 07:54:32.586
    Aug 30 07:54:32.616: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7" in namespace "projected-4698" to be "Succeeded or Failed"
    Aug 30 07:54:32.649: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.498346ms
    Aug 30 07:54:34.663: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046825288s
    Aug 30 07:54:36.661: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044534748s
    Aug 30 07:54:38.687: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.070074668s
    STEP: Saw pod success 08/30/23 07:54:38.687
    Aug 30 07:54:38.687: INFO: Pod "pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7" satisfied condition "Succeeded or Failed"
    Aug 30 07:54:38.735: INFO: Trying to get logs from node 10.135.139.190 pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:54:38.789
    Aug 30 07:54:38.937: INFO: Waiting for pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 to disappear
    Aug 30 07:54:38.955: INFO: Pod pod-projected-configmaps-d3d78665-78a2-48fb-8d7c-fab84cf2cda7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:38.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4698" for this suite. 08/30/23 07:54:38.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:38.997
Aug 30 07:54:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:54:38.999
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:39.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:39.222
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/30/23 07:54:39.232
W0830 07:54:39.270626      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:54:39.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3" in namespace "projected-6785" to be "Succeeded or Failed"
Aug 30 07:54:39.300: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 29.523707ms
Aug 30 07:54:41.312: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041153968s
Aug 30 07:54:43.315: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04458409s
STEP: Saw pod success 08/30/23 07:54:43.315
Aug 30 07:54:43.315: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3" satisfied condition "Succeeded or Failed"
Aug 30 07:54:43.328: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 container client-container: <nil>
STEP: delete the pod 08/30/23 07:54:43.348
Aug 30 07:54:43.383: INFO: Waiting for pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 to disappear
Aug 30 07:54:43.394: INFO: Pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:43.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6785" for this suite. 08/30/23 07:54:43.406
------------------------------
• [4.433 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:38.997
    Aug 30 07:54:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:54:38.999
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:39.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:39.222
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/30/23 07:54:39.232
    W0830 07:54:39.270626      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:54:39.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3" in namespace "projected-6785" to be "Succeeded or Failed"
    Aug 30 07:54:39.300: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 29.523707ms
    Aug 30 07:54:41.312: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041153968s
    Aug 30 07:54:43.315: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04458409s
    STEP: Saw pod success 08/30/23 07:54:43.315
    Aug 30 07:54:43.315: INFO: Pod "downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3" satisfied condition "Succeeded or Failed"
    Aug 30 07:54:43.328: INFO: Trying to get logs from node 10.135.139.190 pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 container client-container: <nil>
    STEP: delete the pod 08/30/23 07:54:43.348
    Aug 30 07:54:43.383: INFO: Waiting for pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 to disappear
    Aug 30 07:54:43.394: INFO: Pod downwardapi-volume-05e2c0ab-9e7d-4b66-a3b1-3409a84d26b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:43.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6785" for this suite. 08/30/23 07:54:43.406
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:43.433
Aug 30 07:54:43.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename svc-latency 08/30/23 07:54:43.434
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:43.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:43.495
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 30 07:54:43.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7156 08/30/23 07:54:43.506
W0830 07:54:43.527090      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0830 07:54:43.527460      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7156, replica count: 1
I0830 07:54:44.579323      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 07:54:45.580154      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0830 07:54:46.580481      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:54:46.718: INFO: Created: latency-svc-9wqgw
Aug 30 07:54:46.734: INFO: Got endpoints: latency-svc-9wqgw [53.518648ms]
Aug 30 07:54:46.774: INFO: Created: latency-svc-d522h
Aug 30 07:54:46.786: INFO: Got endpoints: latency-svc-d522h [49.949225ms]
Aug 30 07:54:46.792: INFO: Created: latency-svc-t6dll
Aug 30 07:54:46.810: INFO: Got endpoints: latency-svc-t6dll [73.929358ms]
Aug 30 07:54:46.813: INFO: Created: latency-svc-lxlrn
Aug 30 07:54:46.842: INFO: Created: latency-svc-tcn2j
Aug 30 07:54:46.842: INFO: Got endpoints: latency-svc-lxlrn [106.390838ms]
Aug 30 07:54:46.851: INFO: Got endpoints: latency-svc-tcn2j [115.011144ms]
Aug 30 07:54:46.859: INFO: Created: latency-svc-pnwfj
Aug 30 07:54:46.870: INFO: Got endpoints: latency-svc-pnwfj [133.787614ms]
Aug 30 07:54:46.873: INFO: Created: latency-svc-t47bp
Aug 30 07:54:46.886: INFO: Got endpoints: latency-svc-t47bp [151.242642ms]
Aug 30 07:54:46.898: INFO: Created: latency-svc-qvqxw
Aug 30 07:54:46.916: INFO: Got endpoints: latency-svc-qvqxw [179.428557ms]
Aug 30 07:54:46.924: INFO: Created: latency-svc-td6l5
Aug 30 07:54:46.939: INFO: Got endpoints: latency-svc-td6l5 [203.711664ms]
Aug 30 07:54:46.948: INFO: Created: latency-svc-ml7dg
Aug 30 07:54:46.967: INFO: Got endpoints: latency-svc-ml7dg [231.226575ms]
Aug 30 07:54:46.976: INFO: Created: latency-svc-b4z92
Aug 30 07:54:46.987: INFO: Got endpoints: latency-svc-b4z92 [250.867114ms]
Aug 30 07:54:47.014: INFO: Created: latency-svc-fmfwz
Aug 30 07:54:47.033: INFO: Got endpoints: latency-svc-fmfwz [298.020379ms]
Aug 30 07:54:47.038: INFO: Created: latency-svc-brr6t
Aug 30 07:54:47.055: INFO: Got endpoints: latency-svc-brr6t [319.375742ms]
Aug 30 07:54:47.062: INFO: Created: latency-svc-8l2lh
Aug 30 07:54:47.082: INFO: Got endpoints: latency-svc-8l2lh [346.179564ms]
Aug 30 07:54:47.088: INFO: Created: latency-svc-7mp76
Aug 30 07:54:47.097: INFO: Got endpoints: latency-svc-7mp76 [361.291127ms]
Aug 30 07:54:47.113: INFO: Created: latency-svc-6xgm4
Aug 30 07:54:47.128: INFO: Created: latency-svc-jx84m
Aug 30 07:54:47.130: INFO: Got endpoints: latency-svc-6xgm4 [393.580105ms]
Aug 30 07:54:47.146: INFO: Got endpoints: latency-svc-jx84m [359.901183ms]
Aug 30 07:54:47.147: INFO: Created: latency-svc-glnsw
Aug 30 07:54:47.163: INFO: Got endpoints: latency-svc-glnsw [352.82078ms]
Aug 30 07:54:47.169: INFO: Created: latency-svc-kjqp9
Aug 30 07:54:47.183: INFO: Got endpoints: latency-svc-kjqp9 [340.694743ms]
Aug 30 07:54:47.191: INFO: Created: latency-svc-bp7st
Aug 30 07:54:47.203: INFO: Got endpoints: latency-svc-bp7st [352.014204ms]
Aug 30 07:54:47.209: INFO: Created: latency-svc-nzbqj
Aug 30 07:54:47.226: INFO: Got endpoints: latency-svc-nzbqj [355.786053ms]
Aug 30 07:54:47.235: INFO: Created: latency-svc-d8bjf
Aug 30 07:54:47.252: INFO: Got endpoints: latency-svc-d8bjf [365.39331ms]
Aug 30 07:54:47.257: INFO: Created: latency-svc-zf7xc
Aug 30 07:54:47.273: INFO: Got endpoints: latency-svc-zf7xc [357.596339ms]
Aug 30 07:54:47.280: INFO: Created: latency-svc-5vgrc
Aug 30 07:54:47.296: INFO: Got endpoints: latency-svc-5vgrc [356.685839ms]
Aug 30 07:54:47.299: INFO: Created: latency-svc-bhncc
Aug 30 07:54:47.321: INFO: Got endpoints: latency-svc-bhncc [353.342895ms]
Aug 30 07:54:47.322: INFO: Created: latency-svc-z7bq9
Aug 30 07:54:47.334: INFO: Got endpoints: latency-svc-z7bq9 [346.489111ms]
Aug 30 07:54:47.341: INFO: Created: latency-svc-frg8v
Aug 30 07:54:47.362: INFO: Got endpoints: latency-svc-frg8v [327.966151ms]
Aug 30 07:54:47.684: INFO: Created: latency-svc-k9hkd
Aug 30 07:54:47.710: INFO: Created: latency-svc-8hv4p
Aug 30 07:54:47.721: INFO: Got endpoints: latency-svc-k9hkd [639.357659ms]
Aug 30 07:54:47.722: INFO: Created: latency-svc-29884
Aug 30 07:54:47.722: INFO: Created: latency-svc-68qcs
Aug 30 07:54:47.722: INFO: Created: latency-svc-nkw98
Aug 30 07:54:47.723: INFO: Created: latency-svc-dmp4j
Aug 30 07:54:47.723: INFO: Created: latency-svc-jrl7z
Aug 30 07:54:47.723: INFO: Created: latency-svc-k9l6k
Aug 30 07:54:47.723: INFO: Created: latency-svc-jf6kj
Aug 30 07:54:47.723: INFO: Created: latency-svc-vppdn
Aug 30 07:54:47.723: INFO: Created: latency-svc-2gnfw
Aug 30 07:54:47.723: INFO: Created: latency-svc-x8h2c
Aug 30 07:54:47.724: INFO: Created: latency-svc-597wj
Aug 30 07:54:47.724: INFO: Created: latency-svc-mn8fk
Aug 30 07:54:47.724: INFO: Created: latency-svc-bxhkv
Aug 30 07:54:47.728: INFO: Got endpoints: latency-svc-8hv4p [630.34214ms]
Aug 30 07:54:47.734: INFO: Got endpoints: latency-svc-2gnfw [604.567821ms]
Aug 30 07:54:47.735: INFO: Got endpoints: latency-svc-jf6kj [571.887889ms]
Aug 30 07:54:47.735: INFO: Got endpoints: latency-svc-k9l6k [414.263667ms]
Aug 30 07:54:47.738: INFO: Got endpoints: latency-svc-jrl7z [512.67405ms]
Aug 30 07:54:47.743: INFO: Got endpoints: latency-svc-dmp4j [687.465512ms]
Aug 30 07:54:47.760: INFO: Got endpoints: latency-svc-nkw98 [508.210447ms]
Aug 30 07:54:47.761: INFO: Got endpoints: latency-svc-bxhkv [464.859172ms]
Aug 30 07:54:47.761: INFO: Got endpoints: latency-svc-mn8fk [558.121781ms]
Aug 30 07:54:47.762: INFO: Got endpoints: latency-svc-x8h2c [488.214303ms]
Aug 30 07:54:47.762: INFO: Got endpoints: latency-svc-vppdn [616.426536ms]
Aug 30 07:54:47.778: INFO: Got endpoints: latency-svc-29884 [594.498329ms]
Aug 30 07:54:47.784: INFO: Created: latency-svc-b4wvn
Aug 30 07:54:47.785: INFO: Got endpoints: latency-svc-597wj [450.903768ms]
Aug 30 07:54:47.785: INFO: Got endpoints: latency-svc-68qcs [423.242561ms]
Aug 30 07:54:47.803: INFO: Got endpoints: latency-svc-b4wvn [81.737327ms]
Aug 30 07:54:47.814: INFO: Created: latency-svc-lvm9v
Aug 30 07:54:47.833: INFO: Got endpoints: latency-svc-lvm9v [105.463817ms]
Aug 30 07:54:47.840: INFO: Created: latency-svc-l84tv
Aug 30 07:54:47.855: INFO: Got endpoints: latency-svc-l84tv [120.241612ms]
Aug 30 07:54:47.864: INFO: Created: latency-svc-xmr4p
Aug 30 07:54:47.879: INFO: Got endpoints: latency-svc-xmr4p [144.109104ms]
Aug 30 07:54:47.886: INFO: Created: latency-svc-nf5hj
Aug 30 07:54:47.899: INFO: Got endpoints: latency-svc-nf5hj [163.946769ms]
Aug 30 07:54:47.909: INFO: Created: latency-svc-fz2b6
Aug 30 07:54:47.929: INFO: Got endpoints: latency-svc-fz2b6 [190.80393ms]
Aug 30 07:54:47.931: INFO: Created: latency-svc-j68v5
Aug 30 07:54:47.948: INFO: Created: latency-svc-x55ng
Aug 30 07:54:47.974: INFO: Got endpoints: latency-svc-x55ng [214.036648ms]
Aug 30 07:54:47.975: INFO: Got endpoints: latency-svc-j68v5 [231.801505ms]
Aug 30 07:54:47.976: INFO: Created: latency-svc-x9m6q
Aug 30 07:54:47.990: INFO: Got endpoints: latency-svc-x9m6q [229.564575ms]
Aug 30 07:54:47.998: INFO: Created: latency-svc-tknbd
Aug 30 07:54:48.020: INFO: Got endpoints: latency-svc-tknbd [258.07179ms]
Aug 30 07:54:48.022: INFO: Created: latency-svc-wqdvm
Aug 30 07:54:48.043: INFO: Created: latency-svc-p9qpm
Aug 30 07:54:48.062: INFO: Got endpoints: latency-svc-wqdvm [300.892439ms]
Aug 30 07:54:48.064: INFO: Got endpoints: latency-svc-p9qpm [302.240374ms]
Aug 30 07:54:48.075: INFO: Created: latency-svc-jgjh5
Aug 30 07:54:48.097: INFO: Got endpoints: latency-svc-jgjh5 [312.134413ms]
Aug 30 07:54:48.097: INFO: Created: latency-svc-sw5cj
Aug 30 07:54:48.112: INFO: Got endpoints: latency-svc-sw5cj [334.078206ms]
Aug 30 07:54:48.118: INFO: Created: latency-svc-xvmpd
Aug 30 07:54:48.133: INFO: Got endpoints: latency-svc-xvmpd [347.8135ms]
Aug 30 07:54:48.145: INFO: Created: latency-svc-vzfk9
Aug 30 07:54:48.167: INFO: Got endpoints: latency-svc-vzfk9 [364.107249ms]
Aug 30 07:54:48.167: INFO: Created: latency-svc-s447n
Aug 30 07:54:48.185: INFO: Got endpoints: latency-svc-s447n [351.044466ms]
Aug 30 07:54:48.189: INFO: Created: latency-svc-pvswf
Aug 30 07:54:48.246: INFO: Got endpoints: latency-svc-pvswf [391.490513ms]
Aug 30 07:54:48.248: INFO: Created: latency-svc-jz828
Aug 30 07:54:48.267: INFO: Got endpoints: latency-svc-jz828 [388.096059ms]
Aug 30 07:54:48.280: INFO: Created: latency-svc-cdrrj
Aug 30 07:54:48.296: INFO: Got endpoints: latency-svc-cdrrj [397.829793ms]
Aug 30 07:54:48.300: INFO: Created: latency-svc-fps2h
Aug 30 07:54:48.349: INFO: Got endpoints: latency-svc-fps2h [419.728123ms]
Aug 30 07:54:48.355: INFO: Created: latency-svc-b9rdb
Aug 30 07:54:48.398: INFO: Got endpoints: latency-svc-b9rdb [422.448636ms]
Aug 30 07:54:48.463: INFO: Created: latency-svc-6fcbz
Aug 30 07:54:48.479: INFO: Got endpoints: latency-svc-6fcbz [504.053286ms]
Aug 30 07:54:48.486: INFO: Created: latency-svc-nkpk7
Aug 30 07:54:48.526: INFO: Got endpoints: latency-svc-nkpk7 [535.295703ms]
Aug 30 07:54:48.544: INFO: Created: latency-svc-csfl5
Aug 30 07:54:48.596: INFO: Got endpoints: latency-svc-csfl5 [575.552459ms]
Aug 30 07:54:48.597: INFO: Created: latency-svc-46zpq
Aug 30 07:54:48.612: INFO: Got endpoints: latency-svc-46zpq [549.344097ms]
Aug 30 07:54:48.618: INFO: Created: latency-svc-v2jpb
Aug 30 07:54:48.638: INFO: Got endpoints: latency-svc-v2jpb [573.961368ms]
Aug 30 07:54:48.644: INFO: Created: latency-svc-svv5b
Aug 30 07:54:48.668: INFO: Got endpoints: latency-svc-svv5b [570.983127ms]
Aug 30 07:54:48.669: INFO: Created: latency-svc-245fg
Aug 30 07:54:48.683: INFO: Got endpoints: latency-svc-245fg [571.030147ms]
Aug 30 07:54:48.690: INFO: Created: latency-svc-rbcg6
Aug 30 07:54:48.703: INFO: Got endpoints: latency-svc-rbcg6 [570.602731ms]
Aug 30 07:54:48.709: INFO: Created: latency-svc-4n2ht
Aug 30 07:54:48.726: INFO: Created: latency-svc-k55vk
Aug 30 07:54:48.727: INFO: Got endpoints: latency-svc-4n2ht [559.538281ms]
Aug 30 07:54:48.740: INFO: Got endpoints: latency-svc-k55vk [555.239421ms]
Aug 30 07:54:48.747: INFO: Created: latency-svc-9xjqh
Aug 30 07:54:48.768: INFO: Got endpoints: latency-svc-9xjqh [521.88877ms]
Aug 30 07:54:48.777: INFO: Created: latency-svc-txh22
Aug 30 07:54:48.791: INFO: Got endpoints: latency-svc-txh22 [523.867067ms]
Aug 30 07:54:48.792: INFO: Created: latency-svc-lgcmk
Aug 30 07:54:48.838: INFO: Got endpoints: latency-svc-lgcmk [541.124979ms]
Aug 30 07:54:48.842: INFO: Created: latency-svc-tp22w
Aug 30 07:54:48.862: INFO: Got endpoints: latency-svc-tp22w [512.707527ms]
Aug 30 07:54:48.879: INFO: Created: latency-svc-dfvrj
Aug 30 07:54:48.902: INFO: Got endpoints: latency-svc-dfvrj [503.762615ms]
Aug 30 07:54:48.910: INFO: Created: latency-svc-d8jhn
Aug 30 07:54:48.933: INFO: Got endpoints: latency-svc-d8jhn [453.296458ms]
Aug 30 07:54:48.935: INFO: Created: latency-svc-nt94n
Aug 30 07:54:48.980: INFO: Got endpoints: latency-svc-nt94n [454.171444ms]
Aug 30 07:54:48.995: INFO: Created: latency-svc-vkj6d
Aug 30 07:54:49.033: INFO: Got endpoints: latency-svc-vkj6d [437.550572ms]
Aug 30 07:54:49.046: INFO: Created: latency-svc-4rspn
Aug 30 07:54:49.086: INFO: Got endpoints: latency-svc-4rspn [474.162695ms]
Aug 30 07:54:49.096: INFO: Created: latency-svc-w622d
Aug 30 07:54:49.113: INFO: Got endpoints: latency-svc-w622d [474.89953ms]
Aug 30 07:54:49.120: INFO: Created: latency-svc-k76rt
Aug 30 07:54:49.142: INFO: Got endpoints: latency-svc-k76rt [473.730615ms]
Aug 30 07:54:49.142: INFO: Created: latency-svc-ddfs7
Aug 30 07:54:49.159: INFO: Got endpoints: latency-svc-ddfs7 [475.68893ms]
Aug 30 07:54:49.169: INFO: Created: latency-svc-gx26p
Aug 30 07:54:49.185: INFO: Got endpoints: latency-svc-gx26p [481.699722ms]
Aug 30 07:54:49.197: INFO: Created: latency-svc-vjpqx
Aug 30 07:54:49.211: INFO: Got endpoints: latency-svc-vjpqx [484.660366ms]
Aug 30 07:54:49.219: INFO: Created: latency-svc-6stjg
Aug 30 07:54:49.248: INFO: Created: latency-svc-q7s9h
Aug 30 07:54:49.248: INFO: Got endpoints: latency-svc-6stjg [508.175995ms]
Aug 30 07:54:49.265: INFO: Got endpoints: latency-svc-q7s9h [496.718703ms]
Aug 30 07:54:49.268: INFO: Created: latency-svc-tp2kk
Aug 30 07:54:49.299: INFO: Created: latency-svc-tpvxm
Aug 30 07:54:49.300: INFO: Got endpoints: latency-svc-tp2kk [508.702072ms]
Aug 30 07:54:49.319: INFO: Got endpoints: latency-svc-tpvxm [481.78126ms]
Aug 30 07:54:49.348: INFO: Created: latency-svc-dvkql
Aug 30 07:54:49.368: INFO: Got endpoints: latency-svc-dvkql [505.921161ms]
Aug 30 07:54:49.380: INFO: Created: latency-svc-kl74c
Aug 30 07:54:49.399: INFO: Got endpoints: latency-svc-kl74c [496.902509ms]
Aug 30 07:54:49.407: INFO: Created: latency-svc-24wd2
Aug 30 07:54:49.425: INFO: Got endpoints: latency-svc-24wd2 [492.423363ms]
Aug 30 07:54:49.449: INFO: Created: latency-svc-c59fv
Aug 30 07:54:49.474: INFO: Got endpoints: latency-svc-c59fv [493.884397ms]
Aug 30 07:54:49.484: INFO: Created: latency-svc-x5r2q
Aug 30 07:54:49.504: INFO: Got endpoints: latency-svc-x5r2q [470.107962ms]
Aug 30 07:54:49.514: INFO: Created: latency-svc-xjq25
Aug 30 07:54:49.533: INFO: Got endpoints: latency-svc-xjq25 [447.13379ms]
Aug 30 07:54:49.546: INFO: Created: latency-svc-xhn6l
Aug 30 07:54:49.564: INFO: Got endpoints: latency-svc-xhn6l [450.686478ms]
Aug 30 07:54:49.571: INFO: Created: latency-svc-s6slh
Aug 30 07:54:49.595: INFO: Got endpoints: latency-svc-s6slh [452.971476ms]
Aug 30 07:54:49.597: INFO: Created: latency-svc-hdndq
Aug 30 07:54:49.616: INFO: Got endpoints: latency-svc-hdndq [457.032708ms]
Aug 30 07:54:49.618: INFO: Created: latency-svc-jnjs6
Aug 30 07:54:49.636: INFO: Got endpoints: latency-svc-jnjs6 [450.502687ms]
Aug 30 07:54:49.883: INFO: Created: latency-svc-j6lf6
Aug 30 07:54:49.905: INFO: Created: latency-svc-582w9
Aug 30 07:54:49.906: INFO: Created: latency-svc-v4dfq
Aug 30 07:54:49.906: INFO: Created: latency-svc-9rxff
Aug 30 07:54:49.907: INFO: Created: latency-svc-dnkkk
Aug 30 07:54:49.907: INFO: Created: latency-svc-jkmrh
Aug 30 07:54:49.907: INFO: Created: latency-svc-5nwzh
Aug 30 07:54:49.908: INFO: Created: latency-svc-dkz2s
Aug 30 07:54:49.908: INFO: Got endpoints: latency-svc-j6lf6 [313.075612ms]
Aug 30 07:54:49.908: INFO: Created: latency-svc-8mhxl
Aug 30 07:54:49.909: INFO: Created: latency-svc-742tl
Aug 30 07:54:49.909: INFO: Created: latency-svc-lz7z5
Aug 30 07:54:49.909: INFO: Created: latency-svc-fphrj
Aug 30 07:54:49.910: INFO: Created: latency-svc-tb4ql
Aug 30 07:54:49.910: INFO: Created: latency-svc-hwl5l
Aug 30 07:54:49.910: INFO: Created: latency-svc-xsz6j
Aug 30 07:54:49.925: INFO: Got endpoints: latency-svc-8mhxl [308.922635ms]
Aug 30 07:54:49.925: INFO: Got endpoints: latency-svc-742tl [361.061974ms]
Aug 30 07:54:49.961: INFO: Got endpoints: latency-svc-dnkkk [324.438394ms]
Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-xsz6j [536.273997ms]
Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-9rxff [563.270476ms]
Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-5nwzh [594.30534ms]
Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-dkz2s [663.899141ms]
Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-jkmrh [460.248846ms]
Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-tb4ql [716.104504ms]
Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-582w9 [426.66627ms]
Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-v4dfq [490.638485ms]
Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-fphrj [645.215484ms]
Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-hwl5l [754.000082ms]
Aug 30 07:54:49.966: INFO: Got endpoints: latency-svc-lz7z5 [700.628446ms]
Aug 30 07:54:49.976: INFO: Created: latency-svc-gxj2l
Aug 30 07:54:50.009: INFO: Got endpoints: latency-svc-gxj2l [100.5467ms]
Aug 30 07:54:50.009: INFO: Created: latency-svc-fcwcd
Aug 30 07:54:50.009: INFO: Got endpoints: latency-svc-fcwcd [82.315006ms]
Aug 30 07:54:50.396: INFO: Created: latency-svc-cnx77
Aug 30 07:54:50.398: INFO: Created: latency-svc-5gdkq
Aug 30 07:54:50.398: INFO: Created: latency-svc-nlqtd
Aug 30 07:54:50.398: INFO: Created: latency-svc-d6qdx
Aug 30 07:54:50.398: INFO: Created: latency-svc-jgkh9
Aug 30 07:54:50.398: INFO: Created: latency-svc-zkbjv
Aug 30 07:54:50.399: INFO: Created: latency-svc-sxn8n
Aug 30 07:54:50.399: INFO: Created: latency-svc-2q5vj
Aug 30 07:54:50.400: INFO: Created: latency-svc-82g2g
Aug 30 07:54:50.400: INFO: Created: latency-svc-zfx5s
Aug 30 07:54:50.438: INFO: Created: latency-svc-lmbxs
Aug 30 07:54:50.439: INFO: Created: latency-svc-2f6zj
Aug 30 07:54:50.439: INFO: Created: latency-svc-8lljj
Aug 30 07:54:50.442: INFO: Created: latency-svc-dzt72
Aug 30 07:54:50.442: INFO: Created: latency-svc-9sbsx
Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-cnx77 [498.133975ms]
Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-d6qdx [459.669423ms]
Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-2q5vj [544.597458ms]
Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-5gdkq [507.408128ms]
Aug 30 07:54:50.470: INFO: Got endpoints: latency-svc-zkbjv [499.538958ms]
Aug 30 07:54:50.501: INFO: Got endpoints: latency-svc-82g2g [536.969579ms]
Aug 30 07:54:50.544: INFO: Got endpoints: latency-svc-sxn8n [579.555699ms]
Aug 30 07:54:50.544: INFO: Got endpoints: latency-svc-zfx5s [583.305469ms]
Aug 30 07:54:50.545: INFO: Got endpoints: latency-svc-jgkh9 [582.842383ms]
Aug 30 07:54:50.545: INFO: Got endpoints: latency-svc-nlqtd [536.157647ms]
Aug 30 07:54:50.571: INFO: Got endpoints: latency-svc-lmbxs [609.615969ms]
Aug 30 07:54:50.614: INFO: Got endpoints: latency-svc-2f6zj [649.567054ms]
Aug 30 07:54:50.615: INFO: Got endpoints: latency-svc-8lljj [651.013036ms]
Aug 30 07:54:50.616: INFO: Got endpoints: latency-svc-9sbsx [651.650719ms]
Aug 30 07:54:50.617: INFO: Got endpoints: latency-svc-dzt72 [651.98874ms]
Aug 30 07:54:50.617: INFO: Created: latency-svc-b6kk2
Aug 30 07:54:50.690: INFO: Got endpoints: latency-svc-b6kk2 [220.087506ms]
Aug 30 07:54:50.721: INFO: Created: latency-svc-kmjqt
Aug 30 07:54:50.763: INFO: Got endpoints: latency-svc-kmjqt [293.664197ms]
Aug 30 07:54:50.769: INFO: Created: latency-svc-ws8d9
Aug 30 07:54:50.803: INFO: Got endpoints: latency-svc-ws8d9 [333.64891ms]
Aug 30 07:54:50.837: INFO: Created: latency-svc-2kvng
Aug 30 07:54:50.847: INFO: Created: latency-svc-d5bvs
Aug 30 07:54:50.895: INFO: Got endpoints: latency-svc-d5bvs [425.466866ms]
Aug 30 07:54:50.895: INFO: Got endpoints: latency-svc-2kvng [425.45787ms]
Aug 30 07:54:50.905: INFO: Created: latency-svc-bdrs9
Aug 30 07:54:50.924: INFO: Got endpoints: latency-svc-bdrs9 [423.013374ms]
Aug 30 07:54:50.944: INFO: Created: latency-svc-rzvdt
Aug 30 07:54:50.971: INFO: Got endpoints: latency-svc-rzvdt [426.005963ms]
Aug 30 07:54:50.977: INFO: Created: latency-svc-lstkk
Aug 30 07:54:51.107: INFO: Got endpoints: latency-svc-lstkk [562.503304ms]
Aug 30 07:54:51.131: INFO: Created: latency-svc-jt69q
Aug 30 07:54:51.156: INFO: Created: latency-svc-52qm4
Aug 30 07:54:51.184: INFO: Got endpoints: latency-svc-jt69q [640.059387ms]
Aug 30 07:54:51.187: INFO: Got endpoints: latency-svc-52qm4 [641.907347ms]
Aug 30 07:54:51.196: INFO: Created: latency-svc-zbrbf
Aug 30 07:54:51.220: INFO: Got endpoints: latency-svc-zbrbf [648.392665ms]
Aug 30 07:54:51.220: INFO: Created: latency-svc-kk9rk
Aug 30 07:54:51.247: INFO: Created: latency-svc-kftgf
Aug 30 07:54:51.249: INFO: Got endpoints: latency-svc-kk9rk [632.976399ms]
Aug 30 07:54:51.272: INFO: Got endpoints: latency-svc-kftgf [655.034358ms]
Aug 30 07:54:51.293: INFO: Created: latency-svc-qbz4l
Aug 30 07:54:51.320: INFO: Got endpoints: latency-svc-qbz4l [704.854348ms]
Aug 30 07:54:51.321: INFO: Created: latency-svc-gsc59
Aug 30 07:54:51.346: INFO: Got endpoints: latency-svc-gsc59 [729.167101ms]
Aug 30 07:54:51.359: INFO: Created: latency-svc-888rw
Aug 30 07:54:51.382: INFO: Got endpoints: latency-svc-888rw [692.504386ms]
Aug 30 07:54:51.393: INFO: Created: latency-svc-bt49c
Aug 30 07:54:51.416: INFO: Got endpoints: latency-svc-bt49c [653.026771ms]
Aug 30 07:54:51.428: INFO: Created: latency-svc-gf9xc
Aug 30 07:54:51.443: INFO: Got endpoints: latency-svc-gf9xc [640.191527ms]
Aug 30 07:54:51.455: INFO: Created: latency-svc-7mjmh
Aug 30 07:54:51.479: INFO: Got endpoints: latency-svc-7mjmh [583.826031ms]
Aug 30 07:54:51.483: INFO: Created: latency-svc-cnxqv
Aug 30 07:54:51.498: INFO: Got endpoints: latency-svc-cnxqv [602.80077ms]
Aug 30 07:54:51.512: INFO: Created: latency-svc-zrwll
Aug 30 07:54:51.560: INFO: Got endpoints: latency-svc-zrwll [635.866004ms]
Aug 30 07:54:51.566: INFO: Created: latency-svc-6xp29
Aug 30 07:54:51.566: INFO: Got endpoints: latency-svc-6xp29 [594.684964ms]
Aug 30 07:54:51.594: INFO: Created: latency-svc-jzdlh
Aug 30 07:54:51.594: INFO: Got endpoints: latency-svc-jzdlh [486.603649ms]
Aug 30 07:54:51.624: INFO: Created: latency-svc-r6ntq
Aug 30 07:54:51.642: INFO: Got endpoints: latency-svc-r6ntq [457.867737ms]
Aug 30 07:54:51.647: INFO: Created: latency-svc-s2scf
Aug 30 07:54:51.665: INFO: Got endpoints: latency-svc-s2scf [477.714138ms]
Aug 30 07:54:51.667: INFO: Created: latency-svc-mw6vx
Aug 30 07:54:51.678: INFO: Got endpoints: latency-svc-mw6vx [458.215577ms]
Aug 30 07:54:51.686: INFO: Created: latency-svc-hbnwp
Aug 30 07:54:51.702: INFO: Got endpoints: latency-svc-hbnwp [452.971701ms]
Aug 30 07:54:51.712: INFO: Created: latency-svc-zw9wl
Aug 30 07:54:51.727: INFO: Got endpoints: latency-svc-zw9wl [455.51878ms]
Aug 30 07:54:51.740: INFO: Created: latency-svc-rbc6p
Aug 30 07:54:51.758: INFO: Created: latency-svc-n8fbw
Aug 30 07:54:51.763: INFO: Got endpoints: latency-svc-rbc6p [443.22083ms]
Aug 30 07:54:51.787: INFO: Got endpoints: latency-svc-n8fbw [441.525483ms]
Aug 30 07:54:51.809: INFO: Created: latency-svc-8hlfs
Aug 30 07:54:51.854: INFO: Got endpoints: latency-svc-8hlfs [470.972746ms]
Aug 30 07:54:51.854: INFO: Created: latency-svc-mgctm
Aug 30 07:54:51.876: INFO: Got endpoints: latency-svc-mgctm [459.314263ms]
Aug 30 07:54:51.884: INFO: Created: latency-svc-rbslc
Aug 30 07:54:51.903: INFO: Got endpoints: latency-svc-rbslc [459.397701ms]
Aug 30 07:54:51.931: INFO: Created: latency-svc-c2k66
Aug 30 07:54:51.945: INFO: Got endpoints: latency-svc-c2k66 [466.226191ms]
Aug 30 07:54:51.960: INFO: Created: latency-svc-gzgsb
Aug 30 07:54:51.974: INFO: Got endpoints: latency-svc-gzgsb [476.854356ms]
Aug 30 07:54:51.982: INFO: Created: latency-svc-dgzkf
Aug 30 07:54:51.998: INFO: Got endpoints: latency-svc-dgzkf [437.195511ms]
Aug 30 07:54:52.006: INFO: Created: latency-svc-896vc
Aug 30 07:54:52.024: INFO: Got endpoints: latency-svc-896vc [458.573734ms]
Aug 30 07:54:52.035: INFO: Created: latency-svc-5h62v
Aug 30 07:54:52.051: INFO: Got endpoints: latency-svc-5h62v [457.799671ms]
Aug 30 07:54:52.060: INFO: Created: latency-svc-m5q7x
Aug 30 07:54:52.099: INFO: Got endpoints: latency-svc-m5q7x [456.809835ms]
Aug 30 07:54:52.101: INFO: Created: latency-svc-x29sc
Aug 30 07:54:52.115: INFO: Got endpoints: latency-svc-x29sc [450.743771ms]
Aug 30 07:54:52.124: INFO: Created: latency-svc-xbm6w
Aug 30 07:54:52.138: INFO: Got endpoints: latency-svc-xbm6w [460.357834ms]
Aug 30 07:54:52.146: INFO: Created: latency-svc-9jq4w
Aug 30 07:54:52.187: INFO: Got endpoints: latency-svc-9jq4w [484.500865ms]
Aug 30 07:54:52.199: INFO: Created: latency-svc-m4cr9
Aug 30 07:54:52.226: INFO: Got endpoints: latency-svc-m4cr9 [498.772897ms]
Aug 30 07:54:52.226: INFO: Created: latency-svc-bt2wh
Aug 30 07:54:52.253: INFO: Got endpoints: latency-svc-bt2wh [490.012441ms]
Aug 30 07:54:52.261: INFO: Created: latency-svc-8m7xf
Aug 30 07:54:52.281: INFO: Got endpoints: latency-svc-8m7xf [493.464229ms]
Aug 30 07:54:52.293: INFO: Created: latency-svc-kqmkv
Aug 30 07:54:52.319: INFO: Got endpoints: latency-svc-kqmkv [464.612865ms]
Aug 30 07:54:52.332: INFO: Created: latency-svc-9r6wq
Aug 30 07:54:52.348: INFO: Got endpoints: latency-svc-9r6wq [472.289484ms]
Aug 30 07:54:52.350: INFO: Created: latency-svc-z8m9b
Aug 30 07:54:52.377: INFO: Created: latency-svc-47957
Aug 30 07:54:52.377: INFO: Got endpoints: latency-svc-z8m9b [474.736846ms]
Aug 30 07:54:52.390: INFO: Got endpoints: latency-svc-47957 [444.817788ms]
Aug 30 07:54:52.395: INFO: Created: latency-svc-thhrg
Aug 30 07:54:52.445: INFO: Got endpoints: latency-svc-thhrg [470.399141ms]
Aug 30 07:54:52.453: INFO: Created: latency-svc-9d7gl
Aug 30 07:54:52.479: INFO: Got endpoints: latency-svc-9d7gl [481.522666ms]
Aug 30 07:54:52.491: INFO: Created: latency-svc-748hr
Aug 30 07:54:52.507: INFO: Got endpoints: latency-svc-748hr [482.396219ms]
Aug 30 07:54:52.530: INFO: Created: latency-svc-rg6fr
Aug 30 07:54:52.544: INFO: Got endpoints: latency-svc-rg6fr [492.083053ms]
Aug 30 07:54:52.546: INFO: Created: latency-svc-ljbqk
Aug 30 07:54:52.566: INFO: Got endpoints: latency-svc-ljbqk [467.431238ms]
Aug 30 07:54:52.584: INFO: Created: latency-svc-qf8wg
Aug 30 07:54:52.627: INFO: Got endpoints: latency-svc-qf8wg [511.601805ms]
Aug 30 07:54:52.635: INFO: Created: latency-svc-wfsk8
Aug 30 07:54:52.652: INFO: Created: latency-svc-kzhds
Aug 30 07:54:52.674: INFO: Got endpoints: latency-svc-wfsk8 [535.238118ms]
Aug 30 07:54:52.677: INFO: Got endpoints: latency-svc-kzhds [489.944239ms]
Aug 30 07:54:52.690: INFO: Created: latency-svc-2tvcb
Aug 30 07:54:52.721: INFO: Got endpoints: latency-svc-2tvcb [494.457441ms]
Aug 30 07:54:52.731: INFO: Created: latency-svc-z9x9k
Aug 30 07:54:52.748: INFO: Got endpoints: latency-svc-z9x9k [494.733448ms]
Aug 30 07:54:52.762: INFO: Created: latency-svc-6d85t
Aug 30 07:54:52.801: INFO: Got endpoints: latency-svc-6d85t [520.241428ms]
Aug 30 07:54:52.807: INFO: Created: latency-svc-w9lpc
Aug 30 07:54:52.819: INFO: Got endpoints: latency-svc-w9lpc [500.572156ms]
Aug 30 07:54:52.844: INFO: Created: latency-svc-c87rs
Aug 30 07:54:52.845: INFO: Created: latency-svc-wn8kq
Aug 30 07:54:52.887: INFO: Created: latency-svc-vdmfb
Aug 30 07:54:52.888: INFO: Got endpoints: latency-svc-wn8kq [511.09347ms]
Aug 30 07:54:52.889: INFO: Got endpoints: latency-svc-c87rs [540.655139ms]
Aug 30 07:54:52.889: INFO: Got endpoints: latency-svc-vdmfb [499.299983ms]
Aug 30 07:54:52.894: INFO: Created: latency-svc-x89ts
Aug 30 07:54:52.910: INFO: Got endpoints: latency-svc-x89ts [465.375435ms]
Aug 30 07:54:52.918: INFO: Created: latency-svc-hvvd7
Aug 30 07:54:52.965: INFO: Got endpoints: latency-svc-hvvd7 [485.608573ms]
Aug 30 07:54:52.977: INFO: Created: latency-svc-fxcsj
Aug 30 07:54:52.992: INFO: Got endpoints: latency-svc-fxcsj [484.906259ms]
Aug 30 07:54:52.992: INFO: Latencies: [49.949225ms 73.929358ms 81.737327ms 82.315006ms 100.5467ms 105.463817ms 106.390838ms 115.011144ms 120.241612ms 133.787614ms 144.109104ms 151.242642ms 163.946769ms 179.428557ms 190.80393ms 203.711664ms 214.036648ms 220.087506ms 229.564575ms 231.226575ms 231.801505ms 250.867114ms 258.07179ms 293.664197ms 298.020379ms 300.892439ms 302.240374ms 308.922635ms 312.134413ms 313.075612ms 319.375742ms 324.438394ms 327.966151ms 333.64891ms 334.078206ms 340.694743ms 346.179564ms 346.489111ms 347.8135ms 351.044466ms 352.014204ms 352.82078ms 353.342895ms 355.786053ms 356.685839ms 357.596339ms 359.901183ms 361.061974ms 361.291127ms 364.107249ms 365.39331ms 388.096059ms 391.490513ms 393.580105ms 397.829793ms 414.263667ms 419.728123ms 422.448636ms 423.013374ms 423.242561ms 425.45787ms 425.466866ms 426.005963ms 426.66627ms 437.195511ms 437.550572ms 441.525483ms 443.22083ms 444.817788ms 447.13379ms 450.502687ms 450.686478ms 450.743771ms 450.903768ms 452.971476ms 452.971701ms 453.296458ms 454.171444ms 455.51878ms 456.809835ms 457.032708ms 457.799671ms 457.867737ms 458.215577ms 458.573734ms 459.314263ms 459.397701ms 459.669423ms 460.248846ms 460.357834ms 464.612865ms 464.859172ms 465.375435ms 466.226191ms 467.431238ms 470.107962ms 470.399141ms 470.972746ms 472.289484ms 473.730615ms 474.162695ms 474.736846ms 474.89953ms 475.68893ms 476.854356ms 477.714138ms 481.522666ms 481.699722ms 481.78126ms 482.396219ms 484.500865ms 484.660366ms 484.906259ms 485.608573ms 486.603649ms 488.214303ms 489.944239ms 490.012441ms 490.638485ms 492.083053ms 492.423363ms 493.464229ms 493.884397ms 494.457441ms 494.733448ms 496.718703ms 496.902509ms 498.133975ms 498.772897ms 499.299983ms 499.538958ms 500.572156ms 503.762615ms 504.053286ms 505.921161ms 507.408128ms 508.175995ms 508.210447ms 508.702072ms 511.09347ms 511.601805ms 512.67405ms 512.707527ms 520.241428ms 521.88877ms 523.867067ms 535.238118ms 535.295703ms 536.157647ms 536.273997ms 536.969579ms 540.655139ms 541.124979ms 544.597458ms 549.344097ms 555.239421ms 558.121781ms 559.538281ms 562.503304ms 563.270476ms 570.602731ms 570.983127ms 571.030147ms 571.887889ms 573.961368ms 575.552459ms 579.555699ms 582.842383ms 583.305469ms 583.826031ms 594.30534ms 594.498329ms 594.684964ms 602.80077ms 604.567821ms 609.615969ms 616.426536ms 630.34214ms 632.976399ms 635.866004ms 639.357659ms 640.059387ms 640.191527ms 641.907347ms 645.215484ms 648.392665ms 649.567054ms 651.013036ms 651.650719ms 651.98874ms 653.026771ms 655.034358ms 663.899141ms 687.465512ms 692.504386ms 700.628446ms 704.854348ms 716.104504ms 729.167101ms 754.000082ms]
Aug 30 07:54:52.992: INFO: 50 %ile: 474.162695ms
Aug 30 07:54:52.992: INFO: 90 %ile: 639.357659ms
Aug 30 07:54:52.992: INFO: 99 %ile: 729.167101ms
Aug 30 07:54:52.992: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 30 07:54:52.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7156" for this suite. 08/30/23 07:54:53.006
------------------------------
• [SLOW TEST] [9.594 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:43.433
    Aug 30 07:54:43.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename svc-latency 08/30/23 07:54:43.434
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:43.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:43.495
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 30 07:54:43.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7156 08/30/23 07:54:43.506
    W0830 07:54:43.527090      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0830 07:54:43.527460      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7156, replica count: 1
    I0830 07:54:44.579323      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0830 07:54:45.580154      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0830 07:54:46.580481      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:54:46.718: INFO: Created: latency-svc-9wqgw
    Aug 30 07:54:46.734: INFO: Got endpoints: latency-svc-9wqgw [53.518648ms]
    Aug 30 07:54:46.774: INFO: Created: latency-svc-d522h
    Aug 30 07:54:46.786: INFO: Got endpoints: latency-svc-d522h [49.949225ms]
    Aug 30 07:54:46.792: INFO: Created: latency-svc-t6dll
    Aug 30 07:54:46.810: INFO: Got endpoints: latency-svc-t6dll [73.929358ms]
    Aug 30 07:54:46.813: INFO: Created: latency-svc-lxlrn
    Aug 30 07:54:46.842: INFO: Created: latency-svc-tcn2j
    Aug 30 07:54:46.842: INFO: Got endpoints: latency-svc-lxlrn [106.390838ms]
    Aug 30 07:54:46.851: INFO: Got endpoints: latency-svc-tcn2j [115.011144ms]
    Aug 30 07:54:46.859: INFO: Created: latency-svc-pnwfj
    Aug 30 07:54:46.870: INFO: Got endpoints: latency-svc-pnwfj [133.787614ms]
    Aug 30 07:54:46.873: INFO: Created: latency-svc-t47bp
    Aug 30 07:54:46.886: INFO: Got endpoints: latency-svc-t47bp [151.242642ms]
    Aug 30 07:54:46.898: INFO: Created: latency-svc-qvqxw
    Aug 30 07:54:46.916: INFO: Got endpoints: latency-svc-qvqxw [179.428557ms]
    Aug 30 07:54:46.924: INFO: Created: latency-svc-td6l5
    Aug 30 07:54:46.939: INFO: Got endpoints: latency-svc-td6l5 [203.711664ms]
    Aug 30 07:54:46.948: INFO: Created: latency-svc-ml7dg
    Aug 30 07:54:46.967: INFO: Got endpoints: latency-svc-ml7dg [231.226575ms]
    Aug 30 07:54:46.976: INFO: Created: latency-svc-b4z92
    Aug 30 07:54:46.987: INFO: Got endpoints: latency-svc-b4z92 [250.867114ms]
    Aug 30 07:54:47.014: INFO: Created: latency-svc-fmfwz
    Aug 30 07:54:47.033: INFO: Got endpoints: latency-svc-fmfwz [298.020379ms]
    Aug 30 07:54:47.038: INFO: Created: latency-svc-brr6t
    Aug 30 07:54:47.055: INFO: Got endpoints: latency-svc-brr6t [319.375742ms]
    Aug 30 07:54:47.062: INFO: Created: latency-svc-8l2lh
    Aug 30 07:54:47.082: INFO: Got endpoints: latency-svc-8l2lh [346.179564ms]
    Aug 30 07:54:47.088: INFO: Created: latency-svc-7mp76
    Aug 30 07:54:47.097: INFO: Got endpoints: latency-svc-7mp76 [361.291127ms]
    Aug 30 07:54:47.113: INFO: Created: latency-svc-6xgm4
    Aug 30 07:54:47.128: INFO: Created: latency-svc-jx84m
    Aug 30 07:54:47.130: INFO: Got endpoints: latency-svc-6xgm4 [393.580105ms]
    Aug 30 07:54:47.146: INFO: Got endpoints: latency-svc-jx84m [359.901183ms]
    Aug 30 07:54:47.147: INFO: Created: latency-svc-glnsw
    Aug 30 07:54:47.163: INFO: Got endpoints: latency-svc-glnsw [352.82078ms]
    Aug 30 07:54:47.169: INFO: Created: latency-svc-kjqp9
    Aug 30 07:54:47.183: INFO: Got endpoints: latency-svc-kjqp9 [340.694743ms]
    Aug 30 07:54:47.191: INFO: Created: latency-svc-bp7st
    Aug 30 07:54:47.203: INFO: Got endpoints: latency-svc-bp7st [352.014204ms]
    Aug 30 07:54:47.209: INFO: Created: latency-svc-nzbqj
    Aug 30 07:54:47.226: INFO: Got endpoints: latency-svc-nzbqj [355.786053ms]
    Aug 30 07:54:47.235: INFO: Created: latency-svc-d8bjf
    Aug 30 07:54:47.252: INFO: Got endpoints: latency-svc-d8bjf [365.39331ms]
    Aug 30 07:54:47.257: INFO: Created: latency-svc-zf7xc
    Aug 30 07:54:47.273: INFO: Got endpoints: latency-svc-zf7xc [357.596339ms]
    Aug 30 07:54:47.280: INFO: Created: latency-svc-5vgrc
    Aug 30 07:54:47.296: INFO: Got endpoints: latency-svc-5vgrc [356.685839ms]
    Aug 30 07:54:47.299: INFO: Created: latency-svc-bhncc
    Aug 30 07:54:47.321: INFO: Got endpoints: latency-svc-bhncc [353.342895ms]
    Aug 30 07:54:47.322: INFO: Created: latency-svc-z7bq9
    Aug 30 07:54:47.334: INFO: Got endpoints: latency-svc-z7bq9 [346.489111ms]
    Aug 30 07:54:47.341: INFO: Created: latency-svc-frg8v
    Aug 30 07:54:47.362: INFO: Got endpoints: latency-svc-frg8v [327.966151ms]
    Aug 30 07:54:47.684: INFO: Created: latency-svc-k9hkd
    Aug 30 07:54:47.710: INFO: Created: latency-svc-8hv4p
    Aug 30 07:54:47.721: INFO: Got endpoints: latency-svc-k9hkd [639.357659ms]
    Aug 30 07:54:47.722: INFO: Created: latency-svc-29884
    Aug 30 07:54:47.722: INFO: Created: latency-svc-68qcs
    Aug 30 07:54:47.722: INFO: Created: latency-svc-nkw98
    Aug 30 07:54:47.723: INFO: Created: latency-svc-dmp4j
    Aug 30 07:54:47.723: INFO: Created: latency-svc-jrl7z
    Aug 30 07:54:47.723: INFO: Created: latency-svc-k9l6k
    Aug 30 07:54:47.723: INFO: Created: latency-svc-jf6kj
    Aug 30 07:54:47.723: INFO: Created: latency-svc-vppdn
    Aug 30 07:54:47.723: INFO: Created: latency-svc-2gnfw
    Aug 30 07:54:47.723: INFO: Created: latency-svc-x8h2c
    Aug 30 07:54:47.724: INFO: Created: latency-svc-597wj
    Aug 30 07:54:47.724: INFO: Created: latency-svc-mn8fk
    Aug 30 07:54:47.724: INFO: Created: latency-svc-bxhkv
    Aug 30 07:54:47.728: INFO: Got endpoints: latency-svc-8hv4p [630.34214ms]
    Aug 30 07:54:47.734: INFO: Got endpoints: latency-svc-2gnfw [604.567821ms]
    Aug 30 07:54:47.735: INFO: Got endpoints: latency-svc-jf6kj [571.887889ms]
    Aug 30 07:54:47.735: INFO: Got endpoints: latency-svc-k9l6k [414.263667ms]
    Aug 30 07:54:47.738: INFO: Got endpoints: latency-svc-jrl7z [512.67405ms]
    Aug 30 07:54:47.743: INFO: Got endpoints: latency-svc-dmp4j [687.465512ms]
    Aug 30 07:54:47.760: INFO: Got endpoints: latency-svc-nkw98 [508.210447ms]
    Aug 30 07:54:47.761: INFO: Got endpoints: latency-svc-bxhkv [464.859172ms]
    Aug 30 07:54:47.761: INFO: Got endpoints: latency-svc-mn8fk [558.121781ms]
    Aug 30 07:54:47.762: INFO: Got endpoints: latency-svc-x8h2c [488.214303ms]
    Aug 30 07:54:47.762: INFO: Got endpoints: latency-svc-vppdn [616.426536ms]
    Aug 30 07:54:47.778: INFO: Got endpoints: latency-svc-29884 [594.498329ms]
    Aug 30 07:54:47.784: INFO: Created: latency-svc-b4wvn
    Aug 30 07:54:47.785: INFO: Got endpoints: latency-svc-597wj [450.903768ms]
    Aug 30 07:54:47.785: INFO: Got endpoints: latency-svc-68qcs [423.242561ms]
    Aug 30 07:54:47.803: INFO: Got endpoints: latency-svc-b4wvn [81.737327ms]
    Aug 30 07:54:47.814: INFO: Created: latency-svc-lvm9v
    Aug 30 07:54:47.833: INFO: Got endpoints: latency-svc-lvm9v [105.463817ms]
    Aug 30 07:54:47.840: INFO: Created: latency-svc-l84tv
    Aug 30 07:54:47.855: INFO: Got endpoints: latency-svc-l84tv [120.241612ms]
    Aug 30 07:54:47.864: INFO: Created: latency-svc-xmr4p
    Aug 30 07:54:47.879: INFO: Got endpoints: latency-svc-xmr4p [144.109104ms]
    Aug 30 07:54:47.886: INFO: Created: latency-svc-nf5hj
    Aug 30 07:54:47.899: INFO: Got endpoints: latency-svc-nf5hj [163.946769ms]
    Aug 30 07:54:47.909: INFO: Created: latency-svc-fz2b6
    Aug 30 07:54:47.929: INFO: Got endpoints: latency-svc-fz2b6 [190.80393ms]
    Aug 30 07:54:47.931: INFO: Created: latency-svc-j68v5
    Aug 30 07:54:47.948: INFO: Created: latency-svc-x55ng
    Aug 30 07:54:47.974: INFO: Got endpoints: latency-svc-x55ng [214.036648ms]
    Aug 30 07:54:47.975: INFO: Got endpoints: latency-svc-j68v5 [231.801505ms]
    Aug 30 07:54:47.976: INFO: Created: latency-svc-x9m6q
    Aug 30 07:54:47.990: INFO: Got endpoints: latency-svc-x9m6q [229.564575ms]
    Aug 30 07:54:47.998: INFO: Created: latency-svc-tknbd
    Aug 30 07:54:48.020: INFO: Got endpoints: latency-svc-tknbd [258.07179ms]
    Aug 30 07:54:48.022: INFO: Created: latency-svc-wqdvm
    Aug 30 07:54:48.043: INFO: Created: latency-svc-p9qpm
    Aug 30 07:54:48.062: INFO: Got endpoints: latency-svc-wqdvm [300.892439ms]
    Aug 30 07:54:48.064: INFO: Got endpoints: latency-svc-p9qpm [302.240374ms]
    Aug 30 07:54:48.075: INFO: Created: latency-svc-jgjh5
    Aug 30 07:54:48.097: INFO: Got endpoints: latency-svc-jgjh5 [312.134413ms]
    Aug 30 07:54:48.097: INFO: Created: latency-svc-sw5cj
    Aug 30 07:54:48.112: INFO: Got endpoints: latency-svc-sw5cj [334.078206ms]
    Aug 30 07:54:48.118: INFO: Created: latency-svc-xvmpd
    Aug 30 07:54:48.133: INFO: Got endpoints: latency-svc-xvmpd [347.8135ms]
    Aug 30 07:54:48.145: INFO: Created: latency-svc-vzfk9
    Aug 30 07:54:48.167: INFO: Got endpoints: latency-svc-vzfk9 [364.107249ms]
    Aug 30 07:54:48.167: INFO: Created: latency-svc-s447n
    Aug 30 07:54:48.185: INFO: Got endpoints: latency-svc-s447n [351.044466ms]
    Aug 30 07:54:48.189: INFO: Created: latency-svc-pvswf
    Aug 30 07:54:48.246: INFO: Got endpoints: latency-svc-pvswf [391.490513ms]
    Aug 30 07:54:48.248: INFO: Created: latency-svc-jz828
    Aug 30 07:54:48.267: INFO: Got endpoints: latency-svc-jz828 [388.096059ms]
    Aug 30 07:54:48.280: INFO: Created: latency-svc-cdrrj
    Aug 30 07:54:48.296: INFO: Got endpoints: latency-svc-cdrrj [397.829793ms]
    Aug 30 07:54:48.300: INFO: Created: latency-svc-fps2h
    Aug 30 07:54:48.349: INFO: Got endpoints: latency-svc-fps2h [419.728123ms]
    Aug 30 07:54:48.355: INFO: Created: latency-svc-b9rdb
    Aug 30 07:54:48.398: INFO: Got endpoints: latency-svc-b9rdb [422.448636ms]
    Aug 30 07:54:48.463: INFO: Created: latency-svc-6fcbz
    Aug 30 07:54:48.479: INFO: Got endpoints: latency-svc-6fcbz [504.053286ms]
    Aug 30 07:54:48.486: INFO: Created: latency-svc-nkpk7
    Aug 30 07:54:48.526: INFO: Got endpoints: latency-svc-nkpk7 [535.295703ms]
    Aug 30 07:54:48.544: INFO: Created: latency-svc-csfl5
    Aug 30 07:54:48.596: INFO: Got endpoints: latency-svc-csfl5 [575.552459ms]
    Aug 30 07:54:48.597: INFO: Created: latency-svc-46zpq
    Aug 30 07:54:48.612: INFO: Got endpoints: latency-svc-46zpq [549.344097ms]
    Aug 30 07:54:48.618: INFO: Created: latency-svc-v2jpb
    Aug 30 07:54:48.638: INFO: Got endpoints: latency-svc-v2jpb [573.961368ms]
    Aug 30 07:54:48.644: INFO: Created: latency-svc-svv5b
    Aug 30 07:54:48.668: INFO: Got endpoints: latency-svc-svv5b [570.983127ms]
    Aug 30 07:54:48.669: INFO: Created: latency-svc-245fg
    Aug 30 07:54:48.683: INFO: Got endpoints: latency-svc-245fg [571.030147ms]
    Aug 30 07:54:48.690: INFO: Created: latency-svc-rbcg6
    Aug 30 07:54:48.703: INFO: Got endpoints: latency-svc-rbcg6 [570.602731ms]
    Aug 30 07:54:48.709: INFO: Created: latency-svc-4n2ht
    Aug 30 07:54:48.726: INFO: Created: latency-svc-k55vk
    Aug 30 07:54:48.727: INFO: Got endpoints: latency-svc-4n2ht [559.538281ms]
    Aug 30 07:54:48.740: INFO: Got endpoints: latency-svc-k55vk [555.239421ms]
    Aug 30 07:54:48.747: INFO: Created: latency-svc-9xjqh
    Aug 30 07:54:48.768: INFO: Got endpoints: latency-svc-9xjqh [521.88877ms]
    Aug 30 07:54:48.777: INFO: Created: latency-svc-txh22
    Aug 30 07:54:48.791: INFO: Got endpoints: latency-svc-txh22 [523.867067ms]
    Aug 30 07:54:48.792: INFO: Created: latency-svc-lgcmk
    Aug 30 07:54:48.838: INFO: Got endpoints: latency-svc-lgcmk [541.124979ms]
    Aug 30 07:54:48.842: INFO: Created: latency-svc-tp22w
    Aug 30 07:54:48.862: INFO: Got endpoints: latency-svc-tp22w [512.707527ms]
    Aug 30 07:54:48.879: INFO: Created: latency-svc-dfvrj
    Aug 30 07:54:48.902: INFO: Got endpoints: latency-svc-dfvrj [503.762615ms]
    Aug 30 07:54:48.910: INFO: Created: latency-svc-d8jhn
    Aug 30 07:54:48.933: INFO: Got endpoints: latency-svc-d8jhn [453.296458ms]
    Aug 30 07:54:48.935: INFO: Created: latency-svc-nt94n
    Aug 30 07:54:48.980: INFO: Got endpoints: latency-svc-nt94n [454.171444ms]
    Aug 30 07:54:48.995: INFO: Created: latency-svc-vkj6d
    Aug 30 07:54:49.033: INFO: Got endpoints: latency-svc-vkj6d [437.550572ms]
    Aug 30 07:54:49.046: INFO: Created: latency-svc-4rspn
    Aug 30 07:54:49.086: INFO: Got endpoints: latency-svc-4rspn [474.162695ms]
    Aug 30 07:54:49.096: INFO: Created: latency-svc-w622d
    Aug 30 07:54:49.113: INFO: Got endpoints: latency-svc-w622d [474.89953ms]
    Aug 30 07:54:49.120: INFO: Created: latency-svc-k76rt
    Aug 30 07:54:49.142: INFO: Got endpoints: latency-svc-k76rt [473.730615ms]
    Aug 30 07:54:49.142: INFO: Created: latency-svc-ddfs7
    Aug 30 07:54:49.159: INFO: Got endpoints: latency-svc-ddfs7 [475.68893ms]
    Aug 30 07:54:49.169: INFO: Created: latency-svc-gx26p
    Aug 30 07:54:49.185: INFO: Got endpoints: latency-svc-gx26p [481.699722ms]
    Aug 30 07:54:49.197: INFO: Created: latency-svc-vjpqx
    Aug 30 07:54:49.211: INFO: Got endpoints: latency-svc-vjpqx [484.660366ms]
    Aug 30 07:54:49.219: INFO: Created: latency-svc-6stjg
    Aug 30 07:54:49.248: INFO: Created: latency-svc-q7s9h
    Aug 30 07:54:49.248: INFO: Got endpoints: latency-svc-6stjg [508.175995ms]
    Aug 30 07:54:49.265: INFO: Got endpoints: latency-svc-q7s9h [496.718703ms]
    Aug 30 07:54:49.268: INFO: Created: latency-svc-tp2kk
    Aug 30 07:54:49.299: INFO: Created: latency-svc-tpvxm
    Aug 30 07:54:49.300: INFO: Got endpoints: latency-svc-tp2kk [508.702072ms]
    Aug 30 07:54:49.319: INFO: Got endpoints: latency-svc-tpvxm [481.78126ms]
    Aug 30 07:54:49.348: INFO: Created: latency-svc-dvkql
    Aug 30 07:54:49.368: INFO: Got endpoints: latency-svc-dvkql [505.921161ms]
    Aug 30 07:54:49.380: INFO: Created: latency-svc-kl74c
    Aug 30 07:54:49.399: INFO: Got endpoints: latency-svc-kl74c [496.902509ms]
    Aug 30 07:54:49.407: INFO: Created: latency-svc-24wd2
    Aug 30 07:54:49.425: INFO: Got endpoints: latency-svc-24wd2 [492.423363ms]
    Aug 30 07:54:49.449: INFO: Created: latency-svc-c59fv
    Aug 30 07:54:49.474: INFO: Got endpoints: latency-svc-c59fv [493.884397ms]
    Aug 30 07:54:49.484: INFO: Created: latency-svc-x5r2q
    Aug 30 07:54:49.504: INFO: Got endpoints: latency-svc-x5r2q [470.107962ms]
    Aug 30 07:54:49.514: INFO: Created: latency-svc-xjq25
    Aug 30 07:54:49.533: INFO: Got endpoints: latency-svc-xjq25 [447.13379ms]
    Aug 30 07:54:49.546: INFO: Created: latency-svc-xhn6l
    Aug 30 07:54:49.564: INFO: Got endpoints: latency-svc-xhn6l [450.686478ms]
    Aug 30 07:54:49.571: INFO: Created: latency-svc-s6slh
    Aug 30 07:54:49.595: INFO: Got endpoints: latency-svc-s6slh [452.971476ms]
    Aug 30 07:54:49.597: INFO: Created: latency-svc-hdndq
    Aug 30 07:54:49.616: INFO: Got endpoints: latency-svc-hdndq [457.032708ms]
    Aug 30 07:54:49.618: INFO: Created: latency-svc-jnjs6
    Aug 30 07:54:49.636: INFO: Got endpoints: latency-svc-jnjs6 [450.502687ms]
    Aug 30 07:54:49.883: INFO: Created: latency-svc-j6lf6
    Aug 30 07:54:49.905: INFO: Created: latency-svc-582w9
    Aug 30 07:54:49.906: INFO: Created: latency-svc-v4dfq
    Aug 30 07:54:49.906: INFO: Created: latency-svc-9rxff
    Aug 30 07:54:49.907: INFO: Created: latency-svc-dnkkk
    Aug 30 07:54:49.907: INFO: Created: latency-svc-jkmrh
    Aug 30 07:54:49.907: INFO: Created: latency-svc-5nwzh
    Aug 30 07:54:49.908: INFO: Created: latency-svc-dkz2s
    Aug 30 07:54:49.908: INFO: Got endpoints: latency-svc-j6lf6 [313.075612ms]
    Aug 30 07:54:49.908: INFO: Created: latency-svc-8mhxl
    Aug 30 07:54:49.909: INFO: Created: latency-svc-742tl
    Aug 30 07:54:49.909: INFO: Created: latency-svc-lz7z5
    Aug 30 07:54:49.909: INFO: Created: latency-svc-fphrj
    Aug 30 07:54:49.910: INFO: Created: latency-svc-tb4ql
    Aug 30 07:54:49.910: INFO: Created: latency-svc-hwl5l
    Aug 30 07:54:49.910: INFO: Created: latency-svc-xsz6j
    Aug 30 07:54:49.925: INFO: Got endpoints: latency-svc-8mhxl [308.922635ms]
    Aug 30 07:54:49.925: INFO: Got endpoints: latency-svc-742tl [361.061974ms]
    Aug 30 07:54:49.961: INFO: Got endpoints: latency-svc-dnkkk [324.438394ms]
    Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-xsz6j [536.273997ms]
    Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-9rxff [563.270476ms]
    Aug 30 07:54:49.962: INFO: Got endpoints: latency-svc-5nwzh [594.30534ms]
    Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-dkz2s [663.899141ms]
    Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-jkmrh [460.248846ms]
    Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-tb4ql [716.104504ms]
    Aug 30 07:54:49.964: INFO: Got endpoints: latency-svc-582w9 [426.66627ms]
    Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-v4dfq [490.638485ms]
    Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-fphrj [645.215484ms]
    Aug 30 07:54:49.965: INFO: Got endpoints: latency-svc-hwl5l [754.000082ms]
    Aug 30 07:54:49.966: INFO: Got endpoints: latency-svc-lz7z5 [700.628446ms]
    Aug 30 07:54:49.976: INFO: Created: latency-svc-gxj2l
    Aug 30 07:54:50.009: INFO: Got endpoints: latency-svc-gxj2l [100.5467ms]
    Aug 30 07:54:50.009: INFO: Created: latency-svc-fcwcd
    Aug 30 07:54:50.009: INFO: Got endpoints: latency-svc-fcwcd [82.315006ms]
    Aug 30 07:54:50.396: INFO: Created: latency-svc-cnx77
    Aug 30 07:54:50.398: INFO: Created: latency-svc-5gdkq
    Aug 30 07:54:50.398: INFO: Created: latency-svc-nlqtd
    Aug 30 07:54:50.398: INFO: Created: latency-svc-d6qdx
    Aug 30 07:54:50.398: INFO: Created: latency-svc-jgkh9
    Aug 30 07:54:50.398: INFO: Created: latency-svc-zkbjv
    Aug 30 07:54:50.399: INFO: Created: latency-svc-sxn8n
    Aug 30 07:54:50.399: INFO: Created: latency-svc-2q5vj
    Aug 30 07:54:50.400: INFO: Created: latency-svc-82g2g
    Aug 30 07:54:50.400: INFO: Created: latency-svc-zfx5s
    Aug 30 07:54:50.438: INFO: Created: latency-svc-lmbxs
    Aug 30 07:54:50.439: INFO: Created: latency-svc-2f6zj
    Aug 30 07:54:50.439: INFO: Created: latency-svc-8lljj
    Aug 30 07:54:50.442: INFO: Created: latency-svc-dzt72
    Aug 30 07:54:50.442: INFO: Created: latency-svc-9sbsx
    Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-cnx77 [498.133975ms]
    Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-d6qdx [459.669423ms]
    Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-2q5vj [544.597458ms]
    Aug 30 07:54:50.469: INFO: Got endpoints: latency-svc-5gdkq [507.408128ms]
    Aug 30 07:54:50.470: INFO: Got endpoints: latency-svc-zkbjv [499.538958ms]
    Aug 30 07:54:50.501: INFO: Got endpoints: latency-svc-82g2g [536.969579ms]
    Aug 30 07:54:50.544: INFO: Got endpoints: latency-svc-sxn8n [579.555699ms]
    Aug 30 07:54:50.544: INFO: Got endpoints: latency-svc-zfx5s [583.305469ms]
    Aug 30 07:54:50.545: INFO: Got endpoints: latency-svc-jgkh9 [582.842383ms]
    Aug 30 07:54:50.545: INFO: Got endpoints: latency-svc-nlqtd [536.157647ms]
    Aug 30 07:54:50.571: INFO: Got endpoints: latency-svc-lmbxs [609.615969ms]
    Aug 30 07:54:50.614: INFO: Got endpoints: latency-svc-2f6zj [649.567054ms]
    Aug 30 07:54:50.615: INFO: Got endpoints: latency-svc-8lljj [651.013036ms]
    Aug 30 07:54:50.616: INFO: Got endpoints: latency-svc-9sbsx [651.650719ms]
    Aug 30 07:54:50.617: INFO: Got endpoints: latency-svc-dzt72 [651.98874ms]
    Aug 30 07:54:50.617: INFO: Created: latency-svc-b6kk2
    Aug 30 07:54:50.690: INFO: Got endpoints: latency-svc-b6kk2 [220.087506ms]
    Aug 30 07:54:50.721: INFO: Created: latency-svc-kmjqt
    Aug 30 07:54:50.763: INFO: Got endpoints: latency-svc-kmjqt [293.664197ms]
    Aug 30 07:54:50.769: INFO: Created: latency-svc-ws8d9
    Aug 30 07:54:50.803: INFO: Got endpoints: latency-svc-ws8d9 [333.64891ms]
    Aug 30 07:54:50.837: INFO: Created: latency-svc-2kvng
    Aug 30 07:54:50.847: INFO: Created: latency-svc-d5bvs
    Aug 30 07:54:50.895: INFO: Got endpoints: latency-svc-d5bvs [425.466866ms]
    Aug 30 07:54:50.895: INFO: Got endpoints: latency-svc-2kvng [425.45787ms]
    Aug 30 07:54:50.905: INFO: Created: latency-svc-bdrs9
    Aug 30 07:54:50.924: INFO: Got endpoints: latency-svc-bdrs9 [423.013374ms]
    Aug 30 07:54:50.944: INFO: Created: latency-svc-rzvdt
    Aug 30 07:54:50.971: INFO: Got endpoints: latency-svc-rzvdt [426.005963ms]
    Aug 30 07:54:50.977: INFO: Created: latency-svc-lstkk
    Aug 30 07:54:51.107: INFO: Got endpoints: latency-svc-lstkk [562.503304ms]
    Aug 30 07:54:51.131: INFO: Created: latency-svc-jt69q
    Aug 30 07:54:51.156: INFO: Created: latency-svc-52qm4
    Aug 30 07:54:51.184: INFO: Got endpoints: latency-svc-jt69q [640.059387ms]
    Aug 30 07:54:51.187: INFO: Got endpoints: latency-svc-52qm4 [641.907347ms]
    Aug 30 07:54:51.196: INFO: Created: latency-svc-zbrbf
    Aug 30 07:54:51.220: INFO: Got endpoints: latency-svc-zbrbf [648.392665ms]
    Aug 30 07:54:51.220: INFO: Created: latency-svc-kk9rk
    Aug 30 07:54:51.247: INFO: Created: latency-svc-kftgf
    Aug 30 07:54:51.249: INFO: Got endpoints: latency-svc-kk9rk [632.976399ms]
    Aug 30 07:54:51.272: INFO: Got endpoints: latency-svc-kftgf [655.034358ms]
    Aug 30 07:54:51.293: INFO: Created: latency-svc-qbz4l
    Aug 30 07:54:51.320: INFO: Got endpoints: latency-svc-qbz4l [704.854348ms]
    Aug 30 07:54:51.321: INFO: Created: latency-svc-gsc59
    Aug 30 07:54:51.346: INFO: Got endpoints: latency-svc-gsc59 [729.167101ms]
    Aug 30 07:54:51.359: INFO: Created: latency-svc-888rw
    Aug 30 07:54:51.382: INFO: Got endpoints: latency-svc-888rw [692.504386ms]
    Aug 30 07:54:51.393: INFO: Created: latency-svc-bt49c
    Aug 30 07:54:51.416: INFO: Got endpoints: latency-svc-bt49c [653.026771ms]
    Aug 30 07:54:51.428: INFO: Created: latency-svc-gf9xc
    Aug 30 07:54:51.443: INFO: Got endpoints: latency-svc-gf9xc [640.191527ms]
    Aug 30 07:54:51.455: INFO: Created: latency-svc-7mjmh
    Aug 30 07:54:51.479: INFO: Got endpoints: latency-svc-7mjmh [583.826031ms]
    Aug 30 07:54:51.483: INFO: Created: latency-svc-cnxqv
    Aug 30 07:54:51.498: INFO: Got endpoints: latency-svc-cnxqv [602.80077ms]
    Aug 30 07:54:51.512: INFO: Created: latency-svc-zrwll
    Aug 30 07:54:51.560: INFO: Got endpoints: latency-svc-zrwll [635.866004ms]
    Aug 30 07:54:51.566: INFO: Created: latency-svc-6xp29
    Aug 30 07:54:51.566: INFO: Got endpoints: latency-svc-6xp29 [594.684964ms]
    Aug 30 07:54:51.594: INFO: Created: latency-svc-jzdlh
    Aug 30 07:54:51.594: INFO: Got endpoints: latency-svc-jzdlh [486.603649ms]
    Aug 30 07:54:51.624: INFO: Created: latency-svc-r6ntq
    Aug 30 07:54:51.642: INFO: Got endpoints: latency-svc-r6ntq [457.867737ms]
    Aug 30 07:54:51.647: INFO: Created: latency-svc-s2scf
    Aug 30 07:54:51.665: INFO: Got endpoints: latency-svc-s2scf [477.714138ms]
    Aug 30 07:54:51.667: INFO: Created: latency-svc-mw6vx
    Aug 30 07:54:51.678: INFO: Got endpoints: latency-svc-mw6vx [458.215577ms]
    Aug 30 07:54:51.686: INFO: Created: latency-svc-hbnwp
    Aug 30 07:54:51.702: INFO: Got endpoints: latency-svc-hbnwp [452.971701ms]
    Aug 30 07:54:51.712: INFO: Created: latency-svc-zw9wl
    Aug 30 07:54:51.727: INFO: Got endpoints: latency-svc-zw9wl [455.51878ms]
    Aug 30 07:54:51.740: INFO: Created: latency-svc-rbc6p
    Aug 30 07:54:51.758: INFO: Created: latency-svc-n8fbw
    Aug 30 07:54:51.763: INFO: Got endpoints: latency-svc-rbc6p [443.22083ms]
    Aug 30 07:54:51.787: INFO: Got endpoints: latency-svc-n8fbw [441.525483ms]
    Aug 30 07:54:51.809: INFO: Created: latency-svc-8hlfs
    Aug 30 07:54:51.854: INFO: Got endpoints: latency-svc-8hlfs [470.972746ms]
    Aug 30 07:54:51.854: INFO: Created: latency-svc-mgctm
    Aug 30 07:54:51.876: INFO: Got endpoints: latency-svc-mgctm [459.314263ms]
    Aug 30 07:54:51.884: INFO: Created: latency-svc-rbslc
    Aug 30 07:54:51.903: INFO: Got endpoints: latency-svc-rbslc [459.397701ms]
    Aug 30 07:54:51.931: INFO: Created: latency-svc-c2k66
    Aug 30 07:54:51.945: INFO: Got endpoints: latency-svc-c2k66 [466.226191ms]
    Aug 30 07:54:51.960: INFO: Created: latency-svc-gzgsb
    Aug 30 07:54:51.974: INFO: Got endpoints: latency-svc-gzgsb [476.854356ms]
    Aug 30 07:54:51.982: INFO: Created: latency-svc-dgzkf
    Aug 30 07:54:51.998: INFO: Got endpoints: latency-svc-dgzkf [437.195511ms]
    Aug 30 07:54:52.006: INFO: Created: latency-svc-896vc
    Aug 30 07:54:52.024: INFO: Got endpoints: latency-svc-896vc [458.573734ms]
    Aug 30 07:54:52.035: INFO: Created: latency-svc-5h62v
    Aug 30 07:54:52.051: INFO: Got endpoints: latency-svc-5h62v [457.799671ms]
    Aug 30 07:54:52.060: INFO: Created: latency-svc-m5q7x
    Aug 30 07:54:52.099: INFO: Got endpoints: latency-svc-m5q7x [456.809835ms]
    Aug 30 07:54:52.101: INFO: Created: latency-svc-x29sc
    Aug 30 07:54:52.115: INFO: Got endpoints: latency-svc-x29sc [450.743771ms]
    Aug 30 07:54:52.124: INFO: Created: latency-svc-xbm6w
    Aug 30 07:54:52.138: INFO: Got endpoints: latency-svc-xbm6w [460.357834ms]
    Aug 30 07:54:52.146: INFO: Created: latency-svc-9jq4w
    Aug 30 07:54:52.187: INFO: Got endpoints: latency-svc-9jq4w [484.500865ms]
    Aug 30 07:54:52.199: INFO: Created: latency-svc-m4cr9
    Aug 30 07:54:52.226: INFO: Got endpoints: latency-svc-m4cr9 [498.772897ms]
    Aug 30 07:54:52.226: INFO: Created: latency-svc-bt2wh
    Aug 30 07:54:52.253: INFO: Got endpoints: latency-svc-bt2wh [490.012441ms]
    Aug 30 07:54:52.261: INFO: Created: latency-svc-8m7xf
    Aug 30 07:54:52.281: INFO: Got endpoints: latency-svc-8m7xf [493.464229ms]
    Aug 30 07:54:52.293: INFO: Created: latency-svc-kqmkv
    Aug 30 07:54:52.319: INFO: Got endpoints: latency-svc-kqmkv [464.612865ms]
    Aug 30 07:54:52.332: INFO: Created: latency-svc-9r6wq
    Aug 30 07:54:52.348: INFO: Got endpoints: latency-svc-9r6wq [472.289484ms]
    Aug 30 07:54:52.350: INFO: Created: latency-svc-z8m9b
    Aug 30 07:54:52.377: INFO: Created: latency-svc-47957
    Aug 30 07:54:52.377: INFO: Got endpoints: latency-svc-z8m9b [474.736846ms]
    Aug 30 07:54:52.390: INFO: Got endpoints: latency-svc-47957 [444.817788ms]
    Aug 30 07:54:52.395: INFO: Created: latency-svc-thhrg
    Aug 30 07:54:52.445: INFO: Got endpoints: latency-svc-thhrg [470.399141ms]
    Aug 30 07:54:52.453: INFO: Created: latency-svc-9d7gl
    Aug 30 07:54:52.479: INFO: Got endpoints: latency-svc-9d7gl [481.522666ms]
    Aug 30 07:54:52.491: INFO: Created: latency-svc-748hr
    Aug 30 07:54:52.507: INFO: Got endpoints: latency-svc-748hr [482.396219ms]
    Aug 30 07:54:52.530: INFO: Created: latency-svc-rg6fr
    Aug 30 07:54:52.544: INFO: Got endpoints: latency-svc-rg6fr [492.083053ms]
    Aug 30 07:54:52.546: INFO: Created: latency-svc-ljbqk
    Aug 30 07:54:52.566: INFO: Got endpoints: latency-svc-ljbqk [467.431238ms]
    Aug 30 07:54:52.584: INFO: Created: latency-svc-qf8wg
    Aug 30 07:54:52.627: INFO: Got endpoints: latency-svc-qf8wg [511.601805ms]
    Aug 30 07:54:52.635: INFO: Created: latency-svc-wfsk8
    Aug 30 07:54:52.652: INFO: Created: latency-svc-kzhds
    Aug 30 07:54:52.674: INFO: Got endpoints: latency-svc-wfsk8 [535.238118ms]
    Aug 30 07:54:52.677: INFO: Got endpoints: latency-svc-kzhds [489.944239ms]
    Aug 30 07:54:52.690: INFO: Created: latency-svc-2tvcb
    Aug 30 07:54:52.721: INFO: Got endpoints: latency-svc-2tvcb [494.457441ms]
    Aug 30 07:54:52.731: INFO: Created: latency-svc-z9x9k
    Aug 30 07:54:52.748: INFO: Got endpoints: latency-svc-z9x9k [494.733448ms]
    Aug 30 07:54:52.762: INFO: Created: latency-svc-6d85t
    Aug 30 07:54:52.801: INFO: Got endpoints: latency-svc-6d85t [520.241428ms]
    Aug 30 07:54:52.807: INFO: Created: latency-svc-w9lpc
    Aug 30 07:54:52.819: INFO: Got endpoints: latency-svc-w9lpc [500.572156ms]
    Aug 30 07:54:52.844: INFO: Created: latency-svc-c87rs
    Aug 30 07:54:52.845: INFO: Created: latency-svc-wn8kq
    Aug 30 07:54:52.887: INFO: Created: latency-svc-vdmfb
    Aug 30 07:54:52.888: INFO: Got endpoints: latency-svc-wn8kq [511.09347ms]
    Aug 30 07:54:52.889: INFO: Got endpoints: latency-svc-c87rs [540.655139ms]
    Aug 30 07:54:52.889: INFO: Got endpoints: latency-svc-vdmfb [499.299983ms]
    Aug 30 07:54:52.894: INFO: Created: latency-svc-x89ts
    Aug 30 07:54:52.910: INFO: Got endpoints: latency-svc-x89ts [465.375435ms]
    Aug 30 07:54:52.918: INFO: Created: latency-svc-hvvd7
    Aug 30 07:54:52.965: INFO: Got endpoints: latency-svc-hvvd7 [485.608573ms]
    Aug 30 07:54:52.977: INFO: Created: latency-svc-fxcsj
    Aug 30 07:54:52.992: INFO: Got endpoints: latency-svc-fxcsj [484.906259ms]
    Aug 30 07:54:52.992: INFO: Latencies: [49.949225ms 73.929358ms 81.737327ms 82.315006ms 100.5467ms 105.463817ms 106.390838ms 115.011144ms 120.241612ms 133.787614ms 144.109104ms 151.242642ms 163.946769ms 179.428557ms 190.80393ms 203.711664ms 214.036648ms 220.087506ms 229.564575ms 231.226575ms 231.801505ms 250.867114ms 258.07179ms 293.664197ms 298.020379ms 300.892439ms 302.240374ms 308.922635ms 312.134413ms 313.075612ms 319.375742ms 324.438394ms 327.966151ms 333.64891ms 334.078206ms 340.694743ms 346.179564ms 346.489111ms 347.8135ms 351.044466ms 352.014204ms 352.82078ms 353.342895ms 355.786053ms 356.685839ms 357.596339ms 359.901183ms 361.061974ms 361.291127ms 364.107249ms 365.39331ms 388.096059ms 391.490513ms 393.580105ms 397.829793ms 414.263667ms 419.728123ms 422.448636ms 423.013374ms 423.242561ms 425.45787ms 425.466866ms 426.005963ms 426.66627ms 437.195511ms 437.550572ms 441.525483ms 443.22083ms 444.817788ms 447.13379ms 450.502687ms 450.686478ms 450.743771ms 450.903768ms 452.971476ms 452.971701ms 453.296458ms 454.171444ms 455.51878ms 456.809835ms 457.032708ms 457.799671ms 457.867737ms 458.215577ms 458.573734ms 459.314263ms 459.397701ms 459.669423ms 460.248846ms 460.357834ms 464.612865ms 464.859172ms 465.375435ms 466.226191ms 467.431238ms 470.107962ms 470.399141ms 470.972746ms 472.289484ms 473.730615ms 474.162695ms 474.736846ms 474.89953ms 475.68893ms 476.854356ms 477.714138ms 481.522666ms 481.699722ms 481.78126ms 482.396219ms 484.500865ms 484.660366ms 484.906259ms 485.608573ms 486.603649ms 488.214303ms 489.944239ms 490.012441ms 490.638485ms 492.083053ms 492.423363ms 493.464229ms 493.884397ms 494.457441ms 494.733448ms 496.718703ms 496.902509ms 498.133975ms 498.772897ms 499.299983ms 499.538958ms 500.572156ms 503.762615ms 504.053286ms 505.921161ms 507.408128ms 508.175995ms 508.210447ms 508.702072ms 511.09347ms 511.601805ms 512.67405ms 512.707527ms 520.241428ms 521.88877ms 523.867067ms 535.238118ms 535.295703ms 536.157647ms 536.273997ms 536.969579ms 540.655139ms 541.124979ms 544.597458ms 549.344097ms 555.239421ms 558.121781ms 559.538281ms 562.503304ms 563.270476ms 570.602731ms 570.983127ms 571.030147ms 571.887889ms 573.961368ms 575.552459ms 579.555699ms 582.842383ms 583.305469ms 583.826031ms 594.30534ms 594.498329ms 594.684964ms 602.80077ms 604.567821ms 609.615969ms 616.426536ms 630.34214ms 632.976399ms 635.866004ms 639.357659ms 640.059387ms 640.191527ms 641.907347ms 645.215484ms 648.392665ms 649.567054ms 651.013036ms 651.650719ms 651.98874ms 653.026771ms 655.034358ms 663.899141ms 687.465512ms 692.504386ms 700.628446ms 704.854348ms 716.104504ms 729.167101ms 754.000082ms]
    Aug 30 07:54:52.992: INFO: 50 %ile: 474.162695ms
    Aug 30 07:54:52.992: INFO: 90 %ile: 639.357659ms
    Aug 30 07:54:52.992: INFO: 99 %ile: 729.167101ms
    Aug 30 07:54:52.992: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:54:52.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7156" for this suite. 08/30/23 07:54:53.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:54:53.029
Aug 30 07:54:53.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename endpointslice 08/30/23 07:54:53.03
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:53.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:53.145
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
W0830 07:54:53.240465      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: referencing a single matching pod 08/30/23 07:54:58.63
STEP: referencing matching pods with named port 08/30/23 07:55:03.666
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/30/23 07:55:08.726
STEP: recreating EndpointSlices after they've been deleted 08/30/23 07:55:13.76
Aug 30 07:55:13.828: INFO: EndpointSlice for Service endpointslice-8698/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:23.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8698" for this suite. 08/30/23 07:55:23.869
------------------------------
• [SLOW TEST] [30.866 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:54:53.029
    Aug 30 07:54:53.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename endpointslice 08/30/23 07:54:53.03
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:54:53.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:54:53.145
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    W0830 07:54:53.240465      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "container1" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "container1" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "container1" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "container1" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: referencing a single matching pod 08/30/23 07:54:58.63
    STEP: referencing matching pods with named port 08/30/23 07:55:03.666
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/30/23 07:55:08.726
    STEP: recreating EndpointSlices after they've been deleted 08/30/23 07:55:13.76
    Aug 30 07:55:13.828: INFO: EndpointSlice for Service endpointslice-8698/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:23.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8698" for this suite. 08/30/23 07:55:23.869
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:23.896
Aug 30 07:55:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:55:23.897
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:24.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:24.077
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-9ad2cc30-cd39-4d5a-88f4-906b106d464b 08/30/23 07:55:24.088
STEP: Creating a pod to test consume configMaps 08/30/23 07:55:24.108
Aug 30 07:55:24.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba" in namespace "configmap-7243" to be "Succeeded or Failed"
Aug 30 07:55:24.164: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385728ms
Aug 30 07:55:26.194: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050244214s
Aug 30 07:55:28.179: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034932325s
STEP: Saw pod success 08/30/23 07:55:28.179
Aug 30 07:55:28.186: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba" satisfied condition "Succeeded or Failed"
Aug 30 07:55:28.198: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba container agnhost-container: <nil>
STEP: delete the pod 08/30/23 07:55:28.219
Aug 30 07:55:28.248: INFO: Waiting for pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba to disappear
Aug 30 07:55:28.260: INFO: Pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:28.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7243" for this suite. 08/30/23 07:55:28.272
------------------------------
• [4.399 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:23.896
    Aug 30 07:55:23.896: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:55:23.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:24.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:24.077
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-9ad2cc30-cd39-4d5a-88f4-906b106d464b 08/30/23 07:55:24.088
    STEP: Creating a pod to test consume configMaps 08/30/23 07:55:24.108
    Aug 30 07:55:24.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba" in namespace "configmap-7243" to be "Succeeded or Failed"
    Aug 30 07:55:24.164: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385728ms
    Aug 30 07:55:26.194: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050244214s
    Aug 30 07:55:28.179: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034932325s
    STEP: Saw pod success 08/30/23 07:55:28.179
    Aug 30 07:55:28.186: INFO: Pod "pod-configmaps-0715da24-87f7-486f-91be-1440581218ba" satisfied condition "Succeeded or Failed"
    Aug 30 07:55:28.198: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba container agnhost-container: <nil>
    STEP: delete the pod 08/30/23 07:55:28.219
    Aug 30 07:55:28.248: INFO: Waiting for pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba to disappear
    Aug 30 07:55:28.260: INFO: Pod pod-configmaps-0715da24-87f7-486f-91be-1440581218ba no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:28.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7243" for this suite. 08/30/23 07:55:28.272
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:28.297
Aug 30 07:55:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename configmap 08/30/23 07:55:28.298
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:28.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:28.354
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-1632/configmap-test-406083db-265e-47d4-8898-30f122f4bcb3 08/30/23 07:55:28.363
STEP: Creating a pod to test consume configMaps 08/30/23 07:55:28.389
Aug 30 07:55:28.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384" in namespace "configmap-1632" to be "Succeeded or Failed"
Aug 30 07:55:28.450: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Pending", Reason="", readiness=false. Elapsed: 31.876512ms
Aug 30 07:55:30.462: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043715617s
Aug 30 07:55:32.463: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044346127s
STEP: Saw pod success 08/30/23 07:55:32.463
Aug 30 07:55:32.463: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384" satisfied condition "Succeeded or Failed"
Aug 30 07:55:32.477: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 container env-test: <nil>
STEP: delete the pod 08/30/23 07:55:32.498
Aug 30 07:55:32.540: INFO: Waiting for pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 to disappear
Aug 30 07:55:32.552: INFO: Pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:32.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1632" for this suite. 08/30/23 07:55:32.565
------------------------------
• [4.291 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:28.297
    Aug 30 07:55:28.297: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename configmap 08/30/23 07:55:28.298
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:28.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:28.354
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-1632/configmap-test-406083db-265e-47d4-8898-30f122f4bcb3 08/30/23 07:55:28.363
    STEP: Creating a pod to test consume configMaps 08/30/23 07:55:28.389
    Aug 30 07:55:28.418: INFO: Waiting up to 5m0s for pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384" in namespace "configmap-1632" to be "Succeeded or Failed"
    Aug 30 07:55:28.450: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Pending", Reason="", readiness=false. Elapsed: 31.876512ms
    Aug 30 07:55:30.462: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043715617s
    Aug 30 07:55:32.463: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044346127s
    STEP: Saw pod success 08/30/23 07:55:32.463
    Aug 30 07:55:32.463: INFO: Pod "pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384" satisfied condition "Succeeded or Failed"
    Aug 30 07:55:32.477: INFO: Trying to get logs from node 10.135.139.190 pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 container env-test: <nil>
    STEP: delete the pod 08/30/23 07:55:32.498
    Aug 30 07:55:32.540: INFO: Waiting for pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 to disappear
    Aug 30 07:55:32.552: INFO: Pod pod-configmaps-8e6df644-f6a6-4c35-ae66-b0b73b509384 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:32.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1632" for this suite. 08/30/23 07:55:32.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:32.593
Aug 30 07:55:32.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 07:55:32.595
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:32.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:32.671
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/30/23 07:55:32.681
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:32.741
STEP: Creating a pod in the namespace 08/30/23 07:55:32.75
STEP: Waiting for the pod to have running status 08/30/23 07:55:33.778
Aug 30 07:55:33.778: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-795" to be "running"
Aug 30 07:55:33.793: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.547203ms
Aug 30 07:55:35.806: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027733085s
Aug 30 07:55:37.810: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032253091s
Aug 30 07:55:37.810: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/30/23 07:55:37.811
STEP: Waiting for the namespace to be removed. 08/30/23 07:55:37.863
STEP: Recreating the namespace 08/30/23 07:55:51.876
STEP: Verifying there are no pods in the namespace 08/30/23 07:55:51.928
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5463" for this suite. 08/30/23 07:55:51.961
STEP: Destroying namespace "nsdeletetest-795" for this suite. 08/30/23 07:55:51.99
Aug 30 07:55:52.010: INFO: Namespace nsdeletetest-795 was already deleted
STEP: Destroying namespace "nsdeletetest-8774" for this suite. 08/30/23 07:55:52.01
------------------------------
• [SLOW TEST] [19.453 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:32.593
    Aug 30 07:55:32.593: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 07:55:32.595
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:32.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:32.671
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/30/23 07:55:32.681
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:32.741
    STEP: Creating a pod in the namespace 08/30/23 07:55:32.75
    STEP: Waiting for the pod to have running status 08/30/23 07:55:33.778
    Aug 30 07:55:33.778: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-795" to be "running"
    Aug 30 07:55:33.793: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.547203ms
    Aug 30 07:55:35.806: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027733085s
    Aug 30 07:55:37.810: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.032253091s
    Aug 30 07:55:37.810: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/30/23 07:55:37.811
    STEP: Waiting for the namespace to be removed. 08/30/23 07:55:37.863
    STEP: Recreating the namespace 08/30/23 07:55:51.876
    STEP: Verifying there are no pods in the namespace 08/30/23 07:55:51.928
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:51.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5463" for this suite. 08/30/23 07:55:51.961
    STEP: Destroying namespace "nsdeletetest-795" for this suite. 08/30/23 07:55:51.99
    Aug 30 07:55:52.010: INFO: Namespace nsdeletetest-795 was already deleted
    STEP: Destroying namespace "nsdeletetest-8774" for this suite. 08/30/23 07:55:52.01
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:52.046
Aug 30 07:55:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:55:52.047
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:52.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:52.102
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:52.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4695" for this suite. 08/30/23 07:55:52.142
------------------------------
• [0.125 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:52.046
    Aug 30 07:55:52.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:55:52.047
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:52.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:52.102
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:52.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4695" for this suite. 08/30/23 07:55:52.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:52.173
Aug 30 07:55:52.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename kubectl 08/30/23 07:55:52.174
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:52.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:52.241
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 30 07:55:52.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 create -f -'
Aug 30 07:55:52.966: INFO: stderr: ""
Aug 30 07:55:52.966: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 30 07:55:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 create -f -'
Aug 30 07:55:53.610: INFO: stderr: ""
Aug 30 07:55:53.610: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/30/23 07:55:53.61
Aug 30 07:55:54.622: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:55:54.622: INFO: Found 0 / 1
Aug 30 07:55:55.624: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:55:55.624: INFO: Found 1 / 1
Aug 30 07:55:55.624: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 30 07:55:55.638: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 30 07:55:55.638: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 30 07:55:55.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe pod agnhost-primary-v95sf'
Aug 30 07:55:55.762: INFO: stderr: ""
Aug 30 07:55:55.762: INFO: stdout: "Name:             agnhost-primary-v95sf\nNamespace:        kubectl-7102\nPriority:         0\nService Account:  default\nNode:             10.135.139.190/10.135.139.190\nStart Time:       Wed, 30 Aug 2023 07:55:53 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d0882b7f23a2b9515854498f53be4273ed12e5079efe6299314bd7780f2eee37\n                  cni.projectcalico.org/podIP: 172.30.58.81/32\n                  cni.projectcalico.org/podIPs: 172.30.58.81/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.58.81\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.58.81\nIPs:\n  IP:           172.30.58.81\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://4138abfafbead3facd2373666da82b7af5660e66af6944df62b808cd2c215219\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 30 Aug 2023 07:55:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m8b7x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-m8b7x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-7102/agnhost-primary-v95sf to 10.135.139.190\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.58.81/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Aug 30 07:55:55.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe rc agnhost-primary'
Aug 30 07:55:55.871: INFO: stderr: ""
Aug 30 07:55:55.871: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7102\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-v95sf\n"
Aug 30 07:55:55.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe service agnhost-primary'
Aug 30 07:55:55.993: INFO: stderr: ""
Aug 30 07:55:55.993: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7102\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.70.173\nIPs:               172.21.70.173\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.58.81:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 30 07:55:56.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe node 10.135.139.183'
Aug 30 07:55:56.384: INFO: stderr: ""
Aug 30 07:55:56.384: INFO: stdout: "Name:               10.135.139.183\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=159.122.81.165\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.135.139.183\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cjnbd95f0l0i88o3pfn0-kubee2epvg0-default-000003ed\n                    ibm-cloud.kubernetes.io/worker-pool-id=cjnbd95f0l0i88o3pfn0-f4c6e9d\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.13.9_1533_openshift\n                    ibm-cloud.kubernetes.io/zone=fra02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.135.139.183\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722990\n                    publicVLAN=2722976\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra02\nAnnotations:        projectcalico.org/IPv4Address: 10.135.139.183/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.86.128\nCreationTimestamp:  Wed, 30 Aug 2023 03:49:23 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.135.139.183\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 30 Aug 2023 07:55:52 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 30 Aug 2023 03:56:57 +0000   Wed, 30 Aug 2023 03:56:57 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:58:35 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.135.139.183\n  ExternalIP:  159.122.81.165\n  Hostname:    10.135.139.183\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16382400Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13594048Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                       554dd72eaee5496898a30179982b0bb5\n  System UUID:                                      52a60e82-a549-9511-b138-b3a8ec81c097\n  Boot ID:                                          1e0c876e-05ba-4e20-9449-b7aeb2db6cd9\n  Kernel Version:                                   4.18.0-477.21.1.el8_8.x86_64\n  OS Image:                                         Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.26.4-3.rhaos4.13.git615a02c.el8\n  Kubelet Version:                                  v1.26.6+6bf3f75\n  Kube-Proxy Version:                               v1.26.6+6bf3f75\nPodCIDR:                                            172.30.2.0/24\nPodCIDRs:                                           172.30.2.0/24\nProviderID:                                         ibm://fee034388aa6435883a1f720010ab3a2///cjnbd95f0l0i88o3pfn0/kube-cjnbd95f0l0i88o3pfn0-kubee2epvg0-default-000003ed\nNon-terminated Pods:                                (58 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-kube-controllers-8c94dd78-pv85v                     10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      3h59m\n  calico-system                                     calico-node-rkdbq                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h59m\n  calico-system                                     calico-typha-6668d4cdd9-hl2qp                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h59m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-687wz       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-fqb6w       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-vxc59       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-system                                        ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         87m\n  kube-system                                       ibm-file-plugin-77c56989c6-nkjrh                           50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     4h16m\n  kube-system                                       ibm-keepalived-watcher-j9sbd                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h6m\n  kube-system                                       ibm-master-proxy-static-10.135.139.183                     26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h5m\n  kube-system                                       ibm-storage-metrics-agent-66b6778cfb-7cpg6                 60m (1%)      540m (13%)  120Mi (0%)       580Mi (4%)     4h12m\n  kube-system                                       ibm-storage-watcher-569f8b7c46-vxtjw                       50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     4h16m\n  kube-system                                       ibmcloud-block-storage-driver-xtxbl                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h6m\n  kube-system                                       ibmcloud-block-storage-plugin-7556db7ff5-s5xzr             50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h16m\n  kube-system                                       vpn-5cf898c745-hk9nt                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h57m\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-6f7b6884b9-2qg9l              10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-node-tuning-operator            tuned-j5t4h                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h53m\n  openshift-cluster-samples-operator                cluster-samples-operator-5db6d764c6-9s952                  20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         4h2m\n  openshift-cluster-storage-operator                cluster-storage-operator-848968879c-br4hq                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-storage-operator                csi-snapshot-controller-b5685b8b7-s7d7c                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         87m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-85b4b8fdc8-htd5f          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-storage-operator                csi-snapshot-webhook-645cd76dd7-bhc7s                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         87m\n  openshift-console-operator                        console-operator-77698fb45f-7tq8z                          20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         4h2m\n  openshift-console                                 console-76ccb968f7-h5h2q                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         87m\n  openshift-console                                 downloads-55ff47758f-p9bfz                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         87m\n  openshift-dns-operator                            dns-operator-54bdb67d9f-8cjg2                              20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         4h2m\n  openshift-dns                                     dns-default-5mmgg                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         3h53m\n  openshift-dns                                     node-resolver-b2rmk                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h53m\n  openshift-image-registry                          cluster-image-registry-operator-77f67cc94-8p5p5            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-image-registry                          node-ca-kjt5c                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h53m\n  openshift-ingress-canary                          ingress-canary-jfxbv                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h53m\n  openshift-ingress-operator                        ingress-operator-cb44b8bc7-2rjr2                           20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         4h2m\n  openshift-ingress                                 router-default-9c97f6b97-tkkf8                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         87m\n  openshift-insights                                insights-operator-7f9b7d96b4-9cv5s                         10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         4h2m\n  openshift-kube-proxy                              openshift-kube-proxy-5hwpc                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h1m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-854564dc54-mj7zl    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-marketplace                             marketplace-operator-dcc4b747b-4bcck                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                              alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         87m\n  openshift-monitoring                              cluster-monitoring-operator-7bc996789-qqt52                10m (0%)      0 (0%)      75Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                              node-exporter-9pzcl                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h52m\n  openshift-monitoring                              prometheus-adapter-5d4fdc4794-gct89                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         87m\n  openshift-monitoring                              prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         87m\n  openshift-monitoring                              prometheus-operator-admission-webhook-748bd6896b-8b8tk     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         87m\n  openshift-monitoring                              thanos-querier-7bd6db4456-jqzhk                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         87m\n  openshift-multus                                  multus-additional-cni-plugins-fd7l2                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h1m\n  openshift-multus                                  multus-admission-controller-68f648d7b7-65hkj               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         87m\n  openshift-multus                                  multus-vllh7                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h1m\n  openshift-multus                                  network-metrics-daemon-lk2w7                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h1m\n  openshift-network-diagnostics                     network-check-source-8dd86ffb8-k4c5v                       10m (0%)      0 (0%)      40Mi (0%)        0 (0%)         4h1m\n  openshift-network-diagnostics                     network-check-target-zvrg2                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h1m\n  openshift-operator-lifecycle-manager              catalog-operator-56d6f4596f-fzjbx                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              olm-operator-79d7d96656-gs9jc                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              package-server-manager-54dcf5867b-z6ksr                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              packageserver-56d55d9ff4-g9wz7                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h53m\n  openshift-roks-metrics                            metrics-7d985d4645-ntm6t                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h19m\n  openshift-roks-metrics                            push-gateway-86f4464769-8nhh2                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h19m\n  openshift-service-ca-operator                     service-ca-operator-fdbd6d689-hvb8m                        10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1610m (41%)      4340m (110%)\n  memory             4910611Ki (36%)  6799648Ki (50%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:              <none>\n"
Aug 30 07:55:56.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe namespace kubectl-7102'
Aug 30 07:55:56.506: INFO: stderr: ""
Aug 30 07:55:56.506: INFO: stdout: "Name:         kubectl-7102\nLabels:       e2e-framework=kubectl\n              e2e-run=8bbbd299-392d-471f-98f9-905027cf33d5\n              kubernetes.io/metadata.name=kubectl-7102\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c69,c29\n              openshift.io/sa.scc.supplemental-groups: 1004750000/10000\n              openshift.io/sa.scc.uid-range: 1004750000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 30 07:55:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7102" for this suite. 08/30/23 07:55:56.519
------------------------------
• [4.371 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:52.173
    Aug 30 07:55:52.173: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename kubectl 08/30/23 07:55:52.174
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:52.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:52.241
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 30 07:55:52.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 create -f -'
    Aug 30 07:55:52.966: INFO: stderr: ""
    Aug 30 07:55:52.966: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 30 07:55:52.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 create -f -'
    Aug 30 07:55:53.610: INFO: stderr: ""
    Aug 30 07:55:53.610: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/30/23 07:55:53.61
    Aug 30 07:55:54.622: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:55:54.622: INFO: Found 0 / 1
    Aug 30 07:55:55.624: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:55:55.624: INFO: Found 1 / 1
    Aug 30 07:55:55.624: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 30 07:55:55.638: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 30 07:55:55.638: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 30 07:55:55.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe pod agnhost-primary-v95sf'
    Aug 30 07:55:55.762: INFO: stderr: ""
    Aug 30 07:55:55.762: INFO: stdout: "Name:             agnhost-primary-v95sf\nNamespace:        kubectl-7102\nPriority:         0\nService Account:  default\nNode:             10.135.139.190/10.135.139.190\nStart Time:       Wed, 30 Aug 2023 07:55:53 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: d0882b7f23a2b9515854498f53be4273ed12e5079efe6299314bd7780f2eee37\n                  cni.projectcalico.org/podIP: 172.30.58.81/32\n                  cni.projectcalico.org/podIPs: 172.30.58.81/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.58.81\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.58.81\nIPs:\n  IP:           172.30.58.81\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://4138abfafbead3facd2373666da82b7af5660e66af6944df62b808cd2c215219\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 30 Aug 2023 07:55:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m8b7x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-m8b7x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-7102/agnhost-primary-v95sf to 10.135.139.190\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.58.81/32] from k8s-pod-network\n  Normal  Pulled          1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Aug 30 07:55:55.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe rc agnhost-primary'
    Aug 30 07:55:55.871: INFO: stderr: ""
    Aug 30 07:55:55.871: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7102\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-v95sf\n"
    Aug 30 07:55:55.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe service agnhost-primary'
    Aug 30 07:55:55.993: INFO: stderr: ""
    Aug 30 07:55:55.993: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7102\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.70.173\nIPs:               172.21.70.173\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.58.81:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 30 07:55:56.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe node 10.135.139.183'
    Aug 30 07:55:56.384: INFO: stderr: ""
    Aug 30 07:55:56.384: INFO: stdout: "Name:               10.135.139.183\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-de\n                    failure-domain.beta.kubernetes.io/zone=fra02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=159.122.81.165\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.135.139.183\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-de\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cjnbd95f0l0i88o3pfn0-kubee2epvg0-default-000003ed\n                    ibm-cloud.kubernetes.io/worker-pool-id=cjnbd95f0l0i88o3pfn0-f4c6e9d\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.13.9_1533_openshift\n                    ibm-cloud.kubernetes.io/zone=fra02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.135.139.183\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2722990\n                    publicVLAN=2722976\n                    topology.kubernetes.io/region=eu-de\n                    topology.kubernetes.io/zone=fra02\nAnnotations:        projectcalico.org/IPv4Address: 10.135.139.183/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.86.128\nCreationTimestamp:  Wed, 30 Aug 2023 03:49:23 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.135.139.183\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 30 Aug 2023 07:55:52 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 30 Aug 2023 03:56:57 +0000   Wed, 30 Aug 2023 03:56:57 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:49:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 30 Aug 2023 07:51:15 +0000   Wed, 30 Aug 2023 03:58:35 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.135.139.183\n  ExternalIP:  159.122.81.165\n  Hostname:    10.135.139.183\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16382400Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13594048Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                       554dd72eaee5496898a30179982b0bb5\n  System UUID:                                      52a60e82-a549-9511-b138-b3a8ec81c097\n  Boot ID:                                          1e0c876e-05ba-4e20-9449-b7aeb2db6cd9\n  Kernel Version:                                   4.18.0-477.21.1.el8_8.x86_64\n  OS Image:                                         Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                                 linux\n  Architecture:                                     amd64\n  Container Runtime Version:                        cri-o://1.26.4-3.rhaos4.13.git615a02c.el8\n  Kubelet Version:                                  v1.26.6+6bf3f75\n  Kube-Proxy Version:                               v1.26.6+6bf3f75\nPodCIDR:                                            172.30.2.0/24\nPodCIDRs:                                           172.30.2.0/24\nProviderID:                                         ibm://fee034388aa6435883a1f720010ab3a2///cjnbd95f0l0i88o3pfn0/kube-cjnbd95f0l0i88o3pfn0-kubee2epvg0-default-000003ed\nNon-terminated Pods:                                (58 in total)\n  Namespace                                         Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                         ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                                     calico-kube-controllers-8c94dd78-pv85v                     10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      3h59m\n  calico-system                                     calico-node-rkdbq                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h59m\n  calico-system                                     calico-typha-6668d4cdd9-hl2qp                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h59m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-687wz       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-fqb6w       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-odf-validation-webhook                        managed-storage-validation-webhooks-5445c9f55f-vxc59       10m (0%)      500m (12%)  10Mi (0%)        500Mi (3%)     4h12m\n  ibm-system                                        ibm-cloud-provider-ip-159-122-65-122-f7869cc9f-sdcxc       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         87m\n  kube-system                                       ibm-file-plugin-77c56989c6-nkjrh                           50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     4h16m\n  kube-system                                       ibm-keepalived-watcher-j9sbd                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h6m\n  kube-system                                       ibm-master-proxy-static-10.135.139.183                     26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h5m\n  kube-system                                       ibm-storage-metrics-agent-66b6778cfb-7cpg6                 60m (1%)      540m (13%)  120Mi (0%)       580Mi (4%)     4h12m\n  kube-system                                       ibm-storage-watcher-569f8b7c46-vxtjw                       50m (1%)      500m (12%)  100Mi (0%)       200Mi (1%)     4h16m\n  kube-system                                       ibmcloud-block-storage-driver-xtxbl                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h6m\n  kube-system                                       ibmcloud-block-storage-plugin-7556db7ff5-s5xzr             50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h16m\n  kube-system                                       vpn-5cf898c745-hk9nt                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h57m\n  openshift-cluster-node-tuning-operator            cluster-node-tuning-operator-6f7b6884b9-2qg9l              10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-node-tuning-operator            tuned-j5t4h                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h53m\n  openshift-cluster-samples-operator                cluster-samples-operator-5db6d764c6-9s952                  20m (0%)      0 (0%)      100Mi (0%)       0 (0%)         4h2m\n  openshift-cluster-storage-operator                cluster-storage-operator-848968879c-br4hq                  10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-storage-operator                csi-snapshot-controller-b5685b8b7-s7d7c                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         87m\n  openshift-cluster-storage-operator                csi-snapshot-controller-operator-85b4b8fdc8-htd5f          10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h2m\n  openshift-cluster-storage-operator                csi-snapshot-webhook-645cd76dd7-bhc7s                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         87m\n  openshift-console-operator                        console-operator-77698fb45f-7tq8z                          20m (0%)      0 (0%)      200Mi (1%)       0 (0%)         4h2m\n  openshift-console                                 console-76ccb968f7-h5h2q                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         87m\n  openshift-console                                 downloads-55ff47758f-p9bfz                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         87m\n  openshift-dns-operator                            dns-operator-54bdb67d9f-8cjg2                              20m (0%)      0 (0%)      69Mi (0%)        0 (0%)         4h2m\n  openshift-dns                                     dns-default-5mmgg                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         3h53m\n  openshift-dns                                     node-resolver-b2rmk                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h53m\n  openshift-image-registry                          cluster-image-registry-operator-77f67cc94-8p5p5            10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-image-registry                          node-ca-kjt5c                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h53m\n  openshift-ingress-canary                          ingress-canary-jfxbv                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h53m\n  openshift-ingress-operator                        ingress-operator-cb44b8bc7-2rjr2                           20m (0%)      0 (0%)      96Mi (0%)        0 (0%)         4h2m\n  openshift-ingress                                 router-default-9c97f6b97-tkkf8                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         87m\n  openshift-insights                                insights-operator-7f9b7d96b4-9cv5s                         10m (0%)      0 (0%)      30Mi (0%)        0 (0%)         4h2m\n  openshift-kube-proxy                              openshift-kube-proxy-5hwpc                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h1m\n  openshift-kube-storage-version-migrator-operator  kube-storage-version-migrator-operator-854564dc54-mj7zl    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-marketplace                             marketplace-operator-dcc4b747b-4bcck                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                              alertmanager-main-1                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         87m\n  openshift-monitoring                              cluster-monitoring-operator-7bc996789-qqt52                10m (0%)      0 (0%)      75Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                              node-exporter-9pzcl                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h52m\n  openshift-monitoring                              prometheus-adapter-5d4fdc4794-gct89                        1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         87m\n  openshift-monitoring                              prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         87m\n  openshift-monitoring                              prometheus-operator-admission-webhook-748bd6896b-8b8tk     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         87m\n  openshift-monitoring                              thanos-querier-7bd6db4456-jqzhk                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         87m\n  openshift-multus                                  multus-additional-cni-plugins-fd7l2                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h1m\n  openshift-multus                                  multus-admission-controller-68f648d7b7-65hkj               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         87m\n  openshift-multus                                  multus-vllh7                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h1m\n  openshift-multus                                  network-metrics-daemon-lk2w7                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h1m\n  openshift-network-diagnostics                     network-check-source-8dd86ffb8-k4c5v                       10m (0%)      0 (0%)      40Mi (0%)        0 (0%)         4h1m\n  openshift-network-diagnostics                     network-check-target-zvrg2                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h1m\n  openshift-operator-lifecycle-manager              catalog-operator-56d6f4596f-fzjbx                          10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              olm-operator-79d7d96656-gs9jc                              10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              package-server-manager-54dcf5867b-z6ksr                    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h2m\n  openshift-operator-lifecycle-manager              packageserver-56d55d9ff4-g9wz7                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h53m\n  openshift-roks-metrics                            metrics-7d985d4645-ntm6t                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h19m\n  openshift-roks-metrics                            push-gateway-86f4464769-8nhh2                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h19m\n  openshift-service-ca-operator                     service-ca-operator-fdbd6d689-hvb8m                        10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         4h2m\n  sonobuoy                                          sonobuoy-systemd-logs-daemon-set-1ef6b4af6127401a-hc9l2    0 (0%)        0 (0%)      0 (0%)           0 (0%)         99m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1610m (41%)      4340m (110%)\n  memory             4910611Ki (36%)  6799648Ki (50%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:              <none>\n"
    Aug 30 07:55:56.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=kubectl-7102 describe namespace kubectl-7102'
    Aug 30 07:55:56.506: INFO: stderr: ""
    Aug 30 07:55:56.506: INFO: stdout: "Name:         kubectl-7102\nLabels:       e2e-framework=kubectl\n              e2e-run=8bbbd299-392d-471f-98f9-905027cf33d5\n              kubernetes.io/metadata.name=kubectl-7102\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c69,c29\n              openshift.io/sa.scc.supplemental-groups: 1004750000/10000\n              openshift.io/sa.scc.uid-range: 1004750000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:55:56.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7102" for this suite. 08/30/23 07:55:56.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:55:56.544
Aug 30 07:55:56.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename secrets 08/30/23 07:55:56.545
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:56.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:56.701
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-f1990667-8a45-4daf-a429-afbee4ea4ff2 08/30/23 07:55:56.71
STEP: Creating a pod to test consume secrets 08/30/23 07:55:56.734
Aug 30 07:55:56.773: INFO: Waiting up to 5m0s for pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7" in namespace "secrets-3132" to be "Succeeded or Failed"
Aug 30 07:55:56.792: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.873532ms
Aug 30 07:55:58.831: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05788857s
Aug 30 07:56:00.805: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031484879s
STEP: Saw pod success 08/30/23 07:56:00.805
Aug 30 07:56:00.805: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7" satisfied condition "Succeeded or Failed"
Aug 30 07:56:00.816: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 container secret-volume-test: <nil>
STEP: delete the pod 08/30/23 07:56:00.834
Aug 30 07:56:00.875: INFO: Waiting for pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 to disappear
Aug 30 07:56:00.888: INFO: Pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 30 07:56:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3132" for this suite. 08/30/23 07:56:00.9
------------------------------
• [4.378 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:55:56.544
    Aug 30 07:55:56.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename secrets 08/30/23 07:55:56.545
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:55:56.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:55:56.701
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-f1990667-8a45-4daf-a429-afbee4ea4ff2 08/30/23 07:55:56.71
    STEP: Creating a pod to test consume secrets 08/30/23 07:55:56.734
    Aug 30 07:55:56.773: INFO: Waiting up to 5m0s for pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7" in namespace "secrets-3132" to be "Succeeded or Failed"
    Aug 30 07:55:56.792: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.873532ms
    Aug 30 07:55:58.831: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05788857s
    Aug 30 07:56:00.805: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031484879s
    STEP: Saw pod success 08/30/23 07:56:00.805
    Aug 30 07:56:00.805: INFO: Pod "pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7" satisfied condition "Succeeded or Failed"
    Aug 30 07:56:00.816: INFO: Trying to get logs from node 10.135.139.190 pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 container secret-volume-test: <nil>
    STEP: delete the pod 08/30/23 07:56:00.834
    Aug 30 07:56:00.875: INFO: Waiting for pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 to disappear
    Aug 30 07:56:00.888: INFO: Pod pod-secrets-5fe5a998-2e22-4c08-9852-4ed4f71983a7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:56:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3132" for this suite. 08/30/23 07:56:00.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:56:00.924
Aug 30 07:56:00.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:56:00.925
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:00.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:00.984
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/30/23 07:56:00.995
Aug 30 07:56:00.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/30/23 07:56:19.419
Aug 30 07:56:19.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
Aug 30 07:56:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:56:49.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1763" for this suite. 08/30/23 07:56:49.039
------------------------------
• [SLOW TEST] [48.135 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:56:00.924
    Aug 30 07:56:00.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename crd-publish-openapi 08/30/23 07:56:00.925
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:00.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:00.984
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/30/23 07:56:00.995
    Aug 30 07:56:00.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/30/23 07:56:19.419
    Aug 30 07:56:19.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    Aug 30 07:56:25.901: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:56:49.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1763" for this suite. 08/30/23 07:56:49.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:56:49.06
Aug 30 07:56:49.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename services 08/30/23 07:56:49.061
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:49.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:49.133
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8165 08/30/23 07:56:49.138
STEP: changing the ExternalName service to type=ClusterIP 08/30/23 07:56:49.159
STEP: creating replication controller externalname-service in namespace services-8165 08/30/23 07:56:49.206
I0830 07:56:49.220807      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8165, replica count: 2
I0830 07:56:52.272036      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 30 07:56:52.272: INFO: Creating new exec pod
Aug 30 07:56:52.289: INFO: Waiting up to 5m0s for pod "execpod5x6hd" in namespace "services-8165" to be "running"
Aug 30 07:56:52.296: INFO: Pod "execpod5x6hd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655467ms
Aug 30 07:56:54.305: INFO: Pod "execpod5x6hd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015487225s
Aug 30 07:56:54.305: INFO: Pod "execpod5x6hd" satisfied condition "running"
Aug 30 07:56:55.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-8165 exec execpod5x6hd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 30 07:56:55.520: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 30 07:56:55.520: INFO: stdout: ""
Aug 30 07:56:55.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-8165 exec execpod5x6hd -- /bin/sh -x -c nc -v -z -w 2 172.21.59.54 80'
Aug 30 07:56:55.733: INFO: stderr: "+ nc -v -z -w 2 172.21.59.54 80\nConnection to 172.21.59.54 80 port [tcp/http] succeeded!\n"
Aug 30 07:56:55.733: INFO: stdout: ""
Aug 30 07:56:55.734: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 30 07:56:55.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8165" for this suite. 08/30/23 07:56:55.81
------------------------------
• [SLOW TEST] [6.776 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:56:49.06
    Aug 30 07:56:49.060: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename services 08/30/23 07:56:49.061
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:49.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:49.133
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8165 08/30/23 07:56:49.138
    STEP: changing the ExternalName service to type=ClusterIP 08/30/23 07:56:49.159
    STEP: creating replication controller externalname-service in namespace services-8165 08/30/23 07:56:49.206
    I0830 07:56:49.220807      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8165, replica count: 2
    I0830 07:56:52.272036      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 30 07:56:52.272: INFO: Creating new exec pod
    Aug 30 07:56:52.289: INFO: Waiting up to 5m0s for pod "execpod5x6hd" in namespace "services-8165" to be "running"
    Aug 30 07:56:52.296: INFO: Pod "execpod5x6hd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.655467ms
    Aug 30 07:56:54.305: INFO: Pod "execpod5x6hd": Phase="Running", Reason="", readiness=true. Elapsed: 2.015487225s
    Aug 30 07:56:54.305: INFO: Pod "execpod5x6hd" satisfied condition "running"
    Aug 30 07:56:55.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-8165 exec execpod5x6hd -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 30 07:56:55.520: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 30 07:56:55.520: INFO: stdout: ""
    Aug 30 07:56:55.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1239079833 --namespace=services-8165 exec execpod5x6hd -- /bin/sh -x -c nc -v -z -w 2 172.21.59.54 80'
    Aug 30 07:56:55.733: INFO: stderr: "+ nc -v -z -w 2 172.21.59.54 80\nConnection to 172.21.59.54 80 port [tcp/http] succeeded!\n"
    Aug 30 07:56:55.733: INFO: stdout: ""
    Aug 30 07:56:55.734: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:56:55.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8165" for this suite. 08/30/23 07:56:55.81
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:56:55.837
Aug 30 07:56:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename init-container 08/30/23 07:56:55.839
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:55.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:55.888
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/30/23 07:56:55.896
Aug 30 07:56:55.896: INFO: PodSpec: initContainers in spec.initContainers
Aug 30 07:57:39.452: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dd55fa66-000c-4ebd-b29e-0d45d721113b", GenerateName:"", Namespace:"init-container-2767", SelfLink:"", UID:"d69176a4-2a77-4b3a-9270-aa7b5c7efa68", ResourceVersion:"136965", Generation:0, CreationTimestamp:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"896155874"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2ffed8fcb9bfa42b832833f40742608d2c9e06b264ab6c268c8121f47829527f", "cni.projectcalico.org/podIP":"172.30.58.107/32", "cni.projectcalico.org/podIPs":"172.30.58.107/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.58.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0ea0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0ed0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0f00), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 57, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0f30), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jfv5c", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00a7e7540), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e900), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e960), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e8a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00a7f8828), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.135.139.190", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008b823f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a7f88e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a7f8900)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00a7f891c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00a7f8920), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00a7d7970), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.135.139.190", PodIP:"172.30.58.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.58.107"}}, StartTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008b824d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008b82540)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://65bf2b76c8e216e20018a8d995f4bc63af228f4ae0afaf63b2833836a6df0150", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00a7e75c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00a7e75a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00a7f8994)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 30 07:57:39.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2767" for this suite. 08/30/23 07:57:39.465
------------------------------
• [SLOW TEST] [43.646 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:56:55.837
    Aug 30 07:56:55.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename init-container 08/30/23 07:56:55.839
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:56:55.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:56:55.888
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/30/23 07:56:55.896
    Aug 30 07:56:55.896: INFO: PodSpec: initContainers in spec.initContainers
    Aug 30 07:57:39.452: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dd55fa66-000c-4ebd-b29e-0d45d721113b", GenerateName:"", Namespace:"init-container-2767", SelfLink:"", UID:"d69176a4-2a77-4b3a-9270-aa7b5c7efa68", ResourceVersion:"136965", Generation:0, CreationTimestamp:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"896155874"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"2ffed8fcb9bfa42b832833f40742608d2c9e06b264ab6c268c8121f47829527f", "cni.projectcalico.org/podIP":"172.30.58.107/32", "cni.projectcalico.org/podIPs":"172.30.58.107/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.58.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0ea0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0ed0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 56, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0f00), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 30, 7, 57, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a7d0f30), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-jfv5c", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00a7e7540), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e900), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e960), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-jfv5c", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00ad4e8a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00a7f8828), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.135.139.190", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008b823f0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a7f88e0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00a7f8900)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00a7f891c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00a7f8920), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00a7d7970), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.135.139.190", PodIP:"172.30.58.107", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.58.107"}}, StartTime:time.Date(2023, time.August, 30, 7, 56, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008b824d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008b82540)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://65bf2b76c8e216e20018a8d995f4bc63af228f4ae0afaf63b2833836a6df0150", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00a7e75c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00a7e75a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00a7f8994)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:57:39.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2767" for this suite. 08/30/23 07:57:39.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:57:39.484
Aug 30 07:57:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename projected 08/30/23 07:57:39.485
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:39.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:39.545
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
Aug 30 07:57:39.572: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ca9e31e7-af3e-41a1-9680-dd250c293aae 08/30/23 07:57:39.572
STEP: Creating the pod 08/30/23 07:57:39.593
Aug 30 07:57:39.611: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c" in namespace "projected-7485" to be "running and ready"
Aug 30 07:57:39.621: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.881439ms
Aug 30 07:57:39.621: INFO: The phase of Pod pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c is Pending, waiting for it to be Running (with Ready = true)
Aug 30 07:57:41.629: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017928294s
Aug 30 07:57:41.629: INFO: The phase of Pod pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c is Running (Ready = true)
Aug 30 07:57:41.629: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-ca9e31e7-af3e-41a1-9680-dd250c293aae 08/30/23 07:57:41.663
STEP: waiting to observe update in volume 08/30/23 07:57:41.676
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 30 07:57:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7485" for this suite. 08/30/23 07:57:43.781
------------------------------
• [4.331 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:57:39.484
    Aug 30 07:57:39.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename projected 08/30/23 07:57:39.485
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:39.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:39.545
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    Aug 30 07:57:39.572: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-ca9e31e7-af3e-41a1-9680-dd250c293aae 08/30/23 07:57:39.572
    STEP: Creating the pod 08/30/23 07:57:39.593
    Aug 30 07:57:39.611: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c" in namespace "projected-7485" to be "running and ready"
    Aug 30 07:57:39.621: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.881439ms
    Aug 30 07:57:39.621: INFO: The phase of Pod pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c is Pending, waiting for it to be Running (with Ready = true)
    Aug 30 07:57:41.629: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c": Phase="Running", Reason="", readiness=true. Elapsed: 2.017928294s
    Aug 30 07:57:41.629: INFO: The phase of Pod pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c is Running (Ready = true)
    Aug 30 07:57:41.629: INFO: Pod "pod-projected-configmaps-bc68ae98-40e8-4651-a3a0-bec5fe51537c" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-ca9e31e7-af3e-41a1-9680-dd250c293aae 08/30/23 07:57:41.663
    STEP: waiting to observe update in volume 08/30/23 07:57:41.676
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:57:43.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7485" for this suite. 08/30/23 07:57:43.781
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:57:43.815
Aug 30 07:57:43.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 07:57:43.816
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:43.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:43.916
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/30/23 07:57:43.953
STEP: Ensuring job reaches completions 08/30/23 07:57:43.976
STEP: Ensuring pods with index for job exist 08/30/23 07:57:53.987
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 07:57:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-901" for this suite. 08/30/23 07:57:54.01
------------------------------
• [SLOW TEST] [10.212 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:57:43.815
    Aug 30 07:57:43.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 07:57:43.816
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:43.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:43.916
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/30/23 07:57:43.953
    STEP: Ensuring job reaches completions 08/30/23 07:57:43.976
    STEP: Ensuring pods with index for job exist 08/30/23 07:57:53.987
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 07:57:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-901" for this suite. 08/30/23 07:57:54.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 07:57:54.028
Aug 30 07:57:54.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename container-probe 08/30/23 07:57:54.029
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:54.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:54.084
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 in namespace container-probe-1288 08/30/23 07:57:54.089
W0830 07:57:54.109268      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 07:57:54.109: INFO: Waiting up to 5m0s for pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1" in namespace "container-probe-1288" to be "not pending"
Aug 30 07:57:54.124: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.862457ms
Aug 30 07:57:56.131: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022151259s
Aug 30 07:57:58.134: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Running", Reason="", readiness=true. Elapsed: 4.025539729s
Aug 30 07:57:58.134: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1" satisfied condition "not pending"
Aug 30 07:57:58.134: INFO: Started pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 in namespace container-probe-1288
STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 07:57:58.134
Aug 30 07:57:58.141: INFO: Initial restart count of pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 is 0
STEP: deleting the pod 08/30/23 08:01:59.403
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 30 08:01:59.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1288" for this suite. 08/30/23 08:01:59.476
------------------------------
• [SLOW TEST] [245.466 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 07:57:54.028
    Aug 30 07:57:54.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename container-probe 08/30/23 07:57:54.029
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 07:57:54.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 07:57:54.084
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 in namespace container-probe-1288 08/30/23 07:57:54.089
    W0830 07:57:54.109268      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 07:57:54.109: INFO: Waiting up to 5m0s for pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1" in namespace "container-probe-1288" to be "not pending"
    Aug 30 07:57:54.124: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.862457ms
    Aug 30 07:57:56.131: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022151259s
    Aug 30 07:57:58.134: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1": Phase="Running", Reason="", readiness=true. Elapsed: 4.025539729s
    Aug 30 07:57:58.134: INFO: Pod "busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1" satisfied condition "not pending"
    Aug 30 07:57:58.134: INFO: Started pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 in namespace container-probe-1288
    STEP: checking the pod's current state and verifying that restartCount is present 08/30/23 07:57:58.134
    Aug 30 07:57:58.141: INFO: Initial restart count of pod busybox-411dc1bb-caea-4404-9bc5-ed79549c3ad1 is 0
    STEP: deleting the pod 08/30/23 08:01:59.403
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 30 08:01:59.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1288" for this suite. 08/30/23 08:01:59.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 08:01:59.509
Aug 30 08:01:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename statefulset 08/30/23 08:01:59.51
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:01:59.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:01:59.563
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2420 08/30/23 08:01:59.57
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2420 08/30/23 08:01:59.583
W0830 08:01:59.614458      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Aug 30 08:01:59.637: INFO: Found 0 stateful pods, waiting for 1
Aug 30 08:02:09.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/30/23 08:02:09.67
STEP: updating a scale subresource 08/30/23 08:02:09.686
STEP: verifying the statefulset Spec.Replicas was modified 08/30/23 08:02:09.727
STEP: Patch a scale subresource 08/30/23 08:02:09.77
STEP: verifying the statefulset Spec.Replicas was modified 08/30/23 08:02:09.833
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 30 08:02:09.857: INFO: Deleting all statefulset in ns statefulset-2420
Aug 30 08:02:09.867: INFO: Scaling statefulset ss to 0
Aug 30 08:02:19.909: INFO: Waiting for statefulset status.replicas updated to 0
Aug 30 08:02:19.918: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 30 08:02:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2420" for this suite. 08/30/23 08:02:20
------------------------------
• [SLOW TEST] [20.510 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 08:01:59.509
    Aug 30 08:01:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename statefulset 08/30/23 08:01:59.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:01:59.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:01:59.563
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2420 08/30/23 08:01:59.57
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2420 08/30/23 08:01:59.583
    W0830 08:01:59.614458      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Aug 30 08:01:59.637: INFO: Found 0 stateful pods, waiting for 1
    Aug 30 08:02:09.646: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/30/23 08:02:09.67
    STEP: updating a scale subresource 08/30/23 08:02:09.686
    STEP: verifying the statefulset Spec.Replicas was modified 08/30/23 08:02:09.727
    STEP: Patch a scale subresource 08/30/23 08:02:09.77
    STEP: verifying the statefulset Spec.Replicas was modified 08/30/23 08:02:09.833
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 30 08:02:09.857: INFO: Deleting all statefulset in ns statefulset-2420
    Aug 30 08:02:09.867: INFO: Scaling statefulset ss to 0
    Aug 30 08:02:19.909: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 30 08:02:19.918: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 30 08:02:19.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2420" for this suite. 08/30/23 08:02:20
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 08:02:20.02
Aug 30 08:02:20.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename namespaces 08/30/23 08:02:20.021
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:02:20.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:02:20.082
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-3730" 08/30/23 08:02:20.088
Aug 30 08:02:20.136: INFO: Namespace "namespaces-3730" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bbbd299-392d-471f-98f9-905027cf33d5", "kubernetes.io/metadata.name":"namespaces-3730", "namespaces-3730":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 30 08:02:20.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3730" for this suite. 08/30/23 08:02:20.163
------------------------------
• [0.164 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 08:02:20.02
    Aug 30 08:02:20.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename namespaces 08/30/23 08:02:20.021
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:02:20.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:02:20.082
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-3730" 08/30/23 08:02:20.088
    Aug 30 08:02:20.136: INFO: Namespace "namespaces-3730" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bbbd299-392d-471f-98f9-905027cf33d5", "kubernetes.io/metadata.name":"namespaces-3730", "namespaces-3730":"updated", "pod-security.kubernetes.io/audit":"privileged", "pod-security.kubernetes.io/audit-version":"v1.24", "pod-security.kubernetes.io/enforce":"baseline", "pod-security.kubernetes.io/warn":"privileged", "pod-security.kubernetes.io/warn-version":"v1.24"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 30 08:02:20.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3730" for this suite. 08/30/23 08:02:20.163
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/30/23 08:02:20.187
Aug 30 08:02:20.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
STEP: Building a namespace api object, basename job 08/30/23 08:02:20.188
STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:02:20.32
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:02:20.33
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/30/23 08:02:20.337
W0830 08:02:20.354330      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to parallelism count is attached to the job 08/30/23 08:02:20.354
STEP: patching /status 08/30/23 08:02:24.363
STEP: updating /status 08/30/23 08:02:24.376
STEP: get /status 08/30/23 08:02:24.396
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 30 08:02:24.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9700" for this suite. 08/30/23 08:02:24.416
------------------------------
• [4.248 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/30/23 08:02:20.187
    Aug 30 08:02:20.187: INFO: >>> kubeConfig: /tmp/kubeconfig-1239079833
    STEP: Building a namespace api object, basename job 08/30/23 08:02:20.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/30/23 08:02:20.32
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/30/23 08:02:20.33
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/30/23 08:02:20.337
    W0830 08:02:20.354330      21 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to parallelism count is attached to the job 08/30/23 08:02:20.354
    STEP: patching /status 08/30/23 08:02:24.363
    STEP: updating /status 08/30/23 08:02:24.376
    STEP: get /status 08/30/23 08:02:24.396
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 30 08:02:24.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9700" for this suite. 08/30/23 08:02:24.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 30 08:02:24.438: INFO: Running AfterSuite actions on node 1
Aug 30 08:02:24.438: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 30 08:02:24.438: INFO: Running AfterSuite actions on node 1
    Aug 30 08:02:24.438: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.113 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 6318.809 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h45m19.310549984s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

