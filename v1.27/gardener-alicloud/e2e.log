Conformance test: not doing test setup.
  I0904 11:30:13.019750    9466 e2e.go:117] Starting e2e run "2b2a2f92-8d37-4c06-a2b1-a50793a17119" on Ginkgo node 1
Running Suite: Kubernetes e2e suite - /go/src/k8s.io/kubernetes/platforms/linux/amd64
=====================================================================================
Random Seed: 1693827012 - will randomize all specs

Will run 378 of 7207 specs
SSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢
  ------------------------------
  Automatically polling progress:
    [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m0.099s)
      test/e2e/apps/statefulset.go:591
      In [It] (Node Runtime: 5m0.001s)
        test/e2e/apps/statefulset.go:591
        At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2151 (Step Runtime: 3m56.407s)
          test/e2e/apps/statefulset.go:687

        Begin Captured GinkgoWriter Output >>
          ...
          Sep  4 11:38:58.328: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
          Sep  4 11:38:58.442: INFO: rc: 1
          Sep  4 11:38:58.442: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
          Command stdout:
          stderr:
          Error from server (NotFound): pods "ss-2" not found
          error:
          exit status 1
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 1178 [sleep]
          time.Sleep(0x2540be400)
            /usr/local/go/src/runtime/time.go:195
          k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc0010f1450, 0x10}, {0xc0010f141c, 0x4}, {0xc0012a2ec0, 0x38}, 0x3?, 0x45d964b800)
            test/e2e/framework/pod/output/output.go:113
          k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x7fab70f73e78?, 0xc003d670b0?}, {0x72c3b50?, 0xc0020df040?}, 0x1?, {0xc0012a2ec0, 0x38})
            test/e2e/framework/statefulset/rest.go:240
        > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x7fab70f73e78, 0xc003d670b0}, {0x72c3b50, 0xc0020df040}, 0x3?)
            test/e2e/apps/statefulset.go:2006
        > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10({0x7fab70f73e78?, 0xc003d670b0})
            test/e2e/apps/statefulset.go:688
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc003d670b0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
  Automatically polling progress:
    [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m20.101s)
      test/e2e/apps/statefulset.go:591
      In [It] (Node Runtime: 5m20.002s)
        test/e2e/apps/statefulset.go:591
        At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2151 (Step Runtime: 4m16.408s)
          test/e2e/apps/statefulset.go:687

        Begin Captured GinkgoWriter Output >>
          ...
          Sep  4 11:39:18.571: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
          Sep  4 11:39:18.699: INFO: rc: 1
          Sep  4 11:39:18.699: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
          Command stdout:
          stderr:
          Error from server (NotFound): pods "ss-2" not found
          error:
          exit status 1
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 1178 [sleep]
          time.Sleep(0x2540be400)
            /usr/local/go/src/runtime/time.go:195
          k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc0010f1450, 0x10}, {0xc0010f141c, 0x4}, {0xc0012a2ec0, 0x38}, 0x3?, 0x45d964b800)
            test/e2e/framework/pod/output/output.go:113
          k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x7fab70f73e78?, 0xc003d670b0?}, {0x72c3b50?, 0xc0020df040?}, 0x1?, {0xc0012a2ec0, 0x38})
            test/e2e/framework/statefulset/rest.go:240
        > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x7fab70f73e78, 0xc003d670b0}, {0x72c3b50, 0xc0020df040}, 0x3?)
            test/e2e/apps/statefulset.go:2006
        > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10({0x7fab70f73e78?, 0xc003d670b0})
            test/e2e/apps/statefulset.go:688
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc003d670b0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
  Automatically polling progress:
    [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 5m40.102s)
      test/e2e/apps/statefulset.go:591
      In [It] (Node Runtime: 5m40.003s)
        test/e2e/apps/statefulset.go:591
        At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2151 (Step Runtime: 4m36.409s)
          test/e2e/apps/statefulset.go:687

        Begin Captured GinkgoWriter Output >>
          ...
          Sep  4 11:39:38.820: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
          Sep  4 11:39:38.946: INFO: rc: 1
          Sep  4 11:39:38.946: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
          Command stdout:
          stderr:
          Error from server (NotFound): pods "ss-2" not found
          error:
          exit status 1
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 1178 [sleep]
          time.Sleep(0x2540be400)
            /usr/local/go/src/runtime/time.go:195
          k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc0010f1450, 0x10}, {0xc0010f141c, 0x4}, {0xc0012a2ec0, 0x38}, 0x3?, 0x45d964b800)
            test/e2e/framework/pod/output/output.go:113
          k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x7fab70f73e78?, 0xc003d670b0?}, {0x72c3b50?, 0xc0020df040?}, 0x1?, {0xc0012a2ec0, 0x38})
            test/e2e/framework/statefulset/rest.go:240
        > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x7fab70f73e78, 0xc003d670b0}, {0x72c3b50, 0xc0020df040}, 0x3?)
            test/e2e/apps/statefulset.go:2006
        > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10({0x7fab70f73e78?, 0xc003d670b0})
            test/e2e/apps/statefulset.go:688
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc003d670b0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
  Automatically polling progress:
    [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance] (Spec Runtime: 6m0.103s)
      test/e2e/apps/statefulset.go:591
      In [It] (Node Runtime: 6m0.005s)
        test/e2e/apps/statefulset.go:591
        At [By Step] Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2151 (Step Runtime: 4m56.411s)
          test/e2e/apps/statefulset.go:687

        Begin Captured GinkgoWriter Output >>
          ...
          Sep  4 11:39:59.067: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
          Sep  4 11:39:59.192: INFO: rc: 1
          Sep  4 11:39:59.192: INFO: Waiting 10s to retry failed RunHostCmd: error running /go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmt6h-le3.it.shoot.staging.k8s-hana.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=statefulset-2151 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
          Command stdout:
          stderr:
          Error from server (NotFound): pods "ss-2" not found
          error:
          exit status 1
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 1178 [sleep]
          time.Sleep(0x2540be400)
            /usr/local/go/src/runtime/time.go:195
          k8s.io/kubernetes/test/e2e/framework/pod/output.RunHostCmdWithRetries({0xc0010f1450, 0x10}, {0xc0010f141c, 0x4}, {0xc0012a2ec0, 0x38}, 0x3?, 0x45d964b800)
            test/e2e/framework/pod/output/output.go:113
          k8s.io/kubernetes/test/e2e/framework/statefulset.ExecInStatefulPods({0x7fab70f73e78?, 0xc003d670b0?}, {0x72c3b50?, 0xc0020df040?}, 0x1?, {0xc0012a2ec0, 0x38})
            test/e2e/framework/statefulset/rest.go:240
        > k8s.io/kubernetes/test/e2e/apps.restoreHTTPProbe({0x7fab70f73e78, 0xc003d670b0}, {0x72c3b50, 0xc0020df040}, 0x3?)
            test/e2e/apps/statefulset.go:2006
        > k8s.io/kubernetes/test/e2e/apps.glob..func11.2.10({0x7fab70f73e78?, 0xc003d670b0})
            test/e2e/apps/statefulset.go:688
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc003d670b0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
â€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
  ------------------------------
  Automatically polling progress:
    [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance] (Spec Runtime: 5m0.085s)
      test/e2e/apps/cronjob.go:97
      In [It] (Node Runtime: 5m0.002s)
        test/e2e/apps/cronjob.go:97
        At [By Step] Ensuring no jobs are scheduled (Step Runtime: 4m59.988s)
          test/e2e/apps/cronjob.go:106

        Begin Captured GinkgoWriter Output >>
          Sep  4 11:41:27.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 1949 [select]
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.waitForWithContext({0x7fab70f73e78, 0xc002d5a0f0}, 0xc001985500, 0x2bcde0a?)
            vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:205
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fab70f73e78, 0xc002d5a0f0}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:260
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x7fab70f73e78, 0xc002d5a0f0}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:85
        > k8s.io/kubernetes/test/e2e/apps.waitForNoJobs({0x7fab70f73e78?, 0xc002d5a0f0?}, {0x72c3b50?, 0xc0020ded00?}, {0xc003363140?, 0xc?}, {0xc003bec2b0?, 0x5?}, 0x0?)
            test/e2e/apps/cronjob.go:622
        > k8s.io/kubernetes/test/e2e/apps.glob..func2.2({0x7fab70f73e78, 0xc002d5a0f0})
            test/e2e/apps/cronjob.go:107
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc002d5a0f0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSâ€¢SSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSâ€¢Sâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSâ€¢SSSSSSâ€¢â€¢SSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSâ€¢SSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢Sâ€¢â€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSâ€¢SSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSâ€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSâ€¢â€¢SSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSâ€¢SSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢Sâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSâ€¢SSSSSSSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢â€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSSSSSSSS
  ------------------------------
  Automatically polling progress:
    [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance] (Spec Runtime: 5m0.182s)
      test/e2e/scheduling/predicates.go:705
      In [It] (Node Runtime: 5m0.001s)
        test/e2e/scheduling/predicates.go:705
        At [By Step] Trying to create another pod(pod5) with hostport 54322 but hostIP 10.250.7.101 on the node which pod4 resides and expect not scheduled (Step Runtime: 4m55.825s)
          test/e2e/scheduling/predicates.go:724

        Begin Captured GinkgoWriter Output >>
          ...
          E0904 12:45:31.730485    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:32.730997    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:33.731861    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:34.732707    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:35.733176    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:36.733867    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:37.734181    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:38.734603    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:39.734973    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:45:40.736043    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 27183 [select]
          k8s.io/kubernetes/vendor/github.com/onsi/gomega/internal.(*AsyncAssertion).match(0xc00468aaf0, {0x726ee90?, 0xc000e15020}, 0x1, {0x0, 0x0, 0x0})
            vendor/github.com/onsi/gomega/internal/async_assertion.go:538
          k8s.io/kubernetes/vendor/github.com/onsi/gomega/internal.(*AsyncAssertion).Should(0xc00468aaf0, {0x726ee90, 0xc000e15020}, {0x0, 0x0, 0x0})
            vendor/github.com/onsi/gomega/internal/async_assertion.go:145
          k8s.io/kubernetes/test/e2e/framework.asyncAssertion.Should({{0x7fab70f73e78, 0xc004ee95f0}, {0xc000e14fe0, 0x1, 0x1}, 0x45d964b800, 0x77359400, 0x0}, {0x726ee90, 0xc000e15020})
            test/e2e/framework/expect.go:234
          k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodCondition({0x7fab70f73e78, 0xc004ee95f0}, {0x72c3b50?, 0xc0048804e0?}, {0xc004ce17c0, 0xf}, {0x6aaa80b, 0x4}, {0x6ac0c9d, 0xb}, ...)
            test/e2e/framework/pod/wait.go:228
          k8s.io/kubernetes/test/e2e/framework/pod.WaitForPodNotPending({0x7fab70f73e78?, 0xc004ee95f0?}, {0x72c3b50?, 0xc0048804e0?}, {0xc004ce17c0?, 0x0?}, {0x6aaa80b?, 0x0?})
            test/e2e/framework/pod/wait.go:507
        > k8s.io/kubernetes/test/e2e/scheduling.createHostPortPodOnNode({0x7fab70f73e78, 0xc004ee95f0}, 0xc000a1b3b0, {0x6aaa80b, 0x4}, {0xc004ce17c0, 0xf}, {0xc00373e960, 0xc}, 0xd432, ...)
            test/e2e/scheduling/predicates.go:1150
        > k8s.io/kubernetes/test/e2e/scheduling.glob..func4.13({0x7fab70f73e78, 0xc004ee95f0})
            test/e2e/scheduling/predicates.go:725
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc004ee95f0})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850

        Begin Additional Progress Reports >>
          expected pod to be not pending, got instead:
              <*v1.Pod | 0xc001389200>: 
                  metadata:
                    creationTimestamp: "2023-09-04T12:40:45Z"
                    managedFields:
                    - apiVersion: v1
                      fieldsType: FieldsV1
                      fieldsV1:
                        f:spec:
                          f:containers:
                            k:{"name":"agnhost"}:
                              .: {}
                              f:args: {}
                              f:image: {}
                              f:imagePullPolicy: {}
                              f:name: {}
                              f:ports:
                                .: {}
                                k:{"containerPort":8080,"protocol":"TCP"}:
                                  .: {}
                                  f:containerPort: {}
                                  f:hostIP: {}
                                  f:hostPort: {}
                                  f:protocol: {}
                              f:readinessProbe:
                                .: {}
                                f:failureThreshold: {}
                                f:httpGet:
                                  .: {}
                                  f:path: {}
                                  f:port: {}
                                  f:scheme: {}
                                f:periodSeconds: {}
                                f:successThreshold: {}
                                f:timeoutSeconds: {}
                              f:resources: {}
                              f:terminationMessagePath: {}
                              f:terminationMessagePolicy: {}
                          f:dnsPolicy: {}
                          f:enableServiceLinks: {}
                          f:nodeSelector: {}
                          f:restartPolicy: {}
                          f:schedulerName: {}
                          f:securityContext: {}
                          f:terminationGracePeriodSeconds: {}
                      manager: e2e.test
                      operation: Update
                      time: "2023-09-04T12:40:45Z"
                    - apiVersion: v1
                      fieldsType: FieldsV1
                      fieldsV1:
                        f:status:
                          f:conditions:
                            .: {}
                            k:{"type":"PodScheduled"}:
                              .: {}
                              f:lastProbeTime: {}
                              f:lastTransitionTime: {}
                              f:message: {}
                              f:reason: {}
                              f:status: {}
                              f:type: {}
                      manager: kube-scheduler
                      operation: Update
                      subresource: status
                      time: "2023-09-04T12:40:45Z"
                    name: pod5
                    namespace: sched-pred-3076
                    resourceVersion: "41346"
                    uid: f15f6198-d626-4efe-a3d7-50e56e619f68
                  spec:
                    containers:
                    - args:
                      - netexec
                      - --http-port=8080
                      - --udp-port=8080
                      env:
                      - name: KUBERNETES_SERVICE_HOST
                        value: api.tmt6h-le3.it.internal.staging.k8s.ondemand.com
                      image: registry.k8s.io/e2e-test-images/agnhost:2.43
                      imagePullPolicy: IfNotPresent
                      name: agnhost
                      ports:
                      - containerPort: 8080
                        hostIP: 10.250.7.101
                        hostPort: 54322
                        protocol: TCP
                      readinessProbe:
                        failureThreshold: 3
                        httpGet:
                          path: /hostname
                          port: 8080
                          scheme: HTTP
                        periodSeconds: 10
                        successThreshold: 1
                        timeoutSeconds: 1
                      resources: {}
                      terminationMessagePath: /dev/termination-log
                      terminationMessagePolicy: File
                      volumeMounts:
                      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
                        name: kube-api-access-xvrln
                        readOnly: true
                    dnsPolicy: ClusterFirst
                    enableServiceLinks: true
                    nodeSelector:
                      kubernetes.io/e2e-a7609a67-b27a-40a8-838e-0c7d65059588: "95"
                    preemptionPolicy: PreemptLowerPriority
                    priority: 0
                    restartPolicy: Always
                    schedulerName: default-scheduler
                    securityContext: {}
                    serviceAccount: default
                    serviceAccountName: default
                    terminationGracePeriodSeconds: 30
                    tolerations:
                    - effect: NoExecute
                      key: node.kubernetes.io/not-ready
                      operator: Exists
                      tolerationSeconds: 300
                    - effect: NoExecute
                      key: node.kubernetes.io/unreachable
                      operator: Exists
                      tolerationSeconds: 300
                    volumes:
                    - name: kube-api-access-xvrln
                      projected:
                        defaultMode: 420
                        sources:
                        - serviceAccountToken:
                            expirationSeconds: 3607
                            path: token
                        - configMap:
                            items:
                            - key: ca.crt
                              path: ca.crt
                            name: kube-root-ca.crt
                        - downwardAPI:
                            items:
                            - fieldRef:
                                apiVersion: v1
                                fieldPath: metadata.namespace
                              path: namespace
                  status:
                    conditions:
                    - lastProbeTime: null
                      lastTransitionTime: "2023-09-04T12:40:45Z"
                      message: '0/2 nodes are available: 1 node(s) didn''t have free ports for the requested
                        pod ports, 1 node(s) didn''t match Pod''s node affinity/selector. preemption:
                        0/2 nodes are available: 1 No preemption victims found for incoming pod, 1 Preemption
                        is not helpful for scheduling..'
                      reason: Unschedulable
                      status: "False"
                      type: PodScheduled
                    phase: Pending
                    qosClass: BestEffort
        << End Additional Progress Reports
  ------------------------------
â€¢SSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSâ€¢SSSâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSâ€¢â€¢SSSSSSâ€¢â€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSS
  ------------------------------
  Automatically polling progress:
    [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] (Spec Runtime: 5m0.084s)
      test/e2e/apps/cronjob.go:125
      In [It] (Node Runtime: 5m0s)
        test/e2e/apps/cronjob.go:125
        At [By Step] Ensuring no more jobs are scheduled (Step Runtime: 4m29.945s)
          test/e2e/apps/cronjob.go:147

        Begin Captured GinkgoWriter Output >>
          ...
          E0904 12:59:21.163374    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:22.163661    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:23.164699    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:24.164975    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:25.165342    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:26.165659    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:27.165760    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:28.166349    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:29.166502    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:30.166872    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 34247 [select]
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.waitForWithContext({0x7fab70f73e78, 0xc006b9a450}, 0xc008636e40, 0x2bcde0a?)
            vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:205
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fab70f73e78, 0xc006b9a450}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:260
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x7fab70f73e78, 0xc006b9a450}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:85
        > k8s.io/kubernetes/test/e2e/apps.waitForActiveJobs({0x7fab70f73e78?, 0xc006b9a450?}, {0x72c3b50?, 0xc00604a1a0?}, {0xc0051c9f10?, 0x0?}, {0xc004caaf48?, 0x0?}, 0x0?)
            test/e2e/apps/cronjob.go:608
        > k8s.io/kubernetes/test/e2e/apps.glob..func2.3({0x7fab70f73e78, 0xc006b9a450})
            test/e2e/apps/cronjob.go:148
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc006b9a450})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
  Automatically polling progress:
    [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance] (Spec Runtime: 5m20.086s)
      test/e2e/apps/cronjob.go:125
      In [It] (Node Runtime: 5m20.001s)
        test/e2e/apps/cronjob.go:125
        At [By Step] Ensuring no more jobs are scheduled (Step Runtime: 4m49.946s)
          test/e2e/apps/cronjob.go:147

        Begin Captured GinkgoWriter Output >>
          ...
          E0904 12:59:41.172082    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:42.172414    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:43.173100    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:44.173462    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:45.173903    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:46.174964    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:47.175947    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:48.176332    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:49.176944    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
          E0904 12:59:50.177191    9466 retrywatcher.go:130] "Watch failed" err="context canceled"
        << End Captured GinkgoWriter Output

        Spec Goroutine
        goroutine 34247 [select]
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.waitForWithContext({0x7fab70f73e78, 0xc006b9a450}, 0xc008636e40, 0x2bcde0a?)
            vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:205
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.poll({0x7fab70f73e78, 0xc006b9a450}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:260
          k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.PollWithContext({0x7fab70f73e78, 0xc006b9a450}, 0x0?, 0x0?, 0x0?)
            vendor/k8s.io/apimachinery/pkg/util/wait/poll.go:85
        > k8s.io/kubernetes/test/e2e/apps.waitForActiveJobs({0x7fab70f73e78?, 0xc006b9a450?}, {0x72c3b50?, 0xc00604a1a0?}, {0xc0051c9f10?, 0x0?}, {0xc004caaf48?, 0x0?}, 0x0?)
            test/e2e/apps/cronjob.go:608
        > k8s.io/kubernetes/test/e2e/apps.glob..func2.3({0x7fab70f73e78, 0xc006b9a450})
            test/e2e/apps/cronjob.go:148
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.extractBodyFunction.func2({0x728f280?, 0xc006b9a450})
            vendor/github.com/onsi/ginkgo/v2/internal/node.go:456
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode.func3()
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:863
          k8s.io/kubernetes/vendor/github.com/onsi/ginkgo/v2/internal.(*Suite).runNode
            vendor/github.com/onsi/ginkgo/v2/internal/suite.go:850
  ------------------------------
â€¢SSSSSSSSSSSSSSSSâ€¢SSâ€¢SSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSâ€¢SSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢Sâ€¢SSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSâ€¢SSSSSSâ€¢SSSâ€¢SSSSSSSSSSSSSSSSSSSSSSSSâ€¢SSSSSSSSSSSSSSSSâ€¢SSSSSSSSâ€¢â€¢SSâ€¢SSSSSSSSSâ€¢SSSSSSSSSSSSSSSSSSSSâ€¢SSâ€¢SSSSSSâ€¢SSSSSSSSSSSSS

Ran 378 of 7207 Specs in 6186.314 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--ginkgo.flakeAttempts is deprecated, use --ginkgo.flake-attempts instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m
  [38;5;11m--ginkgo.dryRun is deprecated, use --ginkgo.dry-run instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m


Ginkgo ran 1 suite in 1h43m7.865545105s
Test Suite Passed
