  I0129 20:17:12.010537      23 e2e.go:117] Starting e2e run "31722b4b-774a-433c-b403-fd73cf822c96" on Ginkgo node 1
  Jan 29 20:17:12.055: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1706559431 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jan 29 20:17:12.276: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:17:12.278: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jan 29 20:17:12.329: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jan 29 20:17:12.333: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Jan 29 20:17:12.333: INFO: e2e test version: v1.27.6
  Jan 29 20:17:12.335: INFO: kube-apiserver version: v1.27.6-ckp
  Jan 29 20:17:12.336: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:17:12.341: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 01/29/24 20:17:12.63
  Jan 29 20:17:12.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:17:12.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:12.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:12.65
  STEP: creating service in namespace services-9677 @ 01/29/24 20:17:12.655
  STEP: creating service affinity-clusterip in namespace services-9677 @ 01/29/24 20:17:12.655
  STEP: creating replication controller affinity-clusterip in namespace services-9677 @ 01/29/24 20:17:12.678
  I0129 20:17:12.688159      23 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9677, replica count: 3
  I0129 20:17:15.740349      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0129 20:17:18.741845      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0129 20:17:21.742848      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0129 20:17:24.744093      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 20:17:24.755: INFO: Creating new exec pod
  Jan 29 20:17:27.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-9677 exec execpod-affinityss6sj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jan 29 20:17:28.041: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jan 29 20:17:28.041: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:17:28.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-9677 exec execpod-affinityss6sj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.254.141 80'
  Jan 29 20:17:28.259: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.254.141 80\nConnection to 10.102.254.141 80 port [tcp/http] succeeded!\n"
  Jan 29 20:17:28.259: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:17:28.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-9677 exec execpod-affinityss6sj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.102.254.141:80/ ; done'
  Jan 29 20:17:28.553: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.102.254.141:80/\n"
  Jan 29 20:17:28.553: INFO: stdout: "\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf\naffinity-clusterip-rlwjf"
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Received response from host: affinity-clusterip-rlwjf
  Jan 29 20:17:28.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 20:17:28.560: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9677, will wait for the garbage collector to delete the pods @ 01/29/24 20:17:28.577
  Jan 29 20:17:28.643: INFO: Deleting ReplicationController affinity-clusterip took: 9.994009ms
  Jan 29 20:17:28.744: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.632975ms
  STEP: Destroying namespace "services-9677" for this suite. @ 01/29/24 20:17:32.962
• [20.339 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 01/29/24 20:17:32.977
  Jan 29 20:17:32.977: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:17:32.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:32.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:32.999
  STEP: Setting up server cert @ 01/29/24 20:17:33.024
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:17:33.82
  STEP: Deploying the webhook pod @ 01/29/24 20:17:33.831
  STEP: Wait for the deployment to be ready @ 01/29/24 20:17:33.847
  Jan 29 20:17:33.854: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/29/24 20:17:35.872
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:17:35.885
  Jan 29 20:17:36.885: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/29/24 20:17:36.892
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/29/24 20:17:36.922
  STEP: Creating a dummy validating-webhook-configuration object @ 01/29/24 20:17:36.946
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 01/29/24 20:17:36.958
  STEP: Creating a dummy mutating-webhook-configuration object @ 01/29/24 20:17:36.965
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 01/29/24 20:17:36.978
  Jan 29 20:17:36.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-710" for this suite. @ 01/29/24 20:17:37.052
  STEP: Destroying namespace "webhook-markers-4911" for this suite. @ 01/29/24 20:17:37.058
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 01/29/24 20:17:37.072
  Jan 29 20:17:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename watch @ 01/29/24 20:17:37.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:37.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:37.091
  STEP: creating a new configmap @ 01/29/24 20:17:37.096
  STEP: modifying the configmap once @ 01/29/24 20:17:37.1
  STEP: modifying the configmap a second time @ 01/29/24 20:17:37.109
  STEP: deleting the configmap @ 01/29/24 20:17:37.119
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 01/29/24 20:17:37.127
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 01/29/24 20:17:37.129
  Jan 29 20:17:37.129: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2714  f8cb7f74-2c4e-4575-8dea-c151e1151ec4 1720 0 2024-01-29 20:17:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-29 20:17:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:17:37.129: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-2714  f8cb7f74-2c4e-4575-8dea-c151e1151ec4 1721 0 2024-01-29 20:17:37 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-29 20:17:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:17:37.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2714" for this suite. @ 01/29/24 20:17:37.133
• [0.067 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 01/29/24 20:17:37.139
  Jan 29 20:17:37.139: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:17:37.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:37.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:37.159
  STEP: Creating projection with secret that has name projected-secret-test-map-7c8cc78e-8abb-47bc-9c94-3b776156435d @ 01/29/24 20:17:37.164
  STEP: Creating a pod to test consume secrets @ 01/29/24 20:17:37.169
  STEP: Saw pod success @ 01/29/24 20:17:41.193
  Jan 29 20:17:41.199: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-73d9ddc2-bc7f-4ca3-b05b-7308a00de136 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:17:41.23
  Jan 29 20:17:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7977" for this suite. @ 01/29/24 20:17:41.25
• [4.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 01/29/24 20:17:41.259
  Jan 29 20:17:41.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context-test @ 01/29/24 20:17:41.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:41.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:41.28
  Jan 29 20:17:45.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-708" for this suite. @ 01/29/24 20:17:45.541
• [4.292 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 01/29/24 20:17:45.553
  Jan 29 20:17:45.553: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:17:45.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:45.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:45.578
  STEP: creating service multi-endpoint-test in namespace services-8743 @ 01/29/24 20:17:45.583
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8743 to expose endpoints map[] @ 01/29/24 20:17:45.602
  Jan 29 20:17:45.611: INFO: successfully validated that service multi-endpoint-test in namespace services-8743 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8743 @ 01/29/24 20:17:45.611
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8743 to expose endpoints map[pod1:[100]] @ 01/29/24 20:17:47.639
  Jan 29 20:17:47.654: INFO: successfully validated that service multi-endpoint-test in namespace services-8743 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-8743 @ 01/29/24 20:17:47.654
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8743 to expose endpoints map[pod1:[100] pod2:[101]] @ 01/29/24 20:17:49.677
  Jan 29 20:17:49.696: INFO: successfully validated that service multi-endpoint-test in namespace services-8743 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 01/29/24 20:17:49.696
  Jan 29 20:17:49.696: INFO: Creating new exec pod
  Jan 29 20:17:52.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8743 exec execpod72x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jan 29 20:17:52.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jan 29 20:17:52.957: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:17:52.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8743 exec execpod72x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.37.69 80'
  Jan 29 20:17:53.170: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.37.69 80\nConnection to 10.107.37.69 80 port [tcp/http] succeeded!\n"
  Jan 29 20:17:53.170: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:17:53.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8743 exec execpod72x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jan 29 20:17:53.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jan 29 20:17:53.384: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:17:53.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8743 exec execpod72x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.37.69 81'
  Jan 29 20:17:53.606: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.37.69 81\nConnection to 10.107.37.69 81 port [tcp/*] succeeded!\n"
  Jan 29 20:17:53.606: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8743 @ 01/29/24 20:17:53.606
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8743 to expose endpoints map[pod2:[101]] @ 01/29/24 20:17:53.62
  Jan 29 20:17:54.737: INFO: successfully validated that service multi-endpoint-test in namespace services-8743 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-8743 @ 01/29/24 20:17:54.737
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8743 to expose endpoints map[] @ 01/29/24 20:17:54.748
  Jan 29 20:17:54.758: INFO: successfully validated that service multi-endpoint-test in namespace services-8743 exposes endpoints map[]
  Jan 29 20:17:54.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8743" for this suite. @ 01/29/24 20:17:54.778
• [9.231 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 01/29/24 20:17:54.785
  Jan 29 20:17:54.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:17:54.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:54.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:54.806
  STEP: Creating projection with secret that has name projected-secret-test-bf779124-e5df-48a4-a0e2-b9b1666975e1 @ 01/29/24 20:17:54.811
  STEP: Creating a pod to test consume secrets @ 01/29/24 20:17:54.815
  STEP: Saw pod success @ 01/29/24 20:17:58.841
  Jan 29 20:17:58.845: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-09d658ca-f06d-4d42-85ed-5053f2b1a180 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:17:58.854
  Jan 29 20:17:58.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-199" for this suite. @ 01/29/24 20:17:58.875
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 01/29/24 20:17:58.884
  Jan 29 20:17:58.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:17:58.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:17:58.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:17:58.906
  STEP: creating service in namespace services-607 @ 01/29/24 20:17:58.91
  STEP: creating service affinity-clusterip-transition in namespace services-607 @ 01/29/24 20:17:58.91
  STEP: creating replication controller affinity-clusterip-transition in namespace services-607 @ 01/29/24 20:17:58.923
  I0129 20:17:58.929479      23 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-607, replica count: 3
  I0129 20:18:01.981056      23 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 20:18:01.989: INFO: Creating new exec pod
  Jan 29 20:18:05.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-607 exec execpod-affinitys2jt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jan 29 20:18:05.232: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jan 29 20:18:05.232: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:18:05.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-607 exec execpod-affinitys2jt6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.23.20 80'
  Jan 29 20:18:05.465: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.23.20 80\nConnection to 10.108.23.20 80 port [tcp/http] succeeded!\n"
  Jan 29 20:18:05.465: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 20:18:05.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-607 exec execpod-affinitys2jt6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.23.20:80/ ; done'
  Jan 29 20:18:05.827: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n"
  Jan 29 20:18:05.827: INFO: stdout: "\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-6t7rh\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-vtxvz\naffinity-clusterip-transition-rzd75"
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.827: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-6t7rh
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-vtxvz
  Jan 29 20:18:05.828: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:05.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-607 exec execpod-affinitys2jt6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.23.20:80/ ; done'
  Jan 29 20:18:06.219: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.23.20:80/\n"
  Jan 29 20:18:06.219: INFO: stdout: "\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75\naffinity-clusterip-transition-rzd75"
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Received response from host: affinity-clusterip-transition-rzd75
  Jan 29 20:18:06.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 20:18:06.227: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-607, will wait for the garbage collector to delete the pods @ 01/29/24 20:18:06.239
  Jan 29 20:18:06.302: INFO: Deleting ReplicationController affinity-clusterip-transition took: 9.22129ms
  Jan 29 20:18:06.403: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.518199ms
  STEP: Destroying namespace "services-607" for this suite. @ 01/29/24 20:18:08.224
• [9.347 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 01/29/24 20:18:08.231
  Jan 29 20:18:08.231: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 01/29/24 20:18:08.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:08.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:08.251
  STEP: creating a target pod @ 01/29/24 20:18:08.256
  STEP: adding an ephemeral container @ 01/29/24 20:18:10.285
  STEP: checking pod container endpoints @ 01/29/24 20:18:12.315
  Jan 29 20:18:12.315: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4151 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:18:12.315: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:18:12.316: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:18:12.316: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4151/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jan 29 20:18:12.453: INFO: Exec stderr: ""
  Jan 29 20:18:12.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-4151" for this suite. @ 01/29/24 20:18:12.469
• [4.245 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 01/29/24 20:18:12.479
  Jan 29 20:18:12.480: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 20:18:12.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:12.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:12.501
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 01/29/24 20:18:12.508
  STEP: When a replication controller with a matching selector is created @ 01/29/24 20:18:22.557
  STEP: Then the orphan pod is adopted @ 01/29/24 20:18:22.564
  Jan 29 20:18:23.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3092" for this suite. @ 01/29/24 20:18:23.579
• [11.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 01/29/24 20:18:23.592
  Jan 29 20:18:23.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 20:18:23.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:23.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:23.612
  STEP: Creating projection with secret that has name secret-emptykey-test-1a34dc70-32dc-4fa4-af0e-0fa0c2363eba @ 01/29/24 20:18:23.616
  Jan 29 20:18:23.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1697" for this suite. @ 01/29/24 20:18:23.624
• [0.039 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 01/29/24 20:18:23.632
  Jan 29 20:18:23.632: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:18:23.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:23.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:23.65
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:18:23.655
  STEP: Saw pod success @ 01/29/24 20:18:27.683
  Jan 29 20:18:27.687: INFO: Trying to get logs from node nodeb29 pod downwardapi-volume-d247b4b7-2227-4492-9323-d284e64a0441 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:18:27.717
  Jan 29 20:18:27.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5624" for this suite. @ 01/29/24 20:18:27.74
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 01/29/24 20:18:27.751
  Jan 29 20:18:27.751: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:18:27.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:27.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:27.77
  STEP: Setting up server cert @ 01/29/24 20:18:27.794
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:18:28.183
  STEP: Deploying the webhook pod @ 01/29/24 20:18:28.194
  STEP: Wait for the deployment to be ready @ 01/29/24 20:18:28.211
  Jan 29 20:18:28.224: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:18:30.24
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:18:30.254
  Jan 29 20:18:31.255: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 01/29/24 20:18:31.261
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 01/29/24 20:18:31.29
  STEP: Creating a configMap that should not be mutated @ 01/29/24 20:18:31.299
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 01/29/24 20:18:31.31
  STEP: Creating a configMap that should be mutated @ 01/29/24 20:18:31.32
  Jan 29 20:18:31.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2091" for this suite. @ 01/29/24 20:18:31.395
  STEP: Destroying namespace "webhook-markers-5352" for this suite. @ 01/29/24 20:18:31.401
• [3.656 seconds]
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 01/29/24 20:18:31.407
  Jan 29 20:18:31.407: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename podtemplate @ 01/29/24 20:18:31.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:31.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:31.425
  Jan 29 20:18:31.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1826" for this suite. @ 01/29/24 20:18:31.468
• [0.071 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 01/29/24 20:18:31.478
  Jan 29 20:18:31.478: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename certificates @ 01/29/24 20:18:31.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:31.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:31.501
  STEP: getting /apis @ 01/29/24 20:18:32.267
  STEP: getting /apis/certificates.k8s.io @ 01/29/24 20:18:32.274
  STEP: getting /apis/certificates.k8s.io/v1 @ 01/29/24 20:18:32.276
  STEP: creating @ 01/29/24 20:18:32.278
  STEP: getting @ 01/29/24 20:18:32.299
  STEP: listing @ 01/29/24 20:18:32.302
  STEP: watching @ 01/29/24 20:18:32.309
  Jan 29 20:18:32.309: INFO: starting watch
  STEP: patching @ 01/29/24 20:18:32.31
  STEP: updating @ 01/29/24 20:18:32.318
  Jan 29 20:18:32.324: INFO: waiting for watch events with expected annotations
  Jan 29 20:18:32.324: INFO: saw patched and updated annotations
  STEP: getting /approval @ 01/29/24 20:18:32.324
  STEP: patching /approval @ 01/29/24 20:18:32.328
  STEP: updating /approval @ 01/29/24 20:18:32.337
  STEP: getting /status @ 01/29/24 20:18:32.343
  STEP: patching /status @ 01/29/24 20:18:32.347
  STEP: updating /status @ 01/29/24 20:18:32.359
  STEP: deleting @ 01/29/24 20:18:32.367
  STEP: deleting a collection @ 01/29/24 20:18:32.378
  Jan 29 20:18:32.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-8015" for this suite. @ 01/29/24 20:18:32.394
• [0.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 01/29/24 20:18:32.401
  Jan 29 20:18:32.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 20:18:32.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:18:32.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:18:32.423
  STEP: Creating service test in namespace statefulset-5197 @ 01/29/24 20:18:32.427
  STEP: Creating a new StatefulSet @ 01/29/24 20:18:32.431
  Jan 29 20:18:32.442: INFO: Found 0 stateful pods, waiting for 3
  Jan 29 20:18:42.449: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:18:42.449: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:18:42.449: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
  Jan 29 20:18:52.451: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:18:52.451: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:18:52.451: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/29/24 20:18:52.466
  Jan 29 20:18:52.491: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/29/24 20:18:52.491
  STEP: Not applying an update when the partition is greater than the number of replicas @ 01/29/24 20:19:02.512
  STEP: Performing a canary update @ 01/29/24 20:19:02.512
  Jan 29 20:19:02.533: INFO: Updating stateful set ss2
  Jan 29 20:19:02.542: INFO: Waiting for Pod statefulset-5197/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 01/29/24 20:19:12.555
  Jan 29 20:19:12.588: INFO: Found 2 stateful pods, waiting for 3
  Jan 29 20:19:22.597: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:19:22.597: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:19:22.597: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 01/29/24 20:19:22.607
  Jan 29 20:19:22.629: INFO: Updating stateful set ss2
  Jan 29 20:19:22.640: INFO: Waiting for Pod statefulset-5197/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 29 20:19:32.679: INFO: Updating stateful set ss2
  Jan 29 20:19:32.687: INFO: Waiting for StatefulSet statefulset-5197/ss2 to complete update
  Jan 29 20:19:32.687: INFO: Waiting for Pod statefulset-5197/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 29 20:19:42.700: INFO: Deleting all statefulset in ns statefulset-5197
  Jan 29 20:19:42.704: INFO: Scaling statefulset ss2 to 0
  Jan 29 20:19:52.730: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:19:52.734: INFO: Deleting statefulset ss2
  Jan 29 20:19:52.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5197" for this suite. @ 01/29/24 20:19:52.755
• [80.362 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 01/29/24 20:19:52.766
  Jan 29 20:19:52.766: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:19:52.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:19:52.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:19:52.785
  STEP: Creating configMap with name configmap-projected-all-test-volume-682e679b-7c41-4761-bbf4-9d3909563328 @ 01/29/24 20:19:52.789
  STEP: Creating secret with name secret-projected-all-test-volume-64ac5203-f425-4e2e-8746-178fd3acd3b7 @ 01/29/24 20:19:52.793
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 01/29/24 20:19:52.799
  STEP: Saw pod success @ 01/29/24 20:19:56.828
  Jan 29 20:19:56.833: INFO: Trying to get logs from node nodea08 pod projected-volume-fd8bd448-b042-453c-b7a7-144de9ed580a container projected-all-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:19:56.857
  Jan 29 20:19:56.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8061" for this suite. @ 01/29/24 20:19:56.875
• [4.118 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 01/29/24 20:19:56.884
  Jan 29 20:19:56.884: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context @ 01/29/24 20:19:56.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:19:56.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:19:56.91
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/29/24 20:19:56.915
  STEP: Saw pod success @ 01/29/24 20:20:00.943
  Jan 29 20:20:00.948: INFO: Trying to get logs from node nodea08 pod security-context-9d940a93-8f61-4ef6-9d2e-131ba71eccf9 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:20:00.958
  Jan 29 20:20:00.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7860" for this suite. @ 01/29/24 20:20:00.979
• [4.101 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 01/29/24 20:20:00.986
  Jan 29 20:20:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 20:20:00.988
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:01.005
  STEP: Creating secret with name secret-test-9c8f9304-abbe-4372-a9fb-f37fa6e71493 @ 01/29/24 20:20:01.009
  STEP: Creating a pod to test consume secrets @ 01/29/24 20:20:01.014
  STEP: Saw pod success @ 01/29/24 20:20:05.041
  Jan 29 20:20:05.045: INFO: Trying to get logs from node nodea08 pod pod-secrets-8dc20496-5770-4dd4-8703-6b3a96720808 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:20:05.055
  Jan 29 20:20:05.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9992" for this suite. @ 01/29/24 20:20:05.076
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 01/29/24 20:20:05.085
  Jan 29 20:20:05.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:20:05.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:05.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:05.107
  STEP: validating api versions @ 01/29/24 20:20:05.116
  Jan 29 20:20:05.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5571 api-versions'
  Jan 29 20:20:05.220: INFO: stderr: ""
  Jan 29 20:20:05.220: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jan 29 20:20:05.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5571" for this suite. @ 01/29/24 20:20:05.226
• [0.150 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 01/29/24 20:20:05.236
  Jan 29 20:20:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 20:20:05.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:05.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:05.256
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:20:05.26
  STEP: Saw pod success @ 01/29/24 20:20:09.292
  Jan 29 20:20:09.296: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-d17fa304-dc43-4730-b3f6-6d1484983b79 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:20:09.305
  Jan 29 20:20:09.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6528" for this suite. @ 01/29/24 20:20:09.329
• [4.099 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 01/29/24 20:20:09.335
  Jan 29 20:20:09.335: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:20:09.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:09.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:09.355
  STEP: Discovering how many secrets are in namespace by default @ 01/29/24 20:20:09.36
  STEP: Counting existing ResourceQuota @ 01/29/24 20:20:14.366
  STEP: Creating a ResourceQuota @ 01/29/24 20:20:19.372
  STEP: Ensuring resource quota status is calculated @ 01/29/24 20:20:19.379
  STEP: Creating a Secret @ 01/29/24 20:20:21.386
  STEP: Ensuring resource quota status captures secret creation @ 01/29/24 20:20:21.402
  STEP: Deleting a secret @ 01/29/24 20:20:23.41
  STEP: Ensuring resource quota status released usage @ 01/29/24 20:20:23.417
  Jan 29 20:20:25.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6962" for this suite. @ 01/29/24 20:20:25.43
• [16.110 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 01/29/24 20:20:25.446
  Jan 29 20:20:25.446: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:20:25.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:25.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:25.465
  STEP: Creating ServiceAccount "e2e-sa-7nckt"  @ 01/29/24 20:20:25.469
  Jan 29 20:20:25.473: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-7nckt"  @ 01/29/24 20:20:25.474
  Jan 29 20:20:25.483: INFO: AutomountServiceAccountToken: true
  Jan 29 20:20:25.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3292" for this suite. @ 01/29/24 20:20:25.488
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 01/29/24 20:20:25.508
  Jan 29 20:20:25.508: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:20:25.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:25.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:25.527
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/29/24 20:20:25.531
  STEP: Saw pod success @ 01/29/24 20:20:29.556
  Jan 29 20:20:29.560: INFO: Trying to get logs from node nodea08 pod pod-9fb9ca73-0cdd-4a4c-8ddd-db8afb198ac0 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:20:29.57
  Jan 29 20:20:29.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1423" for this suite. @ 01/29/24 20:20:29.59
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 01/29/24 20:20:29.597
  Jan 29 20:20:29.597: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-webhook @ 01/29/24 20:20:29.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:29.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:29.62
  STEP: Setting up server cert @ 01/29/24 20:20:29.625
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/29/24 20:20:30.064
  STEP: Deploying the custom resource conversion webhook pod @ 01/29/24 20:20:30.074
  STEP: Wait for the deployment to be ready @ 01/29/24 20:20:30.093
  Jan 29 20:20:30.101: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:20:32.115
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:20:32.131
  Jan 29 20:20:33.131: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 29 20:20:33.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Creating a v1 custom resource @ 01/29/24 20:20:35.745
  STEP: Create a v2 custom resource @ 01/29/24 20:20:35.764
  STEP: List CRs in v1 @ 01/29/24 20:20:35.807
  STEP: List CRs in v2 @ 01/29/24 20:20:35.813
  Jan 29 20:20:35.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5116" for this suite. @ 01/29/24 20:20:36.379
• [6.790 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 01/29/24 20:20:36.39
  Jan 29 20:20:36.390: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:20:36.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:36.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:36.407
  STEP: creating the pod @ 01/29/24 20:20:36.412
  Jan 29 20:20:36.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 create -f -'
  Jan 29 20:20:37.335: INFO: stderr: ""
  Jan 29 20:20:37.335: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 01/29/24 20:20:41.35
  Jan 29 20:20:41.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 label pods pause testing-label=testing-label-value'
  Jan 29 20:20:41.454: INFO: stderr: ""
  Jan 29 20:20:41.454: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 01/29/24 20:20:41.454
  Jan 29 20:20:41.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 get pod pause -L testing-label'
  Jan 29 20:20:41.550: INFO: stderr: ""
  Jan 29 20:20:41.550: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 01/29/24 20:20:41.55
  Jan 29 20:20:41.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 label pods pause testing-label-'
  Jan 29 20:20:41.651: INFO: stderr: ""
  Jan 29 20:20:41.651: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 01/29/24 20:20:41.651
  Jan 29 20:20:41.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 get pod pause -L testing-label'
  Jan 29 20:20:41.744: INFO: stderr: ""
  Jan 29 20:20:41.744: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
  STEP: using delete to clean up resources @ 01/29/24 20:20:41.744
  Jan 29 20:20:41.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 delete --grace-period=0 --force -f -'
  Jan 29 20:20:41.847: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:20:41.847: INFO: stdout: "pod \"pause\" force deleted\n"
  Jan 29 20:20:41.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 get rc,svc -l name=pause --no-headers'
  Jan 29 20:20:41.946: INFO: stderr: "No resources found in kubectl-1632 namespace.\n"
  Jan 29 20:20:41.946: INFO: stdout: ""
  Jan 29 20:20:41.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1632 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 29 20:20:42.037: INFO: stderr: ""
  Jan 29 20:20:42.037: INFO: stdout: ""
  Jan 29 20:20:42.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1632" for this suite. @ 01/29/24 20:20:42.043
• [5.662 seconds]
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 01/29/24 20:20:42.053
  Jan 29 20:20:42.053: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename cronjob @ 01/29/24 20:20:42.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:20:42.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:20:42.073
  STEP: Creating a ForbidConcurrent cronjob @ 01/29/24 20:20:42.078
  STEP: Ensuring a job is scheduled @ 01/29/24 20:20:42.086
  STEP: Ensuring exactly one is scheduled @ 01/29/24 20:21:02.092
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/29/24 20:21:02.096
  STEP: Ensuring no more jobs are scheduled @ 01/29/24 20:21:02.1
  STEP: Removing cronjob @ 01/29/24 20:26:02.111
  Jan 29 20:26:02.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-130" for this suite. @ 01/29/24 20:26:02.125
• [320.080 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 01/29/24 20:26:02.135
  Jan 29 20:26:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 20:26:02.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:26:02.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:26:02.161
  STEP: Creating pod busybox-2e61de17-8a3e-42dc-8ce1-1890f4cb257e in namespace container-probe-2957 @ 01/29/24 20:26:02.166
  Jan 29 20:26:04.185: INFO: Started pod busybox-2e61de17-8a3e-42dc-8ce1-1890f4cb257e in namespace container-probe-2957
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 20:26:04.185
  Jan 29 20:26:04.189: INFO: Initial restart count of pod busybox-2e61de17-8a3e-42dc-8ce1-1890f4cb257e is 0
  Jan 29 20:26:54.362: INFO: Restart count of pod container-probe-2957/busybox-2e61de17-8a3e-42dc-8ce1-1890f4cb257e is now 1 (50.172866563s elapsed)
  Jan 29 20:26:54.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:26:54.368
  STEP: Destroying namespace "container-probe-2957" for this suite. @ 01/29/24 20:26:54.38
• [52.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 01/29/24 20:26:54.391
  Jan 29 20:26:54.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:26:54.393
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:26:54.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:26:54.413
  Jan 29 20:26:54.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9123 version'
  Jan 29 20:26:54.514: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jan 29 20:26:54.514: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.6\", GitCommit:\"741c8db18a52787d734cbe4795f0b4ad860906d6\", GitTreeState:\"clean\", BuildDate:\"2023-09-13T09:21:34Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27+\", GitVersion:\"v1.27.6-ckp\", GitCommit:\"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99\", GitTreeState:\"clean\", BuildDate:\"2023-12-15T09:18:27Z\", GoVersion:\"go1.20.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jan 29 20:26:54.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9123" for this suite. @ 01/29/24 20:26:54.519
• [0.135 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 01/29/24 20:26:54.526
  Jan 29 20:26:54.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 20:26:54.528
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:26:54.549
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:26:54.553
  STEP: Creating secret with name s-test-opt-del-a663a770-f984-42d2-a189-71bfe23e9f62 @ 01/29/24 20:26:54.563
  STEP: Creating secret with name s-test-opt-upd-bd6ad116-32d9-4a94-8d9f-25265b4df4d8 @ 01/29/24 20:26:54.568
  STEP: Creating the pod @ 01/29/24 20:26:54.574
  STEP: Deleting secret s-test-opt-del-a663a770-f984-42d2-a189-71bfe23e9f62 @ 01/29/24 20:26:56.646
  STEP: Updating secret s-test-opt-upd-bd6ad116-32d9-4a94-8d9f-25265b4df4d8 @ 01/29/24 20:26:56.653
  STEP: Creating secret with name s-test-opt-create-d324ca93-1f57-44f7-a0c6-344665a07458 @ 01/29/24 20:26:56.658
  STEP: waiting to observe update in volume @ 01/29/24 20:26:56.663
  Jan 29 20:28:23.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9122" for this suite. @ 01/29/24 20:28:23.277
• [88.759 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 01/29/24 20:28:23.287
  Jan 29 20:28:23.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl-logs @ 01/29/24 20:28:23.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:23.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:23.308
  STEP: creating an pod @ 01/29/24 20:28:23.314
  Jan 29 20:28:23.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jan 29 20:28:23.432: INFO: stderr: ""
  Jan 29 20:28:23.432: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 01/29/24 20:28:23.433
  Jan 29 20:28:23.433: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jan 29 20:28:25.443: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 01/29/24 20:28:25.443
  Jan 29 20:28:25.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator'
  Jan 29 20:28:25.555: INFO: stderr: ""
  Jan 29 20:28:25.555: INFO: stdout: "I0129 20:28:24.125911       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/mknz 201\nI0129 20:28:24.326546       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2cw4 579\nI0129 20:28:24.526005       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w5cj 337\nI0129 20:28:24.726662       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/pxdl 278\nI0129 20:28:24.926137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2tp2 247\nI0129 20:28:25.126785       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/vgw 449\nI0129 20:28:25.326245       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/5fhf 465\nI0129 20:28:25.526858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/kw69 453\n"
  STEP: limiting log lines @ 01/29/24 20:28:25.555
  Jan 29 20:28:25.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator --tail=1'
  Jan 29 20:28:25.665: INFO: stderr: ""
  Jan 29 20:28:25.665: INFO: stdout: "I0129 20:28:25.526858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/kw69 453\n"
  Jan 29 20:28:25.665: INFO: got output "I0129 20:28:25.526858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/kw69 453\n"
  STEP: limiting log bytes @ 01/29/24 20:28:25.665
  Jan 29 20:28:25.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator --limit-bytes=1'
  Jan 29 20:28:25.773: INFO: stderr: ""
  Jan 29 20:28:25.773: INFO: stdout: "I"
  Jan 29 20:28:25.773: INFO: got output "I"
  STEP: exposing timestamps @ 01/29/24 20:28:25.773
  Jan 29 20:28:25.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator --tail=1 --timestamps'
  Jan 29 20:28:25.882: INFO: stderr: ""
  Jan 29 20:28:25.882: INFO: stdout: "2024-01-29T20:28:25.726591557Z I0129 20:28:25.726259       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/rrm 214\n"
  Jan 29 20:28:25.882: INFO: got output "2024-01-29T20:28:25.726591557Z I0129 20:28:25.726259       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/rrm 214\n"
  STEP: restricting to a time range @ 01/29/24 20:28:25.882
  Jan 29 20:28:28.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator --since=1s'
  Jan 29 20:28:28.491: INFO: stderr: ""
  Jan 29 20:28:28.491: INFO: stdout: "I0129 20:28:27.526093       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6rnc 516\nI0129 20:28:27.726725       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/z2n5 506\nI0129 20:28:27.926147       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/clf 343\nI0129 20:28:28.126781       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/96p 509\nI0129 20:28:28.326216       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/wfm 447\n"
  Jan 29 20:28:28.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 logs logs-generator logs-generator --since=24h'
  Jan 29 20:28:28.604: INFO: stderr: ""
  Jan 29 20:28:28.604: INFO: stdout: "I0129 20:28:24.125911       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/mknz 201\nI0129 20:28:24.326546       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/2cw4 579\nI0129 20:28:24.526005       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/w5cj 337\nI0129 20:28:24.726662       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/pxdl 278\nI0129 20:28:24.926137       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/2tp2 247\nI0129 20:28:25.126785       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/vgw 449\nI0129 20:28:25.326245       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/5fhf 465\nI0129 20:28:25.526858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/kw69 453\nI0129 20:28:25.726259       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/rrm 214\nI0129 20:28:25.926806       1 logs_generator.go:76] 9 GET /api/v1/namespaces/default/pods/fkq 207\nI0129 20:28:26.126270       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/jpp2 204\nI0129 20:28:26.326946       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/8djj 583\nI0129 20:28:26.526539       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/gfnm 250\nI0129 20:28:26.725997       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/zxhz 212\nI0129 20:28:26.926570       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/bj8 421\nI0129 20:28:27.126054       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/n62c 240\nI0129 20:28:27.326645       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/dxv2 418\nI0129 20:28:27.526093       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/6rnc 516\nI0129 20:28:27.726725       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/z2n5 506\nI0129 20:28:27.926147       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/clf 343\nI0129 20:28:28.126781       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/96p 509\nI0129 20:28:28.326216       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/ns/pods/wfm 447\nI0129 20:28:28.526680       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/default/pods/qw6 543\n"
  Jan 29 20:28:28.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-logs-597 delete pod logs-generator'
  Jan 29 20:28:31.103: INFO: stderr: ""
  Jan 29 20:28:31.103: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jan 29 20:28:31.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-597" for this suite. @ 01/29/24 20:28:31.109
• [7.828 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 01/29/24 20:28:31.117
  Jan 29 20:28:31.117: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:28:31.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:31.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:31.138
  STEP: Creating configMap with name projected-configmap-test-volume-277c548f-bdaf-4b8a-849d-0b4f4ab0285e @ 01/29/24 20:28:31.143
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:28:31.148
  STEP: Saw pod success @ 01/29/24 20:28:35.175
  Jan 29 20:28:35.179: INFO: Trying to get logs from node nodea08 pod pod-projected-configmaps-edb38294-6251-47b0-b66b-dc412e41d151 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:28:35.188
  Jan 29 20:28:35.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-133" for this suite. @ 01/29/24 20:28:35.21
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 01/29/24 20:28:35.219
  Jan 29 20:28:35.219: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 20:28:35.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:35.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:35.238
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 01/29/24 20:28:35.243
  Jan 29 20:28:35.253: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5809  01b0cb23-1917-4ed3-8b6a-1cd8de5192a9 3949 0 2024-01-29 20:28:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-29 20:28:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phq8z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phq8z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 01/29/24 20:28:37.264
  Jan 29 20:28:37.264: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5809 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:28:37.264: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:28:37.265: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:28:37.265: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5809/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 01/29/24 20:28:37.409
  Jan 29 20:28:37.409: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5809 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:28:37.410: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:28:37.410: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:28:37.410: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5809/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 29 20:28:37.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 20:28:37.556: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-5809" for this suite. @ 01/29/24 20:28:37.567
• [2.355 seconds]
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 01/29/24 20:28:37.574
  Jan 29 20:28:37.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context-test @ 01/29/24 20:28:37.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:37.589
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:37.593
  Jan 29 20:28:41.635: INFO: Got logs for pod "busybox-privileged-false-b4f9fc73-8af6-4038-8a31-d5554b27d154": "ip: RTNETLINK answers: Operation not permitted\n"
  Jan 29 20:28:41.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5915" for this suite. @ 01/29/24 20:28:41.641
• [4.073 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 01/29/24 20:28:41.648
  Jan 29 20:28:41.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:28:41.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:41.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:41.668
  STEP: creating Agnhost RC @ 01/29/24 20:28:41.673
  Jan 29 20:28:41.673: INFO: namespace kubectl-5858
  Jan 29 20:28:41.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5858 create -f -'
  Jan 29 20:28:42.018: INFO: stderr: ""
  Jan 29 20:28:42.018: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/29/24 20:28:42.018
  Jan 29 20:28:43.024: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:28:43.024: INFO: Found 0 / 1
  Jan 29 20:28:44.024: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:28:44.025: INFO: Found 1 / 1
  Jan 29 20:28:44.025: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 29 20:28:44.029: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:28:44.029: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 29 20:28:44.029: INFO: wait on agnhost-primary startup in kubectl-5858 
  Jan 29 20:28:44.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5858 logs agnhost-primary-22prl agnhost-primary'
  Jan 29 20:28:44.141: INFO: stderr: ""
  Jan 29 20:28:44.141: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 01/29/24 20:28:44.141
  Jan 29 20:28:44.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5858 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jan 29 20:28:44.257: INFO: stderr: ""
  Jan 29 20:28:44.257: INFO: stdout: "service/rm2 exposed\n"
  Jan 29 20:28:44.260: INFO: Service rm2 in namespace kubectl-5858 found.
  STEP: exposing service @ 01/29/24 20:28:46.272
  Jan 29 20:28:46.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5858 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jan 29 20:28:46.389: INFO: stderr: ""
  Jan 29 20:28:46.389: INFO: stdout: "service/rm3 exposed\n"
  Jan 29 20:28:46.393: INFO: Service rm3 in namespace kubectl-5858 found.
  Jan 29 20:28:48.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5858" for this suite. @ 01/29/24 20:28:48.408
• [6.767 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 01/29/24 20:28:48.416
  Jan 29 20:28:48.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:28:48.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:28:48.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:28:48.44
  STEP: Counting existing ResourceQuota @ 01/29/24 20:28:48.445
  STEP: Creating a ResourceQuota @ 01/29/24 20:28:53.449
  STEP: Ensuring resource quota status is calculated @ 01/29/24 20:28:53.456
  STEP: Creating a Pod that fits quota @ 01/29/24 20:28:55.462
  STEP: Ensuring ResourceQuota status captures the pod usage @ 01/29/24 20:28:55.483
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 01/29/24 20:28:57.489
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 01/29/24 20:28:57.494
  STEP: Ensuring a pod cannot update its resource requirements @ 01/29/24 20:28:57.498
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 01/29/24 20:28:57.504
  STEP: Deleting the pod @ 01/29/24 20:28:59.511
  STEP: Ensuring resource quota status released the pod usage @ 01/29/24 20:28:59.524
  Jan 29 20:29:01.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9424" for this suite. @ 01/29/24 20:29:01.536
• [13.128 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 01/29/24 20:29:01.544
  Jan 29 20:29:01.544: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:29:01.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:01.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:01.566
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 01/29/24 20:29:01.571
  STEP: Saw pod success @ 01/29/24 20:29:05.597
  Jan 29 20:29:05.601: INFO: Trying to get logs from node nodea08 pod pod-da8b0a8a-fb0a-40b0-bb88-3d00024c986a container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:29:05.617
  Jan 29 20:29:05.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5471" for this suite. @ 01/29/24 20:29:05.639
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 01/29/24 20:29:05.649
  Jan 29 20:29:05.649: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 20:29:05.651
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:05.666
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:05.671
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/29/24 20:29:05.675
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/29/24 20:29:05.675
  STEP: creating a pod to probe DNS @ 01/29/24 20:29:05.675
  STEP: submitting the pod to kubernetes @ 01/29/24 20:29:05.675
  STEP: retrieving the pod @ 01/29/24 20:29:23.75
  STEP: looking for the results for each expected name from probers @ 01/29/24 20:29:23.754
  Jan 29 20:29:23.779: INFO: DNS probes using dns-6863/dns-test-c32df9db-9fa1-4d07-8563-ccabcde02363 succeeded

  Jan 29 20:29:23.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:29:23.785
  STEP: Destroying namespace "dns-6863" for this suite. @ 01/29/24 20:29:23.799
• [18.155 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 01/29/24 20:29:23.806
  Jan 29 20:29:23.806: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename conformance-tests @ 01/29/24 20:29:23.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:23.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:23.827
  STEP: Getting node addresses @ 01/29/24 20:29:23.831
  Jan 29 20:29:23.831: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jan 29 20:29:23.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3946" for this suite. @ 01/29/24 20:29:23.842
• [0.042 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 01/29/24 20:29:23.848
  Jan 29 20:29:23.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:29:23.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:23.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:23.865
  STEP: Setting up server cert @ 01/29/24 20:29:23.889
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:29:25.285
  STEP: Deploying the webhook pod @ 01/29/24 20:29:25.296
  STEP: Wait for the deployment to be ready @ 01/29/24 20:29:25.311
  Jan 29 20:29:25.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:29:27.335
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:29:27.348
  Jan 29 20:29:28.349: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 29 20:29:28.356: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2253-crds.webhook.example.com via the AdmissionRegistration API @ 01/29/24 20:29:28.881
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/29/24 20:29:28.907
  Jan 29 20:29:30.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-722" for this suite. @ 01/29/24 20:29:31.5
  STEP: Destroying namespace "webhook-markers-3832" for this suite. @ 01/29/24 20:29:31.509
• [7.667 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 01/29/24 20:29:31.516
  Jan 29 20:29:31.516: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:29:31.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:31.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:31.54
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/29/24 20:29:31.544
  STEP: Saw pod success @ 01/29/24 20:29:35.569
  Jan 29 20:29:35.573: INFO: Trying to get logs from node nodea08 pod pod-50235b26-d123-466f-93cf-e88bc6d736f7 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:29:35.583
  Jan 29 20:29:35.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8579" for this suite. @ 01/29/24 20:29:35.607
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 01/29/24 20:29:35.616
  Jan 29 20:29:35.616: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:29:35.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:35.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:35.637
  STEP: creating Agnhost RC @ 01/29/24 20:29:35.642
  Jan 29 20:29:35.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5786 create -f -'
  Jan 29 20:29:36.103: INFO: stderr: ""
  Jan 29 20:29:36.103: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/29/24 20:29:36.103
  Jan 29 20:29:37.110: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:29:37.110: INFO: Found 0 / 1
  Jan 29 20:29:38.110: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:29:38.110: INFO: Found 1 / 1
  Jan 29 20:29:38.110: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 01/29/24 20:29:38.11
  Jan 29 20:29:38.114: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:29:38.114: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 29 20:29:38.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5786 patch pod agnhost-primary-vsv2g -p {"metadata":{"annotations":{"x":"y"}}}'
  Jan 29 20:29:38.228: INFO: stderr: ""
  Jan 29 20:29:38.228: INFO: stdout: "pod/agnhost-primary-vsv2g patched\n"
  STEP: checking annotations @ 01/29/24 20:29:38.228
  Jan 29 20:29:38.232: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 20:29:38.232: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 29 20:29:38.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5786" for this suite. @ 01/29/24 20:29:38.238
• [2.629 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 01/29/24 20:29:38.245
  Jan 29 20:29:38.245: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 20:29:38.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:38.268
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:38.273
  STEP: Creating a pod to test substitution in container's command @ 01/29/24 20:29:38.278
  STEP: Saw pod success @ 01/29/24 20:29:42.304
  Jan 29 20:29:42.308: INFO: Trying to get logs from node nodea08 pod var-expansion-520d8399-84c3-4880-958b-f904c2fe321b container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 20:29:42.319
  Jan 29 20:29:42.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5765" for this suite. @ 01/29/24 20:29:42.342
• [4.104 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 01/29/24 20:29:42.349
  Jan 29 20:29:42.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 20:29:42.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:29:42.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:29:42.375
  STEP: Creating pod busybox-7e545533-4260-4a05-93f3-0abdfe260500 in namespace container-probe-7425 @ 01/29/24 20:29:42.381
  Jan 29 20:29:44.399: INFO: Started pod busybox-7e545533-4260-4a05-93f3-0abdfe260500 in namespace container-probe-7425
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 20:29:44.399
  Jan 29 20:29:44.406: INFO: Initial restart count of pod busybox-7e545533-4260-4a05-93f3-0abdfe260500 is 0
  Jan 29 20:33:45.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:33:45.219
  STEP: Destroying namespace "container-probe-7425" for this suite. @ 01/29/24 20:33:45.232
• [242.890 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 01/29/24 20:33:45.241
  Jan 29 20:33:45.241: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:33:45.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:33:45.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:33:45.262
  STEP: Setting up server cert @ 01/29/24 20:33:45.284
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:33:45.603
  STEP: Deploying the webhook pod @ 01/29/24 20:33:45.613
  STEP: Wait for the deployment to be ready @ 01/29/24 20:33:45.626
  Jan 29 20:33:45.634: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:33:47.655
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:33:47.672
  Jan 29 20:33:48.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 01/29/24 20:33:48.678
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 01/29/24 20:33:48.681
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 01/29/24 20:33:48.681
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 01/29/24 20:33:48.681
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 01/29/24 20:33:48.684
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/29/24 20:33:48.684
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/29/24 20:33:48.686
  Jan 29 20:33:48.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5001" for this suite. @ 01/29/24 20:33:48.729
  STEP: Destroying namespace "webhook-markers-1488" for this suite. @ 01/29/24 20:33:48.735
• [3.500 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 01/29/24 20:33:48.742
  Jan 29 20:33:48.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 20:33:48.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:33:48.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:33:48.759
  Jan 29 20:33:48.775: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jan 29 20:33:53.780: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 20:33:53.781
  Jan 29 20:33:53.781: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 01/29/24 20:33:53.791
  Jan 29 20:33:53.802: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2173  893dc6a2-49bf-4f00-b69d-50a0decf5e16 4874 1 2024-01-29 20:33:53 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2024-01-29 20:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dd3258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 29 20:33:53.806: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jan 29 20:33:53.806: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jan 29 20:33:53.806: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2173  87520672-c028-40ab-8fce-5d873e2986c9 4877 1 2024-01-29 20:33:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 893dc6a2-49bf-4f00-b69d-50a0decf5e16 0xc003dd392f 0xc003dd3940}] [] [{e2e.test Update apps/v1 2024-01-29 20:33:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:33:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-29 20:33:53 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"893dc6a2-49bf-4f00-b69d-50a0decf5e16\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003dd39f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:33:53.810: INFO: Pod "test-cleanup-controller-hzhvn" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-hzhvn test-cleanup-controller- deployment-2173  d2dfbe94-c4a4-4da8-9e97-cdca315bb813 4853 0 2024-01-29 20:33:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 87520672-c028-40ab-8fce-5d873e2986c9 0xc003dd3cdf 0xc003dd3cf0}] [] [{kube-controller-manager Update v1 2024-01-29 20:33:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87520672-c028-40ab-8fce-5d873e2986c9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:33:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g8qgd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g8qgd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:33:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:33:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:33:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.44,StartTime:2024-01-29 20:33:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:33:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d06fb90850c42948c8bc55ac67c14d24b480d02892241bad1ba28a9d0d63ce3c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.44,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:33:53.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2173" for this suite. @ 01/29/24 20:33:53.815
• [5.085 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 01/29/24 20:33:53.828
  Jan 29 20:33:53.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pod-network-test @ 01/29/24 20:33:53.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:33:53.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:33:53.845
  STEP: Performing setup for networking test in namespace pod-network-test-1056 @ 01/29/24 20:33:53.849
  STEP: creating a selector @ 01/29/24 20:33:53.849
  STEP: Creating the service pods in kubernetes @ 01/29/24 20:33:53.849
  Jan 29 20:33:53.849: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/29/24 20:34:05.926
  Jan 29 20:34:07.945: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 29 20:34:07.945: INFO: Breadth first check of 10.244.1.45 on host 192.168.100.28...
  Jan 29 20:34:07.950: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.46:9080/dial?request=hostname&protocol=http&host=10.244.1.45&port=8083&tries=1'] Namespace:pod-network-test-1056 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:34:07.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:34:07.951: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:34:07.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1056/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.45%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 29 20:34:08.079: INFO: Waiting for responses: map[]
  Jan 29 20:34:08.079: INFO: reached 10.244.1.45 after 0/1 tries
  Jan 29 20:34:08.079: INFO: Breadth first check of 10.244.2.12 on host 192.168.100.129...
  Jan 29 20:34:08.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.46:9080/dial?request=hostname&protocol=http&host=10.244.2.12&port=8083&tries=1'] Namespace:pod-network-test-1056 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:34:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:34:08.084: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:34:08.084: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1056/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.46%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.2.12%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 29 20:34:08.204: INFO: Waiting for responses: map[]
  Jan 29 20:34:08.204: INFO: reached 10.244.2.12 after 0/1 tries
  Jan 29 20:34:08.204: INFO: Going to retry 0 out of 2 pods....
  Jan 29 20:34:08.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1056" for this suite. @ 01/29/24 20:34:08.21
• [14.390 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 01/29/24 20:34:08.218
  Jan 29 20:34:08.218: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 20:34:08.22
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:08.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:08.24
  Jan 29 20:34:08.245: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  W0129 20:34:08.246350      23 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc0013e1590 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0129 20:34:10.814682      23 warnings.go:70] unknown field "alpha"
  W0129 20:34:10.814722      23 warnings.go:70] unknown field "beta"
  W0129 20:34:10.814729      23 warnings.go:70] unknown field "delta"
  W0129 20:34:10.814736      23 warnings.go:70] unknown field "epsilon"
  W0129 20:34:10.814742      23 warnings.go:70] unknown field "gamma"
  Jan 29 20:34:11.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9156" for this suite. @ 01/29/24 20:34:11.379
• [3.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 01/29/24 20:34:11.388
  Jan 29 20:34:11.388: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename watch @ 01/29/24 20:34:11.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:11.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:11.409
  STEP: creating a watch on configmaps @ 01/29/24 20:34:11.414
  STEP: creating a new configmap @ 01/29/24 20:34:11.416
  STEP: modifying the configmap once @ 01/29/24 20:34:11.421
  STEP: closing the watch once it receives two notifications @ 01/29/24 20:34:11.431
  Jan 29 20:34:11.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8529  598761d4-0cd9-4406-b271-e6c538ea22ad 4995 0 2024-01-29 20:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-29 20:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:34:11.431: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8529  598761d4-0cd9-4406-b271-e6c538ea22ad 4997 0 2024-01-29 20:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-29 20:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 01/29/24 20:34:11.432
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 01/29/24 20:34:11.439
  STEP: deleting the configmap @ 01/29/24 20:34:11.441
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 01/29/24 20:34:11.447
  Jan 29 20:34:11.447: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8529  598761d4-0cd9-4406-b271-e6c538ea22ad 4998 0 2024-01-29 20:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-29 20:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:34:11.447: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8529  598761d4-0cd9-4406-b271-e6c538ea22ad 4999 0 2024-01-29 20:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-29 20:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:34:11.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8529" for this suite. @ 01/29/24 20:34:11.452
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 01/29/24 20:34:11.462
  Jan 29 20:34:11.462: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 20:34:11.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:11.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:11.483
  STEP: set up a multi version CRD @ 01/29/24 20:34:11.488
  Jan 29 20:34:11.489: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: rename a version @ 01/29/24 20:34:15.457
  STEP: check the new version name is served @ 01/29/24 20:34:15.481
  STEP: check the old version name is removed @ 01/29/24 20:34:16.889
  STEP: check the other version is not changed @ 01/29/24 20:34:17.71
  Jan 29 20:34:20.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8635" for this suite. @ 01/29/24 20:34:20.768
• [9.314 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 01/29/24 20:34:20.777
  Jan 29 20:34:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename init-container @ 01/29/24 20:34:20.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:20.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:20.797
  STEP: creating the pod @ 01/29/24 20:34:20.802
  Jan 29 20:34:20.802: INFO: PodSpec: initContainers in spec.initContainers
  Jan 29 20:34:24.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3403" for this suite. @ 01/29/24 20:34:24.44
• [3.670 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 01/29/24 20:34:24.455
  Jan 29 20:34:24.455: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:34:24.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:24.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:24.476
  STEP: Creating configMap with name configmap-test-volume-75c021dd-b4db-4456-a559-8c5dca40e810 @ 01/29/24 20:34:24.481
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:34:24.486
  STEP: Saw pod success @ 01/29/24 20:34:28.511
  Jan 29 20:34:28.516: INFO: Trying to get logs from node nodea08 pod pod-configmaps-0619bbd0-68ac-468a-8904-94bdea98ae63 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:34:28.544
  Jan 29 20:34:28.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2630" for this suite. @ 01/29/24 20:34:28.566
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 01/29/24 20:34:28.573
  Jan 29 20:34:28.573: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:34:28.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:34:28.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:34:28.591
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-f3b94abf-41ca-4730-b1e3-870a8370a18a @ 01/29/24 20:34:28.6
  STEP: Creating the pod @ 01/29/24 20:34:28.604
  STEP: Updating configmap projected-configmap-test-upd-f3b94abf-41ca-4730-b1e3-870a8370a18a @ 01/29/24 20:34:30.634
  STEP: waiting to observe update in volume @ 01/29/24 20:34:30.642
  Jan 29 20:36:01.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-732" for this suite. @ 01/29/24 20:36:01.258
• [92.694 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 01/29/24 20:36:01.268
  Jan 29 20:36:01.268: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:36:01.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:01.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:01.288
  Jan 29 20:36:01.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3475" for this suite. @ 01/29/24 20:36:01.341
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 01/29/24 20:36:01.35
  Jan 29 20:36:01.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/29/24 20:36:01.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:01.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:01.376
  STEP: create the container to handle the HTTPGet hook request. @ 01/29/24 20:36:01.385
  STEP: create the pod with lifecycle hook @ 01/29/24 20:36:03.415
  STEP: delete the pod with lifecycle hook @ 01/29/24 20:36:05.442
  STEP: check prestop hook @ 01/29/24 20:36:09.464
  Jan 29 20:36:09.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1719" for this suite. @ 01/29/24 20:36:09.5
• [8.156 seconds]
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 01/29/24 20:36:09.507
  Jan 29 20:36:09.507: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename proxy @ 01/29/24 20:36:09.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:09.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:09.528
  STEP: starting an echo server on multiple ports @ 01/29/24 20:36:09.547
  STEP: creating replication controller proxy-service-pdm6t in namespace proxy-7594 @ 01/29/24 20:36:09.547
  I0129 20:36:09.556043      23 runners.go:194] Created replication controller with name: proxy-service-pdm6t, namespace: proxy-7594, replica count: 1
  I0129 20:36:10.607914      23 runners.go:194] proxy-service-pdm6t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0129 20:36:11.608378      23 runners.go:194] proxy-service-pdm6t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0129 20:36:12.609682      23 runners.go:194] proxy-service-pdm6t Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 20:36:12.616: INFO: setup took 3.081959938s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 01/29/24 20:36:12.616
  Jan 29 20:36:12.626: INFO: (0) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 9.825381ms)
  Jan 29 20:36:12.626: INFO: (0) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 9.917657ms)
  Jan 29 20:36:12.626: INFO: (0) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 10.036257ms)
  Jan 29 20:36:12.628: INFO: (0) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 11.438262ms)
  Jan 29 20:36:12.628: INFO: (0) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 11.88394ms)
  Jan 29 20:36:12.628: INFO: (0) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 12.108347ms)
  Jan 29 20:36:12.629: INFO: (0) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 13.02413ms)
  Jan 29 20:36:12.629: INFO: (0) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 12.983916ms)
  Jan 29 20:36:12.629: INFO: (0) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 13.15852ms)
  Jan 29 20:36:12.629: INFO: (0) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 13.604182ms)
  Jan 29 20:36:12.630: INFO: (0) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 13.363824ms)
  Jan 29 20:36:12.637: INFO: (0) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 21.286599ms)
  Jan 29 20:36:12.638: INFO: (0) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 21.360311ms)
  Jan 29 20:36:12.638: INFO: (0) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 21.46869ms)
  Jan 29 20:36:12.638: INFO: (0) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 21.693578ms)
  Jan 29 20:36:12.639: INFO: (0) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 23.096405ms)
  Jan 29 20:36:12.644: INFO: (1) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 4.218793ms)
  Jan 29 20:36:12.647: INFO: (1) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 8.049385ms)
  Jan 29 20:36:12.647: INFO: (1) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 7.96618ms)
  Jan 29 20:36:12.647: INFO: (1) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 8.008063ms)
  Jan 29 20:36:12.647: INFO: (1) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 7.966807ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 8.347313ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 8.401323ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 8.72657ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 8.868353ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 8.845185ms)
  Jan 29 20:36:12.648: INFO: (1) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 8.891684ms)
  Jan 29 20:36:12.649: INFO: (1) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 9.524969ms)
  Jan 29 20:36:12.649: INFO: (1) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 9.945575ms)
  Jan 29 20:36:12.649: INFO: (1) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 9.857318ms)
  Jan 29 20:36:12.650: INFO: (1) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 10.194078ms)
  Jan 29 20:36:12.650: INFO: (1) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 10.498074ms)
  Jan 29 20:36:12.654: INFO: (2) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 3.746228ms)
  Jan 29 20:36:12.654: INFO: (2) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.879518ms)
  Jan 29 20:36:12.656: INFO: (2) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.792072ms)
  Jan 29 20:36:12.656: INFO: (2) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.87666ms)
  Jan 29 20:36:12.656: INFO: (2) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.175369ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 6.651711ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.840105ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.797057ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 6.775119ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.786138ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.833671ms)
  Jan 29 20:36:12.657: INFO: (2) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.921419ms)
  Jan 29 20:36:12.658: INFO: (2) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 7.349019ms)
  Jan 29 20:36:12.658: INFO: (2) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 7.53096ms)
  Jan 29 20:36:12.658: INFO: (2) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 7.679139ms)
  Jan 29 20:36:12.658: INFO: (2) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 7.601004ms)
  Jan 29 20:36:12.667: INFO: (3) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 8.399012ms)
  Jan 29 20:36:12.667: INFO: (3) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 8.42872ms)
  Jan 29 20:36:12.670: INFO: (3) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 11.253096ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 12.277242ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 12.50844ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 12.428549ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 12.431979ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 12.529651ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 12.581767ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 12.666376ms)
  Jan 29 20:36:12.671: INFO: (3) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 12.643178ms)
  Jan 29 20:36:12.672: INFO: (3) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 13.327706ms)
  Jan 29 20:36:12.672: INFO: (3) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 13.586676ms)
  Jan 29 20:36:12.672: INFO: (3) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 13.504013ms)
  Jan 29 20:36:12.672: INFO: (3) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 13.394348ms)
  Jan 29 20:36:12.672: INFO: (3) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 13.607829ms)
  Jan 29 20:36:12.676: INFO: (4) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 4.544485ms)
  Jan 29 20:36:12.676: INFO: (4) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.668386ms)
  Jan 29 20:36:12.677: INFO: (4) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.533826ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.56178ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.70632ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.574068ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.727622ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.730454ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.703248ms)
  Jan 29 20:36:12.678: INFO: (4) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.905551ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.697624ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.81881ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.976985ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.896145ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.899523ms)
  Jan 29 20:36:12.679: INFO: (4) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 7.081722ms)
  Jan 29 20:36:12.682: INFO: (5) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 2.922571ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.454893ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 4.56021ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 4.303178ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 4.804538ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.921734ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 4.963497ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.709708ms)
  Jan 29 20:36:12.685: INFO: (5) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.691192ms)
  Jan 29 20:36:12.686: INFO: (5) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.700631ms)
  Jan 29 20:36:12.686: INFO: (5) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 5.862326ms)
  Jan 29 20:36:12.686: INFO: (5) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 5.544348ms)
  Jan 29 20:36:12.686: INFO: (5) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.472756ms)
  Jan 29 20:36:12.686: INFO: (5) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.936656ms)
  Jan 29 20:36:12.687: INFO: (5) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 7.094618ms)
  Jan 29 20:36:12.687: INFO: (5) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.392603ms)
  Jan 29 20:36:12.691: INFO: (6) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.268424ms)
  Jan 29 20:36:12.691: INFO: (6) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 4.45727ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 4.9014ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 4.867825ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.001214ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.316226ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.485465ms)
  Jan 29 20:36:12.692: INFO: (6) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.738011ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.77438ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 5.80114ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.807665ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.318778ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.240103ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.595173ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.556664ms)
  Jan 29 20:36:12.693: INFO: (6) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.630914ms)
  Jan 29 20:36:12.697: INFO: (7) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.506896ms)
  Jan 29 20:36:12.697: INFO: (7) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.510276ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.322004ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 6.2922ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.29197ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.445862ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.546863ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.554817ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 6.647955ms)
  Jan 29 20:36:12.700: INFO: (7) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.613076ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.69568ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 6.831794ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.812232ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 7.299948ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 7.105273ms)
  Jan 29 20:36:12.701: INFO: (7) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 7.0677ms)
  Jan 29 20:36:12.704: INFO: (8) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 3.185942ms)
  Jan 29 20:36:12.705: INFO: (8) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 3.666276ms)
  Jan 29 20:36:12.706: INFO: (8) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.268117ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.360945ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.492446ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.537687ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.564096ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.574601ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.213184ms)
  Jan 29 20:36:12.707: INFO: (8) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.285454ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.489358ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.427015ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.539912ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 6.604435ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.625366ms)
  Jan 29 20:36:12.708: INFO: (8) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.966726ms)
  Jan 29 20:36:12.713: INFO: (9) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 4.836813ms)
  Jan 29 20:36:12.713: INFO: (9) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.430426ms)
  Jan 29 20:36:12.714: INFO: (9) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 5.411282ms)
  Jan 29 20:36:12.714: INFO: (9) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.005644ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.457179ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.000925ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.08711ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.602517ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 4.833635ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.748656ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 5.287853ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 7.073154ms)
  Jan 29 20:36:12.715: INFO: (9) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 6.418759ms)
  Jan 29 20:36:12.716: INFO: (9) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 7.013137ms)
  Jan 29 20:36:12.716: INFO: (9) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.165248ms)
  Jan 29 20:36:12.716: INFO: (9) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.707382ms)
  Jan 29 20:36:12.720: INFO: (10) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.691453ms)
  Jan 29 20:36:12.720: INFO: (10) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.089214ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.521102ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.899485ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 4.983749ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.132969ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.091697ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.053256ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.222799ms)
  Jan 29 20:36:12.721: INFO: (10) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.156007ms)
  Jan 29 20:36:12.722: INFO: (10) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.327247ms)
  Jan 29 20:36:12.722: INFO: (10) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.235109ms)
  Jan 29 20:36:12.723: INFO: (10) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.317361ms)
  Jan 29 20:36:12.723: INFO: (10) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.431423ms)
  Jan 29 20:36:12.723: INFO: (10) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.764571ms)
  Jan 29 20:36:12.723: INFO: (10) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.684563ms)
  Jan 29 20:36:12.727: INFO: (11) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 4.042823ms)
  Jan 29 20:36:12.729: INFO: (11) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.481767ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.558407ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.541151ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.540357ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.605151ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 6.702042ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.781022ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 6.710959ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 6.91411ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.888738ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.892589ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.840501ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.867955ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 7.009803ms)
  Jan 29 20:36:12.730: INFO: (11) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.889443ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.748863ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.874056ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.259981ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.929917ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.947702ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.021363ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 6.09863ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.149307ms)
  Jan 29 20:36:12.736: INFO: (12) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 6.241694ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.457746ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.495338ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 6.532694ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 7.181866ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.91101ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 7.103128ms)
  Jan 29 20:36:12.737: INFO: (12) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 7.210179ms)
  Jan 29 20:36:12.741: INFO: (13) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 3.722207ms)
  Jan 29 20:36:12.741: INFO: (13) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.799588ms)
  Jan 29 20:36:12.741: INFO: (13) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 3.843668ms)
  Jan 29 20:36:12.741: INFO: (13) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.043313ms)
  Jan 29 20:36:12.743: INFO: (13) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.360313ms)
  Jan 29 20:36:12.743: INFO: (13) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.19859ms)
  Jan 29 20:36:12.743: INFO: (13) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 5.494794ms)
  Jan 29 20:36:12.743: INFO: (13) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.396626ms)
  Jan 29 20:36:12.743: INFO: (13) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 5.642962ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.0297ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 6.549299ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.711995ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.918128ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.76552ms)
  Jan 29 20:36:12.744: INFO: (13) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.712549ms)
  Jan 29 20:36:12.746: INFO: (13) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 8.007853ms)
  Jan 29 20:36:12.749: INFO: (14) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 3.221578ms)
  Jan 29 20:36:12.749: INFO: (14) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 3.242609ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.02681ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.101778ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.448896ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 5.760599ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.568902ms)
  Jan 29 20:36:12.751: INFO: (14) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.53913ms)
  Jan 29 20:36:12.752: INFO: (14) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.965981ms)
  Jan 29 20:36:12.752: INFO: (14) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.082132ms)
  Jan 29 20:36:12.752: INFO: (14) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.296134ms)
  Jan 29 20:36:12.753: INFO: (14) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.666866ms)
  Jan 29 20:36:12.753: INFO: (14) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.583605ms)
  Jan 29 20:36:12.753: INFO: (14) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.645885ms)
  Jan 29 20:36:12.753: INFO: (14) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 6.911436ms)
  Jan 29 20:36:12.753: INFO: (14) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 6.66696ms)
  Jan 29 20:36:12.759: INFO: (15) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.021498ms)
  Jan 29 20:36:12.759: INFO: (15) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 5.916941ms)
  Jan 29 20:36:12.759: INFO: (15) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.128797ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 6.810519ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 6.861761ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.940322ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 7.061868ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 7.119412ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 7.062853ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 7.125788ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 7.114903ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 7.314755ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 7.344053ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 7.275155ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 7.239571ms)
  Jan 29 20:36:12.760: INFO: (15) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 7.454566ms)
  Jan 29 20:36:12.764: INFO: (16) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 3.607819ms)
  Jan 29 20:36:12.764: INFO: (16) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 3.782333ms)
  Jan 29 20:36:12.765: INFO: (16) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 4.530105ms)
  Jan 29 20:36:12.765: INFO: (16) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.52216ms)
  Jan 29 20:36:12.765: INFO: (16) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 4.600358ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.629669ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 5.709921ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.604321ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 5.833096ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 5.751919ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.631199ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.55595ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.604613ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 5.550916ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 5.828015ms)
  Jan 29 20:36:12.766: INFO: (16) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 5.748972ms)
  Jan 29 20:36:12.770: INFO: (17) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 3.112735ms)
  Jan 29 20:36:12.772: INFO: (17) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 4.91007ms)
  Jan 29 20:36:12.772: INFO: (17) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.164561ms)
  Jan 29 20:36:12.772: INFO: (17) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.254269ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.746449ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.916783ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.792827ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 6.035883ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.306062ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.277111ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 6.213875ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.728066ms)
  Jan 29 20:36:12.773: INFO: (17) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.74871ms)
  Jan 29 20:36:12.774: INFO: (17) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.794286ms)
  Jan 29 20:36:12.774: INFO: (17) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.914869ms)
  Jan 29 20:36:12.774: INFO: (17) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.935811ms)
  Jan 29 20:36:12.778: INFO: (18) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 4.516123ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.250451ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.281392ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.34586ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.363648ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.463548ms)
  Jan 29 20:36:12.779: INFO: (18) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.562754ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.591507ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.817746ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.056783ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 6.102603ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.243907ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 6.269603ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 6.433783ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 6.41654ms)
  Jan 29 20:36:12.780: INFO: (18) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.487161ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.072948ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:460/proxy/: tls baz (200; 5.2285ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86/proxy/rewriteme">test</a> (200; 5.13127ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:462/proxy/: tls qux (200; 5.491558ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.177881ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:162/proxy/: bar (200; 5.182197ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/https:proxy-service-pdm6t-l6j86:443/proxy/tlsrewritem... (200; 5.434801ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:160/proxy/: foo (200; 5.55197ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/http:proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">... (200; 5.529152ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/: <a href="/api/v1/namespaces/proxy-7594/pods/proxy-service-pdm6t-l6j86:1080/proxy/rewriteme">test<... (200; 5.565883ms)
  Jan 29 20:36:12.786: INFO: (19) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname2/proxy/: bar (200; 5.797338ms)
  Jan 29 20:36:12.788: INFO: (19) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname1/proxy/: foo (200; 6.821299ms)
  Jan 29 20:36:12.788: INFO: (19) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname2/proxy/: tls qux (200; 6.921683ms)
  Jan 29 20:36:12.788: INFO: (19) /api/v1/namespaces/proxy-7594/services/proxy-service-pdm6t:portname1/proxy/: foo (200; 6.898035ms)
  Jan 29 20:36:12.788: INFO: (19) /api/v1/namespaces/proxy-7594/services/http:proxy-service-pdm6t:portname2/proxy/: bar (200; 7.001265ms)
  Jan 29 20:36:12.788: INFO: (19) /api/v1/namespaces/proxy-7594/services/https:proxy-service-pdm6t:tlsportname1/proxy/: tls baz (200; 7.185293ms)
  Jan 29 20:36:12.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-pdm6t in namespace proxy-7594, will wait for the garbage collector to delete the pods @ 01/29/24 20:36:12.792
  Jan 29 20:36:12.853: INFO: Deleting ReplicationController proxy-service-pdm6t took: 7.298381ms
  Jan 29 20:36:12.954: INFO: Terminating ReplicationController proxy-service-pdm6t pods took: 100.761198ms
  STEP: Destroying namespace "proxy-7594" for this suite. @ 01/29/24 20:36:14.855
• [5.354 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 01/29/24 20:36:14.861
  Jan 29 20:36:14.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename watch @ 01/29/24 20:36:14.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:14.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:14.881
  STEP: creating a watch on configmaps with a certain label @ 01/29/24 20:36:14.886
  STEP: creating a new configmap @ 01/29/24 20:36:14.888
  STEP: modifying the configmap once @ 01/29/24 20:36:14.893
  STEP: changing the label value of the configmap @ 01/29/24 20:36:14.903
  STEP: Expecting to observe a delete notification for the watched object @ 01/29/24 20:36:14.911
  Jan 29 20:36:14.911: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5438 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:36:14.912: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5439 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:36:14.912: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5440 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 01/29/24 20:36:14.912
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 01/29/24 20:36:14.92
  STEP: changing the label value of the configmap back @ 01/29/24 20:36:24.92
  STEP: modifying the configmap a third time @ 01/29/24 20:36:24.932
  STEP: deleting the configmap @ 01/29/24 20:36:24.943
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 01/29/24 20:36:24.95
  Jan 29 20:36:24.951: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5472 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:36:24.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5473 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:36:24.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7897  46fcb000-a164-4964-a3d9-5bcca8165167 5474 0 2024-01-29 20:36:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-29 20:36:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:36:24.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7897" for this suite. @ 01/29/24 20:36:24.957
• [10.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 01/29/24 20:36:24.965
  Jan 29 20:36:24.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/29/24 20:36:24.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:24.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:24.988
  Jan 29 20:36:24.992: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:36:31.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-797" for this suite. @ 01/29/24 20:36:31.271
• [6.314 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 01/29/24 20:36:31.285
  Jan 29 20:36:31.285: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 20:36:31.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:31.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:31.306
  Jan 29 20:36:31.310: INFO: Creating deployment "test-recreate-deployment"
  Jan 29 20:36:31.316: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jan 29 20:36:31.324: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jan 29 20:36:33.333: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jan 29 20:36:33.337: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jan 29 20:36:33.349: INFO: Updating deployment test-recreate-deployment
  Jan 29 20:36:33.349: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jan 29 20:36:33.428: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1010  63fe98b7-1033-4c6a-b3e8-1a044692aa31 5586 2 2024-01-29 20:36:31 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fcad08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-29 20:36:33 +0000 UTC,LastTransitionTime:2024-01-29 20:36:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2024-01-29 20:36:33 +0000 UTC,LastTransitionTime:2024-01-29 20:36:31 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 29 20:36:33.431: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1010  9ef40b3d-4c4a-4e9e-a0b4-1b725aca3b83 5584 1 2024-01-29 20:36:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 63fe98b7-1033-4c6a-b3e8-1a044692aa31 0xc004fcb0a7 0xc004fcb0a8}] [] [{kube-controller-manager Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63fe98b7-1033-4c6a-b3e8-1a044692aa31\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fcb148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:36:33.431: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jan 29 20:36:33.432: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1010  7dfe289c-9e2f-4589-bbe4-97914800a024 5574 2 2024-01-29 20:36:31 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 63fe98b7-1033-4c6a-b3e8-1a044692aa31 0xc004fcb1a7 0xc004fcb1a8}] [] [{kube-controller-manager Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63fe98b7-1033-4c6a-b3e8-1a044692aa31\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fcb258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:36:33.435: INFO: Pod "test-recreate-deployment-54757ffd6c-xv92w" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-xv92w test-recreate-deployment-54757ffd6c- deployment-1010  7e0199e1-bd25-4c18-a4c0-9a5a0ded2a90 5585 0 2024-01-29 20:36:33 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 9ef40b3d-4c4a-4e9e-a0b4-1b725aca3b83 0xc004fcb6d7 0xc004fcb6d8}] [] [{kube-controller-manager Update v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9ef40b3d-4c4a-4e9e-a0b4-1b725aca3b83\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q6d75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q6d75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:36:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:36:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:36:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:,StartTime:2024-01-29 20:36:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:36:33.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1010" for this suite. @ 01/29/24 20:36:33.441
• [2.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 01/29/24 20:36:33.448
  Jan 29 20:36:33.448: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:36:33.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:33.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:33.466
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:36:33.471
  STEP: Saw pod success @ 01/29/24 20:36:37.497
  Jan 29 20:36:37.502: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-c6002906-ca10-4651-9135-533a7261b46c container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:36:37.512
  Jan 29 20:36:37.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2114" for this suite. @ 01/29/24 20:36:37.536
• [4.097 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 01/29/24 20:36:37.545
  Jan 29 20:36:37.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:36:37.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:37.561
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:37.567
  STEP: creating the pod @ 01/29/24 20:36:37.571
  STEP: submitting the pod to kubernetes @ 01/29/24 20:36:37.571
  STEP: verifying the pod is in kubernetes @ 01/29/24 20:36:39.594
  STEP: updating the pod @ 01/29/24 20:36:39.598
  Jan 29 20:36:40.114: INFO: Successfully updated pod "pod-update-eec4ef49-c44a-409b-a75e-64a361c1292c"
  STEP: verifying the updated pod is in kubernetes @ 01/29/24 20:36:40.118
  Jan 29 20:36:40.123: INFO: Pod update OK
  Jan 29 20:36:40.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9196" for this suite. @ 01/29/24 20:36:40.129
• [2.590 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 01/29/24 20:36:40.136
  Jan 29 20:36:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:36:40.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:40.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:40.154
  STEP: Setting up server cert @ 01/29/24 20:36:40.174
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:36:40.792
  STEP: Deploying the webhook pod @ 01/29/24 20:36:40.802
  STEP: Wait for the deployment to be ready @ 01/29/24 20:36:40.814
  Jan 29 20:36:40.825: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:36:42.839
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:36:42.856
  Jan 29 20:36:43.856: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 01/29/24 20:36:43.862
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/29/24 20:36:43.862
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 01/29/24 20:36:43.888
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 01/29/24 20:36:44.901
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/29/24 20:36:44.901
  STEP: Having no error when timeout is longer than webhook latency @ 01/29/24 20:36:45.937
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/29/24 20:36:45.937
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 01/29/24 20:36:50.984
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/29/24 20:36:50.984
  Jan 29 20:36:56.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2908" for this suite. @ 01/29/24 20:36:56.076
  STEP: Destroying namespace "webhook-markers-2230" for this suite. @ 01/29/24 20:36:56.081
• [15.950 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 01/29/24 20:36:56.086
  Jan 29 20:36:56.086: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/29/24 20:36:56.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:36:56.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:36:56.105
  STEP: Creating 50 configmaps @ 01/29/24 20:36:56.109
  STEP: Creating RC which spawns configmap-volume pods @ 01/29/24 20:36:56.344
  Jan 29 20:36:56.470: INFO: Pod name wrapped-volume-race-f9c4961c-f0c9-4a1d-8202-0c58fc05014c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/29/24 20:36:56.47
  STEP: Creating RC which spawns configmap-volume pods @ 01/29/24 20:36:58.527
  Jan 29 20:36:58.549: INFO: Pod name wrapped-volume-race-d3e6774c-bd42-47ab-aa63-5fb8aa6f179b: Found 0 pods out of 5
  Jan 29 20:37:03.563: INFO: Pod name wrapped-volume-race-d3e6774c-bd42-47ab-aa63-5fb8aa6f179b: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/29/24 20:37:03.563
  STEP: Creating RC which spawns configmap-volume pods @ 01/29/24 20:37:03.593
  Jan 29 20:37:03.615: INFO: Pod name wrapped-volume-race-40e9a24f-d6f4-48f3-8bfb-fd88d4dbc534: Found 0 pods out of 5
  Jan 29 20:37:08.633: INFO: Pod name wrapped-volume-race-40e9a24f-d6f4-48f3-8bfb-fd88d4dbc534: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/29/24 20:37:08.633
  Jan 29 20:37:08.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-40e9a24f-d6f4-48f3-8bfb-fd88d4dbc534 in namespace emptydir-wrapper-6218, will wait for the garbage collector to delete the pods @ 01/29/24 20:37:08.664
  Jan 29 20:37:08.728: INFO: Deleting ReplicationController wrapped-volume-race-40e9a24f-d6f4-48f3-8bfb-fd88d4dbc534 took: 9.235322ms
  Jan 29 20:37:08.829: INFO: Terminating ReplicationController wrapped-volume-race-40e9a24f-d6f4-48f3-8bfb-fd88d4dbc534 pods took: 100.993525ms
  STEP: deleting ReplicationController wrapped-volume-race-d3e6774c-bd42-47ab-aa63-5fb8aa6f179b in namespace emptydir-wrapper-6218, will wait for the garbage collector to delete the pods @ 01/29/24 20:37:10.43
  Jan 29 20:37:10.494: INFO: Deleting ReplicationController wrapped-volume-race-d3e6774c-bd42-47ab-aa63-5fb8aa6f179b took: 7.562515ms
  Jan 29 20:37:10.594: INFO: Terminating ReplicationController wrapped-volume-race-d3e6774c-bd42-47ab-aa63-5fb8aa6f179b pods took: 100.744827ms
  STEP: deleting ReplicationController wrapped-volume-race-f9c4961c-f0c9-4a1d-8202-0c58fc05014c in namespace emptydir-wrapper-6218, will wait for the garbage collector to delete the pods @ 01/29/24 20:37:11.495
  Jan 29 20:37:11.560: INFO: Deleting ReplicationController wrapped-volume-race-f9c4961c-f0c9-4a1d-8202-0c58fc05014c took: 8.007168ms
  Jan 29 20:37:11.661: INFO: Terminating ReplicationController wrapped-volume-race-f9c4961c-f0c9-4a1d-8202-0c58fc05014c pods took: 100.980985ms
  STEP: Cleaning up the configMaps @ 01/29/24 20:37:12.663
  STEP: Destroying namespace "emptydir-wrapper-6218" for this suite. @ 01/29/24 20:37:12.936
• [16.855 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 01/29/24 20:37:12.945
  Jan 29 20:37:12.945: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 20:37:12.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:37:12.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:37:12.969
  STEP: Creating replication controller my-hostname-basic-03f3dbe6-63df-444a-86cb-bb3b368f8c17 @ 01/29/24 20:37:12.974
  Jan 29 20:37:12.984: INFO: Pod name my-hostname-basic-03f3dbe6-63df-444a-86cb-bb3b368f8c17: Found 0 pods out of 1
  Jan 29 20:37:17.990: INFO: Pod name my-hostname-basic-03f3dbe6-63df-444a-86cb-bb3b368f8c17: Found 1 pods out of 1
  Jan 29 20:37:17.990: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-03f3dbe6-63df-444a-86cb-bb3b368f8c17" are running
  Jan 29 20:37:17.993: INFO: Pod "my-hostname-basic-03f3dbe6-63df-444a-86cb-bb3b368f8c17-6927f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 20:37:12 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 20:37:14 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 20:37:14 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 20:37:12 +0000 UTC Reason: Message:}])
  Jan 29 20:37:17.994: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/29/24 20:37:17.994
  Jan 29 20:37:18.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9472" for this suite. @ 01/29/24 20:37:18.012
• [5.073 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 01/29/24 20:37:18.018
  Jan 29 20:37:18.018: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 20:37:18.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:37:18.031
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:37:18.035
  STEP: Create a Replicaset @ 01/29/24 20:37:18.043
  STEP: Verify that the required pods have come up. @ 01/29/24 20:37:18.048
  Jan 29 20:37:18.052: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 29 20:37:23.058: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 20:37:23.058
  STEP: Getting /status @ 01/29/24 20:37:23.058
  Jan 29 20:37:23.062: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 01/29/24 20:37:23.062
  Jan 29 20:37:23.072: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 01/29/24 20:37:23.072
  Jan 29 20:37:23.075: INFO: Observed &ReplicaSet event: ADDED
  Jan 29 20:37:23.075: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.075: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.075: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.075: INFO: Found replicaset test-rs in namespace replicaset-921 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 29 20:37:23.076: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 01/29/24 20:37:23.076
  Jan 29 20:37:23.076: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 29 20:37:23.082: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 01/29/24 20:37:23.082
  Jan 29 20:37:23.085: INFO: Observed &ReplicaSet event: ADDED
  Jan 29 20:37:23.085: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.085: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.085: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.086: INFO: Observed replicaset test-rs in namespace replicaset-921 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 20:37:23.086: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 29 20:37:23.086: INFO: Found replicaset test-rs in namespace replicaset-921 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jan 29 20:37:23.086: INFO: Replicaset test-rs has a patched status
  Jan 29 20:37:23.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-921" for this suite. @ 01/29/24 20:37:23.089
• [5.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 01/29/24 20:37:23.099
  Jan 29 20:37:23.099: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename taint-multiple-pods @ 01/29/24 20:37:23.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:37:23.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:37:23.116
  Jan 29 20:37:23.120: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 29 20:38:23.151: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 20:38:23.156: INFO: Starting informer...
  STEP: Starting pods... @ 01/29/24 20:38:23.156
  Jan 29 20:38:23.380: INFO: Pod1 is running on nodea08. Tainting Node
  Jan 29 20:38:25.607: INFO: Pod2 is running on nodea08. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/29/24 20:38:25.607
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/29/24 20:38:25.625
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 01/29/24 20:38:25.631
  Jan 29 20:38:31.538: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jan 29 20:38:51.612: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jan 29 20:38:51.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/29/24 20:38:51.633
  STEP: Destroying namespace "taint-multiple-pods-7626" for this suite. @ 01/29/24 20:38:51.637
• [88.544 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 01/29/24 20:38:51.644
  Jan 29 20:38:51.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 20:38:51.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:38:51.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:38:51.662
  STEP: Creating a test headless service @ 01/29/24 20:38:51.667
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2073.svc.cluster.local;sleep 1; done
   @ 01/29/24 20:38:51.676
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2073.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2073.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2073.svc.cluster.local;sleep 1; done
   @ 01/29/24 20:38:51.676
  STEP: creating a pod to probe DNS @ 01/29/24 20:38:51.676
  STEP: submitting the pod to kubernetes @ 01/29/24 20:38:51.676
  STEP: retrieving the pod @ 01/29/24 20:38:53.694
  STEP: looking for the results for each expected name from probers @ 01/29/24 20:38:53.702
  Jan 29 20:38:53.709: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:53.714: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:53.728: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:53.733: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:53.741: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:38:58.751: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:58.756: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:58.772: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:58.778: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:38:58.788: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:39:03.750: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:03.755: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:03.771: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:03.777: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:03.789: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:39:08.750: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:08.758: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:08.775: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:08.780: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:08.791: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:39:13.750: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:13.756: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:13.771: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:13.776: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:13.786: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:39:18.748: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:18.753: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:18.769: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:18.774: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local from pod dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4: the server could not find the requested resource (get pods dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4)
  Jan 29 20:39:18.784: INFO: Lookups using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2073.svc.cluster.local]

  Jan 29 20:39:23.790: INFO: DNS probes using dns-2073/dns-test-4de24a56-ca52-4764-8939-6692fe42dfc4 succeeded

  Jan 29 20:39:23.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:39:23.796
  STEP: deleting the test headless service @ 01/29/24 20:39:23.812
  STEP: Destroying namespace "dns-2073" for this suite. @ 01/29/24 20:39:23.825
• [32.187 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 01/29/24 20:39:23.832
  Jan 29 20:39:23.832: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 20:39:23.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:23.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:23.861
  STEP: Creating service test in namespace statefulset-8945 @ 01/29/24 20:39:23.866
  STEP: Creating statefulset ss in namespace statefulset-8945 @ 01/29/24 20:39:23.872
  Jan 29 20:39:23.884: INFO: Found 0 stateful pods, waiting for 1
  Jan 29 20:39:33.893: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 01/29/24 20:39:33.902
  STEP: updating a scale subresource @ 01/29/24 20:39:33.906
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/29/24 20:39:33.914
  STEP: Patch a scale subresource @ 01/29/24 20:39:33.917
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/29/24 20:39:33.926
  Jan 29 20:39:33.929: INFO: Deleting all statefulset in ns statefulset-8945
  Jan 29 20:39:33.933: INFO: Scaling statefulset ss to 0
  Jan 29 20:39:43.955: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:39:43.959: INFO: Deleting statefulset ss
  Jan 29 20:39:43.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8945" for this suite. @ 01/29/24 20:39:43.98
• [20.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 01/29/24 20:39:43.99
  Jan 29 20:39:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename server-version @ 01/29/24 20:39:43.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:44.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:44.011
  STEP: Request ServerVersion @ 01/29/24 20:39:44.016
  STEP: Confirm major version @ 01/29/24 20:39:44.018
  Jan 29 20:39:44.018: INFO: Major version: 1
  STEP: Confirm minor version @ 01/29/24 20:39:44.018
  Jan 29 20:39:44.018: INFO: cleanMinorVersion: 27
  Jan 29 20:39:44.018: INFO: Minor version: 27+
  Jan 29 20:39:44.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-1185" for this suite. @ 01/29/24 20:39:44.024
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 01/29/24 20:39:44.031
  Jan 29 20:39:44.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:39:44.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:44.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:44.051
  Jan 29 20:39:44.077: INFO: created pod pod-service-account-defaultsa
  Jan 29 20:39:44.077: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jan 29 20:39:44.085: INFO: created pod pod-service-account-mountsa
  Jan 29 20:39:44.085: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jan 29 20:39:44.091: INFO: created pod pod-service-account-nomountsa
  Jan 29 20:39:44.091: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jan 29 20:39:44.097: INFO: created pod pod-service-account-defaultsa-mountspec
  Jan 29 20:39:44.097: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jan 29 20:39:44.102: INFO: created pod pod-service-account-mountsa-mountspec
  Jan 29 20:39:44.102: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jan 29 20:39:44.110: INFO: created pod pod-service-account-nomountsa-mountspec
  Jan 29 20:39:44.110: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jan 29 20:39:44.117: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jan 29 20:39:44.117: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jan 29 20:39:44.123: INFO: created pod pod-service-account-mountsa-nomountspec
  Jan 29 20:39:44.123: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jan 29 20:39:44.152: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jan 29 20:39:44.152: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jan 29 20:39:44.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8759" for this suite. @ 01/29/24 20:39:44.161
• [0.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 01/29/24 20:39:44.195
  Jan 29 20:39:44.195: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 20:39:44.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:44.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:44.226
  STEP: creating a Namespace @ 01/29/24 20:39:44.23
  STEP: patching the Namespace @ 01/29/24 20:39:44.244
  STEP: get the Namespace and ensuring it has the label @ 01/29/24 20:39:44.249
  Jan 29 20:39:44.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4707" for this suite. @ 01/29/24 20:39:44.255
  STEP: Destroying namespace "nspatchtest-5a279687-2582-4931-8dc4-c8bbd88a5511-5083" for this suite. @ 01/29/24 20:39:44.261
• [0.071 seconds]
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 01/29/24 20:39:44.266
  Jan 29 20:39:44.266: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 20:39:44.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:44.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:44.28
  Jan 29 20:39:44.316: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e2f74042-548b-4a02-812a-9ad095f16b47", Controller:(*bool)(0xc004027eda), BlockOwnerDeletion:(*bool)(0xc004027edb)}}
  Jan 29 20:39:44.321: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5dabf1f3-b929-44b5-bdc4-509880a4640a", Controller:(*bool)(0xc00508a10e), BlockOwnerDeletion:(*bool)(0xc00508a10f)}}
  Jan 29 20:39:44.332: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"772bdc6e-c2a2-405a-b27f-b3b044f580cb", Controller:(*bool)(0xc00508a336), BlockOwnerDeletion:(*bool)(0xc00508a337)}}
  Jan 29 20:39:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3045" for this suite. @ 01/29/24 20:39:49.35
• [5.093 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 01/29/24 20:39:49.359
  Jan 29 20:39:49.359: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/29/24 20:39:49.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:49.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:49.374
  STEP: fetching the /apis discovery document @ 01/29/24 20:39:49.377
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 01/29/24 20:39:49.379
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 01/29/24 20:39:49.379
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 01/29/24 20:39:49.379
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 01/29/24 20:39:49.38
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 01/29/24 20:39:49.38
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 01/29/24 20:39:49.382
  Jan 29 20:39:49.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4923" for this suite. @ 01/29/24 20:39:49.387
• [0.147 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 01/29/24 20:39:49.506
  Jan 29 20:39:49.506: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename events @ 01/29/24 20:39:49.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:49.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:49.535
  STEP: creating a test event @ 01/29/24 20:39:49.54
  STEP: listing all events in all namespaces @ 01/29/24 20:39:49.549
  STEP: patching the test event @ 01/29/24 20:39:49.556
  STEP: fetching the test event @ 01/29/24 20:39:49.562
  STEP: updating the test event @ 01/29/24 20:39:49.565
  STEP: getting the test event @ 01/29/24 20:39:49.574
  STEP: deleting the test event @ 01/29/24 20:39:49.577
  STEP: listing all events in all namespaces @ 01/29/24 20:39:49.582
  Jan 29 20:39:49.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3000" for this suite. @ 01/29/24 20:39:49.593
• [0.094 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 01/29/24 20:39:49.6
  Jan 29 20:39:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-runtime @ 01/29/24 20:39:49.602
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:49.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:49.643
  STEP: create the container @ 01/29/24 20:39:49.648
  W0129 20:39:49.656497      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/29/24 20:39:49.656
  STEP: get the container status @ 01/29/24 20:39:52.677
  STEP: the container should be terminated @ 01/29/24 20:39:52.682
  STEP: the termination message should be set @ 01/29/24 20:39:52.682
  Jan 29 20:39:52.682: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 01/29/24 20:39:52.682
  Jan 29 20:39:52.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6616" for this suite. @ 01/29/24 20:39:52.71
• [3.116 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 01/29/24 20:39:52.717
  Jan 29 20:39:52.717: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:39:52.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:39:52.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:39:52.738
  STEP: creating all guestbook components @ 01/29/24 20:39:52.744
  Jan 29 20:39:52.744: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jan 29 20:39:52.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:53.239: INFO: stderr: ""
  Jan 29 20:39:53.239: INFO: stdout: "service/agnhost-replica created\n"
  Jan 29 20:39:53.239: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jan 29 20:39:53.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:53.567: INFO: stderr: ""
  Jan 29 20:39:53.567: INFO: stdout: "service/agnhost-primary created\n"
  Jan 29 20:39:53.567: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jan 29 20:39:53.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:53.904: INFO: stderr: ""
  Jan 29 20:39:53.904: INFO: stdout: "service/frontend created\n"
  Jan 29 20:39:53.904: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jan 29 20:39:53.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:54.215: INFO: stderr: ""
  Jan 29 20:39:54.215: INFO: stdout: "deployment.apps/frontend created\n"
  Jan 29 20:39:54.215: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 29 20:39:54.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:54.518: INFO: stderr: ""
  Jan 29 20:39:54.518: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jan 29 20:39:54.518: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 29 20:39:54.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 create -f -'
  Jan 29 20:39:54.830: INFO: stderr: ""
  Jan 29 20:39:54.830: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 01/29/24 20:39:54.83
  Jan 29 20:39:54.830: INFO: Waiting for all frontend pods to be Running.
  Jan 29 20:39:59.884: INFO: Waiting for frontend to serve content.
  Jan 29 20:39:59.897: INFO: Trying to add a new entry to the guestbook.
  Jan 29 20:39:59.912: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 01/29/24 20:39:59.926
  Jan 29 20:39:59.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.044: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.044: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 01/29/24 20:40:00.044
  Jan 29 20:40:00.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.164: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.164: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/29/24 20:40:00.164
  Jan 29 20:40:00.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.269: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.269: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/29/24 20:40:00.269
  Jan 29 20:40:00.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.364: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.364: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/29/24 20:40:00.364
  Jan 29 20:40:00.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.531: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.531: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/29/24 20:40:00.531
  Jan 29 20:40:00.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-4050 delete --grace-period=0 --force -f -'
  Jan 29 20:40:00.651: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:40:00.652: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jan 29 20:40:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4050" for this suite. @ 01/29/24 20:40:00.658
• [7.950 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 01/29/24 20:40:00.672
  Jan 29 20:40:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:40:00.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:00.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:00.689
  STEP: create deployment with httpd image @ 01/29/24 20:40:00.693
  Jan 29 20:40:00.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9170 create -f -'
  Jan 29 20:40:01.004: INFO: stderr: ""
  Jan 29 20:40:01.004: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 01/29/24 20:40:01.004
  Jan 29 20:40:01.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9170 diff -f -'
  Jan 29 20:40:01.284: INFO: rc: 1
  Jan 29 20:40:01.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9170 delete -f -'
  Jan 29 20:40:01.379: INFO: stderr: ""
  Jan 29 20:40:01.379: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jan 29 20:40:01.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9170" for this suite. @ 01/29/24 20:40:01.384
• [0.718 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 01/29/24 20:40:01.391
  Jan 29 20:40:01.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subpath @ 01/29/24 20:40:01.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:01.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:01.409
  STEP: Setting up data @ 01/29/24 20:40:01.414
  STEP: Creating pod pod-subpath-test-secret-bk7b @ 01/29/24 20:40:01.425
  STEP: Creating a pod to test atomic-volume-subpath @ 01/29/24 20:40:01.425
  STEP: Saw pod success @ 01/29/24 20:40:25.52
  Jan 29 20:40:25.527: INFO: Trying to get logs from node nodeb29 pod pod-subpath-test-secret-bk7b container test-container-subpath-secret-bk7b: <nil>
  STEP: delete the pod @ 01/29/24 20:40:25.553
  STEP: Deleting pod pod-subpath-test-secret-bk7b @ 01/29/24 20:40:25.569
  Jan 29 20:40:25.569: INFO: Deleting pod "pod-subpath-test-secret-bk7b" in namespace "subpath-5975"
  Jan 29 20:40:25.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5975" for this suite. @ 01/29/24 20:40:25.577
• [24.193 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 01/29/24 20:40:25.584
  Jan 29 20:40:25.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-runtime @ 01/29/24 20:40:25.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:25.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:25.604
  STEP: create the container @ 01/29/24 20:40:25.609
  W0129 20:40:25.616914      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 01/29/24 20:40:25.617
  STEP: get the container status @ 01/29/24 20:40:29.643
  STEP: the container should be terminated @ 01/29/24 20:40:29.648
  STEP: the termination message should be set @ 01/29/24 20:40:29.648
  Jan 29 20:40:29.648: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/29/24 20:40:29.648
  Jan 29 20:40:29.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9560" for this suite. @ 01/29/24 20:40:29.67
• [4.093 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 01/29/24 20:40:29.677
  Jan 29 20:40:29.677: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:40:29.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:29.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:29.698
  Jan 29 20:40:29.706: INFO: Got root ca configmap in namespace "svcaccounts-5494"
  Jan 29 20:40:29.713: INFO: Deleted root ca configmap in namespace "svcaccounts-5494"
  STEP: waiting for a new root ca configmap created @ 01/29/24 20:40:30.214
  Jan 29 20:40:30.218: INFO: Recreated root ca configmap in namespace "svcaccounts-5494"
  Jan 29 20:40:30.256: INFO: Updated root ca configmap in namespace "svcaccounts-5494"
  STEP: waiting for the root ca configmap reconciled @ 01/29/24 20:40:30.756
  Jan 29 20:40:30.760: INFO: Reconciled root ca configmap in namespace "svcaccounts-5494"
  Jan 29 20:40:30.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5494" for this suite. @ 01/29/24 20:40:30.765
• [1.093 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 01/29/24 20:40:30.77
  Jan 29 20:40:30.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 20:40:30.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:30.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:30.786
  Jan 29 20:40:30.790: INFO: Creating deployment "webserver-deployment"
  Jan 29 20:40:30.796: INFO: Waiting for observed generation 1
  Jan 29 20:40:32.805: INFO: Waiting for all required pods to come up
  Jan 29 20:40:32.814: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 01/29/24 20:40:32.814
  Jan 29 20:40:32.814: INFO: Waiting for deployment "webserver-deployment" to complete
  Jan 29 20:40:32.822: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jan 29 20:40:32.835: INFO: Updating deployment webserver-deployment
  Jan 29 20:40:32.835: INFO: Waiting for observed generation 2
  Jan 29 20:40:34.844: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jan 29 20:40:34.847: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jan 29 20:40:34.850: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 29 20:40:34.859: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jan 29 20:40:34.859: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jan 29 20:40:34.863: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 29 20:40:34.871: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jan 29 20:40:34.871: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jan 29 20:40:34.880: INFO: Updating deployment webserver-deployment
  Jan 29 20:40:34.881: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jan 29 20:40:34.888: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jan 29 20:40:34.892: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jan 29 20:40:34.932: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2308  87404788-521f-43ea-8592-39e2d6054586 7541 3 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0045549e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2024-01-29 20:40:32 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-29 20:40:34 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jan 29 20:40:34.937: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2308  b5233bed-14c3-4340-bdeb-6a1688a421ed 7536 3 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 87404788-521f-43ea-8592-39e2d6054586 0xc004554f17 0xc004554f18}] [] [{kube-controller-manager Update apps/v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87404788-521f-43ea-8592-39e2d6054586\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004554fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:40:34.937: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jan 29 20:40:34.938: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2308  b2618c33-0019-44e0-a273-0395757d561d 7533 3 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 87404788-521f-43ea-8592-39e2d6054586 0xc004554e27 0xc004554e28}] [] [{kube-controller-manager Update apps/v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87404788-521f-43ea-8592-39e2d6054586\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004554eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:40:34.949: INFO: Pod "webserver-deployment-67bd4bf6dc-2mp6p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2mp6p webserver-deployment-67bd4bf6dc- deployment-2308  b76e1ac0-f267-42aa-8aca-a88439b99d33 7562 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c6fa0 0xc0048c6fa1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tchcl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tchcl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.950: INFO: Pod "webserver-deployment-67bd4bf6dc-4j6p2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4j6p2 webserver-deployment-67bd4bf6dc- deployment-2308  a7e93395-b260-4378-8f55-0b50a59ae95b 7436 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c70d7 0xc0048c70d8}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vr9mw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vr9mw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.24,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://56b26886dcb88d35b6c5acbe9735c46da3e28d47713c905cf025beba3c7de4dd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.950: INFO: Pod "webserver-deployment-67bd4bf6dc-4zjqw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4zjqw webserver-deployment-67bd4bf6dc- deployment-2308  2f09f629-cfb5-481e-960d-090b498f46f5 7561 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c72c0 0xc0048c72c1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbvp6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbvp6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.951: INFO: Pod "webserver-deployment-67bd4bf6dc-5rggm" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5rggm webserver-deployment-67bd4bf6dc- deployment-2308  7776eb19-1270-4931-b831-d87e077f7ac3 7452 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7420 0xc0048c7421}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.87\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs5c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs5c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.87,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7696f009c5efa990c0124801103bcdffd0be6dd3a58093ed42423cc197199fad,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.87,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.951: INFO: Pod "webserver-deployment-67bd4bf6dc-6cnm7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6cnm7 webserver-deployment-67bd4bf6dc- deployment-2308  b5ed72df-56e5-47dd-97ed-14840c1809c7 7566 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7620 0xc0048c7621}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xcct4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xcct4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.951: INFO: Pod "webserver-deployment-67bd4bf6dc-7d97b" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7d97b webserver-deployment-67bd4bf6dc- deployment-2308  3bf800fd-a7f4-418b-abb5-cbee0349eaf5 7544 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7780 0xc0048c7781}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n674r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n674r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.952: INFO: Pod "webserver-deployment-67bd4bf6dc-8t5fs" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8t5fs webserver-deployment-67bd4bf6dc- deployment-2308  1fd2642d-63dd-4ea0-bfdf-cc1467ed35fb 7425 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c78e0 0xc0048c78e1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdlxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdlxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.26,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://00fbb550f44f012ad391cd584329b60f631a8da0e64ec93bbfa4b3ab52f50c42,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.952: INFO: Pod "webserver-deployment-67bd4bf6dc-92hws" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-92hws webserver-deployment-67bd4bf6dc- deployment-2308  6abc32d7-1c41-4aad-aba2-c6aa01902e32 7433 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7ad0 0xc0048c7ad1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vspg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vspg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.23,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://505bb0e847e73d8fde4a8505d497d0ee1cf89c317bbda8885ec1bbf0845c73e8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.23,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.952: INFO: Pod "webserver-deployment-67bd4bf6dc-dc2gc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dc2gc webserver-deployment-67bd4bf6dc- deployment-2308  0ea4d012-e652-4008-9104-f2a7e978e98e 7555 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7cb0 0xc0048c7cb1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ff9lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ff9lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.953: INFO: Pod "webserver-deployment-67bd4bf6dc-fjkh4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fjkh4 webserver-deployment-67bd4bf6dc- deployment-2308  a151c02d-04ac-488e-b12c-be0313a038de 7550 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7de7 0xc0048c7de8}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p52z2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p52z2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.953: INFO: Pod "webserver-deployment-67bd4bf6dc-gkhjk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gkhjk webserver-deployment-67bd4bf6dc- deployment-2308  3461ca17-97ea-47e8-91ed-cb1322272e0d 7431 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc0048c7f50 0xc0048c7f51}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85bwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85bwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.22,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c4ef03c855cf3278c986b6b83ab07865de1119aab43565e3dbc2b0b6f7f6dea3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.22,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.953: INFO: Pod "webserver-deployment-67bd4bf6dc-hswh4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hswh4 webserver-deployment-67bd4bf6dc- deployment-2308  74e33253-5aed-4acf-8364-8c72722d8164 7554 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd2380 0xc003dd2381}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srglc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srglc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.954: INFO: Pod "webserver-deployment-67bd4bf6dc-jr6sk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jr6sk webserver-deployment-67bd4bf6dc- deployment-2308  a6f895f7-c07f-4128-95ad-6fd958140a14 7443 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd2767 0xc003dd2768}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k67s6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k67s6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.88,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://45d205a8c28e168f2a665d9ef7051b10a8a1d6c8025432d50412ce80535041bc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.88,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.954: INFO: Pod "webserver-deployment-67bd4bf6dc-swhtp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-swhtp webserver-deployment-67bd4bf6dc- deployment-2308  ed45f38e-f820-4a46-a476-f0cb5a504091 7553 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd2d80 0xc003dd2d81}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p25cl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p25cl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.954: INFO: Pod "webserver-deployment-67bd4bf6dc-t9xxb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-t9xxb webserver-deployment-67bd4bf6dc- deployment-2308  595f169f-ff7e-4a0a-a6f0-2243e1c08a3b 7558 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3240 0xc003dd3241}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xg8w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xg8w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.955: INFO: Pod "webserver-deployment-67bd4bf6dc-tcxz8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tcxz8 webserver-deployment-67bd4bf6dc- deployment-2308  ae9ede7c-7b28-4367-8a34-4103b57cce7c 7556 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3650 0xc003dd3651}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6f9w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6f9w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.955: INFO: Pod "webserver-deployment-67bd4bf6dc-w7xm4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w7xm4 webserver-deployment-67bd4bf6dc- deployment-2308  537957c2-9603-4286-8aa5-8118265f0cb6 7449 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3867 0xc003dd3868}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcw8x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcw8x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.85,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://97d3bde9de26d8f07e1b4425ece7c4c011715a5661ebc24d1a8a51c66524d075,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.85,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.955: INFO: Pod "webserver-deployment-67bd4bf6dc-wzc7w" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wzc7w webserver-deployment-67bd4bf6dc- deployment-2308  6cbaa1b0-9ba4-44d7-bfbd-e58e5ce764bc 7427 0 2024-01-29 20:40:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3a50 0xc003dd3a51}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.25\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lt475,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lt475,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.25,StartTime:2024-01-29 20:40:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:40:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f60dc1f93995d0c89e06722ab8e943284492a48981c042c07f8714eb9dc23349,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.25,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.956: INFO: Pod "webserver-deployment-67bd4bf6dc-x9mtz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x9mtz webserver-deployment-67bd4bf6dc- deployment-2308  d5aa8595-60a9-47f4-9fbc-83c6fd02b293 7563 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3c30 0xc003dd3c31}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbf8t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbf8t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.956: INFO: Pod "webserver-deployment-67bd4bf6dc-zmd4h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zmd4h webserver-deployment-67bd4bf6dc- deployment-2308  23fe13b2-294e-4bd9-a105-e06f3ad4927a 7537 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc b2618c33-0019-44e0-a273-0395757d561d 0xc003dd3d90 0xc003dd3d91}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2618c33-0019-44e0-a273-0395757d561d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vsp7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vsp7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.956: INFO: Pod "webserver-deployment-7b75d79cf5-746r2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-746r2 webserver-deployment-7b75d79cf5- deployment-2308  1220ca63-8abc-4aa3-b679-0d4333a7fa03 7560 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc003dd3ef0 0xc003dd3ef1}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttmms,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttmms,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.957: INFO: Pod "webserver-deployment-7b75d79cf5-dmt7q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-dmt7q webserver-deployment-7b75d79cf5- deployment-2308  f88a8e19-f720-47f8-889b-aa4de012dcb3 7503 0 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc004672010 0xc004672011}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4mh7s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4mh7s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:,StartTime:2024-01-29 20:40:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.957: INFO: Pod "webserver-deployment-7b75d79cf5-hpljj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hpljj webserver-deployment-7b75d79cf5- deployment-2308  858deb8b-9cc8-4adc-8612-72ee8f0589a5 7477 0 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc004672207 0xc004672208}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79jmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79jmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:,StartTime:2024-01-29 20:40:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.958: INFO: Pod "webserver-deployment-7b75d79cf5-vbhqq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vbhqq webserver-deployment-7b75d79cf5- deployment-2308  dc713b8c-b461-4320-a51d-aef2c350f7fd 7483 0 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc0046723f7 0xc0046723f8}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb4jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb4jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:,StartTime:2024-01-29 20:40:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.958: INFO: Pod "webserver-deployment-7b75d79cf5-vzrw5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vzrw5 webserver-deployment-7b75d79cf5- deployment-2308  652cff83-78a9-4c3a-ad61-3ba7a66638c1 7501 0 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc0046725e7 0xc0046725e8}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmffw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmffw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:,StartTime:2024-01-29 20:40:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.958: INFO: Pod "webserver-deployment-7b75d79cf5-w6g97" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w6g97 webserver-deployment-7b75d79cf5- deployment-2308  2419ed98-674c-477e-9e7d-e85dca8be517 7557 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc004672807 0xc004672808}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlvbk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlvbk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.959: INFO: Pod "webserver-deployment-7b75d79cf5-x7zhq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-x7zhq webserver-deployment-7b75d79cf5- deployment-2308  dfc18a88-e645-4c3e-86ac-c4c05b2fcaa5 7559 0 2024-01-29 20:40:34 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc004672967 0xc004672968}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kb5nb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kb5nb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.959: INFO: Pod "webserver-deployment-7b75d79cf5-x8dps" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-x8dps webserver-deployment-7b75d79cf5- deployment-2308  434cde27-2174-44c6-8fcc-2e628c308cb7 7478 0 2024-01-29 20:40:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 b5233bed-14c3-4340-bdeb-6a1688a421ed 0xc004672ab7 0xc004672ab8}] [] [{kube-controller-manager Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b5233bed-14c3-4340-bdeb-6a1688a421ed\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:40:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gskv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gskv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:40:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:,StartTime:2024-01-29 20:40:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:40:34.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2308" for this suite. @ 01/29/24 20:40:34.966
• [4.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 01/29/24 20:40:34.992
  Jan 29 20:40:34.992: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/29/24 20:40:34.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:35.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:35.011
  Jan 29 20:40:35.015: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:40:35.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9236" for this suite. @ 01/29/24 20:40:35.573
• [0.588 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 01/29/24 20:40:35.582
  Jan 29 20:40:35.582: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename podtemplate @ 01/29/24 20:40:35.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:35.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:35.601
  STEP: Create a pod template @ 01/29/24 20:40:35.607
  STEP: Replace a pod template @ 01/29/24 20:40:35.612
  Jan 29 20:40:35.622: INFO: Found updated podtemplate annotation: "true"

  Jan 29 20:40:35.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-5967" for this suite. @ 01/29/24 20:40:35.626
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 01/29/24 20:40:35.644
  Jan 29 20:40:35.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/29/24 20:40:35.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:35.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:35.659
  STEP: create the container to handle the HTTPGet hook request. @ 01/29/24 20:40:35.668
  STEP: create the pod with lifecycle hook @ 01/29/24 20:40:37.691
  STEP: delete the pod with lifecycle hook @ 01/29/24 20:40:39.714
  STEP: check prestop hook @ 01/29/24 20:40:41.734
  Jan 29 20:40:41.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9572" for this suite. @ 01/29/24 20:40:41.748
• [6.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 01/29/24 20:40:41.756
  Jan 29 20:40:41.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:40:41.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:41.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:41.777
  STEP: creating the pod @ 01/29/24 20:40:41.781
  STEP: setting up watch @ 01/29/24 20:40:41.781
  STEP: submitting the pod to kubernetes @ 01/29/24 20:40:41.884
  STEP: verifying the pod is in kubernetes @ 01/29/24 20:40:41.895
  STEP: verifying pod creation was observed @ 01/29/24 20:40:41.899
  STEP: deleting the pod gracefully @ 01/29/24 20:40:43.915
  STEP: verifying pod deletion was observed @ 01/29/24 20:40:43.924
  Jan 29 20:40:45.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-111" for this suite. @ 01/29/24 20:40:45.221
• [3.470 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 01/29/24 20:40:45.227
  Jan 29 20:40:45.227: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:40:45.229
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:45.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:45.248
  STEP: Counting existing ResourceQuota @ 01/29/24 20:40:45.253
  STEP: Creating a ResourceQuota @ 01/29/24 20:40:50.258
  STEP: Ensuring resource quota status is calculated @ 01/29/24 20:40:50.267
  Jan 29 20:40:52.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2000" for this suite. @ 01/29/24 20:40:52.281
• [7.062 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 01/29/24 20:40:52.29
  Jan 29 20:40:52.290: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:40:52.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:40:52.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:40:52.315
  STEP: Counting existing ResourceQuota @ 01/29/24 20:40:52.319
  STEP: Creating a ResourceQuota @ 01/29/24 20:40:57.324
  STEP: Ensuring resource quota status is calculated @ 01/29/24 20:40:57.332
  STEP: Creating a ReplicaSet @ 01/29/24 20:40:59.339
  STEP: Ensuring resource quota status captures replicaset creation @ 01/29/24 20:40:59.355
  STEP: Deleting a ReplicaSet @ 01/29/24 20:41:01.368
  STEP: Ensuring resource quota status released usage @ 01/29/24 20:41:01.374
  Jan 29 20:41:03.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9031" for this suite. @ 01/29/24 20:41:03.386
• [11.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 01/29/24 20:41:03.394
  Jan 29 20:41:03.394: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 20:41:03.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:03.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:03.417
  Jan 29 20:41:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  W0129 20:41:05.988018      23 warnings.go:70] unknown field "alpha"
  W0129 20:41:05.988071      23 warnings.go:70] unknown field "beta"
  W0129 20:41:05.988082      23 warnings.go:70] unknown field "delta"
  W0129 20:41:05.988093      23 warnings.go:70] unknown field "epsilon"
  W0129 20:41:05.988103      23 warnings.go:70] unknown field "gamma"
  Jan 29 20:41:06.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2402" for this suite. @ 01/29/24 20:41:06.548
• [3.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 01/29/24 20:41:06.556
  Jan 29 20:41:06.556: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename cronjob @ 01/29/24 20:41:06.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:06.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:06.577
  STEP: Creating a cronjob @ 01/29/24 20:41:06.582
  STEP: creating @ 01/29/24 20:41:06.582
  STEP: getting @ 01/29/24 20:41:06.59
  STEP: listing @ 01/29/24 20:41:06.594
  STEP: watching @ 01/29/24 20:41:06.597
  Jan 29 20:41:06.598: INFO: starting watch
  STEP: cluster-wide listing @ 01/29/24 20:41:06.6
  STEP: cluster-wide watching @ 01/29/24 20:41:06.603
  Jan 29 20:41:06.604: INFO: starting watch
  STEP: patching @ 01/29/24 20:41:06.606
  STEP: updating @ 01/29/24 20:41:06.613
  Jan 29 20:41:06.622: INFO: waiting for watch events with expected annotations
  Jan 29 20:41:06.622: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/29/24 20:41:06.622
  STEP: updating /status @ 01/29/24 20:41:06.629
  STEP: get /status @ 01/29/24 20:41:06.638
  STEP: deleting @ 01/29/24 20:41:06.642
  STEP: deleting a collection @ 01/29/24 20:41:06.656
  Jan 29 20:41:06.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9347" for this suite. @ 01/29/24 20:41:06.673
• [0.123 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 01/29/24 20:41:06.68
  Jan 29 20:41:06.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 20:41:06.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:06.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:06.7
  STEP: Given a ReplicationController is created @ 01/29/24 20:41:06.705
  STEP: When the matched label of one of its pods change @ 01/29/24 20:41:06.713
  Jan 29 20:41:06.719: INFO: Pod name pod-release: Found 0 pods out of 1
  Jan 29 20:41:11.722: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/29/24 20:41:11.734
  Jan 29 20:41:12.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7910" for this suite. @ 01/29/24 20:41:12.751
• [6.079 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 01/29/24 20:41:12.759
  Jan 29 20:41:12.759: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 20:41:12.76
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:12.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:12.779
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 01/29/24 20:41:12.784
  Jan 29 20:41:12.784: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:41:14.258: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:41:20.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9337" for this suite. @ 01/29/24 20:41:20.205
• [7.452 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 01/29/24 20:41:20.212
  Jan 29 20:41:20.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:41:20.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:20.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:20.234
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:41:20.239
  STEP: Saw pod success @ 01/29/24 20:41:24.263
  Jan 29 20:41:24.267: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-a8d9d5f3-219e-4fa0-aca5-8d4f83042449 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:41:24.296
  Jan 29 20:41:24.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1520" for this suite. @ 01/29/24 20:41:24.316
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 01/29/24 20:41:24.325
  Jan 29 20:41:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename ingress @ 01/29/24 20:41:24.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:24.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:24.347
  STEP: getting /apis @ 01/29/24 20:41:24.352
  STEP: getting /apis/networking.k8s.io @ 01/29/24 20:41:24.359
  STEP: getting /apis/networking.k8s.iov1 @ 01/29/24 20:41:24.361
  STEP: creating @ 01/29/24 20:41:24.363
  STEP: getting @ 01/29/24 20:41:24.379
  STEP: listing @ 01/29/24 20:41:24.383
  STEP: watching @ 01/29/24 20:41:24.386
  Jan 29 20:41:24.386: INFO: starting watch
  STEP: cluster-wide listing @ 01/29/24 20:41:24.388
  STEP: cluster-wide watching @ 01/29/24 20:41:24.392
  Jan 29 20:41:24.392: INFO: starting watch
  STEP: patching @ 01/29/24 20:41:24.394
  STEP: updating @ 01/29/24 20:41:24.4
  Jan 29 20:41:24.407: INFO: waiting for watch events with expected annotations
  Jan 29 20:41:24.407: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/29/24 20:41:24.407
  STEP: updating /status @ 01/29/24 20:41:24.412
  STEP: get /status @ 01/29/24 20:41:24.424
  STEP: deleting @ 01/29/24 20:41:24.427
  STEP: deleting a collection @ 01/29/24 20:41:24.44
  Jan 29 20:41:24.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9541" for this suite. @ 01/29/24 20:41:24.458
• [0.138 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 01/29/24 20:41:24.463
  Jan 29 20:41:24.463: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/29/24 20:41:24.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:24.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:24.48
  Jan 29 20:41:24.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:41:27.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2707" for this suite. @ 01/29/24 20:41:27.604
• [3.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 01/29/24 20:41:27.614
  Jan 29 20:41:27.614: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 20:41:27.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:27.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:27.634
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8843.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8843.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 01/29/24 20:41:27.638
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8843.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8843.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 01/29/24 20:41:27.638
  STEP: creating a pod to probe /etc/hosts @ 01/29/24 20:41:27.638
  STEP: submitting the pod to kubernetes @ 01/29/24 20:41:27.638
  STEP: retrieving the pod @ 01/29/24 20:41:29.657
  STEP: looking for the results for each expected name from probers @ 01/29/24 20:41:29.661
  Jan 29 20:41:29.695: INFO: DNS probes using dns-8843/dns-test-95bbe257-be57-498c-a01c-8a8613e49277 succeeded

  Jan 29 20:41:29.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:41:29.699
  STEP: Destroying namespace "dns-8843" for this suite. @ 01/29/24 20:41:29.712
• [2.105 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 01/29/24 20:41:29.72
  Jan 29 20:41:29.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename containers @ 01/29/24 20:41:29.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:29.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:29.737
  STEP: Creating a pod to test override all @ 01/29/24 20:41:29.742
  STEP: Saw pod success @ 01/29/24 20:41:33.767
  Jan 29 20:41:33.771: INFO: Trying to get logs from node nodea08 pod client-containers-3efcb9a4-68a5-4dea-ad0b-784c657c9169 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:41:33.78
  Jan 29 20:41:33.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9012" for this suite. @ 01/29/24 20:41:33.8
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 01/29/24 20:41:33.808
  Jan 29 20:41:33.808: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:41:33.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:33.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:33.827
  STEP: Creating configMap with name configmap-test-volume-f6401c9c-7f15-42b8-a171-5c9f36d6e97c @ 01/29/24 20:41:33.832
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:41:33.839
  STEP: Saw pod success @ 01/29/24 20:41:37.864
  Jan 29 20:41:37.868: INFO: Trying to get logs from node nodeb29 pod pod-configmaps-2a23936b-524e-4988-b880-4b43b43a0d0e container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:41:37.882
  Jan 29 20:41:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6097" for this suite. @ 01/29/24 20:41:37.906
• [4.103 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 01/29/24 20:41:37.911
  Jan 29 20:41:37.911: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 20:41:37.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:37.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:37.932
  STEP: Creating a test namespace @ 01/29/24 20:41:37.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:37.949
  STEP: Creating a pod in the namespace @ 01/29/24 20:41:37.953
  STEP: Waiting for the pod to have running status @ 01/29/24 20:41:37.962
  STEP: Deleting the namespace @ 01/29/24 20:41:39.971
  STEP: Waiting for the namespace to be removed. @ 01/29/24 20:41:39.977
  STEP: Recreating the namespace @ 01/29/24 20:41:50.983
  STEP: Verifying there are no pods in the namespace @ 01/29/24 20:41:50.999
  Jan 29 20:41:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1757" for this suite. @ 01/29/24 20:41:51.009
  STEP: Destroying namespace "nsdeletetest-11" for this suite. @ 01/29/24 20:41:51.015
  Jan 29 20:41:51.019: INFO: Namespace nsdeletetest-11 was already deleted
  STEP: Destroying namespace "nsdeletetest-7045" for this suite. @ 01/29/24 20:41:51.019
• [13.116 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 01/29/24 20:41:51.028
  Jan 29 20:41:51.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:41:51.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:41:51.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:41:51.056
  STEP: Creating resourceQuota "e2e-rq-status-hrv9z" @ 01/29/24 20:41:51.063
  Jan 29 20:41:51.071: INFO: Resource quota "e2e-rq-status-hrv9z" reports spec: hard cpu limit of 500m
  Jan 29 20:41:51.071: INFO: Resource quota "e2e-rq-status-hrv9z" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-hrv9z" /status @ 01/29/24 20:41:51.071
  STEP: Confirm /status for "e2e-rq-status-hrv9z" resourceQuota via watch @ 01/29/24 20:41:51.08
  Jan 29 20:41:51.082: INFO: observed resourceQuota "e2e-rq-status-hrv9z" in namespace "resourcequota-6490" with hard status: v1.ResourceList(nil)
  Jan 29 20:41:51.082: INFO: Found resourceQuota "e2e-rq-status-hrv9z" in namespace "resourcequota-6490" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 29 20:41:51.082: INFO: ResourceQuota "e2e-rq-status-hrv9z" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 01/29/24 20:41:51.086
  Jan 29 20:41:51.092: INFO: Resource quota "e2e-rq-status-hrv9z" reports spec: hard cpu limit of 1
  Jan 29 20:41:51.092: INFO: Resource quota "e2e-rq-status-hrv9z" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-hrv9z" /status @ 01/29/24 20:41:51.092
  STEP: Confirm /status for "e2e-rq-status-hrv9z" resourceQuota via watch @ 01/29/24 20:41:51.099
  Jan 29 20:41:51.101: INFO: observed resourceQuota "e2e-rq-status-hrv9z" in namespace "resourcequota-6490" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 29 20:41:51.102: INFO: Found resourceQuota "e2e-rq-status-hrv9z" in namespace "resourcequota-6490" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jan 29 20:41:51.102: INFO: ResourceQuota "e2e-rq-status-hrv9z" /status was patched
  STEP: Get "e2e-rq-status-hrv9z" /status @ 01/29/24 20:41:51.102
  Jan 29 20:41:51.106: INFO: Resourcequota "e2e-rq-status-hrv9z" reports status: hard cpu of 1
  Jan 29 20:41:51.106: INFO: Resourcequota "e2e-rq-status-hrv9z" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-hrv9z" /status before checking Spec is unchanged @ 01/29/24 20:41:51.11
  Jan 29 20:41:51.117: INFO: Resourcequota "e2e-rq-status-hrv9z" reports status: hard cpu of 2
  Jan 29 20:41:51.117: INFO: Resourcequota "e2e-rq-status-hrv9z" reports status: hard memory of 2Gi
  Jan 29 20:41:51.119: INFO: Found resourceQuota "e2e-rq-status-hrv9z" in namespace "resourcequota-6490" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jan 29 20:42:16.132: INFO: ResourceQuota "e2e-rq-status-hrv9z" Spec was unchanged and /status reset
  Jan 29 20:42:16.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6490" for this suite. @ 01/29/24 20:42:16.139
• [25.119 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 01/29/24 20:42:16.147
  Jan 29 20:42:16.147: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 20:42:16.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:16.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:16.165
  Jan 29 20:42:38.257: INFO: Container started at 2024-01-29 20:42:16 +0000 UTC, pod became ready at 2024-01-29 20:42:36 +0000 UTC
  Jan 29 20:42:38.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1570" for this suite. @ 01/29/24 20:42:38.264
• [22.125 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 01/29/24 20:42:38.275
  Jan 29 20:42:38.275: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 20:42:38.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:38.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:38.295
  STEP: Creating a pod to test substitution in container's args @ 01/29/24 20:42:38.3
  STEP: Saw pod success @ 01/29/24 20:42:42.323
  Jan 29 20:42:42.327: INFO: Trying to get logs from node nodea08 pod var-expansion-fc0422c5-df28-462e-a365-72704f40a563 container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 20:42:42.337
  Jan 29 20:42:42.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5058" for this suite. @ 01/29/24 20:42:42.36
• [4.092 seconds]
------------------------------
SSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 01/29/24 20:42:42.367
  Jan 29 20:42:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename lease-test @ 01/29/24 20:42:42.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:42.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:42.389
  Jan 29 20:42:42.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6368" for this suite. @ 01/29/24 20:42:42.462
• [0.101 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 01/29/24 20:42:42.469
  Jan 29 20:42:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:42:42.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:42.486
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:42.49
  Jan 29 20:42:42.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: creating the pod @ 01/29/24 20:42:42.496
  STEP: submitting the pod to kubernetes @ 01/29/24 20:42:42.497
  Jan 29 20:42:44.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8418" for this suite. @ 01/29/24 20:42:44.547
• [2.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 01/29/24 20:42:44.557
  Jan 29 20:42:44.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 20:42:44.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:44.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:44.579
  STEP: Updating Namespace "namespaces-5596" @ 01/29/24 20:42:44.585
  Jan 29 20:42:44.595: INFO: Namespace "namespaces-5596" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"31722b4b-774a-433c-b403-fd73cf822c96", "kubernetes.io/metadata.name":"namespaces-5596", "namespaces-5596":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jan 29 20:42:44.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5596" for this suite. @ 01/29/24 20:42:44.6
• [0.051 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 01/29/24 20:42:44.609
  Jan 29 20:42:44.609: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-pred @ 01/29/24 20:42:44.611
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:44.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:44.631
  Jan 29 20:42:44.636: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 29 20:42:44.644: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 20:42:44.648: INFO: 
  Logging pods the apiserver thinks is on node nodea08 before test
  Jan 29 20:42:44.656: INFO: test-webserver-c24fd2b6-899c-4878-b3f9-edaae0fd851a from container-probe-1570 started at 2024-01-29 20:42:16 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container test-webserver ready: false, restart count 0
  Jan 29 20:42:44.656: INFO: kube-flannel-ds-444bq from kube-flannel started at 2024-01-29 20:38:51 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: kube-proxy-5wdv6 from kube-system started at 2024-01-29 20:10:26 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: pod-logs-websocket-a642ec6e-55fe-4269-81d3-ba1d6415679a from pods-8418 started at 2024-01-29 20:42:42 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container main ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: sonobuoy from sonobuoy started at 2024-01-29 20:16:38 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: sonobuoy-e2e-job-4fcfaf908eac4396 from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container e2e ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 20:42:44.656: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 29 20:42:44.656: INFO: 
  Logging pods the apiserver thinks is on node nodeb29 before test
  Jan 29 20:42:44.663: INFO: kube-flannel-ds-4qmbv from kube-flannel started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.663: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 20:42:44.663: INFO: kube-proxy-6xx6l from kube-system started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 20:42:44.663: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 20:42:44.663: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-5zbgb from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 20:42:44.663: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 20:42:44.663: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/29/24 20:42:44.664
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/29/24 20:42:46.686
  STEP: Trying to apply a random label on the found node. @ 01/29/24 20:42:46.702
  STEP: verifying the node has the label kubernetes.io/e2e-4c912eee-026a-414b-b873-4652da0a184d 42 @ 01/29/24 20:42:46.717
  STEP: Trying to relaunch the pod, now with labels. @ 01/29/24 20:42:46.721
  STEP: removing the label kubernetes.io/e2e-4c912eee-026a-414b-b873-4652da0a184d off the node nodea08 @ 01/29/24 20:42:48.739
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-4c912eee-026a-414b-b873-4652da0a184d @ 01/29/24 20:42:48.755
  Jan 29 20:42:48.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5919" for this suite. @ 01/29/24 20:42:48.764
• [4.160 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 01/29/24 20:42:48.77
  Jan 29 20:42:48.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:42:48.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:48.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:48.79
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/29/24 20:42:48.794
  STEP: Saw pod success @ 01/29/24 20:42:52.824
  Jan 29 20:42:52.828: INFO: Trying to get logs from node nodea08 pod pod-7440dc68-bf9b-4bce-9bf8-3d43186ba0fd container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:42:52.838
  Jan 29 20:42:52.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1262" for this suite. @ 01/29/24 20:42:52.861
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 01/29/24 20:42:52.87
  Jan 29 20:42:52.870: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:42:52.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:52.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:52.891
  STEP: Creating configMap configmap-1737/configmap-test-dbba6e08-df61-4a80-957f-5e00de9607cd @ 01/29/24 20:42:52.895
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:42:52.901
  STEP: Saw pod success @ 01/29/24 20:42:56.925
  Jan 29 20:42:56.930: INFO: Trying to get logs from node nodeb29 pod pod-configmaps-eadd3261-0237-4bb3-ae34-43653bf9aa11 container env-test: <nil>
  STEP: delete the pod @ 01/29/24 20:42:56.941
  Jan 29 20:42:56.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1737" for this suite. @ 01/29/24 20:42:56.965
• [4.101 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 01/29/24 20:42:56.972
  Jan 29 20:42:56.972: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 20:42:56.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:42:56.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:42:56.996
  STEP: Creating service test in namespace statefulset-3168 @ 01/29/24 20:42:57.002
  STEP: Creating stateful set ss in namespace statefulset-3168 @ 01/29/24 20:42:57.008
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3168 @ 01/29/24 20:42:57.019
  Jan 29 20:42:57.023: INFO: Found 0 stateful pods, waiting for 1
  Jan 29 20:43:07.031: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 01/29/24 20:43:07.031
  Jan 29 20:43:07.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 20:43:07.277: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 20:43:07.277: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 20:43:07.277: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 20:43:07.283: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jan 29 20:43:17.289: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 20:43:17.289: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:43:17.309: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
  Jan 29 20:43:17.309: INFO: ss-0  nodea08  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC  }]
  Jan 29 20:43:17.309: INFO: 
  Jan 29 20:43:17.309: INFO: StatefulSet ss has not reached scale 3, at 1
  Jan 29 20:43:18.316: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993669386s
  Jan 29 20:43:19.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987061344s
  Jan 29 20:43:20.332: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980126467s
  Jan 29 20:43:21.339: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.971245526s
  Jan 29 20:43:22.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963020001s
  Jan 29 20:43:23.354: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.956738137s
  Jan 29 20:43:24.366: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.948656287s
  Jan 29 20:43:25.372: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936895968s
  Jan 29 20:43:26.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 931.539385ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3168 @ 01/29/24 20:43:27.379
  Jan 29 20:43:27.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 20:43:27.623: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 20:43:27.623: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 20:43:27.623: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 20:43:27.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 20:43:27.845: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 29 20:43:27.845: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 20:43:27.845: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 20:43:27.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 20:43:28.078: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 29 20:43:28.078: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 20:43:28.078: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 20:43:28.084: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Jan 29 20:43:38.092: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:43:38.092: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 20:43:38.092: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 01/29/24 20:43:38.092
  Jan 29 20:43:38.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 20:43:38.334: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 20:43:38.334: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 20:43:38.334: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 20:43:38.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 20:43:38.563: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 20:43:38.564: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 20:43:38.564: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 20:43:38.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-3168 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 20:43:38.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 20:43:38.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 20:43:38.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 20:43:38.764: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:43:38.769: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  Jan 29 20:43:48.782: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 20:43:48.782: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 20:43:48.782: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 20:43:48.798: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
  Jan 29 20:43:48.798: INFO: ss-0  nodea08  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC  }]
  Jan 29 20:43:48.798: INFO: ss-1  nodea08  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC  }]
  Jan 29 20:43:48.798: INFO: ss-2  nodeb29  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC  }]
  Jan 29 20:43:48.799: INFO: 
  Jan 29 20:43:48.799: INFO: StatefulSet ss has not reached scale 0, at 3
  Jan 29 20:43:49.804: INFO: POD   NODE     PHASE      GRACE  CONDITIONS
  Jan 29 20:43:49.804: INFO: ss-0  nodea08  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:42:57 +0000 UTC  }]
  Jan 29 20:43:49.804: INFO: ss-1  nodea08  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:38 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:43:17 +0000 UTC  }]
  Jan 29 20:43:49.804: INFO: 
  Jan 29 20:43:49.804: INFO: StatefulSet ss has not reached scale 0, at 2
  Jan 29 20:43:50.809: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989034567s
  Jan 29 20:43:51.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.983819266s
  Jan 29 20:43:52.820: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.978326509s
  Jan 29 20:43:53.827: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972719882s
  Jan 29 20:43:54.832: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966193127s
  Jan 29 20:43:55.838: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.960909094s
  Jan 29 20:43:56.843: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.955213074s
  Jan 29 20:43:57.848: INFO: Verifying statefulset ss doesn't scale past 0 for another 949.819104ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3168 @ 01/29/24 20:43:58.849
  Jan 29 20:43:58.854: INFO: Scaling statefulset ss to 0
  Jan 29 20:43:58.867: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:43:58.871: INFO: Deleting all statefulset in ns statefulset-3168
  Jan 29 20:43:58.874: INFO: Scaling statefulset ss to 0
  Jan 29 20:43:58.885: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 20:43:58.889: INFO: Deleting statefulset ss
  Jan 29 20:43:58.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3168" for this suite. @ 01/29/24 20:43:58.906
• [61.940 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 01/29/24 20:43:58.912
  Jan 29 20:43:58.913: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename watch @ 01/29/24 20:43:58.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:43:58.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:43:58.933
  STEP: getting a starting resourceVersion @ 01/29/24 20:43:58.937
  STEP: starting a background goroutine to produce watch events @ 01/29/24 20:43:58.94
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 01/29/24 20:43:58.94
  Jan 29 20:44:01.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2354" for this suite. @ 01/29/24 20:44:01.77
• [2.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 01/29/24 20:44:01.824
  Jan 29 20:44:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/29/24 20:44:01.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:01.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:01.844
  STEP: create the container to handle the HTTPGet hook request. @ 01/29/24 20:44:01.854
  STEP: create the pod with lifecycle hook @ 01/29/24 20:44:03.879
  STEP: check poststart hook @ 01/29/24 20:44:05.906
  STEP: delete the pod with lifecycle hook @ 01/29/24 20:44:05.916
  Jan 29 20:44:07.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1165" for this suite. @ 01/29/24 20:44:07.943
• [6.128 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 01/29/24 20:44:07.953
  Jan 29 20:44:07.953: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:44:07.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:07.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:07.974
  Jan 29 20:44:07.979: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: creating the pod @ 01/29/24 20:44:07.98
  STEP: submitting the pod to kubernetes @ 01/29/24 20:44:07.98
  Jan 29 20:44:10.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2782" for this suite. @ 01/29/24 20:44:10.158
• [2.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 01/29/24 20:44:10.167
  Jan 29 20:44:10.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename runtimeclass @ 01/29/24 20:44:10.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:10.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:10.19
  Jan 29 20:44:12.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7764" for this suite. @ 01/29/24 20:44:12.232
• [2.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 01/29/24 20:44:12.242
  Jan 29 20:44:12.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:44:12.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:12.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:12.262
  STEP: creating pod @ 01/29/24 20:44:12.266
  Jan 29 20:44:14.293: INFO: Pod pod-hostip-8012e949-f647-4359-bbff-5a533d518265 has hostIP: 192.168.100.129
  Jan 29 20:44:14.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9873" for this suite. @ 01/29/24 20:44:14.299
• [2.065 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 01/29/24 20:44:14.308
  Jan 29 20:44:14.308: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/29/24 20:44:14.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:14.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:14.329
  Jan 29 20:44:16.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 01/29/24 20:44:16.376
  STEP: Cleaning up the configmap @ 01/29/24 20:44:16.382
  STEP: Cleaning up the pod @ 01/29/24 20:44:16.388
  STEP: Destroying namespace "emptydir-wrapper-6124" for this suite. @ 01/29/24 20:44:16.401
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 01/29/24 20:44:16.409
  Jan 29 20:44:16.410: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 20:44:16.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:16.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:16.431
  STEP: Creating a pod to test downward api env vars @ 01/29/24 20:44:16.436
  STEP: Saw pod success @ 01/29/24 20:44:20.46
  Jan 29 20:44:20.463: INFO: Trying to get logs from node nodea08 pod downward-api-26e78819-7e11-4eab-8e8b-2a1068e5e94e container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 20:44:20.472
  Jan 29 20:44:20.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1723" for this suite. @ 01/29/24 20:44:20.489
• [4.084 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 01/29/24 20:44:20.494
  Jan 29 20:44:20.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 20:44:20.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:20.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:20.512
  STEP: Creating secret with name secret-test-6f716a0b-9b40-48bc-99f2-8173b335f71a @ 01/29/24 20:44:20.531
  STEP: Creating a pod to test consume secrets @ 01/29/24 20:44:20.535
  STEP: Saw pod success @ 01/29/24 20:44:24.556
  Jan 29 20:44:24.559: INFO: Trying to get logs from node nodea08 pod pod-secrets-359c8a86-962c-44ef-b393-8fb33bb831fc container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:44:24.567
  Jan 29 20:44:24.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7582" for this suite. @ 01/29/24 20:44:24.584
  STEP: Destroying namespace "secret-namespace-7937" for this suite. @ 01/29/24 20:44:24.589
• [4.099 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 01/29/24 20:44:24.595
  Jan 29 20:44:24.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:44:24.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:24.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:24.611
  STEP: Creating the pod @ 01/29/24 20:44:24.615
  Jan 29 20:44:27.163: INFO: Successfully updated pod "annotationupdatee8a44f58-8856-4cb5-b563-f67e67b7c9d1"
  Jan 29 20:44:29.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9984" for this suite. @ 01/29/24 20:44:29.191
• [4.604 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 01/29/24 20:44:29.199
  Jan 29 20:44:29.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:44:29.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:29.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:29.22
  STEP: creating a ServiceAccount @ 01/29/24 20:44:29.225
  STEP: watching for the ServiceAccount to be added @ 01/29/24 20:44:29.233
  STEP: patching the ServiceAccount @ 01/29/24 20:44:29.237
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 01/29/24 20:44:29.244
  STEP: deleting the ServiceAccount @ 01/29/24 20:44:29.248
  Jan 29 20:44:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9442" for this suite. @ 01/29/24 20:44:29.262
• [0.069 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 01/29/24 20:44:29.269
  Jan 29 20:44:29.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption @ 01/29/24 20:44:29.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:44:29.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:44:29.287
  Jan 29 20:44:29.304: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 29 20:45:29.338: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/29/24 20:45:29.342
  Jan 29 20:45:29.342: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/29/24 20:45:29.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:29.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:29.366
  Jan 29 20:45:29.388: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jan 29 20:45:29.392: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jan 29 20:45:29.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 20:45:29.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-154" for this suite. @ 01/29/24 20:45:29.464
  STEP: Destroying namespace "sched-preemption-7916" for this suite. @ 01/29/24 20:45:29.472
• [60.209 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 01/29/24 20:45:29.478
  Jan 29 20:45:29.478: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:45:29.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:29.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:29.496
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/29/24 20:45:29.501
  Jan 29 20:45:29.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1907 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jan 29 20:45:29.624: INFO: stderr: ""
  Jan 29 20:45:29.624: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/29/24 20:45:29.624
  Jan 29 20:45:29.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1907 delete pods e2e-test-httpd-pod'
  Jan 29 20:45:31.396: INFO: stderr: ""
  Jan 29 20:45:31.396: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 29 20:45:31.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1907" for this suite. @ 01/29/24 20:45:31.401
• [1.929 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 01/29/24 20:45:31.409
  Jan 29 20:45:31.409: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 20:45:31.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:31.424
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:31.429
  Jan 29 20:45:31.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 01/29/24 20:45:32.853
  Jan 29 20:45:32.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 create -f -'
  Jan 29 20:45:33.643: INFO: stderr: ""
  Jan 29 20:45:33.643: INFO: stdout: "e2e-test-crd-publish-openapi-4646-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 29 20:45:33.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 delete e2e-test-crd-publish-openapi-4646-crds test-foo'
  Jan 29 20:45:33.748: INFO: stderr: ""
  Jan 29 20:45:33.748: INFO: stdout: "e2e-test-crd-publish-openapi-4646-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jan 29 20:45:33.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 apply -f -'
  Jan 29 20:45:34.033: INFO: stderr: ""
  Jan 29 20:45:34.033: INFO: stdout: "e2e-test-crd-publish-openapi-4646-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 29 20:45:34.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 delete e2e-test-crd-publish-openapi-4646-crds test-foo'
  Jan 29 20:45:34.132: INFO: stderr: ""
  Jan 29 20:45:34.132: INFO: stdout: "e2e-test-crd-publish-openapi-4646-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 01/29/24 20:45:34.132
  Jan 29 20:45:34.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 create -f -'
  Jan 29 20:45:34.410: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 01/29/24 20:45:34.41
  Jan 29 20:45:34.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 create -f -'
  Jan 29 20:45:34.684: INFO: rc: 1
  Jan 29 20:45:34.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 apply -f -'
  Jan 29 20:45:34.956: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 01/29/24 20:45:34.956
  Jan 29 20:45:34.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 create -f -'
  Jan 29 20:45:35.218: INFO: rc: 1
  Jan 29 20:45:35.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 --namespace=crd-publish-openapi-3887 apply -f -'
  Jan 29 20:45:35.508: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 01/29/24 20:45:35.508
  Jan 29 20:45:35.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 explain e2e-test-crd-publish-openapi-4646-crds'
  Jan 29 20:45:35.771: INFO: stderr: ""
  Jan 29 20:45:35.771: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4646-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 01/29/24 20:45:35.772
  Jan 29 20:45:35.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 explain e2e-test-crd-publish-openapi-4646-crds.metadata'
  Jan 29 20:45:36.045: INFO: stderr: ""
  Jan 29 20:45:36.045: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4646-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jan 29 20:45:36.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 explain e2e-test-crd-publish-openapi-4646-crds.spec'
  Jan 29 20:45:36.309: INFO: stderr: ""
  Jan 29 20:45:36.309: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4646-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jan 29 20:45:36.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 explain e2e-test-crd-publish-openapi-4646-crds.spec.bars'
  Jan 29 20:45:36.579: INFO: stderr: ""
  Jan 29 20:45:36.580: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4646-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 01/29/24 20:45:36.581
  Jan 29 20:45:36.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-3887 explain e2e-test-crd-publish-openapi-4646-crds.spec.bars2'
  Jan 29 20:45:36.832: INFO: rc: 1
  Jan 29 20:45:38.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3887" for this suite. @ 01/29/24 20:45:38.28
• [6.878 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 01/29/24 20:45:38.289
  Jan 29 20:45:38.289: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 20:45:38.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:38.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:38.308
  STEP: Setting up server cert @ 01/29/24 20:45:38.33
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 20:45:38.797
  STEP: Deploying the webhook pod @ 01/29/24 20:45:38.807
  STEP: Wait for the deployment to be ready @ 01/29/24 20:45:38.82
  Jan 29 20:45:38.829: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 20:45:40.844
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 20:45:40.859
  Jan 29 20:45:41.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 01/29/24 20:45:41.867
  STEP: Creating a custom resource definition that should be denied by the webhook @ 01/29/24 20:45:41.893
  Jan 29 20:45:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:45:41.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1928" for this suite. @ 01/29/24 20:45:41.965
  STEP: Destroying namespace "webhook-markers-4002" for this suite. @ 01/29/24 20:45:41.971
• [3.687 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 01/29/24 20:45:41.978
  Jan 29 20:45:41.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption @ 01/29/24 20:45:41.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:41.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:41.998
  STEP: Creating a pdb that targets all three pods in a test replica set @ 01/29/24 20:45:42.003
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:45:42.009
  STEP: First trying to evict a pod which shouldn't be evictable @ 01/29/24 20:45:44.026
  STEP: Waiting for all pods to be running @ 01/29/24 20:45:44.026
  Jan 29 20:45:44.030: INFO: pods: 0 < 3
  STEP: locating a running pod @ 01/29/24 20:45:46.039
  STEP: Updating the pdb to allow a pod to be evicted @ 01/29/24 20:45:46.051
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:45:46.062
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/29/24 20:45:48.071
  STEP: Waiting for all pods to be running @ 01/29/24 20:45:48.071
  STEP: Waiting for the pdb to observed all healthy pods @ 01/29/24 20:45:48.076
  STEP: Patching the pdb to disallow a pod to be evicted @ 01/29/24 20:45:48.104
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:45:48.127
  STEP: Waiting for all pods to be running @ 01/29/24 20:45:50.137
  STEP: locating a running pod @ 01/29/24 20:45:50.142
  STEP: Deleting the pdb to allow a pod to be evicted @ 01/29/24 20:45:50.154
  STEP: Waiting for the pdb to be deleted @ 01/29/24 20:45:50.161
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/29/24 20:45:50.164
  STEP: Waiting for all pods to be running @ 01/29/24 20:45:50.164
  Jan 29 20:45:50.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5241" for this suite. @ 01/29/24 20:45:50.187
• [8.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 01/29/24 20:45:50.198
  Jan 29 20:45:50.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:45:50.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:50.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:50.216
  STEP: Creating projection with secret that has name projected-secret-test-5dd68ec2-4753-4105-88e3-0180bbd61945 @ 01/29/24 20:45:50.221
  STEP: Creating a pod to test consume secrets @ 01/29/24 20:45:50.226
  STEP: Saw pod success @ 01/29/24 20:45:54.253
  Jan 29 20:45:54.257: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-03dc268f-8f13-4c38-b46f-00c44450d406 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:45:54.267
  Jan 29 20:45:54.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3762" for this suite. @ 01/29/24 20:45:54.292
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 01/29/24 20:45:54.307
  Jan 29 20:45:54.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption @ 01/29/24 20:45:54.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:54.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:54.33
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:45:54.34
  STEP: Waiting for all pods to be running @ 01/29/24 20:45:56.378
  Jan 29 20:45:56.382: INFO: running pods: 0 < 3
  Jan 29 20:45:58.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4748" for this suite. @ 01/29/24 20:45:58.4
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 01/29/24 20:45:58.409
  Jan 29 20:45:58.409: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:45:58.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:58.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:58.429
  STEP: creating an Endpoint @ 01/29/24 20:45:58.438
  STEP: waiting for available Endpoint @ 01/29/24 20:45:58.443
  STEP: listing all Endpoints @ 01/29/24 20:45:58.446
  STEP: updating the Endpoint @ 01/29/24 20:45:58.449
  STEP: fetching the Endpoint @ 01/29/24 20:45:58.456
  STEP: patching the Endpoint @ 01/29/24 20:45:58.46
  STEP: fetching the Endpoint @ 01/29/24 20:45:58.471
  STEP: deleting the Endpoint by Collection @ 01/29/24 20:45:58.475
  STEP: waiting for Endpoint deletion @ 01/29/24 20:45:58.483
  STEP: fetching the Endpoint @ 01/29/24 20:45:58.486
  Jan 29 20:45:58.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-784" for this suite. @ 01/29/24 20:45:58.495
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 01/29/24 20:45:58.503
  Jan 29 20:45:58.503: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename containers @ 01/29/24 20:45:58.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:45:58.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:45:58.522
  STEP: Creating a pod to test override arguments @ 01/29/24 20:45:58.527
  STEP: Saw pod success @ 01/29/24 20:46:02.549
  Jan 29 20:46:02.554: INFO: Trying to get logs from node nodea08 pod client-containers-9b679b3d-ed6a-42be-909f-26b2e49531f0 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:46:02.563
  Jan 29 20:46:02.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5290" for this suite. @ 01/29/24 20:46:02.584
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 01/29/24 20:46:02.591
  Jan 29 20:46:02.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 20:46:02.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:46:02.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:46:02.61
  STEP: Creating a test headless service @ 01/29/24 20:46:02.615
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5480.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5480.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 158.207.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.207.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.207.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.207.158_tcp@PTR;sleep 1; done
   @ 01/29/24 20:46:02.638
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5480.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5480.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5480.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5480.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5480.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 158.207.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.207.158_udp@PTR;check="$$(dig +tcp +noall +answer +search 158.207.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.207.158_tcp@PTR;sleep 1; done
   @ 01/29/24 20:46:02.638
  STEP: creating a pod to probe DNS @ 01/29/24 20:46:02.638
  STEP: submitting the pod to kubernetes @ 01/29/24 20:46:02.638
  STEP: retrieving the pod @ 01/29/24 20:46:04.657
  STEP: looking for the results for each expected name from probers @ 01/29/24 20:46:04.662
  Jan 29 20:46:04.668: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.674: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.679: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.687: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.715: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.721: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.726: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.731: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:04.750: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:09.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.768: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.800: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.805: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.810: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:09.838: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:14.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.764: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.776: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.802: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.807: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.812: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.818: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:14.842: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:19.759: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.764: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.774: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.798: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.803: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.808: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.813: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:19.835: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:24.757: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.763: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.769: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.775: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.800: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.806: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.812: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.818: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:24.849: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:29.758: INFO: Unable to read wheezy_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.765: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.770: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.776: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.803: INFO: Unable to read jessie_udp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.808: INFO: Unable to read jessie_tcp@dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.813: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.818: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local from pod dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379: the server could not find the requested resource (get pods dns-test-af249341-cbe6-4444-b9cc-210cee5b7379)
  Jan 29 20:46:29.839: INFO: Lookups using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 failed for: [wheezy_udp@dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@dns-test-service.dns-5480.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_udp@dns-test-service.dns-5480.svc.cluster.local jessie_tcp@dns-test-service.dns-5480.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5480.svc.cluster.local]

  Jan 29 20:46:34.834: INFO: DNS probes using dns-5480/dns-test-af249341-cbe6-4444-b9cc-210cee5b7379 succeeded

  Jan 29 20:46:34.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:46:34.842
  STEP: deleting the test service @ 01/29/24 20:46:34.857
  STEP: deleting the test headless service @ 01/29/24 20:46:34.875
  STEP: Destroying namespace "dns-5480" for this suite. @ 01/29/24 20:46:34.888
• [32.303 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 01/29/24 20:46:34.896
  Jan 29 20:46:34.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 20:46:34.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:46:34.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:46:34.911
  STEP: Creating pod test-webserver-7a69ffcc-9bc3-4e71-a55a-dcbe7441c809 in namespace container-probe-1844 @ 01/29/24 20:46:34.916
  Jan 29 20:46:36.935: INFO: Started pod test-webserver-7a69ffcc-9bc3-4e71-a55a-dcbe7441c809 in namespace container-probe-1844
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 20:46:36.935
  Jan 29 20:46:36.939: INFO: Initial restart count of pod test-webserver-7a69ffcc-9bc3-4e71-a55a-dcbe7441c809 is 0
  Jan 29 20:50:37.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 20:50:37.757
  STEP: Destroying namespace "container-probe-1844" for this suite. @ 01/29/24 20:50:37.774
• [242.927 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 01/29/24 20:50:37.824
  Jan 29 20:50:37.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:50:37.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:37.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:37.844
  STEP: Creating configMap with name configmap-test-upd-41a76961-b3b3-455e-ac53-c8c1af55f9e4 @ 01/29/24 20:50:37.852
  STEP: Creating the pod @ 01/29/24 20:50:37.856
  STEP: Updating configmap configmap-test-upd-41a76961-b3b3-455e-ac53-c8c1af55f9e4 @ 01/29/24 20:50:39.911
  STEP: waiting to observe update in volume @ 01/29/24 20:50:39.918
  Jan 29 20:50:43.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8487" for this suite. @ 01/29/24 20:50:43.96
• [6.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 01/29/24 20:50:43.969
  Jan 29 20:50:43.969: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:50:43.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:43.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:43.989
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/29/24 20:50:43.993
  Jan 29 20:50:43.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1148 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 29 20:50:44.105: INFO: stderr: ""
  Jan 29 20:50:44.105: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 01/29/24 20:50:44.105
  Jan 29 20:50:44.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1148 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jan 29 20:50:44.214: INFO: stderr: ""
  Jan 29 20:50:44.214: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/29/24 20:50:44.214
  Jan 29 20:50:44.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1148 delete pods e2e-test-httpd-pod'
  Jan 29 20:50:46.580: INFO: stderr: ""
  Jan 29 20:50:46.580: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 29 20:50:46.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1148" for this suite. @ 01/29/24 20:50:46.586
• [2.623 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 01/29/24 20:50:46.594
  Jan 29 20:50:46.594: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 20:50:46.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:46.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:46.613
  STEP: Creating a pod to test downward api env vars @ 01/29/24 20:50:46.619
  STEP: Saw pod success @ 01/29/24 20:50:50.645
  Jan 29 20:50:50.649: INFO: Trying to get logs from node nodea08 pod downward-api-727227d9-e7fb-4a2b-850f-99a3d3470645 container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 20:50:50.661
  Jan 29 20:50:50.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4783" for this suite. @ 01/29/24 20:50:50.683
• [4.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 01/29/24 20:50:50.689
  Jan 29 20:50:50.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 20:50:50.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:50.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:50.711
  Jan 29 20:50:50.716: INFO: Creating simple deployment test-new-deployment
  Jan 29 20:50:50.733: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 01/29/24 20:50:52.751
  STEP: updating a scale subresource @ 01/29/24 20:50:52.754
  STEP: verifying the deployment Spec.Replicas was modified @ 01/29/24 20:50:52.762
  STEP: Patch a scale subresource @ 01/29/24 20:50:52.766
  Jan 29 20:50:52.790: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-3380  22deefa9-1a91-44fa-b2f3-4926ca73485b 10518 3 2024-01-29 20:50:50 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-29 20:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 20:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040269d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-29 20:50:51 +0000 UTC,LastTransitionTime:2024-01-29 20:50:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2024-01-29 20:50:51 +0000 UTC,LastTransitionTime:2024-01-29 20:50:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 29 20:50:52.798: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-3380  26d982f4-c608-45dd-b8bb-eede98529231 10523 3 2024-01-29 20:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 22deefa9-1a91-44fa-b2f3-4926ca73485b 0xc004026e37 0xc004026e38}] [] [{kube-controller-manager Update apps/v1 2024-01-29 20:50:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-29 20:50:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"22deefa9-1a91-44fa-b2f3-4926ca73485b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004026ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 20:50:52.817: INFO: Pod "test-new-deployment-67bd4bf6dc-4tj6q" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-4tj6q test-new-deployment-67bd4bf6dc- deployment-3380  c0826e05-727d-4f9c-a01e-9ccdee06f4aa 10526 0 2024-01-29 20:50:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 26d982f4-c608-45dd-b8bb-eede98529231 0xc00381b867 0xc00381b868}] [] [{kube-controller-manager Update v1 2024-01-29 20:50:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26d982f4-c608-45dd-b8bb-eede98529231\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:50:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sqxxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sqxxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:,StartTime:2024-01-29 20:50:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:50:52.818: INFO: Pod "test-new-deployment-67bd4bf6dc-jvxsq" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-jvxsq test-new-deployment-67bd4bf6dc- deployment-3380  18910540-d14c-4446-974f-8e62786a81bc 10504 0 2024-01-29 20:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 26d982f4-c608-45dd-b8bb-eede98529231 0xc00381ba47 0xc00381ba48}] [] [{kube-controller-manager Update v1 2024-01-29 20:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"26d982f4-c608-45dd-b8bb-eede98529231\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 20:50:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ln9jd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ln9jd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 20:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.126,StartTime:2024-01-29 20:50:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 20:50:51 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c26d5b6e8259c03a227a2383772725f2446d31f3dc02ba65814560018916954e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.126,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 20:50:52.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3380" for this suite. @ 01/29/24 20:50:52.822
• [2.138 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 01/29/24 20:50:52.828
  Jan 29 20:50:52.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:50:52.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:52.843
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:52.847
  STEP: reading a file in the container @ 01/29/24 20:50:54.899
  Jan 29 20:50:54.899: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9390 pod-service-account-8b0c5404-c0ee-47e5-923f-38e9689ec02a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 01/29/24 20:50:55.141
  Jan 29 20:50:55.141: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9390 pod-service-account-8b0c5404-c0ee-47e5-923f-38e9689ec02a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 01/29/24 20:50:55.346
  Jan 29 20:50:55.346: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9390 pod-service-account-8b0c5404-c0ee-47e5-923f-38e9689ec02a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jan 29 20:50:55.561: INFO: Got root ca configmap in namespace "svcaccounts-9390"
  Jan 29 20:50:55.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9390" for this suite. @ 01/29/24 20:50:55.572
• [2.751 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 01/29/24 20:50:55.579
  Jan 29 20:50:55.579: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 20:50:55.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:50:55.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:50:55.598
  Jan 29 20:51:55.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7901" for this suite. @ 01/29/24 20:51:55.626
• [60.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 01/29/24 20:51:55.639
  Jan 29 20:51:55.639: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption @ 01/29/24 20:51:55.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:51:55.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:51:55.662
  Jan 29 20:51:55.682: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 29 20:52:55.720: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/29/24 20:52:55.724
  Jan 29 20:52:55.750: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 29 20:52:55.756: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 29 20:52:55.777: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 29 20:52:55.784: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/29/24 20:52:55.784
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 01/29/24 20:52:57.809
  Jan 29 20:53:01.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9131" for this suite. @ 01/29/24 20:53:01.896
• [66.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 01/29/24 20:53:01.959
  Jan 29 20:53:01.959: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pod-network-test @ 01/29/24 20:53:01.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:01.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:01.983
  STEP: Performing setup for networking test in namespace pod-network-test-4489 @ 01/29/24 20:53:01.989
  STEP: creating a selector @ 01/29/24 20:53:01.989
  STEP: Creating the service pods in kubernetes @ 01/29/24 20:53:01.989
  Jan 29 20:53:01.989: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/29/24 20:53:14.088
  Jan 29 20:53:16.131: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 29 20:53:16.131: INFO: Going to poll 10.244.1.132 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Jan 29 20:53:16.136: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.132 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4489 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:53:16.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:53:16.137: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:53:16.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4489/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.132+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 29 20:53:17.264: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 29 20:53:17.264: INFO: Going to poll 10.244.2.41 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Jan 29 20:53:17.269: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.41 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4489 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:53:17.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:53:17.270: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:53:17.271: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4489/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.2.41+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 29 20:53:18.377: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 29 20:53:18.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4489" for this suite. @ 01/29/24 20:53:18.386
• [16.435 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 01/29/24 20:53:18.397
  Jan 29 20:53:18.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 20:53:18.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:18.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:18.42
  STEP: Creating configMap with name configmap-test-volume-b7b94d48-5edc-4e04-9e39-0db321bd62dd @ 01/29/24 20:53:18.426
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:53:18.431
  STEP: Saw pod success @ 01/29/24 20:53:22.459
  Jan 29 20:53:22.463: INFO: Trying to get logs from node nodea08 pod pod-configmaps-4e661b43-7504-43a3-8bb9-5bc9f9f9ba53 container configmap-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:53:22.492
  Jan 29 20:53:22.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8000" for this suite. @ 01/29/24 20:53:22.514
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 01/29/24 20:53:22.53
  Jan 29 20:53:22.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename events @ 01/29/24 20:53:22.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:22.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:22.55
  STEP: Create set of events @ 01/29/24 20:53:22.555
  Jan 29 20:53:22.560: INFO: created test-event-1
  Jan 29 20:53:22.565: INFO: created test-event-2
  Jan 29 20:53:22.570: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 01/29/24 20:53:22.57
  STEP: delete collection of events @ 01/29/24 20:53:22.574
  Jan 29 20:53:22.574: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/29/24 20:53:22.668
  Jan 29 20:53:22.668: INFO: requesting list of events to confirm quantity
  Jan 29 20:53:22.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9366" for this suite. @ 01/29/24 20:53:22.679
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 01/29/24 20:53:22.688
  Jan 29 20:53:22.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename watch @ 01/29/24 20:53:22.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:22.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:22.708
  STEP: creating a watch on configmaps with label A @ 01/29/24 20:53:22.713
  STEP: creating a watch on configmaps with label B @ 01/29/24 20:53:22.715
  STEP: creating a watch on configmaps with label A or B @ 01/29/24 20:53:22.717
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 01/29/24 20:53:22.719
  Jan 29 20:53:22.725: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11034 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:22.725: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11034 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 01/29/24 20:53:22.726
  Jan 29 20:53:22.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11035 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:22.734: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11035 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 01/29/24 20:53:22.734
  Jan 29 20:53:22.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11036 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:22.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11036 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 01/29/24 20:53:22.743
  Jan 29 20:53:22.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11037 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:22.748: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4513  6e3b391e-751f-4adf-93ee-4b64e480809f 11037 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 01/29/24 20:53:22.748
  Jan 29 20:53:22.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4513  6c73eab2-6667-40a0-89b4-4027938874e9 11038 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:22.753: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4513  6c73eab2-6667-40a0-89b4-4027938874e9 11038 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 01/29/24 20:53:32.753
  Jan 29 20:53:32.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4513  6c73eab2-6667-40a0-89b4-4027938874e9 11105 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:32.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4513  6c73eab2-6667-40a0-89b4-4027938874e9 11105 0 2024-01-29 20:53:22 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-29 20:53:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 29 20:53:42.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4513" for this suite. @ 01/29/24 20:53:42.771
• [20.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 01/29/24 20:53:42.781
  Jan 29 20:53:42.781: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:53:42.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:42.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:42.803
  STEP: Creating a ResourceQuota with terminating scope @ 01/29/24 20:53:42.808
  STEP: Ensuring ResourceQuota status is calculated @ 01/29/24 20:53:42.813
  STEP: Creating a ResourceQuota with not terminating scope @ 01/29/24 20:53:44.818
  STEP: Ensuring ResourceQuota status is calculated @ 01/29/24 20:53:44.829
  STEP: Creating a long running pod @ 01/29/24 20:53:46.835
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 01/29/24 20:53:46.854
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 01/29/24 20:53:48.86
  STEP: Deleting the pod @ 01/29/24 20:53:50.867
  STEP: Ensuring resource quota status released the pod usage @ 01/29/24 20:53:50.879
  STEP: Creating a terminating pod @ 01/29/24 20:53:52.886
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 01/29/24 20:53:52.901
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 01/29/24 20:53:54.907
  STEP: Deleting the pod @ 01/29/24 20:53:56.912
  STEP: Ensuring resource quota status released the pod usage @ 01/29/24 20:53:56.926
  Jan 29 20:53:58.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1954" for this suite. @ 01/29/24 20:53:58.939
• [16.167 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 01/29/24 20:53:58.949
  Jan 29 20:53:58.949: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:53:58.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:53:58.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:53:58.972
  STEP: Create set of pods @ 01/29/24 20:53:58.977
  Jan 29 20:53:58.987: INFO: created test-pod-1
  Jan 29 20:53:58.992: INFO: created test-pod-2
  Jan 29 20:53:59.000: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 01/29/24 20:53:59
  STEP: waiting for all pods to be deleted @ 01/29/24 20:54:01.047
  Jan 29 20:54:01.051: INFO: Pod quantity 3 is different from expected quantity 0
  Jan 29 20:54:02.058: INFO: Pod quantity 3 is different from expected quantity 0
  Jan 29 20:54:03.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9004" for this suite. @ 01/29/24 20:54:03.064
• [4.121 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 01/29/24 20:54:03.07
  Jan 29 20:54:03.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 20:54:03.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:54:03.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:54:03.088
  STEP: Creating a simple DaemonSet "daemon-set" @ 01/29/24 20:54:03.109
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 20:54:03.115
  Jan 29 20:54:03.120: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:03.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:54:03.123: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:54:04.128: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:04.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:54:04.131: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:54:05.131: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:05.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 20:54:05.136: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 01/29/24 20:54:05.14
  Jan 29 20:54:05.160: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:05.164: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 20:54:05.164: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:54:06.172: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:06.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 20:54:06.177: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:54:07.172: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:54:07.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 20:54:07.177: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 01/29/24 20:54:07.177
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 20:54:07.184
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4767, will wait for the garbage collector to delete the pods @ 01/29/24 20:54:07.184
  Jan 29 20:54:07.247: INFO: Deleting DaemonSet.extensions daemon-set took: 7.378804ms
  Jan 29 20:54:07.347: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.281233ms
  Jan 29 20:54:08.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:54:08.652: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 20:54:08.657: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11321"},"items":null}

  Jan 29 20:54:08.661: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11321"},"items":null}

  Jan 29 20:54:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4767" for this suite. @ 01/29/24 20:54:08.681
• [5.617 seconds]
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 01/29/24 20:54:08.688
  Jan 29 20:54:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename taint-single-pod @ 01/29/24 20:54:08.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:54:08.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:54:08.71
  Jan 29 20:54:08.714: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 29 20:55:08.746: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 20:55:08.750: INFO: Starting informer...
  STEP: Starting pod... @ 01/29/24 20:55:08.75
  Jan 29 20:55:08.971: INFO: Pod is running on nodea08. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/29/24 20:55:08.971
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/29/24 20:55:08.989
  STEP: Waiting short time to make sure Pod is queued for deletion @ 01/29/24 20:55:08.993
  Jan 29 20:55:08.993: INFO: Pod wasn't evicted. Proceeding
  Jan 29 20:55:08.993: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/29/24 20:55:09.011
  STEP: Waiting some time to make sure that toleration time passed. @ 01/29/24 20:55:09.018
  Jan 29 20:56:24.019: INFO: Pod wasn't evicted. Test successful
  Jan 29 20:56:24.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-2334" for this suite. @ 01/29/24 20:56:24.027
• [135.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 01/29/24 20:56:24.039
  Jan 29 20:56:24.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pod-network-test @ 01/29/24 20:56:24.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:56:24.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:56:24.071
  STEP: Performing setup for networking test in namespace pod-network-test-5476 @ 01/29/24 20:56:24.075
  STEP: creating a selector @ 01/29/24 20:56:24.076
  STEP: Creating the service pods in kubernetes @ 01/29/24 20:56:24.076
  Jan 29 20:56:24.076: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/29/24 20:56:36.177
  Jan 29 20:56:38.199: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 29 20:56:38.200: INFO: Breadth first check of 10.244.1.141 on host 192.168.100.28...
  Jan 29 20:56:38.205: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.142:9080/dial?request=hostname&protocol=udp&host=10.244.1.141&port=8081&tries=1'] Namespace:pod-network-test-5476 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:56:38.205: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:56:38.206: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:56:38.206: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5476/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.142%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.141%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 29 20:56:38.338: INFO: Waiting for responses: map[]
  Jan 29 20:56:38.338: INFO: reached 10.244.1.141 after 0/1 tries
  Jan 29 20:56:38.338: INFO: Breadth first check of 10.244.2.43 on host 192.168.100.129...
  Jan 29 20:56:38.343: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.142:9080/dial?request=hostname&protocol=udp&host=10.244.2.43&port=8081&tries=1'] Namespace:pod-network-test-5476 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 20:56:38.343: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:56:38.344: INFO: ExecWithOptions: Clientset creation
  Jan 29 20:56:38.344: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5476/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.142%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.2.43%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 29 20:56:38.451: INFO: Waiting for responses: map[]
  Jan 29 20:56:38.451: INFO: reached 10.244.2.43 after 0/1 tries
  Jan 29 20:56:38.451: INFO: Going to retry 0 out of 2 pods....
  Jan 29 20:56:38.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5476" for this suite. @ 01/29/24 20:56:38.457
• [14.426 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 01/29/24 20:56:38.466
  Jan 29 20:56:38.466: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubelet-test @ 01/29/24 20:56:38.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:56:38.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:56:38.486
  Jan 29 20:56:42.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6171" for this suite. @ 01/29/24 20:56:42.517
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 01/29/24 20:56:42.525
  Jan 29 20:56:42.525: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename containers @ 01/29/24 20:56:42.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:56:42.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:56:42.546
  Jan 29 20:56:44.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6599" for this suite. @ 01/29/24 20:56:44.607
• [2.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 01/29/24 20:56:44.616
  Jan 29 20:56:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption @ 01/29/24 20:56:44.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:56:44.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:56:44.639
  Jan 29 20:56:44.661: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 29 20:57:44.699: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/29/24 20:57:44.703
  Jan 29 20:57:44.703: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/29/24 20:57:44.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:57:44.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:57:44.723
  STEP: Finding an available node @ 01/29/24 20:57:44.728
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/29/24 20:57:44.728
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/29/24 20:57:46.752
  Jan 29 20:57:46.796: INFO: found a healthy node: nodea08
  Jan 29 20:57:52.880: INFO: pods created so far: [1 1 1]
  Jan 29 20:57:52.880: INFO: length of pods created so far: 3
  Jan 29 20:57:54.893: INFO: pods created so far: [2 2 1]
  Jan 29 20:58:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 20:58:01.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1490" for this suite. @ 01/29/24 20:58:01.991
  STEP: Destroying namespace "sched-preemption-7778" for this suite. @ 01/29/24 20:58:01.997
• [77.388 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 01/29/24 20:58:02.005
  Jan 29 20:58:02.005: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 20:58:02.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:02.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:02.024
  STEP: Creating simple DaemonSet "daemon-set" @ 01/29/24 20:58:02.047
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 20:58:02.055
  Jan 29 20:58:02.061: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:02.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:58:02.065: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:58:03.072: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:03.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:58:03.077: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:58:04.072: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:04.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 20:58:04.077: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 01/29/24 20:58:04.081
  Jan 29 20:58:04.100: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:04.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 20:58:04.104: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:58:05.112: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:05.116: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 20:58:05.117: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:58:06.113: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:06.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 20:58:06.119: INFO: Node nodea08 is running 0 daemon pod, expected 1
  Jan 29 20:58:07.111: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 20:58:07.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 20:58:07.115: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 20:58:07.118
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9544, will wait for the garbage collector to delete the pods @ 01/29/24 20:58:07.118
  Jan 29 20:58:07.178: INFO: Deleting DaemonSet.extensions daemon-set took: 5.833668ms
  Jan 29 20:58:07.280: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.243263ms
  Jan 29 20:58:08.383: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 20:58:08.384: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 20:58:08.387: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12118"},"items":null}

  Jan 29 20:58:08.390: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12118"},"items":null}

  Jan 29 20:58:08.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9544" for this suite. @ 01/29/24 20:58:08.407
• [6.410 seconds]
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 01/29/24 20:58:08.415
  Jan 29 20:58:08.415: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:58:08.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:08.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:08.431
  STEP: creating the pod @ 01/29/24 20:58:08.436
  STEP: submitting the pod to kubernetes @ 01/29/24 20:58:08.436
  STEP: verifying QOS class is set on the pod @ 01/29/24 20:58:08.448
  Jan 29 20:58:08.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1165" for this suite. @ 01/29/24 20:58:08.456
• [0.048 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 01/29/24 20:58:08.463
  Jan 29 20:58:08.464: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:58:08.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:08.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:08.48
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/29/24 20:58:08.484
  STEP: Saw pod success @ 01/29/24 20:58:12.507
  Jan 29 20:58:12.511: INFO: Trying to get logs from node nodea08 pod pod-5aefe0ea-379a-48bc-8ff9-9b357554d3a5 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 20:58:12.52
  Jan 29 20:58:12.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1974" for this suite. @ 01/29/24 20:58:12.541
• [4.084 seconds]
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 01/29/24 20:58:12.548
  Jan 29 20:58:12.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 20:58:12.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:12.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:12.568
  STEP: creating a Pod with a static label @ 01/29/24 20:58:12.578
  STEP: watching for Pod to be ready @ 01/29/24 20:58:12.585
  Jan 29 20:58:12.587: INFO: observed Pod pod-test in namespace pods-4267 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jan 29 20:58:12.590: INFO: observed Pod pod-test in namespace pods-4267 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC  }]
  Jan 29 20:58:12.602: INFO: observed Pod pod-test in namespace pods-4267 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC  }]
  Jan 29 20:58:14.322: INFO: Found Pod pod-test in namespace pods-4267 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-29 20:58:12 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 01/29/24 20:58:14.327
  STEP: getting the Pod and ensuring that it's patched @ 01/29/24 20:58:14.342
  STEP: replacing the Pod's status Ready condition to False @ 01/29/24 20:58:14.345
  STEP: check the Pod again to ensure its Ready conditions are False @ 01/29/24 20:58:14.364
  STEP: deleting the Pod via a Collection with a LabelSelector @ 01/29/24 20:58:14.364
  STEP: watching for the Pod to be deleted @ 01/29/24 20:58:14.391
  Jan 29 20:58:14.393: INFO: observed event type MODIFIED
  Jan 29 20:58:15.175: INFO: observed event type MODIFIED
  Jan 29 20:58:16.704: INFO: observed event type MODIFIED
  Jan 29 20:58:17.334: INFO: observed event type MODIFIED
  Jan 29 20:58:17.345: INFO: observed event type MODIFIED
  Jan 29 20:58:17.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4267" for this suite. @ 01/29/24 20:58:17.359
• [4.816 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 01/29/24 20:58:17.366
  Jan 29 20:58:17.366: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 20:58:17.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:17.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:17.388
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 01/29/24 20:58:17.393
  Jan 29 20:58:17.394: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 01/29/24 20:58:23.545
  Jan 29 20:58:23.547: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:58:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 20:58:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7723" for this suite. @ 01/29/24 20:58:30.883
• [13.523 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 01/29/24 20:58:30.894
  Jan 29 20:58:30.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 20:58:30.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:30.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:30.914
  STEP: Creating a pod to test service account token:  @ 01/29/24 20:58:30.918
  STEP: Saw pod success @ 01/29/24 20:58:34.939
  Jan 29 20:58:34.944: INFO: Trying to get logs from node nodea08 pod test-pod-b2f268f1-8f89-404f-b6d2-72e8787d036f container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 20:58:34.953
  Jan 29 20:58:34.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4918" for this suite. @ 01/29/24 20:58:34.973
• [4.085 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 01/29/24 20:58:34.979
  Jan 29 20:58:34.979: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename runtimeclass @ 01/29/24 20:58:34.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:34.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:34.999
  STEP: Deleting RuntimeClass runtimeclass-5101-delete-me @ 01/29/24 20:58:35.013
  STEP: Waiting for the RuntimeClass to disappear @ 01/29/24 20:58:35.018
  Jan 29 20:58:35.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5101" for this suite. @ 01/29/24 20:58:35.032
• [0.059 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 01/29/24 20:58:35.038
  Jan 29 20:58:35.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename events @ 01/29/24 20:58:35.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:35.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:35.055
  STEP: creating a test event @ 01/29/24 20:58:35.06
  STEP: listing events in all namespaces @ 01/29/24 20:58:35.066
  STEP: listing events in test namespace @ 01/29/24 20:58:35.076
  STEP: listing events with field selection filtering on source @ 01/29/24 20:58:35.079
  STEP: listing events with field selection filtering on reportingController @ 01/29/24 20:58:35.082
  STEP: getting the test event @ 01/29/24 20:58:35.086
  STEP: patching the test event @ 01/29/24 20:58:35.089
  STEP: getting the test event @ 01/29/24 20:58:35.101
  STEP: updating the test event @ 01/29/24 20:58:35.104
  STEP: getting the test event @ 01/29/24 20:58:35.113
  STEP: deleting the test event @ 01/29/24 20:58:35.116
  STEP: listing events in all namespaces @ 01/29/24 20:58:35.123
  STEP: listing events in test namespace @ 01/29/24 20:58:35.13
  Jan 29 20:58:35.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6426" for this suite. @ 01/29/24 20:58:35.139
• [0.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 01/29/24 20:58:35.146
  Jan 29 20:58:35.146: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:58:35.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:35.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:35.166
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:58:35.171
  STEP: Saw pod success @ 01/29/24 20:58:39.203
  Jan 29 20:58:39.208: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-b1b7d546-0e48-4bf1-92df-4572c3a305e8 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:58:39.217
  Jan 29 20:58:39.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6643" for this suite. @ 01/29/24 20:58:39.242
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 01/29/24 20:58:39.25
  Jan 29 20:58:39.250: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename ingressclass @ 01/29/24 20:58:39.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:39.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:39.269
  STEP: getting /apis @ 01/29/24 20:58:39.274
  STEP: getting /apis/networking.k8s.io @ 01/29/24 20:58:39.281
  STEP: getting /apis/networking.k8s.iov1 @ 01/29/24 20:58:39.283
  STEP: creating @ 01/29/24 20:58:39.285
  STEP: getting @ 01/29/24 20:58:39.301
  STEP: listing @ 01/29/24 20:58:39.305
  STEP: watching @ 01/29/24 20:58:39.308
  Jan 29 20:58:39.308: INFO: starting watch
  STEP: patching @ 01/29/24 20:58:39.31
  STEP: updating @ 01/29/24 20:58:39.318
  Jan 29 20:58:39.324: INFO: waiting for watch events with expected annotations
  Jan 29 20:58:39.324: INFO: saw patched and updated annotations
  STEP: deleting @ 01/29/24 20:58:39.324
  STEP: deleting a collection @ 01/29/24 20:58:39.336
  Jan 29 20:58:39.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-8712" for this suite. @ 01/29/24 20:58:39.354
• [0.110 seconds]
------------------------------
S
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 01/29/24 20:58:39.36
  Jan 29 20:58:39.360: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:58:39.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:39.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:39.376
  Jan 29 20:58:39.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1534" for this suite. @ 01/29/24 20:58:39.39
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 01/29/24 20:58:39.398
  Jan 29 20:58:39.398: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 20:58:39.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:39.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:39.416
  STEP: creating a Service @ 01/29/24 20:58:39.424
  STEP: watching for the Service to be added @ 01/29/24 20:58:39.437
  Jan 29 20:58:39.439: INFO: Found Service test-service-ms5tv in namespace services-852 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jan 29 20:58:39.439: INFO: Service test-service-ms5tv created
  STEP: Getting /status @ 01/29/24 20:58:39.439
  Jan 29 20:58:39.444: INFO: Service test-service-ms5tv has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 01/29/24 20:58:39.444
  STEP: watching for the Service to be patched @ 01/29/24 20:58:39.449
  Jan 29 20:58:39.451: INFO: observed Service test-service-ms5tv in namespace services-852 with annotations: map[] & LoadBalancer: {[]}
  Jan 29 20:58:39.451: INFO: Found Service test-service-ms5tv in namespace services-852 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jan 29 20:58:39.451: INFO: Service test-service-ms5tv has service status patched
  STEP: updating the ServiceStatus @ 01/29/24 20:58:39.452
  Jan 29 20:58:39.461: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 01/29/24 20:58:39.461
  Jan 29 20:58:39.463: INFO: Observed Service test-service-ms5tv in namespace services-852 with annotations: map[] & Conditions: {[]}
  Jan 29 20:58:39.463: INFO: Observed event: &Service{ObjectMeta:{test-service-ms5tv  services-852  94946b4d-6e65-466a-96d0-05952b706d2d 12358 0 2024-01-29 20:58:39 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-29 20:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-29 20:58:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.107.118.118,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.107.118.118],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jan 29 20:58:39.463: INFO: Found Service test-service-ms5tv in namespace services-852 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 29 20:58:39.463: INFO: Service test-service-ms5tv has service status updated
  STEP: patching the service @ 01/29/24 20:58:39.463
  STEP: watching for the Service to be patched @ 01/29/24 20:58:39.477
  Jan 29 20:58:39.479: INFO: observed Service test-service-ms5tv in namespace services-852 with labels: map[test-service-static:true]
  Jan 29 20:58:39.479: INFO: observed Service test-service-ms5tv in namespace services-852 with labels: map[test-service-static:true]
  Jan 29 20:58:39.479: INFO: observed Service test-service-ms5tv in namespace services-852 with labels: map[test-service-static:true]
  Jan 29 20:58:39.479: INFO: Found Service test-service-ms5tv in namespace services-852 with labels: map[test-service:patched test-service-static:true]
  Jan 29 20:58:39.479: INFO: Service test-service-ms5tv patched
  STEP: deleting the service @ 01/29/24 20:58:39.479
  STEP: watching for the Service to be deleted @ 01/29/24 20:58:39.492
  Jan 29 20:58:39.494: INFO: Observed event: ADDED
  Jan 29 20:58:39.494: INFO: Observed event: MODIFIED
  Jan 29 20:58:39.494: INFO: Observed event: MODIFIED
  Jan 29 20:58:39.494: INFO: Observed event: MODIFIED
  Jan 29 20:58:39.495: INFO: Found Service test-service-ms5tv in namespace services-852 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jan 29 20:58:39.495: INFO: Service test-service-ms5tv deleted
  Jan 29 20:58:39.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-852" for this suite. @ 01/29/24 20:58:39.499
• [0.106 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 01/29/24 20:58:39.505
  Jan 29 20:58:39.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 20:58:39.506
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:39.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:39.523
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:58:39.527
  STEP: Saw pod success @ 01/29/24 20:58:43.55
  Jan 29 20:58:43.555: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-00a61bc3-c248-4f3b-a5d6-582e19d831e5 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:58:43.565
  Jan 29 20:58:43.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3030" for this suite. @ 01/29/24 20:58:43.589
• [4.090 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 01/29/24 20:58:43.596
  Jan 29 20:58:43.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 20:58:43.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:58:43.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:58:43.619
  STEP: creating a replication controller @ 01/29/24 20:58:43.625
  Jan 29 20:58:43.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 create -f -'
  Jan 29 20:58:44.438: INFO: stderr: ""
  Jan 29 20:58:44.438: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/29/24 20:58:44.438
  Jan 29 20:58:44.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:58:44.545: INFO: stderr: ""
  Jan 29 20:58:44.545: INFO: stdout: "update-demo-nautilus-qs55x update-demo-nautilus-vb8mr "
  Jan 29 20:58:44.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:44.638: INFO: stderr: ""
  Jan 29 20:58:44.638: INFO: stdout: ""
  Jan 29 20:58:44.638: INFO: update-demo-nautilus-qs55x is created but not running
  Jan 29 20:58:49.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:58:49.752: INFO: stderr: ""
  Jan 29 20:58:49.752: INFO: stdout: "update-demo-nautilus-qs55x update-demo-nautilus-vb8mr "
  Jan 29 20:58:49.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:49.852: INFO: stderr: ""
  Jan 29 20:58:49.852: INFO: stdout: ""
  Jan 29 20:58:49.852: INFO: update-demo-nautilus-qs55x is created but not running
  Jan 29 20:58:54.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:58:54.964: INFO: stderr: ""
  Jan 29 20:58:54.964: INFO: stdout: "update-demo-nautilus-qs55x update-demo-nautilus-vb8mr "
  Jan 29 20:58:54.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:55.062: INFO: stderr: ""
  Jan 29 20:58:55.062: INFO: stdout: "true"
  Jan 29 20:58:55.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 20:58:55.158: INFO: stderr: ""
  Jan 29 20:58:55.158: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 20:58:55.158: INFO: validating pod update-demo-nautilus-qs55x
  Jan 29 20:58:55.165: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 20:58:55.166: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 20:58:55.166: INFO: update-demo-nautilus-qs55x is verified up and running
  Jan 29 20:58:55.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-vb8mr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:55.260: INFO: stderr: ""
  Jan 29 20:58:55.260: INFO: stdout: "true"
  Jan 29 20:58:55.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-vb8mr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 20:58:55.358: INFO: stderr: ""
  Jan 29 20:58:55.359: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 20:58:55.359: INFO: validating pod update-demo-nautilus-vb8mr
  Jan 29 20:58:55.365: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 20:58:55.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 20:58:55.366: INFO: update-demo-nautilus-vb8mr is verified up and running
  STEP: scaling down the replication controller @ 01/29/24 20:58:55.366
  Jan 29 20:58:55.368: INFO: scanned /root for discovery docs: <nil>
  Jan 29 20:58:55.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jan 29 20:58:56.490: INFO: stderr: ""
  Jan 29 20:58:56.490: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/29/24 20:58:56.49
  Jan 29 20:58:56.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:58:56.593: INFO: stderr: ""
  Jan 29 20:58:56.593: INFO: stdout: "update-demo-nautilus-qs55x "
  Jan 29 20:58:56.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:56.688: INFO: stderr: ""
  Jan 29 20:58:56.688: INFO: stdout: "true"
  Jan 29 20:58:56.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 20:58:56.784: INFO: stderr: ""
  Jan 29 20:58:56.784: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 20:58:56.784: INFO: validating pod update-demo-nautilus-qs55x
  Jan 29 20:58:56.789: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 20:58:56.790: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 20:58:56.790: INFO: update-demo-nautilus-qs55x is verified up and running
  STEP: scaling up the replication controller @ 01/29/24 20:58:56.79
  Jan 29 20:58:56.792: INFO: scanned /root for discovery docs: <nil>
  Jan 29 20:58:56.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jan 29 20:58:57.917: INFO: stderr: ""
  Jan 29 20:58:57.917: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/29/24 20:58:57.917
  Jan 29 20:58:57.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:58:58.020: INFO: stderr: ""
  Jan 29 20:58:58.021: INFO: stdout: "update-demo-nautilus-6rbzs update-demo-nautilus-qs55x "
  Jan 29 20:58:58.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-6rbzs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:58:58.125: INFO: stderr: ""
  Jan 29 20:58:58.126: INFO: stdout: ""
  Jan 29 20:58:58.126: INFO: update-demo-nautilus-6rbzs is created but not running
  Jan 29 20:59:03.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 20:59:03.235: INFO: stderr: ""
  Jan 29 20:59:03.235: INFO: stdout: "update-demo-nautilus-6rbzs update-demo-nautilus-qs55x "
  Jan 29 20:59:03.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-6rbzs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:59:03.336: INFO: stderr: ""
  Jan 29 20:59:03.336: INFO: stdout: "true"
  Jan 29 20:59:03.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-6rbzs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 20:59:03.433: INFO: stderr: ""
  Jan 29 20:59:03.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 20:59:03.433: INFO: validating pod update-demo-nautilus-6rbzs
  Jan 29 20:59:03.441: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 20:59:03.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 20:59:03.441: INFO: update-demo-nautilus-6rbzs is verified up and running
  Jan 29 20:59:03.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 20:59:03.535: INFO: stderr: ""
  Jan 29 20:59:03.535: INFO: stdout: "true"
  Jan 29 20:59:03.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods update-demo-nautilus-qs55x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 20:59:03.629: INFO: stderr: ""
  Jan 29 20:59:03.629: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 20:59:03.629: INFO: validating pod update-demo-nautilus-qs55x
  Jan 29 20:59:03.634: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 20:59:03.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 20:59:03.634: INFO: update-demo-nautilus-qs55x is verified up and running
  STEP: using delete to clean up resources @ 01/29/24 20:59:03.634
  Jan 29 20:59:03.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 delete --grace-period=0 --force -f -'
  Jan 29 20:59:03.735: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 20:59:03.736: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 29 20:59:03.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get rc,svc -l name=update-demo --no-headers'
  Jan 29 20:59:03.847: INFO: stderr: "No resources found in kubectl-9383 namespace.\n"
  Jan 29 20:59:03.847: INFO: stdout: ""
  Jan 29 20:59:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-9383 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 29 20:59:03.952: INFO: stderr: ""
  Jan 29 20:59:03.952: INFO: stdout: ""
  Jan 29 20:59:03.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9383" for this suite. @ 01/29/24 20:59:03.958
• [20.369 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 01/29/24 20:59:03.965
  Jan 29 20:59:03.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 20:59:03.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:03.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:03.983
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 20:59:03.989
  STEP: Saw pod success @ 01/29/24 20:59:08.039
  Jan 29 20:59:08.043: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-fe2c59a0-6e2f-44d3-a173-7c51d649b90c container client-container: <nil>
  STEP: delete the pod @ 01/29/24 20:59:08.054
  Jan 29 20:59:08.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-893" for this suite. @ 01/29/24 20:59:08.075
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 01/29/24 20:59:08.083
  Jan 29 20:59:08.083: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption @ 01/29/24 20:59:08.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:08.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:08.103
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:59:08.113
  STEP: Updating PodDisruptionBudget status @ 01/29/24 20:59:10.122
  STEP: Waiting for all pods to be running @ 01/29/24 20:59:10.133
  Jan 29 20:59:10.137: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 01/29/24 20:59:12.146
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:59:12.16
  STEP: Patching PodDisruptionBudget status @ 01/29/24 20:59:12.168
  STEP: Waiting for the pdb to be processed @ 01/29/24 20:59:12.178
  Jan 29 20:59:12.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2208" for this suite. @ 01/29/24 20:59:12.187
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 01/29/24 20:59:12.197
  Jan 29 20:59:12.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subpath @ 01/29/24 20:59:12.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:12.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:12.216
  STEP: Setting up data @ 01/29/24 20:59:12.221
  STEP: Creating pod pod-subpath-test-configmap-s5mv @ 01/29/24 20:59:12.231
  STEP: Creating a pod to test atomic-volume-subpath @ 01/29/24 20:59:12.231
  STEP: Saw pod success @ 01/29/24 20:59:36.338
  Jan 29 20:59:36.343: INFO: Trying to get logs from node nodea08 pod pod-subpath-test-configmap-s5mv container test-container-subpath-configmap-s5mv: <nil>
  STEP: delete the pod @ 01/29/24 20:59:36.354
  STEP: Deleting pod pod-subpath-test-configmap-s5mv @ 01/29/24 20:59:36.371
  Jan 29 20:59:36.371: INFO: Deleting pod "pod-subpath-test-configmap-s5mv" in namespace "subpath-9710"
  Jan 29 20:59:36.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9710" for this suite. @ 01/29/24 20:59:36.38
• [24.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 01/29/24 20:59:36.388
  Jan 29 20:59:36.388: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 20:59:36.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:36.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:36.409
  STEP: Creating a ResourceQuota with best effort scope @ 01/29/24 20:59:36.415
  STEP: Ensuring ResourceQuota status is calculated @ 01/29/24 20:59:36.42
  STEP: Creating a ResourceQuota with not best effort scope @ 01/29/24 20:59:38.426
  STEP: Ensuring ResourceQuota status is calculated @ 01/29/24 20:59:38.433
  STEP: Creating a best-effort pod @ 01/29/24 20:59:40.438
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 01/29/24 20:59:40.454
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 01/29/24 20:59:42.461
  STEP: Deleting the pod @ 01/29/24 20:59:44.466
  STEP: Ensuring resource quota status released the pod usage @ 01/29/24 20:59:44.479
  STEP: Creating a not best-effort pod @ 01/29/24 20:59:46.485
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 01/29/24 20:59:46.508
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 01/29/24 20:59:48.513
  STEP: Deleting the pod @ 01/29/24 20:59:50.52
  STEP: Ensuring resource quota status released the pod usage @ 01/29/24 20:59:50.533
  Jan 29 20:59:52.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1696" for this suite. @ 01/29/24 20:59:52.547
• [16.167 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 01/29/24 20:59:52.555
  Jan 29 20:59:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 20:59:52.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:52.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:52.579
  STEP: Creating configMap with name projected-configmap-test-volume-386a66ed-e8a0-412d-8983-b0587dc817ea @ 01/29/24 20:59:52.584
  STEP: Creating a pod to test consume configMaps @ 01/29/24 20:59:52.589
  STEP: Saw pod success @ 01/29/24 20:59:56.616
  Jan 29 20:59:56.621: INFO: Trying to get logs from node nodea08 pod pod-projected-configmaps-af8d4566-260e-498c-a750-16f2bc21c047 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 20:59:56.631
  Jan 29 20:59:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1374" for this suite. @ 01/29/24 20:59:56.656
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 01/29/24 20:59:56.666
  Jan 29 20:59:56.666: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 20:59:56.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 20:59:56.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 20:59:56.697
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/29/24 20:59:56.702
  STEP: Saw pod success @ 01/29/24 21:00:00.729
  Jan 29 21:00:00.734: INFO: Trying to get logs from node nodea08 pod pod-4c2d9fa6-9020-4eb3-929c-273c3e30792b container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:00:00.743
  Jan 29 21:00:00.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-394" for this suite. @ 01/29/24 21:00:00.764
• [4.104 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 01/29/24 21:00:00.771
  Jan 29 21:00:00.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 21:00:00.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:00.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:00.797
  Jan 29 21:00:00.802: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 01/29/24 21:00:01.816
  STEP: Checking rc "condition-test" has the desired failure condition set @ 01/29/24 21:00:01.822
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 01/29/24 21:00:02.832
  Jan 29 21:00:02.841: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 01/29/24 21:00:02.841
  Jan 29 21:00:03.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1861" for this suite. @ 01/29/24 21:00:03.857
• [3.094 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 01/29/24 21:00:03.865
  Jan 29 21:00:03.865: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 21:00:03.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:03.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:03.889
  STEP: Saw pod success @ 01/29/24 21:00:09.968
  Jan 29 21:00:09.974: INFO: Trying to get logs from node nodeb29 pod client-envvars-db96f9b1-99ea-4e5e-8100-85723960959c container env3cont: <nil>
  STEP: delete the pod @ 01/29/24 21:00:10.004
  Jan 29 21:00:10.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4016" for this suite. @ 01/29/24 21:00:10.029
• [6.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 01/29/24 21:00:10.04
  Jan 29 21:00:10.040: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:00:10.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:10.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:10.062
  STEP: fetching services @ 01/29/24 21:00:10.067
  Jan 29 21:00:10.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-324" for this suite. @ 01/29/24 21:00:10.077
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 01/29/24 21:00:10.085
  Jan 29 21:00:10.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 21:00:10.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:10.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:10.108
  Jan 29 21:00:10.121: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jan 29 21:00:15.126: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 21:00:15.126
  Jan 29 21:00:15.127: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jan 29 21:00:17.133: INFO: Creating deployment "test-rollover-deployment"
  Jan 29 21:00:17.146: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jan 29 21:00:19.158: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jan 29 21:00:19.167: INFO: Ensure that both replica sets have 1 created replica
  Jan 29 21:00:19.174: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jan 29 21:00:19.187: INFO: Updating deployment test-rollover-deployment
  Jan 29 21:00:19.187: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jan 29 21:00:21.196: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jan 29 21:00:21.205: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jan 29 21:00:21.212: INFO: all replica sets need to contain the pod-template-hash label
  Jan 29 21:00:21.212: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 29 21:00:23.224: INFO: all replica sets need to contain the pod-template-hash label
  Jan 29 21:00:23.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 29 21:00:25.226: INFO: all replica sets need to contain the pod-template-hash label
  Jan 29 21:00:25.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 29 21:00:27.224: INFO: all replica sets need to contain the pod-template-hash label
  Jan 29 21:00:27.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 29 21:00:29.226: INFO: all replica sets need to contain the pod-template-hash label
  Jan 29 21:00:29.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 0, 20, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 0, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 29 21:00:31.224: INFO: 
  Jan 29 21:00:31.224: INFO: Ensure that both old replica sets have no replicas
  Jan 29 21:00:31.236: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9838  0c61df04-ce8a-4692-808b-41d6e5657a53 13073 2 2024-01-29 21:00:17 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-29 21:00:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059da858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-29 21:00:17 +0000 UTC,LastTransitionTime:2024-01-29 21:00:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2024-01-29 21:00:30 +0000 UTC,LastTransitionTime:2024-01-29 21:00:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 29 21:00:31.241: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-9838  ba694e67-edb9-4a27-a136-6f2f42ba7e94 13063 2 2024-01-29 21:00:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0c61df04-ce8a-4692-808b-41d6e5657a53 0xc0059dad27 0xc0059dad28}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:00:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c61df04-ce8a-4692-808b-41d6e5657a53\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:00:30 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059dadd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:00:31.241: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jan 29 21:00:31.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9838  193979e5-ce17-47f8-8787-595d113e181d 13072 2 2024-01-29 21:00:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0c61df04-ce8a-4692-808b-41d6e5657a53 0xc0059dabf7 0xc0059dabf8}] [] [{e2e.test Update apps/v1 2024-01-29 21:00:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:00:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c61df04-ce8a-4692-808b-41d6e5657a53\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:00:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0059dacb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:00:31.241: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-9838  3102ba52-346f-4f60-849f-30d0d2cfad0e 13035 2 2024-01-29 21:00:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0c61df04-ce8a-4692-808b-41d6e5657a53 0xc0059dae47 0xc0059dae48}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:00:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0c61df04-ce8a-4692-808b-41d6e5657a53\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:00:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0059daef8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:00:31.246: INFO: Pod "test-rollover-deployment-57777854c9-2ddhd" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-2ddhd test-rollover-deployment-57777854c9- deployment-9838  fbb9b44f-e442-411e-80be-810ace5005f4 13045 0 2024-01-29 21:00:19 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 ba694e67-edb9-4a27-a136-6f2f42ba7e94 0xc0059db457 0xc0059db458}] [] [{kube-controller-manager Update v1 2024-01-29 21:00:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ba694e67-edb9-4a27-a136-6f2f42ba7e94\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:00:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.167\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xsbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xsbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:00:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:00:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:00:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.167,StartTime:2024-01-29 21:00:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:00:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://5e59dba9f4f7ebc23e7dc4e8937815a8c065b6f25af590f92e6019c53630f74d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.167,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 21:00:31.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9838" for this suite. @ 01/29/24 21:00:31.251
• [21.173 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 01/29/24 21:00:31.258
  Jan 29 21:00:31.258: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename endpointslicemirroring @ 01/29/24 21:00:31.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:31.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:31.28
  STEP: mirroring a new custom Endpoint @ 01/29/24 21:00:31.298
  Jan 29 21:00:31.307: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 01/29/24 21:00:33.314
  Jan 29 21:00:33.323: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 01/29/24 21:00:35.33
  Jan 29 21:00:35.340: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jan 29 21:00:37.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-5467" for this suite. @ 01/29/24 21:00:37.361
• [6.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 01/29/24 21:00:37.372
  Jan 29 21:00:37.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:00:37.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:37.388
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:37.394
  STEP: Creating a job @ 01/29/24 21:00:37.399
  STEP: Ensuring job reaches completions @ 01/29/24 21:00:37.41
  Jan 29 21:00:47.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9702" for this suite. @ 01/29/24 21:00:47.422
• [10.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 01/29/24 21:00:47.431
  Jan 29 21:00:47.431: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pod-network-test @ 01/29/24 21:00:47.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:00:47.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:00:47.451
  STEP: Performing setup for networking test in namespace pod-network-test-9482 @ 01/29/24 21:00:47.456
  STEP: creating a selector @ 01/29/24 21:00:47.456
  STEP: Creating the service pods in kubernetes @ 01/29/24 21:00:47.456
  Jan 29 21:00:47.456: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/29/24 21:01:09.565
  Jan 29 21:01:11.601: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 29 21:01:11.601: INFO: Going to poll 10.244.1.172 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Jan 29 21:01:11.605: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.172:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9482 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:01:11.605: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:01:11.606: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:01:11.606: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9482/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.172%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 29 21:01:11.740: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 29 21:01:11.740: INFO: Going to poll 10.244.2.49 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Jan 29 21:01:11.744: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.49:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9482 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:01:11.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:01:11.745: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:01:11.745: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9482/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.2.49%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 29 21:01:11.856: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 29 21:01:11.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9482" for this suite. @ 01/29/24 21:01:11.864
• [24.442 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 01/29/24 21:01:11.874
  Jan 29 21:01:11.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename cronjob @ 01/29/24 21:01:11.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:01:11.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:01:11.896
  STEP: Creating a suspended cronjob @ 01/29/24 21:01:11.902
  STEP: Ensuring no jobs are scheduled @ 01/29/24 21:01:11.91
  STEP: Ensuring no job exists by listing jobs explicitly @ 01/29/24 21:06:11.922
  STEP: Removing cronjob @ 01/29/24 21:06:11.926
  Jan 29 21:06:11.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5588" for this suite. @ 01/29/24 21:06:11.941
• [300.073 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 01/29/24 21:06:11.949
  Jan 29 21:06:11.949: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:06:11.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:11.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:11.972
  STEP: Creating secret with name secret-test-ca19ecc8-2791-4d33-b52d-53f3359404ad @ 01/29/24 21:06:11.977
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:06:11.984
  STEP: Saw pod success @ 01/29/24 21:06:16.009
  Jan 29 21:06:16.014: INFO: Trying to get logs from node nodea08 pod pod-secrets-fde4c725-63cc-486b-8fbd-01b2dacbd3a4 container secret-env-test: <nil>
  STEP: delete the pod @ 01/29/24 21:06:16.044
  Jan 29 21:06:16.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8855" for this suite. @ 01/29/24 21:06:16.066
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 01/29/24 21:06:16.076
  Jan 29 21:06:16.076: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 21:06:16.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:16.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:16.097
  STEP: Creating a test namespace @ 01/29/24 21:06:16.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:16.113
  STEP: Creating a service in the namespace @ 01/29/24 21:06:16.117
  STEP: Deleting the namespace @ 01/29/24 21:06:16.129
  STEP: Waiting for the namespace to be removed. @ 01/29/24 21:06:16.137
  STEP: Recreating the namespace @ 01/29/24 21:06:22.146
  STEP: Verifying there is no service in the namespace @ 01/29/24 21:06:22.165
  Jan 29 21:06:22.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2309" for this suite. @ 01/29/24 21:06:22.175
  STEP: Destroying namespace "nsdeletetest-1854" for this suite. @ 01/29/24 21:06:22.182
  Jan 29 21:06:22.185: INFO: Namespace nsdeletetest-1854 was already deleted
  STEP: Destroying namespace "nsdeletetest-652" for this suite. @ 01/29/24 21:06:22.185
• [6.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 01/29/24 21:06:22.196
  Jan 29 21:06:22.196: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:06:22.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:22.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:22.218
  STEP: create the rc @ 01/29/24 21:06:22.227
  W0129 21:06:22.234076      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/29/24 21:06:28.238
  STEP: wait for the rc to be deleted @ 01/29/24 21:06:28.246
  Jan 29 21:06:29.263: INFO: 80 pods remaining
  Jan 29 21:06:29.263: INFO: 80 pods has nil DeletionTimestamp
  Jan 29 21:06:29.263: INFO: 
  Jan 29 21:06:30.265: INFO: 71 pods remaining
  Jan 29 21:06:30.266: INFO: 70 pods has nil DeletionTimestamp
  Jan 29 21:06:30.266: INFO: 
  Jan 29 21:06:31.262: INFO: 60 pods remaining
  Jan 29 21:06:31.262: INFO: 60 pods has nil DeletionTimestamp
  Jan 29 21:06:31.262: INFO: 
  Jan 29 21:06:32.263: INFO: 40 pods remaining
  Jan 29 21:06:32.263: INFO: 40 pods has nil DeletionTimestamp
  Jan 29 21:06:32.263: INFO: 
  Jan 29 21:06:33.259: INFO: 31 pods remaining
  Jan 29 21:06:33.259: INFO: 30 pods has nil DeletionTimestamp
  Jan 29 21:06:33.259: INFO: 
  Jan 29 21:06:34.260: INFO: 20 pods remaining
  Jan 29 21:06:34.260: INFO: 20 pods has nil DeletionTimestamp
  Jan 29 21:06:34.260: INFO: 
  STEP: Gathering metrics @ 01/29/24 21:06:35.254
  Jan 29 21:06:35.389: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:06:35.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4614" for this suite. @ 01/29/24 21:06:35.394
• [13.204 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 01/29/24 21:06:35.401
  Jan 29 21:06:35.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:06:35.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:35.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:35.418
  STEP: Creating secret with name secret-test-56620ffc-2615-4440-a768-23c0fea1f3a0 @ 01/29/24 21:06:35.422
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:06:35.426
  STEP: Saw pod success @ 01/29/24 21:06:39.449
  Jan 29 21:06:39.452: INFO: Trying to get logs from node nodea08 pod pod-secrets-96512591-fa86-4655-abbe-9a2ff835a8c4 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:06:39.462
  Jan 29 21:06:39.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4790" for this suite. @ 01/29/24 21:06:39.485
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 01/29/24 21:06:39.502
  Jan 29 21:06:39.502: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:06:39.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:39.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:39.521
  STEP: creating service in namespace services-5770 @ 01/29/24 21:06:39.526
  STEP: creating service affinity-nodeport-transition in namespace services-5770 @ 01/29/24 21:06:39.526
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5770 @ 01/29/24 21:06:39.542
  I0129 21:06:39.549344      23 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5770, replica count: 3
  I0129 21:06:42.600275      23 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 21:06:42.615: INFO: Creating new exec pod
  Jan 29 21:06:45.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jan 29 21:06:45.877: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jan 29 21:06:45.877: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:06:45.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.82.200 80'
  Jan 29 21:06:46.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.82.200 80\nConnection to 10.104.82.200 80 port [tcp/http] succeeded!\n"
  Jan 29 21:06:46.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:06:46.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.28 31985'
  Jan 29 21:06:46.316: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.28 31985\nConnection to 192.168.100.28 31985 port [tcp/*] succeeded!\n"
  Jan 29 21:06:46.316: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:06:46.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.129 31985'
  Jan 29 21:06:46.544: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.129 31985\nConnection to 192.168.100.129 31985 port [tcp/*] succeeded!\n"
  Jan 29 21:06:46.544: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:06:46.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.28:31985/ ; done'
  Jan 29 21:06:46.934: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n"
  Jan 29 21:06:46.934: INFO: stdout: "\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-6js2z\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-6js2z\naffinity-nodeport-transition-6js2z\naffinity-nodeport-transition-7vg4h\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-7vg4h\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-7vg4h\naffinity-nodeport-transition-6js2z\naffinity-nodeport-transition-8kxtr"
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-6js2z
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-6js2z
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-6js2z
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-7vg4h
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-7vg4h
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.934: INFO: Received response from host: affinity-nodeport-transition-7vg4h
  Jan 29 21:06:46.935: INFO: Received response from host: affinity-nodeport-transition-6js2z
  Jan 29 21:06:46.935: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:46.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5770 exec execpod-affinityf6nv4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.28:31985/ ; done'
  Jan 29 21:06:47.301: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:31985/\n"
  Jan 29 21:06:47.301: INFO: stdout: "\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr\naffinity-nodeport-transition-8kxtr"
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Received response from host: affinity-nodeport-transition-8kxtr
  Jan 29 21:06:47.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:06:47.308: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5770, will wait for the garbage collector to delete the pods @ 01/29/24 21:06:47.323
  Jan 29 21:06:47.385: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.535828ms
  Jan 29 21:06:47.485: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.608396ms
  STEP: Destroying namespace "services-5770" for this suite. @ 01/29/24 21:06:50.009
• [10.515 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 01/29/24 21:06:50.018
  Jan 29 21:06:50.018: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:06:50.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:50.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:50.036
  STEP: creating a collection of services @ 01/29/24 21:06:50.041
  Jan 29 21:06:50.041: INFO: Creating e2e-svc-a-6qn4v
  Jan 29 21:06:50.055: INFO: Creating e2e-svc-b-ldd84
  Jan 29 21:06:50.069: INFO: Creating e2e-svc-c-2cxcg
  STEP: deleting service collection @ 01/29/24 21:06:50.087
  Jan 29 21:06:50.123: INFO: Collection of services has been deleted
  Jan 29 21:06:50.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7003" for this suite. @ 01/29/24 21:06:50.128
• [0.121 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 01/29/24 21:06:50.14
  Jan 29 21:06:50.140: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:06:50.142
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:50.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:50.16
  STEP: Creating the pod @ 01/29/24 21:06:50.164
  Jan 29 21:06:52.718: INFO: Successfully updated pod "annotationupdate69f50843-55a9-4963-bf89-9b1354c3ed58"
  Jan 29 21:06:56.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9519" for this suite. @ 01/29/24 21:06:56.761
• [6.630 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 01/29/24 21:06:56.773
  Jan 29 21:06:56.773: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:06:56.775
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:06:56.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:06:56.794
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3975 @ 01/29/24 21:06:56.799
  STEP: changing the ExternalName service to type=NodePort @ 01/29/24 21:06:56.804
  STEP: creating replication controller externalname-service in namespace services-3975 @ 01/29/24 21:06:56.828
  I0129 21:06:56.836010      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3975, replica count: 2
  I0129 21:06:59.888531      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 21:06:59.888: INFO: Creating new exec pod
  Jan 29 21:07:02.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-3975 exec execpodlmnm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 29 21:07:03.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 29 21:07:03.153: INFO: stdout: "externalname-service-sbw62"
  Jan 29 21:07:03.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-3975 exec execpodlmnm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.196.89 80'
  Jan 29 21:07:03.377: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.196.89 80\nConnection to 10.103.196.89 80 port [tcp/http] succeeded!\n"
  Jan 29 21:07:03.378: INFO: stdout: "externalname-service-sbw62"
  Jan 29 21:07:03.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-3975 exec execpodlmnm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.28 32159'
  Jan 29 21:07:03.589: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.28 32159\nConnection to 192.168.100.28 32159 port [tcp/*] succeeded!\n"
  Jan 29 21:07:03.590: INFO: stdout: "externalname-service-sbw62"
  Jan 29 21:07:03.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-3975 exec execpodlmnm2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.129 32159'
  Jan 29 21:07:03.823: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.129 32159\nConnection to 192.168.100.129 32159 port [tcp/*] succeeded!\n"
  Jan 29 21:07:03.823: INFO: stdout: "externalname-service-wrfsl"
  Jan 29 21:07:03.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:07:03.829: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3975" for this suite. @ 01/29/24 21:07:03.851
• [7.084 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 01/29/24 21:07:03.857
  Jan 29 21:07:03.857: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:07:03.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:07:03.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:07:03.874
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:07:03.879
  STEP: Saw pod success @ 01/29/24 21:07:07.905
  Jan 29 21:07:07.909: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-acd7cb80-e51d-4dd4-b7c2-0df89f66f2d5 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:07:07.919
  Jan 29 21:07:07.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-82" for this suite. @ 01/29/24 21:07:07.94
• [4.090 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 01/29/24 21:07:07.949
  Jan 29 21:07:07.949: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:07:07.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:07:07.964
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:07:07.968
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1598 @ 01/29/24 21:07:07.972
  STEP: changing the ExternalName service to type=ClusterIP @ 01/29/24 21:07:07.976
  STEP: creating replication controller externalname-service in namespace services-1598 @ 01/29/24 21:07:07.994
  I0129 21:07:08.002171      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1598, replica count: 2
  I0129 21:07:11.053757      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 21:07:11.053: INFO: Creating new exec pod
  Jan 29 21:07:14.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-1598 exec execpod56454 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 29 21:07:14.330: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 29 21:07:14.330: INFO: stdout: ""
  Jan 29 21:07:15.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-1598 exec execpod56454 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 29 21:07:15.552: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 29 21:07:15.552: INFO: stdout: "externalname-service-rwlrl"
  Jan 29 21:07:15.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-1598 exec execpod56454 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.185.63 80'
  Jan 29 21:07:15.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.185.63 80\nConnection to 10.105.185.63 80 port [tcp/http] succeeded!\n"
  Jan 29 21:07:15.762: INFO: stdout: "externalname-service-rwlrl"
  Jan 29 21:07:15.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:07:15.768: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-1598" for this suite. @ 01/29/24 21:07:15.801
• [7.867 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 01/29/24 21:07:15.816
  Jan 29 21:07:15.816: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 21:07:15.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:07:15.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:07:15.835
  STEP: Create a ReplicaSet @ 01/29/24 21:07:15.839
  STEP: Verify that the required pods have come up @ 01/29/24 21:07:15.847
  Jan 29 21:07:15.852: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jan 29 21:07:20.857: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 01/29/24 21:07:20.857
  Jan 29 21:07:20.861: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 01/29/24 21:07:20.861
  STEP: DeleteCollection of the ReplicaSets @ 01/29/24 21:07:20.864
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 01/29/24 21:07:20.871
  Jan 29 21:07:20.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1111" for this suite. @ 01/29/24 21:07:20.881
• [5.072 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 01/29/24 21:07:20.889
  Jan 29 21:07:20.889: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 21:07:20.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:07:20.902
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:07:20.906
  STEP: Creating namespace "e2e-ns-xvjgr" @ 01/29/24 21:07:20.909
  Jan 29 21:07:20.919: INFO: Namespace "e2e-ns-xvjgr-8853" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-xvjgr-8853" @ 01/29/24 21:07:20.919
  Jan 29 21:07:20.927: INFO: Namespace "e2e-ns-xvjgr-8853" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-xvjgr-8853" @ 01/29/24 21:07:20.927
  Jan 29 21:07:20.934: INFO: Namespace "e2e-ns-xvjgr-8853" has []v1.FinalizerName{"kubernetes"}
  Jan 29 21:07:20.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3477" for this suite. @ 01/29/24 21:07:20.939
  STEP: Destroying namespace "e2e-ns-xvjgr-8853" for this suite. @ 01/29/24 21:07:20.943
• [0.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 01/29/24 21:07:20.948
  Jan 29 21:07:20.948: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-pred @ 01/29/24 21:07:20.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:07:20.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:07:20.965
  Jan 29 21:07:20.969: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 29 21:07:20.976: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 21:07:20.979: INFO: 
  Logging pods the apiserver thinks is on node nodea08 before test
  Jan 29 21:07:20.985: INFO: kube-flannel-ds-s6bhq from kube-flannel started at 2024-01-29 20:55:09 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:07:20.985: INFO: kube-proxy-5wdv6 from kube-system started at 2024-01-29 20:10:26 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:07:20.985: INFO: execpod56454 from services-1598 started at 2024-01-29 21:07:11 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container agnhost-container ready: true, restart count 0
  Jan 29 21:07:20.985: INFO: externalname-service-rwlrl from services-1598 started at 2024-01-29 21:07:08 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container externalname-service ready: true, restart count 0
  Jan 29 21:07:20.985: INFO: sonobuoy from sonobuoy started at 2024-01-29 20:16:38 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 29 21:07:20.985: INFO: sonobuoy-e2e-job-4fcfaf908eac4396 from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:07:20.985: INFO: 	Container e2e ready: true, restart count 0
  Jan 29 21:07:20.986: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:07:20.986: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:07:20.986: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:07:20.986: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 29 21:07:20.986: INFO: 
  Logging pods the apiserver thinks is on node nodeb29 before test
  Jan 29 21:07:20.991: INFO: kube-flannel-ds-4qmbv from kube-flannel started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.991: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:07:20.991: INFO: kube-proxy-6xx6l from kube-system started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.991: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:07:20.991: INFO: externalname-service-q66hc from services-1598 started at 2024-01-29 21:07:08 +0000 UTC (1 container statuses recorded)
  Jan 29 21:07:20.991: INFO: 	Container externalname-service ready: true, restart count 0
  Jan 29 21:07:20.991: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-5zbgb from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:07:20.991: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:07:20.991: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/29/24 21:07:20.991
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/29/24 21:07:23.014
  STEP: Trying to apply a random label on the found node. @ 01/29/24 21:07:23.031
  STEP: verifying the node has the label kubernetes.io/e2e-73666bf7-20f1-4422-8c7e-2564cac196b5 95 @ 01/29/24 21:07:23.044
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 01/29/24 21:07:23.048
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.100.28 on the node which pod4 resides and expect not scheduled @ 01/29/24 21:07:25.066
  STEP: removing the label kubernetes.io/e2e-73666bf7-20f1-4422-8c7e-2564cac196b5 off the node nodea08 @ 01/29/24 21:12:25.078
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-73666bf7-20f1-4422-8c7e-2564cac196b5 @ 01/29/24 21:12:25.097
  Jan 29 21:12:25.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1658" for this suite. @ 01/29/24 21:12:25.107
• [304.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 01/29/24 21:12:25.12
  Jan 29 21:12:25.120: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename endpointslice @ 01/29/24 21:12:25.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:12:25.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:12:25.14
  Jan 29 21:12:25.152: INFO: Endpoints addresses: [192.168.100.86] , ports: [6443]
  Jan 29 21:12:25.152: INFO: EndpointSlices addresses: [192.168.100.86] , ports: [6443]
  Jan 29 21:12:25.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6507" for this suite. @ 01/29/24 21:12:25.157
• [0.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 01/29/24 21:12:25.167
  Jan 29 21:12:25.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:12:25.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:12:25.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:12:25.185
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:12:25.19
  STEP: Saw pod success @ 01/29/24 21:12:29.216
  Jan 29 21:12:29.221: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-15c6c16c-0363-429e-a299-5d84aef96550 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:12:29.251
  Jan 29 21:12:29.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1193" for this suite. @ 01/29/24 21:12:29.272
• [4.111 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 01/29/24 21:12:29.279
  Jan 29 21:12:29.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 21:12:29.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:12:29.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:12:29.295
  STEP: Creating pod test-grpc-3c066315-5fc2-42d3-af40-644f831b218b in namespace container-probe-3519 @ 01/29/24 21:12:29.3
  Jan 29 21:12:31.316: INFO: Started pod test-grpc-3c066315-5fc2-42d3-af40-644f831b218b in namespace container-probe-3519
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 21:12:31.316
  Jan 29 21:12:31.321: INFO: Initial restart count of pod test-grpc-3c066315-5fc2-42d3-af40-644f831b218b is 0
  Jan 29 21:13:35.545: INFO: Restart count of pod container-probe-3519/test-grpc-3c066315-5fc2-42d3-af40-644f831b218b is now 1 (1m4.224346275s elapsed)
  Jan 29 21:13:35.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:13:35.552
  STEP: Destroying namespace "container-probe-3519" for this suite. @ 01/29/24 21:13:35.565
• [66.294 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 01/29/24 21:13:35.574
  Jan 29 21:13:35.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:13:35.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:35.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:35.596
  STEP: Setting up server cert @ 01/29/24 21:13:35.617
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:13:35.941
  STEP: Deploying the webhook pod @ 01/29/24 21:13:35.952
  STEP: Wait for the deployment to be ready @ 01/29/24 21:13:35.967
  Jan 29 21:13:35.975: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 21:13:37.991
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:13:38.007
  Jan 29 21:13:39.007: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/29/24 21:13:39.013
  STEP: create a pod that should be denied by the webhook @ 01/29/24 21:13:39.045
  STEP: create a pod that causes the webhook to hang @ 01/29/24 21:13:39.065
  STEP: create a configmap that should be denied by the webhook @ 01/29/24 21:13:49.079
  STEP: create a configmap that should be admitted by the webhook @ 01/29/24 21:13:49.114
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/29/24 21:13:49.129
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/29/24 21:13:49.139
  STEP: create a namespace that bypass the webhook @ 01/29/24 21:13:49.148
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 01/29/24 21:13:49.161
  Jan 29 21:13:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-669" for this suite. @ 01/29/24 21:13:49.219
  STEP: Destroying namespace "webhook-markers-8431" for this suite. @ 01/29/24 21:13:49.223
  STEP: Destroying namespace "exempted-namespace-3779" for this suite. @ 01/29/24 21:13:49.227
• [13.658 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 01/29/24 21:13:49.232
  Jan 29 21:13:49.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:13:49.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:49.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:49.25
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:13:49.254
  STEP: Saw pod success @ 01/29/24 21:13:53.279
  Jan 29 21:13:53.284: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-b1a1cc2d-b7f3-40f7-97c9-ace1c69b5e8b container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:13:53.294
  Jan 29 21:13:53.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-73" for this suite. @ 01/29/24 21:13:53.316
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 01/29/24 21:13:53.326
  Jan 29 21:13:53.326: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-pred @ 01/29/24 21:13:53.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:53.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:53.348
  Jan 29 21:13:53.357: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 29 21:13:53.366: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 21:13:53.369: INFO: 
  Logging pods the apiserver thinks is on node nodea08 before test
  Jan 29 21:13:53.381: INFO: kube-flannel-ds-s6bhq from kube-flannel started at 2024-01-29 20:55:09 +0000 UTC (1 container statuses recorded)
  Jan 29 21:13:53.381: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: kube-proxy-5wdv6 from kube-system started at 2024-01-29 20:10:26 +0000 UTC (1 container statuses recorded)
  Jan 29 21:13:53.381: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: sonobuoy from sonobuoy started at 2024-01-29 20:16:38 +0000 UTC (1 container statuses recorded)
  Jan 29 21:13:53.381: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: sonobuoy-e2e-job-4fcfaf908eac4396 from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:13:53.381: INFO: 	Container e2e ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:13:53.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 29 21:13:53.381: INFO: 
  Logging pods the apiserver thinks is on node nodeb29 before test
  Jan 29 21:13:53.387: INFO: kube-flannel-ds-4qmbv from kube-flannel started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:13:53.387: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:13:53.387: INFO: kube-proxy-6xx6l from kube-system started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:13:53.387: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:13:53.387: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-5zbgb from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:13:53.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:13:53.387: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 01/29/24 21:13:53.387
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17aeeddaf05fb444], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 01/29/24 21:13:53.419
  Jan 29 21:13:54.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2332" for this suite. @ 01/29/24 21:13:54.42
• [1.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 01/29/24 21:13:54.429
  Jan 29 21:13:54.429: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename limitrange @ 01/29/24 21:13:54.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:54.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:54.446
  STEP: Creating LimitRange "e2e-limitrange-vbmgm" in namespace "limitrange-8605" @ 01/29/24 21:13:54.451
  STEP: Creating another limitRange in another namespace @ 01/29/24 21:13:54.458
  Jan 29 21:13:54.470: INFO: Namespace "e2e-limitrange-vbmgm-2169" created
  Jan 29 21:13:54.470: INFO: Creating LimitRange "e2e-limitrange-vbmgm" in namespace "e2e-limitrange-vbmgm-2169"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vbmgm" @ 01/29/24 21:13:54.476
  Jan 29 21:13:54.478: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-vbmgm" in "limitrange-8605" namespace @ 01/29/24 21:13:54.478
  Jan 29 21:13:54.488: INFO: LimitRange "e2e-limitrange-vbmgm" has been patched
  STEP: Delete LimitRange "e2e-limitrange-vbmgm" by Collection with labelSelector: "e2e-limitrange-vbmgm=patched" @ 01/29/24 21:13:54.488
  STEP: Confirm that the limitRange "e2e-limitrange-vbmgm" has been deleted @ 01/29/24 21:13:54.494
  Jan 29 21:13:54.494: INFO: Requesting list of LimitRange to confirm quantity
  Jan 29 21:13:54.497: INFO: Found 0 LimitRange with label "e2e-limitrange-vbmgm=patched"
  Jan 29 21:13:54.497: INFO: LimitRange "e2e-limitrange-vbmgm" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vbmgm" @ 01/29/24 21:13:54.497
  Jan 29 21:13:54.501: INFO: Found 1 limitRange
  Jan 29 21:13:54.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8605" for this suite. @ 01/29/24 21:13:54.505
  STEP: Destroying namespace "e2e-limitrange-vbmgm-2169" for this suite. @ 01/29/24 21:13:54.509
• [0.085 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 01/29/24 21:13:54.515
  Jan 29 21:13:54.515: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename containers @ 01/29/24 21:13:54.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:54.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:54.534
  STEP: Creating a pod to test override command @ 01/29/24 21:13:54.539
  STEP: Saw pod success @ 01/29/24 21:13:58.562
  Jan 29 21:13:58.566: INFO: Trying to get logs from node nodea08 pod client-containers-58cc3926-d353-4a3c-b706-fbb61e81e1f4 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:13:58.574
  Jan 29 21:13:58.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9852" for this suite. @ 01/29/24 21:13:58.597
• [4.090 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 01/29/24 21:13:58.605
  Jan 29 21:13:58.605: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subpath @ 01/29/24 21:13:58.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:13:58.621
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:13:58.626
  STEP: Setting up data @ 01/29/24 21:13:58.631
  STEP: Creating pod pod-subpath-test-configmap-tt7c @ 01/29/24 21:13:58.642
  STEP: Creating a pod to test atomic-volume-subpath @ 01/29/24 21:13:58.642
  STEP: Saw pod success @ 01/29/24 21:14:22.741
  Jan 29 21:14:22.746: INFO: Trying to get logs from node nodea08 pod pod-subpath-test-configmap-tt7c container test-container-subpath-configmap-tt7c: <nil>
  STEP: delete the pod @ 01/29/24 21:14:22.757
  STEP: Deleting pod pod-subpath-test-configmap-tt7c @ 01/29/24 21:14:22.772
  Jan 29 21:14:22.772: INFO: Deleting pod "pod-subpath-test-configmap-tt7c" in namespace "subpath-1577"
  Jan 29 21:14:22.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1577" for this suite. @ 01/29/24 21:14:22.78
• [24.181 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 01/29/24 21:14:22.786
  Jan 29 21:14:22.786: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubelet-test @ 01/29/24 21:14:22.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:22.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:22.806
  Jan 29 21:14:22.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2600" for this suite. @ 01/29/24 21:14:22.839
• [0.058 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 01/29/24 21:14:22.845
  Jan 29 21:14:22.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:14:22.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:22.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:22.865
  STEP: Creating secret with name secret-test-map-7f67670c-b448-4c88-827d-a72af1d67f54 @ 01/29/24 21:14:22.869
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:14:22.875
  STEP: Saw pod success @ 01/29/24 21:14:26.901
  Jan 29 21:14:26.905: INFO: Trying to get logs from node nodea08 pod pod-secrets-eb5f4553-8ad7-4d09-ac8b-fabf4ba88e57 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:14:26.915
  Jan 29 21:14:26.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7986" for this suite. @ 01/29/24 21:14:26.939
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 01/29/24 21:14:26.947
  Jan 29 21:14:26.947: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:14:26.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:26.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:26.965
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/29/24 21:14:26.97
  STEP: Saw pod success @ 01/29/24 21:14:30.997
  Jan 29 21:14:31.002: INFO: Trying to get logs from node nodea08 pod pod-9705ffcc-fcd0-48a6-bdeb-9b67bfeb6b29 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:14:31.012
  Jan 29 21:14:31.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8688" for this suite. @ 01/29/24 21:14:31.037
• [4.096 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 01/29/24 21:14:31.043
  Jan 29 21:14:31.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-webhook @ 01/29/24 21:14:31.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:31.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:31.063
  STEP: Setting up server cert @ 01/29/24 21:14:31.069
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/29/24 21:14:31.579
  STEP: Deploying the custom resource conversion webhook pod @ 01/29/24 21:14:31.589
  STEP: Wait for the deployment to be ready @ 01/29/24 21:14:31.604
  Jan 29 21:14:31.613: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/29/24 21:14:33.628
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:14:33.642
  Jan 29 21:14:34.643: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 29 21:14:34.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Creating a v1 custom resource @ 01/29/24 21:14:37.253
  STEP: v2 custom resource should be converted @ 01/29/24 21:14:37.259
  Jan 29 21:14:37.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6161" for this suite. @ 01/29/24 21:14:37.833
• [6.798 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 01/29/24 21:14:37.842
  Jan 29 21:14:37.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubelet-test @ 01/29/24 21:14:37.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:37.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:37.862
  STEP: Waiting for pod completion @ 01/29/24 21:14:37.874
  Jan 29 21:14:41.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8020" for this suite. @ 01/29/24 21:14:41.905
• [4.069 seconds]
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 01/29/24 21:14:41.91
  Jan 29 21:14:41.910: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:14:41.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:14:41.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:14:41.931
  STEP: creating the pod @ 01/29/24 21:14:41.936
  STEP: waiting for pod running @ 01/29/24 21:14:41.945
  STEP: creating a file in subpath @ 01/29/24 21:14:43.956
  Jan 29 21:14:43.960: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2406 PodName:var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:14:43.960: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:14:43.961: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:14:43.962: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2406/pods/var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 01/29/24 21:14:44.082
  Jan 29 21:14:44.087: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2406 PodName:var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:14:44.087: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:14:44.087: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:14:44.088: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2406/pods/var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 01/29/24 21:14:44.205
  Jan 29 21:14:44.724: INFO: Successfully updated pod "var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12"
  STEP: waiting for annotated pod running @ 01/29/24 21:14:44.724
  STEP: deleting the pod gracefully @ 01/29/24 21:14:44.728
  Jan 29 21:14:44.728: INFO: Deleting pod "var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12" in namespace "var-expansion-2406"
  Jan 29 21:14:44.736: INFO: Wait up to 5m0s for pod "var-expansion-e4c267b8-352e-4f0f-887c-19d04afb0d12" to be fully deleted
  Jan 29 21:15:16.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2406" for this suite. @ 01/29/24 21:15:16.854
• [34.951 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 01/29/24 21:15:16.864
  Jan 29 21:15:16.864: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename discovery @ 01/29/24 21:15:16.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:16.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:16.888
  STEP: Setting up server cert @ 01/29/24 21:15:16.896
  Jan 29 21:15:17.438: INFO: Checking APIGroup: apiregistration.k8s.io
  Jan 29 21:15:17.440: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jan 29 21:15:17.440: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jan 29 21:15:17.440: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jan 29 21:15:17.440: INFO: Checking APIGroup: apps
  Jan 29 21:15:17.442: INFO: PreferredVersion.GroupVersion: apps/v1
  Jan 29 21:15:17.442: INFO: Versions found [{apps/v1 v1}]
  Jan 29 21:15:17.442: INFO: apps/v1 matches apps/v1
  Jan 29 21:15:17.442: INFO: Checking APIGroup: events.k8s.io
  Jan 29 21:15:17.444: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jan 29 21:15:17.444: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jan 29 21:15:17.445: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jan 29 21:15:17.445: INFO: Checking APIGroup: authentication.k8s.io
  Jan 29 21:15:17.447: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jan 29 21:15:17.447: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jan 29 21:15:17.447: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jan 29 21:15:17.447: INFO: Checking APIGroup: authorization.k8s.io
  Jan 29 21:15:17.449: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jan 29 21:15:17.449: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jan 29 21:15:17.449: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jan 29 21:15:17.449: INFO: Checking APIGroup: autoscaling
  Jan 29 21:15:17.451: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jan 29 21:15:17.451: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jan 29 21:15:17.451: INFO: autoscaling/v2 matches autoscaling/v2
  Jan 29 21:15:17.451: INFO: Checking APIGroup: batch
  Jan 29 21:15:17.453: INFO: PreferredVersion.GroupVersion: batch/v1
  Jan 29 21:15:17.453: INFO: Versions found [{batch/v1 v1}]
  Jan 29 21:15:17.453: INFO: batch/v1 matches batch/v1
  Jan 29 21:15:17.453: INFO: Checking APIGroup: certificates.k8s.io
  Jan 29 21:15:17.455: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jan 29 21:15:17.455: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jan 29 21:15:17.455: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jan 29 21:15:17.455: INFO: Checking APIGroup: networking.k8s.io
  Jan 29 21:15:17.457: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jan 29 21:15:17.457: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jan 29 21:15:17.457: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jan 29 21:15:17.457: INFO: Checking APIGroup: policy
  Jan 29 21:15:17.459: INFO: PreferredVersion.GroupVersion: policy/v1
  Jan 29 21:15:17.460: INFO: Versions found [{policy/v1 v1}]
  Jan 29 21:15:17.460: INFO: policy/v1 matches policy/v1
  Jan 29 21:15:17.460: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jan 29 21:15:17.462: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jan 29 21:15:17.462: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jan 29 21:15:17.462: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jan 29 21:15:17.462: INFO: Checking APIGroup: storage.k8s.io
  Jan 29 21:15:17.464: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jan 29 21:15:17.464: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jan 29 21:15:17.464: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jan 29 21:15:17.464: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jan 29 21:15:17.466: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jan 29 21:15:17.466: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jan 29 21:15:17.466: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jan 29 21:15:17.466: INFO: Checking APIGroup: apiextensions.k8s.io
  Jan 29 21:15:17.468: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jan 29 21:15:17.468: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jan 29 21:15:17.468: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jan 29 21:15:17.468: INFO: Checking APIGroup: scheduling.k8s.io
  Jan 29 21:15:17.470: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jan 29 21:15:17.470: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jan 29 21:15:17.470: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jan 29 21:15:17.470: INFO: Checking APIGroup: coordination.k8s.io
  Jan 29 21:15:17.472: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jan 29 21:15:17.472: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jan 29 21:15:17.472: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jan 29 21:15:17.472: INFO: Checking APIGroup: node.k8s.io
  Jan 29 21:15:17.474: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jan 29 21:15:17.474: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jan 29 21:15:17.474: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jan 29 21:15:17.474: INFO: Checking APIGroup: discovery.k8s.io
  Jan 29 21:15:17.476: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jan 29 21:15:17.476: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jan 29 21:15:17.476: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jan 29 21:15:17.476: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jan 29 21:15:17.478: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jan 29 21:15:17.478: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jan 29 21:15:17.478: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jan 29 21:15:17.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-8111" for this suite. @ 01/29/24 21:15:17.484
• [0.627 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 01/29/24 21:15:17.494
  Jan 29 21:15:17.494: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-pred @ 01/29/24 21:15:17.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:17.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:17.517
  Jan 29 21:15:17.522: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 29 21:15:17.530: INFO: Waiting for terminating namespaces to be deleted...
  Jan 29 21:15:17.534: INFO: 
  Logging pods the apiserver thinks is on node nodea08 before test
  Jan 29 21:15:17.541: INFO: kube-flannel-ds-s6bhq from kube-flannel started at 2024-01-29 20:55:09 +0000 UTC (1 container statuses recorded)
  Jan 29 21:15:17.541: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: kube-proxy-5wdv6 from kube-system started at 2024-01-29 20:10:26 +0000 UTC (1 container statuses recorded)
  Jan 29 21:15:17.541: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: sonobuoy from sonobuoy started at 2024-01-29 20:16:38 +0000 UTC (1 container statuses recorded)
  Jan 29 21:15:17.541: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: sonobuoy-e2e-job-4fcfaf908eac4396 from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:15:17.541: INFO: 	Container e2e ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:15:17.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 29 21:15:17.541: INFO: 
  Logging pods the apiserver thinks is on node nodeb29 before test
  Jan 29 21:15:17.547: INFO: kube-flannel-ds-4qmbv from kube-flannel started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:15:17.547: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 29 21:15:17.548: INFO: kube-proxy-6xx6l from kube-system started at 2024-01-29 20:11:22 +0000 UTC (1 container statuses recorded)
  Jan 29 21:15:17.548: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 29 21:15:17.548: INFO: sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-5zbgb from sonobuoy started at 2024-01-29 20:16:44 +0000 UTC (2 container statuses recorded)
  Jan 29 21:15:17.548: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 29 21:15:17.548: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node nodea08 @ 01/29/24 21:15:17.57
  STEP: verifying the node has the label node nodeb29 @ 01/29/24 21:15:17.584
  Jan 29 21:15:17.596: INFO: Pod kube-flannel-ds-4qmbv requesting resource cpu=100m on Node nodeb29
  Jan 29 21:15:17.596: INFO: Pod kube-flannel-ds-s6bhq requesting resource cpu=100m on Node nodea08
  Jan 29 21:15:17.596: INFO: Pod kube-proxy-5wdv6 requesting resource cpu=0m on Node nodea08
  Jan 29 21:15:17.596: INFO: Pod kube-proxy-6xx6l requesting resource cpu=0m on Node nodeb29
  Jan 29 21:15:17.596: INFO: Pod sonobuoy requesting resource cpu=0m on Node nodea08
  Jan 29 21:15:17.596: INFO: Pod sonobuoy-e2e-job-4fcfaf908eac4396 requesting resource cpu=0m on Node nodea08
  Jan 29 21:15:17.596: INFO: Pod sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-5zbgb requesting resource cpu=0m on Node nodeb29
  Jan 29 21:15:17.596: INFO: Pod sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf requesting resource cpu=0m on Node nodea08
  STEP: Starting Pods to consume most of the cluster CPU. @ 01/29/24 21:15:17.596
  Jan 29 21:15:17.596: INFO: Creating a pod which consumes cpu=11130m on Node nodea08
  Jan 29 21:15:17.605: INFO: Creating a pod which consumes cpu=5530m on Node nodeb29
  STEP: Creating another pod that requires unavailable amount of CPU. @ 01/29/24 21:15:19.628
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f.17aeedee8b77791c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3316/filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f to nodeb29] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f.17aeedeead1da167], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f.17aeedeeae34700c], Reason = [Created], Message = [Created container filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f.17aeedeeb56e69cf], Reason = [Started], Message = [Started container filler-pod-2f3f9f4b-ed02-43d7-91aa-b9a307435a8f] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9.17aeedee8b69c43b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3316/filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9 to nodea08] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9.17aeedeeac6566ed], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9.17aeedeead615673], Reason = [Created], Message = [Created container filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9.17aeedeeb5ccc298], Reason = [Started], Message = [Started container filler-pod-ccca1808-21f9-4278-b632-4d23fa3516a9] @ 01/29/24 21:15:19.634
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17aeedef044699ee], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] @ 01/29/24 21:15:19.65
  STEP: removing the label node off the node nodea08 @ 01/29/24 21:15:20.651
  STEP: verifying the node doesn't have the label node @ 01/29/24 21:15:20.671
  STEP: removing the label node off the node nodeb29 @ 01/29/24 21:15:20.676
  STEP: verifying the node doesn't have the label node @ 01/29/24 21:15:20.691
  Jan 29 21:15:20.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3316" for this suite. @ 01/29/24 21:15:20.7
• [3.212 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 01/29/24 21:15:20.707
  Jan 29 21:15:20.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:15:20.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:20.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:20.728
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:15:20.733
  STEP: Saw pod success @ 01/29/24 21:15:24.758
  Jan 29 21:15:24.762: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-0ec4199e-ab92-429f-8675-a624337cdcbc container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:15:24.771
  Jan 29 21:15:24.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3187" for this suite. @ 01/29/24 21:15:24.795
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 01/29/24 21:15:24.804
  Jan 29 21:15:24.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sysctl @ 01/29/24 21:15:24.805
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:24.819
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:24.825
  STEP: Creating a pod with one valid and two invalid sysctls @ 01/29/24 21:15:24.83
  Jan 29 21:15:24.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3838" for this suite. @ 01/29/24 21:15:24.841
• [0.043 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 01/29/24 21:15:24.848
  Jan 29 21:15:24.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 21:15:24.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:24.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:24.871
  STEP: Create a pod @ 01/29/24 21:15:24.875
  STEP: patching /status @ 01/29/24 21:15:26.893
  Jan 29 21:15:26.903: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jan 29 21:15:26.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9871" for this suite. @ 01/29/24 21:15:26.908
• [2.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 01/29/24 21:15:26.918
  Jan 29 21:15:26.918: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:15:26.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:26.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:26.938
  STEP: Creating the pod @ 01/29/24 21:15:26.942
  Jan 29 21:15:29.491: INFO: Successfully updated pod "labelsupdate1039104a-d551-4cba-8d0e-d465285e64cd"
  Jan 29 21:15:31.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-699" for this suite. @ 01/29/24 21:15:31.521
• [4.610 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 01/29/24 21:15:31.53
  Jan 29 21:15:31.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context-test @ 01/29/24 21:15:31.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:31.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:31.55
  Jan 29 21:15:35.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3305" for this suite. @ 01/29/24 21:15:35.589
• [4.069 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 01/29/24 21:15:35.599
  Jan 29 21:15:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename init-container @ 01/29/24 21:15:35.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:35.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:35.627
  STEP: creating the pod @ 01/29/24 21:15:35.632
  Jan 29 21:15:35.632: INFO: PodSpec: initContainers in spec.initContainers
  Jan 29 21:15:39.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-183" for this suite. @ 01/29/24 21:15:39.201
• [3.609 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 01/29/24 21:15:39.21
  Jan 29 21:15:39.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 21:15:39.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:15:39.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:15:39.229
  STEP: Creating a test externalName service @ 01/29/24 21:15:39.233
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:15:39.238
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:15:39.238
  STEP: creating a pod to probe DNS @ 01/29/24 21:15:39.238
  STEP: submitting the pod to kubernetes @ 01/29/24 21:15:39.238
  STEP: retrieving the pod @ 01/29/24 21:15:41.26
  STEP: looking for the results for each expected name from probers @ 01/29/24 21:15:41.264
  Jan 29 21:15:41.275: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-06b4ea99-a80e-4c40-b147-f747eb217e32 contains '' instead of 'foo.example.com.'
  Jan 29 21:15:41.275: INFO: Lookups using dns-8800/dns-test-06b4ea99-a80e-4c40-b147-f747eb217e32 failed for: [jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:15:46.292: INFO: DNS probes using dns-test-06b4ea99-a80e-4c40-b147-f747eb217e32 succeeded

  STEP: changing the externalName to bar.example.com @ 01/29/24 21:15:46.292
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:15:46.302
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:15:46.302
  STEP: creating a second pod to probe DNS @ 01/29/24 21:15:46.302
  STEP: submitting the pod to kubernetes @ 01/29/24 21:15:46.302
  STEP: retrieving the pod @ 01/29/24 21:15:48.321
  STEP: looking for the results for each expected name from probers @ 01/29/24 21:15:48.325
  Jan 29 21:15:48.332: INFO: File wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:48.338: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:48.338: INFO: Lookups using dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 failed for: [wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:15:53.347: INFO: File wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:53.353: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:53.353: INFO: Lookups using dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 failed for: [wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:15:58.348: INFO: File wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:58.353: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:15:58.353: INFO: Lookups using dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 failed for: [wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:16:03.346: INFO: File wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:16:03.351: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:16:03.351: INFO: Lookups using dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 failed for: [wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:16:08.348: INFO: File wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:16:08.358: INFO: File jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local from pod  dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 29 21:16:08.358: INFO: Lookups using dns-8800/dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 failed for: [wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local]

  Jan 29 21:16:13.350: INFO: DNS probes using dns-test-7104f956-4835-4305-91b0-e3e10d1579e0 succeeded

  STEP: changing the service to type=ClusterIP @ 01/29/24 21:16:13.351
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:16:13.372
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8800.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8800.svc.cluster.local; sleep 1; done
   @ 01/29/24 21:16:13.372
  STEP: creating a third pod to probe DNS @ 01/29/24 21:16:13.372
  STEP: submitting the pod to kubernetes @ 01/29/24 21:16:13.375
  STEP: retrieving the pod @ 01/29/24 21:16:15.399
  STEP: looking for the results for each expected name from probers @ 01/29/24 21:16:15.403
  Jan 29 21:16:15.416: INFO: DNS probes using dns-test-2f802732-359d-4d65-bfd6-17cb26027271 succeeded

  Jan 29 21:16:15.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:16:15.421
  STEP: deleting the pod @ 01/29/24 21:16:15.435
  STEP: deleting the pod @ 01/29/24 21:16:15.447
  STEP: deleting the test externalName service @ 01/29/24 21:16:15.459
  STEP: Destroying namespace "dns-8800" for this suite. @ 01/29/24 21:16:15.473
• [36.269 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 01/29/24 21:16:15.479
  Jan 29 21:16:15.479: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 21:16:15.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:16:15.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:16:15.496
  STEP: Creating service test in namespace statefulset-6661 @ 01/29/24 21:16:15.501
  STEP: Creating a new StatefulSet @ 01/29/24 21:16:15.506
  Jan 29 21:16:15.516: INFO: Found 0 stateful pods, waiting for 3
  Jan 29 21:16:25.525: INFO: Found 1 stateful pods, waiting for 3
  Jan 29 21:16:35.527: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:16:35.527: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:16:35.527: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:16:35.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6661 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 21:16:35.784: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:16:35.784: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:16:35.784: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/29/24 21:16:45.807
  Jan 29 21:16:45.828: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/29/24 21:16:45.828
  STEP: Updating Pods in reverse ordinal order @ 01/29/24 21:16:55.85
  Jan 29 21:16:55.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6661 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 21:16:56.081: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:16:56.081: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:16:56.081: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 01/29/24 21:17:06.113
  Jan 29 21:17:06.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6661 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 21:17:06.347: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:17:06.347: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:17:06.347: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 21:17:16.394: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 01/29/24 21:17:26.417
  Jan 29 21:17:26.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6661 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 21:17:26.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:17:26.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:17:26.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 21:17:36.675: INFO: Deleting all statefulset in ns statefulset-6661
  Jan 29 21:17:36.678: INFO: Scaling statefulset ss2 to 0
  Jan 29 21:17:46.708: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:17:46.713: INFO: Deleting statefulset ss2
  Jan 29 21:17:46.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6661" for this suite. @ 01/29/24 21:17:46.734
• [91.263 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 01/29/24 21:17:46.744
  Jan 29 21:17:46.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 21:17:46.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:17:46.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:17:46.763
  STEP: Creating pod liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 in namespace container-probe-5011 @ 01/29/24 21:17:46.768
  Jan 29 21:17:48.785: INFO: Started pod liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 in namespace container-probe-5011
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 21:17:48.786
  Jan 29 21:17:48.790: INFO: Initial restart count of pod liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is 0
  Jan 29 21:17:58.830: INFO: Restart count of pod container-probe-5011/liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is now 1 (10.03982986s elapsed)
  Jan 29 21:18:18.908: INFO: Restart count of pod container-probe-5011/liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is now 2 (30.118361749s elapsed)
  Jan 29 21:18:38.976: INFO: Restart count of pod container-probe-5011/liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is now 3 (50.186206278s elapsed)
  Jan 29 21:18:59.044: INFO: Restart count of pod container-probe-5011/liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is now 4 (1m10.253907332s elapsed)
  Jan 29 21:20:01.251: INFO: Restart count of pod container-probe-5011/liveness-e60f4497-c59d-41b1-a3e1-7a9d6ac84b09 is now 5 (2m12.46093547s elapsed)
  Jan 29 21:20:01.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:20:01.258
  STEP: Destroying namespace "container-probe-5011" for this suite. @ 01/29/24 21:20:01.271
• [134.534 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 01/29/24 21:20:01.279
  Jan 29 21:20:01.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:20:01.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:01.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:01.303
  STEP: creating a replication controller @ 01/29/24 21:20:01.308
  Jan 29 21:20:01.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 create -f -'
  Jan 29 21:20:01.745: INFO: stderr: ""
  Jan 29 21:20:01.745: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/29/24 21:20:01.746
  Jan 29 21:20:01.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 21:20:01.846: INFO: stderr: ""
  Jan 29 21:20:01.846: INFO: stdout: "update-demo-nautilus-sq4qc update-demo-nautilus-wzs9f "
  Jan 29 21:20:01.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods update-demo-nautilus-sq4qc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 21:20:01.942: INFO: stderr: ""
  Jan 29 21:20:01.942: INFO: stdout: ""
  Jan 29 21:20:01.942: INFO: update-demo-nautilus-sq4qc is created but not running
  Jan 29 21:20:06.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 29 21:20:07.064: INFO: stderr: ""
  Jan 29 21:20:07.064: INFO: stdout: "update-demo-nautilus-sq4qc update-demo-nautilus-wzs9f "
  Jan 29 21:20:07.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods update-demo-nautilus-sq4qc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 21:20:07.162: INFO: stderr: ""
  Jan 29 21:20:07.162: INFO: stdout: "true"
  Jan 29 21:20:07.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods update-demo-nautilus-sq4qc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 21:20:07.267: INFO: stderr: ""
  Jan 29 21:20:07.267: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 21:20:07.267: INFO: validating pod update-demo-nautilus-sq4qc
  Jan 29 21:20:22.559: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 21:20:22.560: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 21:20:22.560: INFO: update-demo-nautilus-sq4qc is verified up and running
  Jan 29 21:20:22.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods update-demo-nautilus-wzs9f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 29 21:20:22.666: INFO: stderr: ""
  Jan 29 21:20:22.666: INFO: stdout: "true"
  Jan 29 21:20:22.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods update-demo-nautilus-wzs9f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 29 21:20:22.765: INFO: stderr: ""
  Jan 29 21:20:22.765: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 29 21:20:22.765: INFO: validating pod update-demo-nautilus-wzs9f
  Jan 29 21:20:22.772: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 29 21:20:22.772: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 29 21:20:22.773: INFO: update-demo-nautilus-wzs9f is verified up and running
  STEP: using delete to clean up resources @ 01/29/24 21:20:22.773
  Jan 29 21:20:22.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 delete --grace-period=0 --force -f -'
  Jan 29 21:20:22.869: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 29 21:20:22.869: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 29 21:20:22.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get rc,svc -l name=update-demo --no-headers'
  Jan 29 21:20:22.980: INFO: stderr: "No resources found in kubectl-433 namespace.\n"
  Jan 29 21:20:22.980: INFO: stdout: ""
  Jan 29 21:20:22.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-433 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 29 21:20:23.093: INFO: stderr: ""
  Jan 29 21:20:23.093: INFO: stdout: ""
  Jan 29 21:20:23.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-433" for this suite. @ 01/29/24 21:20:23.098
• [21.826 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 01/29/24 21:20:23.106
  Jan 29 21:20:23.106: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-runtime @ 01/29/24 21:20:23.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:23.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:23.128
  STEP: create the container @ 01/29/24 21:20:23.133
  W0129 21:20:23.141831      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/29/24 21:20:23.142
  STEP: get the container status @ 01/29/24 21:20:26.161
  STEP: the container should be terminated @ 01/29/24 21:20:26.166
  STEP: the termination message should be set @ 01/29/24 21:20:26.166
  Jan 29 21:20:26.166: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 01/29/24 21:20:26.166
  Jan 29 21:20:26.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-703" for this suite. @ 01/29/24 21:20:26.191
• [3.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 01/29/24 21:20:26.2
  Jan 29 21:20:26.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename runtimeclass @ 01/29/24 21:20:26.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:26.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:26.218
  STEP: getting /apis @ 01/29/24 21:20:26.223
  STEP: getting /apis/node.k8s.io @ 01/29/24 21:20:26.23
  STEP: getting /apis/node.k8s.io/v1 @ 01/29/24 21:20:26.232
  STEP: creating @ 01/29/24 21:20:26.234
  STEP: watching @ 01/29/24 21:20:26.252
  Jan 29 21:20:26.252: INFO: starting watch
  STEP: getting @ 01/29/24 21:20:26.259
  STEP: listing @ 01/29/24 21:20:26.263
  STEP: patching @ 01/29/24 21:20:26.267
  STEP: updating @ 01/29/24 21:20:26.273
  Jan 29 21:20:26.278: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 01/29/24 21:20:26.278
  STEP: deleting a collection @ 01/29/24 21:20:26.29
  Jan 29 21:20:26.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5512" for this suite. @ 01/29/24 21:20:26.31
• [0.117 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 01/29/24 21:20:26.317
  Jan 29 21:20:26.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename events @ 01/29/24 21:20:26.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:26.341
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:26.345
  STEP: Create set of events @ 01/29/24 21:20:26.35
  STEP: get a list of Events with a label in the current namespace @ 01/29/24 21:20:26.366
  STEP: delete a list of events @ 01/29/24 21:20:26.37
  Jan 29 21:20:26.370: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/29/24 21:20:26.383
  Jan 29 21:20:26.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9223" for this suite. @ 01/29/24 21:20:26.39
• [0.083 seconds]
------------------------------
SSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 01/29/24 21:20:26.4
  Jan 29 21:20:26.400: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 21:20:26.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:26.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:26.421
  STEP: Creating a test headless service @ 01/29/24 21:20:26.426
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2835.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2835.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 01/29/24 21:20:26.432
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2835.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2835.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 01/29/24 21:20:26.432
  STEP: creating a pod to probe DNS @ 01/29/24 21:20:26.432
  STEP: submitting the pod to kubernetes @ 01/29/24 21:20:26.432
  STEP: retrieving the pod @ 01/29/24 21:20:28.452
  STEP: looking for the results for each expected name from probers @ 01/29/24 21:20:28.456
  Jan 29 21:20:28.476: INFO: DNS probes using dns-2835/dns-test-af8a44d6-f533-4ce0-9e62-4cb3037d9600 succeeded

  Jan 29 21:20:28.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:20:28.481
  STEP: deleting the test headless service @ 01/29/24 21:20:28.496
  STEP: Destroying namespace "dns-2835" for this suite. @ 01/29/24 21:20:28.506
• [2.111 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 01/29/24 21:20:28.512
  Jan 29 21:20:28.512: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 21:20:28.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:28.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:28.533
  STEP: Creating service test in namespace statefulset-8268 @ 01/29/24 21:20:28.537
  STEP: Looking for a node to schedule stateful set and pod @ 01/29/24 21:20:28.543
  STEP: Creating pod with conflicting port in namespace statefulset-8268 @ 01/29/24 21:20:28.547
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8268 @ 01/29/24 21:20:28.555
  STEP: Creating statefulset with conflicting port in namespace statefulset-8268 @ 01/29/24 21:20:30.564
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8268 @ 01/29/24 21:20:30.573
  Jan 29 21:20:30.588: INFO: Observed stateful pod in namespace: statefulset-8268, name: ss-0, uid: ebf3cef2-78ac-4c01-b0c9-1bf24f8d237b, status phase: Pending. Waiting for statefulset controller to delete.
  Jan 29 21:20:30.601: INFO: Observed stateful pod in namespace: statefulset-8268, name: ss-0, uid: ebf3cef2-78ac-4c01-b0c9-1bf24f8d237b, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 29 21:20:30.609: INFO: Observed stateful pod in namespace: statefulset-8268, name: ss-0, uid: ebf3cef2-78ac-4c01-b0c9-1bf24f8d237b, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 29 21:20:30.611: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8268
  STEP: Removing pod with conflicting port in namespace statefulset-8268 @ 01/29/24 21:20:30.611
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8268 and will be in running state @ 01/29/24 21:20:30.624
  Jan 29 21:20:32.635: INFO: Deleting all statefulset in ns statefulset-8268
  Jan 29 21:20:32.639: INFO: Scaling statefulset ss to 0
  Jan 29 21:20:42.661: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:20:42.666: INFO: Deleting statefulset ss
  Jan 29 21:20:42.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8268" for this suite. @ 01/29/24 21:20:42.686
• [14.179 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 01/29/24 21:20:42.692
  Jan 29 21:20:42.692: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:20:42.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:42.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:42.713
  STEP: Setting up server cert @ 01/29/24 21:20:42.735
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:20:42.992
  STEP: Deploying the webhook pod @ 01/29/24 21:20:43.004
  STEP: Wait for the deployment to be ready @ 01/29/24 21:20:43.017
  Jan 29 21:20:43.023: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/29/24 21:20:45.037
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:20:45.052
  Jan 29 21:20:46.053: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 01/29/24 21:20:46.059
  STEP: create a pod that should be updated by the webhook @ 01/29/24 21:20:46.09
  Jan 29 21:20:46.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5833" for this suite. @ 01/29/24 21:20:46.157
  STEP: Destroying namespace "webhook-markers-992" for this suite. @ 01/29/24 21:20:46.165
• [3.477 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 01/29/24 21:20:46.171
  Jan 29 21:20:46.171: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:20:46.174
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:46.185
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:46.189
  STEP: Creating projection with secret that has name projected-secret-test-6281ad3f-b079-42f2-a18a-9feaed4e8601 @ 01/29/24 21:20:46.194
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:20:46.199
  STEP: Saw pod success @ 01/29/24 21:20:50.224
  Jan 29 21:20:50.228: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-dd7eec65-012f-4da1-bb25-ad78730105e6 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:20:50.259
  Jan 29 21:20:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1895" for this suite. @ 01/29/24 21:20:50.283
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 01/29/24 21:20:50.293
  Jan 29 21:20:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:20:50.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:20:50.308
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:20:50.313
  STEP: Creating configMap with name cm-test-opt-del-1b046c25-adea-483f-a627-d726023736a5 @ 01/29/24 21:20:50.323
  STEP: Creating configMap with name cm-test-opt-upd-5c5273ec-5dc0-480f-9561-9ca8ff76ad0f @ 01/29/24 21:20:50.329
  STEP: Creating the pod @ 01/29/24 21:20:50.335
  STEP: Deleting configmap cm-test-opt-del-1b046c25-adea-483f-a627-d726023736a5 @ 01/29/24 21:20:52.387
  STEP: Updating configmap cm-test-opt-upd-5c5273ec-5dc0-480f-9561-9ca8ff76ad0f @ 01/29/24 21:20:52.394
  STEP: Creating configMap with name cm-test-opt-create-b15c8d86-a37c-4876-9950-ce90d7de7a0c @ 01/29/24 21:20:52.399
  STEP: waiting to observe update in volume @ 01/29/24 21:20:52.407
  Jan 29 21:22:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6893" for this suite. @ 01/29/24 21:22:00.911
• [70.625 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 01/29/24 21:22:00.919
  Jan 29 21:22:00.919: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:22:00.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:22:00.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:22:00.941
  STEP: Creating Pod @ 01/29/24 21:22:00.946
  STEP: Reading file content from the nginx-container @ 01/29/24 21:22:02.964
  Jan 29 21:22:02.964: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5382 PodName:pod-sharedvolume-91361be4-f631-4928-929a-8bc7703cfa35 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:22:02.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:22:02.965: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:22:02.965: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-5382/pods/pod-sharedvolume-91361be4-f631-4928-929a-8bc7703cfa35/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jan 29 21:22:03.089: INFO: Exec stderr: ""
  Jan 29 21:22:03.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5382" for this suite. @ 01/29/24 21:22:03.095
• [2.182 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 01/29/24 21:22:03.101
  Jan 29 21:22:03.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:22:03.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:22:03.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:22:03.119
  STEP: Creating configMap configmap-7444/configmap-test-d8446196-7f2b-4dbd-89c5-11a049afcde0 @ 01/29/24 21:22:03.123
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:22:03.129
  STEP: Saw pod success @ 01/29/24 21:22:07.152
  Jan 29 21:22:07.156: INFO: Trying to get logs from node nodea08 pod pod-configmaps-899c21bf-3dd3-4ea7-9c8f-61a775eb8cea container env-test: <nil>
  STEP: delete the pod @ 01/29/24 21:22:07.165
  Jan 29 21:22:07.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7444" for this suite. @ 01/29/24 21:22:07.188
• [4.092 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 01/29/24 21:22:07.194
  Jan 29 21:22:07.194: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 21:22:07.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:22:07.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:22:07.216
  STEP: Creating pod test-grpc-bad89a80-247c-46f6-89c6-f8e173d604f6 in namespace container-probe-4062 @ 01/29/24 21:22:07.221
  Jan 29 21:22:09.241: INFO: Started pod test-grpc-bad89a80-247c-46f6-89c6-f8e173d604f6 in namespace container-probe-4062
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 21:22:09.241
  Jan 29 21:22:09.245: INFO: Initial restart count of pod test-grpc-bad89a80-247c-46f6-89c6-f8e173d604f6 is 0
  Jan 29 21:26:10.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:26:10.061
  STEP: Destroying namespace "container-probe-4062" for this suite. @ 01/29/24 21:26:10.08
• [242.893 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 01/29/24 21:26:10.087
  Jan 29 21:26:10.087: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename cronjob @ 01/29/24 21:26:10.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:26:10.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:26:10.107
  STEP: Creating a ReplaceConcurrent cronjob @ 01/29/24 21:26:10.112
  STEP: Ensuring a job is scheduled @ 01/29/24 21:26:10.117
  STEP: Ensuring exactly one is scheduled @ 01/29/24 21:27:02.123
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/29/24 21:27:02.128
  STEP: Ensuring the job is replaced with a new one @ 01/29/24 21:27:02.133
  STEP: Removing cronjob @ 01/29/24 21:28:00.14
  Jan 29 21:28:00.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3015" for this suite. @ 01/29/24 21:28:00.153
• [110.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 01/29/24 21:28:00.163
  Jan 29 21:28:00.163: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 21:28:00.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:00.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:00.185
  STEP: creating a ReplicationController @ 01/29/24 21:28:00.193
  STEP: waiting for RC to be added @ 01/29/24 21:28:00.199
  STEP: waiting for available Replicas @ 01/29/24 21:28:00.2
  STEP: patching ReplicationController @ 01/29/24 21:28:01.45
  STEP: waiting for RC to be modified @ 01/29/24 21:28:01.463
  STEP: patching ReplicationController status @ 01/29/24 21:28:01.463
  STEP: waiting for RC to be modified @ 01/29/24 21:28:01.47
  STEP: waiting for available Replicas @ 01/29/24 21:28:01.47
  STEP: fetching ReplicationController status @ 01/29/24 21:28:01.476
  STEP: patching ReplicationController scale @ 01/29/24 21:28:01.48
  STEP: waiting for RC to be modified @ 01/29/24 21:28:01.488
  STEP: waiting for ReplicationController's scale to be the max amount @ 01/29/24 21:28:01.488
  STEP: fetching ReplicationController; ensuring that it's patched @ 01/29/24 21:28:02.795
  STEP: updating ReplicationController status @ 01/29/24 21:28:02.799
  STEP: waiting for RC to be modified @ 01/29/24 21:28:02.809
  STEP: listing all ReplicationControllers @ 01/29/24 21:28:02.809
  STEP: checking that ReplicationController has expected values @ 01/29/24 21:28:02.812
  STEP: deleting ReplicationControllers by collection @ 01/29/24 21:28:02.812
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 01/29/24 21:28:02.82
  Jan 29 21:28:02.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0129 21:28:02.863025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-3345" for this suite. @ 01/29/24 21:28:02.868
• [2.710 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 01/29/24 21:28:02.879
  Jan 29 21:28:02.879: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:28:02.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:02.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:02.897
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/29/24 21:28:02.902
  E0129 21:28:03.864093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:04.864974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:05.865368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:06.865982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:28:06.927
  Jan 29 21:28:06.931: INFO: Trying to get logs from node nodeb29 pod pod-ad66736b-6c81-48dd-a929-a91b51081791 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:28:06.959
  Jan 29 21:28:06.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6198" for this suite. @ 01/29/24 21:28:06.994
• [4.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 01/29/24 21:28:07.014
  Jan 29 21:28:07.014: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 21:28:07.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:07.027
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:07.031
  STEP: apply creating a deployment @ 01/29/24 21:28:07.036
  Jan 29 21:28:07.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5084" for this suite. @ 01/29/24 21:28:07.055
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 01/29/24 21:28:07.064
  Jan 29 21:28:07.064: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename init-container @ 01/29/24 21:28:07.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:07.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:07.083
  STEP: creating the pod @ 01/29/24 21:28:07.088
  Jan 29 21:28:07.088: INFO: PodSpec: initContainers in spec.initContainers
  E0129 21:28:07.866210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:08.866840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:09.866873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:10.867912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:28:11.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4852" for this suite. @ 01/29/24 21:28:11.734
• [4.676 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 01/29/24 21:28:11.742
  Jan 29 21:28:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename proxy @ 01/29/24 21:28:11.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:11.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:11.759
  Jan 29 21:28:11.763: INFO: Creating pod...
  E0129 21:28:11.868779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:12.869671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:28:13.780: INFO: Creating service...
  Jan 29 21:28:13.796: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=DELETE
  Jan 29 21:28:13.806: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 29 21:28:13.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=OPTIONS
  Jan 29 21:28:13.811: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 29 21:28:13.811: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=PATCH
  Jan 29 21:28:13.815: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 29 21:28:13.815: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=POST
  Jan 29 21:28:13.819: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 29 21:28:13.819: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=PUT
  Jan 29 21:28:13.822: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 29 21:28:13.822: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=DELETE
  Jan 29 21:28:13.828: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 29 21:28:13.828: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jan 29 21:28:13.833: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 29 21:28:13.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=PATCH
  Jan 29 21:28:13.839: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 29 21:28:13.839: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=POST
  Jan 29 21:28:13.844: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 29 21:28:13.844: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=PUT
  Jan 29 21:28:13.848: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 29 21:28:13.848: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=GET
  Jan 29 21:28:13.851: INFO: http.Client request:GET StatusCode:301
  Jan 29 21:28:13.851: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=GET
  Jan 29 21:28:13.855: INFO: http.Client request:GET StatusCode:301
  Jan 29 21:28:13.855: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/pods/agnhost/proxy?method=HEAD
  Jan 29 21:28:13.857: INFO: http.Client request:HEAD StatusCode:301
  Jan 29 21:28:13.858: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5616/services/e2e-proxy-test-service/proxy?method=HEAD
  Jan 29 21:28:13.861: INFO: http.Client request:HEAD StatusCode:301
  Jan 29 21:28:13.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-5616" for this suite. @ 01/29/24 21:28:13.867
  E0129 21:28:13.870052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [2.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 01/29/24 21:28:13.875
  Jan 29 21:28:13.875: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subpath @ 01/29/24 21:28:13.876
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:13.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:13.895
  STEP: Setting up data @ 01/29/24 21:28:13.899
  STEP: Creating pod pod-subpath-test-downwardapi-vrhr @ 01/29/24 21:28:13.908
  STEP: Creating a pod to test atomic-volume-subpath @ 01/29/24 21:28:13.908
  E0129 21:28:14.871162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:15.871813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:16.872915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:17.873773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:18.874200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:19.874764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:20.875677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:21.876219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:22.877196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:23.877783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:24.878311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:25.878836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:26.879591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:27.879744      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:28.880717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:29.881408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:30.882482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:31.883119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:32.883790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:33.884313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:34.884778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:35.885271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:36.885459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:37.886390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:28:37.991
  Jan 29 21:28:37.995: INFO: Trying to get logs from node nodeb29 pod pod-subpath-test-downwardapi-vrhr container test-container-subpath-downwardapi-vrhr: <nil>
  STEP: delete the pod @ 01/29/24 21:28:38.008
  STEP: Deleting pod pod-subpath-test-downwardapi-vrhr @ 01/29/24 21:28:38.024
  Jan 29 21:28:38.024: INFO: Deleting pod "pod-subpath-test-downwardapi-vrhr" in namespace "subpath-8443"
  Jan 29 21:28:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8443" for this suite. @ 01/29/24 21:28:38.034
• [24.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 01/29/24 21:28:38.049
  Jan 29 21:28:38.049: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:28:38.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:38.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:38.07
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/29/24 21:28:38.075
  Jan 29 21:28:38.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-8475 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 29 21:28:38.183: INFO: stderr: ""
  Jan 29 21:28:38.184: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 01/29/24 21:28:38.184
  E0129 21:28:38.886608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:39.887069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:40.887587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:41.888146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:42.888317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/29/24 21:28:43.235
  Jan 29 21:28:43.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-8475 get pod e2e-test-httpd-pod -o json'
  Jan 29 21:28:43.333: INFO: stderr: ""
  Jan 29 21:28:43.333: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2024-01-29T21:28:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8475\",\n        \"resourceVersion\": \"20279\",\n        \"uid\": \"2b3a776e-d149-4aa9-ac80-47416d154699\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-wzqbk\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"nodea08\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-wzqbk\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-29T21:28:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-29T21:28:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-29T21:28:38Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-29T21:28:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://c74e813923136a88db08d2ed6b56b7b1abb0ec63105d7132eff9efcde9c738a6\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-29T21:28:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.100.28\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.31\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.31\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-29T21:28:38Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 01/29/24 21:28:43.333
  Jan 29 21:28:43.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-8475 replace -f -'
  Jan 29 21:28:43.690: INFO: stderr: ""
  Jan 29 21:28:43.690: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 01/29/24 21:28:43.69
  Jan 29 21:28:43.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-8475 delete pods e2e-test-httpd-pod'
  E0129 21:28:43.889159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:44.889934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:45.890454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:28:46.003: INFO: stderr: ""
  Jan 29 21:28:46.003: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 29 21:28:46.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8475" for this suite. @ 01/29/24 21:28:46.009
• [7.966 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 01/29/24 21:28:46.016
  Jan 29 21:28:46.016: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename init-container @ 01/29/24 21:28:46.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:28:46.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:28:46.035
  STEP: creating the pod @ 01/29/24 21:28:46.039
  Jan 29 21:28:46.040: INFO: PodSpec: initContainers in spec.initContainers
  E0129 21:28:46.890790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:47.891235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:48.891575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:49.892147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:50.892804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:51.893384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:52.893586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:53.894061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:54.894596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:55.895127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:56.895639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:57.896629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:58.897134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:28:59.897729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:00.900414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:01.900620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:02.900841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:03.901534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:04.902153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:05.902646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:06.903265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:07.904103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:08.904673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:09.905259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:10.905772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:11.906284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:12.906614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:13.906873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:14.907339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:15.907832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:16.908045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:17.908714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:18.909086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:19.909259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:20.909720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:21.909989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:22.910861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:23.911143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:24.911654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:25.912102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:26.912287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:27.913144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:28.913362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:29.913815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:29:30.159: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7b731584-8518-4e87-b2a1-f7428ebd382f", GenerateName:"", Namespace:"init-container-1542", SelfLink:"", UID:"949691cf-d78c-4958-8514-94ac7424b287", ResourceVersion:"20418", Generation:0, CreationTimestamp:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"40058782"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a2690), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 29, 21, 29, 30, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0046a26f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-5nmgn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e059e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5nmgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5nmgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-5nmgn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0055b0ed8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"nodea08", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009064d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0055b0f60)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0055b0f80)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0055b0f88), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0055b0f8c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0073416d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.100.28", PodIP:"10.244.1.32", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.32"}}, StartTime:time.Date(2024, time.January, 29, 21, 28, 46, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009065b0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000906620)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://a4b356fc47da0b898e930b334acb4c7887a4506ad0a8e058578ac57d2b99b933", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e05a60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e05a40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0055b100f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jan 29 21:29:30.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1542" for this suite. @ 01/29/24 21:29:30.166
• [44.157 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 01/29/24 21:29:30.173
  Jan 29 21:29:30.174: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubelet-test @ 01/29/24 21:29:30.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:29:30.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:29:30.195
  E0129 21:29:30.914705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:31.914801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:29:32.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2218" for this suite. @ 01/29/24 21:29:32.26
• [2.093 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 01/29/24 21:29:32.267
  Jan 29 21:29:32.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename runtimeclass @ 01/29/24 21:29:32.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:29:32.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:29:32.286
  E0129 21:29:32.915022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:33.916048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:29:34.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9452" for this suite. @ 01/29/24 21:29:34.327
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 01/29/24 21:29:34.335
  Jan 29 21:29:34.335: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:29:34.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:29:34.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:29:34.355
  STEP: Creating a pod to test substitution in volume subpath @ 01/29/24 21:29:34.36
  E0129 21:29:34.916739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:35.917376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:36.917396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:37.918518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:29:38.384
  Jan 29 21:29:38.388: INFO: Trying to get logs from node nodeb29 pod var-expansion-81a63bd0-be7b-4e1c-8330-73c1f7a5aec1 container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 21:29:38.399
  Jan 29 21:29:38.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8740" for this suite. @ 01/29/24 21:29:38.419
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 01/29/24 21:29:38.428
  Jan 29 21:29:38.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 21:29:38.43
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:29:38.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:29:38.447
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 01/29/24 21:29:38.452
  Jan 29 21:29:38.465: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0129 21:29:38.919002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:39.918680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:40.919364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:41.920282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:42.920524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:29:43.473: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 21:29:43.473
  STEP: getting scale subresource @ 01/29/24 21:29:43.474
  STEP: updating a scale subresource @ 01/29/24 21:29:43.477
  STEP: verifying the replicaset Spec.Replicas was modified @ 01/29/24 21:29:43.485
  STEP: Patch a scale subresource @ 01/29/24 21:29:43.488
  Jan 29 21:29:43.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2430" for this suite. @ 01/29/24 21:29:43.509
• [5.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 01/29/24 21:29:43.52
  Jan 29 21:29:43.520: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 21:29:43.522
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:29:43.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:29:43.536
  E0129 21:29:43.920875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:44.921102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:45.922332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:46.922691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:47.923081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:48.924190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:49.925245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:50.925578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:51.925642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:52.925982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:53.927147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:54.927856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:55.928788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:56.929060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:57.930309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:58.930327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:29:59.930713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 01/29/24 21:30:00.544
  E0129 21:30:00.931689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:01.932027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:02.933172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:03.933939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:04.934443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/29/24 21:30:05.55
  STEP: Ensuring resource quota status is calculated @ 01/29/24 21:30:05.559
  E0129 21:30:05.934540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:06.935037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 01/29/24 21:30:07.565
  STEP: Ensuring resource quota status captures configMap creation @ 01/29/24 21:30:07.58
  E0129 21:30:07.935912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:08.936461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 01/29/24 21:30:09.587
  STEP: Ensuring resource quota status released usage @ 01/29/24 21:30:09.594
  E0129 21:30:09.937269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:10.937884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:30:11.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7539" for this suite. @ 01/29/24 21:30:11.609
• [28.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 01/29/24 21:30:11.621
  Jan 29 21:30:11.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption @ 01/29/24 21:30:11.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:30:11.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:30:11.648
  STEP: Creating a kubernetes client @ 01/29/24 21:30:11.653
  Jan 29 21:30:11.653: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption-2 @ 01/29/24 21:30:11.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:30:11.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:30:11.674
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:30:11.684
  E0129 21:30:11.938806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:12.939964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:30:13.701
  E0129 21:30:13.940455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:14.940964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:30:15.722
  E0129 21:30:15.941020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:16.941624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 01/29/24 21:30:17.735
  STEP: listing a collection of PDBs in namespace disruption-5140 @ 01/29/24 21:30:17.74
  STEP: deleting a collection of PDBs @ 01/29/24 21:30:17.744
  STEP: Waiting for the PDB collection to be deleted @ 01/29/24 21:30:17.758
  Jan 29 21:30:17.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:30:17.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5673" for this suite. @ 01/29/24 21:30:17.773
  STEP: Destroying namespace "disruption-5140" for this suite. @ 01/29/24 21:30:17.78
• [6.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 01/29/24 21:30:17.787
  Jan 29 21:30:17.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:30:17.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:30:17.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:30:17.808
  STEP: Creating secret with name s-test-opt-del-de164514-6df3-4f29-ae13-d0b882207993 @ 01/29/24 21:30:17.818
  STEP: Creating secret with name s-test-opt-upd-935379fa-aa58-4921-93c6-b8ac8f9a2052 @ 01/29/24 21:30:17.823
  STEP: Creating the pod @ 01/29/24 21:30:17.829
  E0129 21:30:17.941820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:18.941936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-de164514-6df3-4f29-ae13-d0b882207993 @ 01/29/24 21:30:19.882
  STEP: Updating secret s-test-opt-upd-935379fa-aa58-4921-93c6-b8ac8f9a2052 @ 01/29/24 21:30:19.889
  STEP: Creating secret with name s-test-opt-create-14afac17-551a-46cf-9a87-803cd765572a @ 01/29/24 21:30:19.895
  STEP: waiting to observe update in volume @ 01/29/24 21:30:19.9
  E0129 21:30:19.942832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:20.943393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:21.944040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:22.944359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:23.945446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:24.945971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:25.946375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:26.946972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:27.947050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:28.947856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:29.948332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:30.948912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:31.949409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:32.950617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:33.951083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:34.951453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:35.952115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:36.952161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:37.953074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:38.953713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:39.954564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:40.955213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:41.955874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:42.956297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:43.956799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:44.957680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:45.958278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:46.958438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:47.959515      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:48.960355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:49.960961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:50.961903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:51.962384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:52.963112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:53.963658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:54.964288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:55.964845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:56.965388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:57.966372      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:58.966554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:30:59.967055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:00.968811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:01.967858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:02.968135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:03.968674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:04.968937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:05.969467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:06.970334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:07.970745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:08.971567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:09.971904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:10.972049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:11.972568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:12.972683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:13.973374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:14.974133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:15.974687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:16.975344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:17.976503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:18.976832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:19.977325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:20.977661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:21.978127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:22.978817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:23.979262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:24.979973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:25.980451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:26.980974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:27.981890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:28.981963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:29.982420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:30.983148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:31.983643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:32.984702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:33.985246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:31:34.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6411" for this suite. @ 01/29/24 21:31:34.42
• [76.640 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 01/29/24 21:31:34.428
  Jan 29 21:31:34.428: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subpath @ 01/29/24 21:31:34.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:31:34.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:31:34.447
  STEP: Setting up data @ 01/29/24 21:31:34.452
  STEP: Creating pod pod-subpath-test-projected-gpb4 @ 01/29/24 21:31:34.463
  STEP: Creating a pod to test atomic-volume-subpath @ 01/29/24 21:31:34.463
  E0129 21:31:34.985313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:35.986186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:36.987369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:37.988366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:38.988919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:39.989536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:40.990253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:41.990913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:42.991091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:43.991695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:44.991893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:45.992480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:46.993163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:47.994138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:48.994109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:49.994779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:50.995692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:51.995748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:52.996219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:53.996880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:54.997976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:31:55.998659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:31:56.544
  Jan 29 21:31:56.549: INFO: Trying to get logs from node nodeb29 pod pod-subpath-test-projected-gpb4 container test-container-subpath-projected-gpb4: <nil>
  STEP: delete the pod @ 01/29/24 21:31:56.576
  STEP: Deleting pod pod-subpath-test-projected-gpb4 @ 01/29/24 21:31:56.594
  Jan 29 21:31:56.594: INFO: Deleting pod "pod-subpath-test-projected-gpb4" in namespace "subpath-575"
  Jan 29 21:31:56.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-575" for this suite. @ 01/29/24 21:31:56.602
• [22.182 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 01/29/24 21:31:56.611
  Jan 29 21:31:56.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 21:31:56.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:31:56.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:31:56.63
  STEP: creating a Deployment @ 01/29/24 21:31:56.638
  STEP: waiting for Deployment to be created @ 01/29/24 21:31:56.646
  STEP: waiting for all Replicas to be Ready @ 01/29/24 21:31:56.649
  Jan 29 21:31:56.652: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.652: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.660: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.660: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.672: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.672: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.688: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 29 21:31:56.688: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0129 21:31:56.998872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:31:57.671: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jan 29 21:31:57.671: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0129 21:31:57.999784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:31:58.266: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 01/29/24 21:31:58.266
  W0129 21:31:58.275074      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 29 21:31:58.277: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 01/29/24 21:31:58.277
  Jan 29 21:31:58.280: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 0
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.281: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.287: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.287: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.325: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.325: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:58.334: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:31:58.334: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:31:58.344: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:31:58.344: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  E0129 21:31:58.999861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:31:59.690: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:59.690: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:31:59.712: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  STEP: listing Deployments @ 01/29/24 21:31:59.712
  Jan 29 21:31:59.719: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 01/29/24 21:31:59.719
  Jan 29 21:31:59.737: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 01/29/24 21:31:59.737
  Jan 29 21:31:59.748: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 29 21:31:59.748: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 29 21:31:59.763: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 29 21:31:59.779: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0129 21:32:00.000412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:00.723: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 29 21:32:00.744: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 29 21:32:00.764: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0129 21:32:01.001502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:02.002585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:02.318: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 01/29/24 21:32:02.337
  STEP: fetching the DeploymentStatus @ 01/29/24 21:32:02.346
  Jan 29 21:32:02.352: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:32:02.352: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:32:02.352: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:32:02.353: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 1
  Jan 29 21:32:02.353: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:32:02.353: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:32:02.353: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 2
  Jan 29 21:32:02.353: INFO: observed Deployment test-deployment in namespace deployment-9850 with ReadyReplicas 3
  STEP: deleting the Deployment @ 01/29/24 21:32:02.353
  Jan 29 21:32:02.363: INFO: observed event type MODIFIED
  Jan 29 21:32:02.363: INFO: observed event type MODIFIED
  Jan 29 21:32:02.363: INFO: observed event type MODIFIED
  Jan 29 21:32:02.363: INFO: observed event type MODIFIED
  Jan 29 21:32:02.363: INFO: observed event type MODIFIED
  Jan 29 21:32:02.364: INFO: observed event type MODIFIED
  Jan 29 21:32:02.364: INFO: observed event type MODIFIED
  Jan 29 21:32:02.364: INFO: observed event type MODIFIED
  Jan 29 21:32:02.364: INFO: observed event type MODIFIED
  Jan 29 21:32:02.369: INFO: Log out all the ReplicaSets if there is no deployment created
  Jan 29 21:32:02.373: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-9850  00213aef-47fc-41a2-8e96-524a3c66b077 20954 3 2024-01-29 21:31:56 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 2890d391-219e-4039-b4d0-604fc17e2520 0xc0049c6707 0xc0049c6708}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:31:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2890d391-219e-4039-b4d0-604fc17e2520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:31:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c6790 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jan 29 21:32:02.378: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-9850  4d3ed187-6fc2-4321-9ba7-2db8fafa6b57 21044 4 2024-01-29 21:31:58 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 2890d391-219e-4039-b4d0-604fc17e2520 0xc0049c67f7 0xc0049c67f8}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:32:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2890d391-219e-4039-b4d0-604fc17e2520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:32:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c6880 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jan 29 21:32:02.381: INFO: pod: "test-deployment-5b5dcbcd95-crzsq":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-crzsq test-deployment-5b5dcbcd95- deployment-9850  cb20cfb1-469e-49db-a023-9fa42b6d663a 21038 0 2024-01-29 21:31:58 +0000 UTC 2024-01-29 21:32:03 +0000 UTC 0xc0049c6e88 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 4d3ed187-6fc2-4321-9ba7-2db8fafa6b57 0xc0049c6eb7 0xc0049c6eb8}] [] [{kube-controller-manager Update v1 2024-01-29 21:31:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4d3ed187-6fc2-4321-9ba7-2db8fafa6b57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:31:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c8cpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c8cpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.37,StartTime:2024-01-29 21:31:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:31:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/coredgeio/pause:3.9,ImageID:docker.io/coredgeio/pause@sha256:0fc1f3b764be56f7c881a69cbd553ae25a2b5523c6901fbacb8270307c29d0c4,ContainerID:containerd://27b5051ad772311ebe680e0bbf5a1c46ca58fa3e90ff6db02a66a75b1e9d74e3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.37,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 29 21:32:02.382: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-9850  a538eac2-c41d-47e5-b3d3-1f67b4904e2d 21036 2 2024-01-29 21:31:59 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 2890d391-219e-4039-b4d0-604fc17e2520 0xc0049c68e7 0xc0049c68e8}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:32:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2890d391-219e-4039-b4d0-604fc17e2520\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:32:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c6970 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jan 29 21:32:02.387: INFO: pod: "test-deployment-6fc78d85c6-4wd98":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-4wd98 test-deployment-6fc78d85c6- deployment-9850  94781a34-5284-4304-8a52-e8dddcd02f17 21050 0 2024-01-29 21:31:59 +0000 UTC 2024-01-29 21:32:03 +0000 UTC 0xc003f0e938 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 a538eac2-c41d-47e5-b3d3-1f67b4904e2d 0xc003f0e967 0xc003f0e968}] [] [{kube-controller-manager Update v1 2024-01-29 21:31:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a538eac2-c41d-47e5-b3d3-1f67b4904e2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:32:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.38\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlztj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlztj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:31:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.38,StartTime:2024-01-29 21:31:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:32:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://69729a8d9887fb82025ea68e605c96bed1c17cb3e1eddee83c3537fb82292abf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.38,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 29 21:32:02.388: INFO: pod: "test-deployment-6fc78d85c6-nq44x":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-nq44x test-deployment-6fc78d85c6- deployment-9850  18d71471-a68c-4729-942b-3467ba2a58f8 21051 0 2024-01-29 21:32:00 +0000 UTC 2024-01-29 21:32:03 +0000 UTC 0xc003f0eb40 map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 a538eac2-c41d-47e5-b3d3-1f67b4904e2d 0xc003f0eb77 0xc003f0eb78}] [] [{kube-controller-manager Update v1 2024-01-29 21:32:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a538eac2-c41d-47e5-b3d3-1f67b4904e2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:32:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlg2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlg2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodeb29,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:32:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.129,PodIP:10.244.2.117,StartTime:2024-01-29 21:32:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:32:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0a410c8b9dfa2f7d432f67abf0b8eafca5de6d257d1138f3651f81ed91061a7a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.2.117,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 29 21:32:02.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9850" for this suite. @ 01/29/24 21:32:02.393
• [5.788 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 01/29/24 21:32:02.399
  Jan 29 21:32:02.399: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:32:02.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:02.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:02.414
  STEP: Setting up server cert @ 01/29/24 21:32:02.435
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:32:02.753
  STEP: Deploying the webhook pod @ 01/29/24 21:32:02.764
  STEP: Wait for the deployment to be ready @ 01/29/24 21:32:02.779
  Jan 29 21:32:02.787: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:32:03.003340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:04.004307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:32:04.801
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:32:04.815
  E0129 21:32:05.005185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:05.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 29 21:32:05.821: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:32:06.006272      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1584-crds.webhook.example.com via the AdmissionRegistration API @ 01/29/24 21:32:06.338
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/29/24 21:32:06.367
  E0129 21:32:07.006946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:08.007362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:08.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2334" for this suite. @ 01/29/24 21:32:08.966
  STEP: Destroying namespace "webhook-markers-2960" for this suite. @ 01/29/24 21:32:08.974
• [6.584 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 01/29/24 21:32:08.985
  Jan 29 21:32:08.985: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sysctl @ 01/29/24 21:32:08.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:09.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:09.006
  E0129 21:32:09.007521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 01/29/24 21:32:09.011
  STEP: Watching for error events or started pod @ 01/29/24 21:32:09.019
  E0129 21:32:10.008379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:11.008940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 01/29/24 21:32:11.026
  E0129 21:32:12.009029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:13.009976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 01/29/24 21:32:13.042
  STEP: Getting logs from the pod @ 01/29/24 21:32:13.042
  STEP: Checking that the sysctl is actually updated @ 01/29/24 21:32:13.052
  Jan 29 21:32:13.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9800" for this suite. @ 01/29/24 21:32:13.058
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 01/29/24 21:32:13.067
  Jan 29 21:32:13.067: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:32:13.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:13.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:13.088
  STEP: creating a secret @ 01/29/24 21:32:13.093
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 01/29/24 21:32:13.098
  STEP: patching the secret @ 01/29/24 21:32:13.102
  STEP: deleting the secret using a LabelSelector @ 01/29/24 21:32:13.114
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 01/29/24 21:32:13.121
  Jan 29 21:32:13.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6753" for this suite. @ 01/29/24 21:32:13.131
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 01/29/24 21:32:13.139
  Jan 29 21:32:13.139: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:32:13.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:13.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:13.159
  STEP: creating a ConfigMap @ 01/29/24 21:32:13.164
  STEP: fetching the ConfigMap @ 01/29/24 21:32:13.169
  STEP: patching the ConfigMap @ 01/29/24 21:32:13.172
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 01/29/24 21:32:13.177
  STEP: deleting the ConfigMap by collection with a label selector @ 01/29/24 21:32:13.182
  STEP: listing all ConfigMaps in test namespace @ 01/29/24 21:32:13.189
  Jan 29 21:32:13.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7256" for this suite. @ 01/29/24 21:32:13.197
• [0.066 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 01/29/24 21:32:13.206
  Jan 29 21:32:13.206: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:32:13.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:13.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:13.224
  STEP: Setting up server cert @ 01/29/24 21:32:13.246
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:32:13.566
  STEP: Deploying the webhook pod @ 01/29/24 21:32:13.573
  STEP: Wait for the deployment to be ready @ 01/29/24 21:32:13.587
  Jan 29 21:32:13.602: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:32:14.010382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:15.010781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:32:15.618
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:32:15.63
  E0129 21:32:16.011725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:16.631: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/29/24 21:32:16.714
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/29/24 21:32:16.754
  STEP: Deleting the collection of validation webhooks @ 01/29/24 21:32:16.785
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/29/24 21:32:16.827
  Jan 29 21:32:16.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3357" for this suite. @ 01/29/24 21:32:16.871
  STEP: Destroying namespace "webhook-markers-2472" for this suite. @ 01/29/24 21:32:16.876
• [3.677 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 01/29/24 21:32:16.883
  Jan 29 21:32:16.883: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename podtemplate @ 01/29/24 21:32:16.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:16.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:16.901
  STEP: Create set of pod templates @ 01/29/24 21:32:16.905
  Jan 29 21:32:16.911: INFO: created test-podtemplate-1
  Jan 29 21:32:16.921: INFO: created test-podtemplate-2
  Jan 29 21:32:16.925: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 01/29/24 21:32:16.925
  STEP: delete collection of pod templates @ 01/29/24 21:32:16.93
  Jan 29 21:32:16.930: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 01/29/24 21:32:16.943
  Jan 29 21:32:16.943: INFO: requesting list of pod templates to confirm quantity
  Jan 29 21:32:16.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8707" for this suite. @ 01/29/24 21:32:16.952
• [0.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 01/29/24 21:32:16.96
  Jan 29 21:32:16.960: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:32:16.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:16.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:16.982
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:32:16.987
  E0129 21:32:17.011769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:18.012437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:19.012660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:20.012888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:32:21.012
  E0129 21:32:21.013314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:21.016: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-1fd516cb-5819-4377-ad50-45e8df653d61 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:32:21.025
  Jan 29 21:32:21.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4985" for this suite. @ 01/29/24 21:32:21.046
• [4.092 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 01/29/24 21:32:21.052
  Jan 29 21:32:21.052: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:32:21.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:21.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:21.07
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:32:21.075
  E0129 21:32:22.014084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:23.014912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:24.015151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:25.015687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:32:25.103
  Jan 29 21:32:25.108: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-2f3294fc-da16-422d-b688-b49ecc42fbe5 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:32:25.117
  Jan 29 21:32:25.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1377" for this suite. @ 01/29/24 21:32:25.137
• [4.092 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 01/29/24 21:32:25.144
  Jan 29 21:32:25.144: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 21:32:25.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:25.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:25.165
  Jan 29 21:32:25.170: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:32:26.015944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/29/24 21:32:26.588
  Jan 29 21:32:26.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 create -f -'
  E0129 21:32:27.015971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:27.378: INFO: stderr: ""
  Jan 29 21:32:27.378: INFO: stdout: "e2e-test-crd-publish-openapi-9492-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 29 21:32:27.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9492-crds test-cr'
  Jan 29 21:32:27.486: INFO: stderr: ""
  Jan 29 21:32:27.486: INFO: stdout: "e2e-test-crd-publish-openapi-9492-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jan 29 21:32:27.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 apply -f -'
  Jan 29 21:32:27.757: INFO: stderr: ""
  Jan 29 21:32:27.757: INFO: stdout: "e2e-test-crd-publish-openapi-9492-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 29 21:32:27.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-4502 --namespace=crd-publish-openapi-4502 delete e2e-test-crd-publish-openapi-9492-crds test-cr'
  Jan 29 21:32:27.864: INFO: stderr: ""
  Jan 29 21:32:27.864: INFO: stdout: "e2e-test-crd-publish-openapi-9492-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/29/24 21:32:27.864
  Jan 29 21:32:27.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-4502 explain e2e-test-crd-publish-openapi-9492-crds'
  E0129 21:32:28.016142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:28.139: INFO: stderr: ""
  Jan 29 21:32:28.139: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-9492-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0129 21:32:29.017028      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:29.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4502" for this suite. @ 01/29/24 21:32:29.616
• [4.478 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 01/29/24 21:32:29.625
  Jan 29 21:32:29.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename limitrange @ 01/29/24 21:32:29.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:29.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:29.648
  STEP: Creating a LimitRange @ 01/29/24 21:32:29.654
  STEP: Setting up watch @ 01/29/24 21:32:29.654
  STEP: Submitting a LimitRange @ 01/29/24 21:32:29.76
  STEP: Verifying LimitRange creation was observed @ 01/29/24 21:32:29.767
  STEP: Fetching the LimitRange to ensure it has proper values @ 01/29/24 21:32:29.767
  Jan 29 21:32:29.771: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 29 21:32:29.771: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 01/29/24 21:32:29.771
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 01/29/24 21:32:29.78
  Jan 29 21:32:29.785: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 29 21:32:29.785: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 01/29/24 21:32:29.785
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 01/29/24 21:32:29.794
  Jan 29 21:32:29.797: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jan 29 21:32:29.797: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 01/29/24 21:32:29.797
  STEP: Failing to create a Pod with more than max resources @ 01/29/24 21:32:29.801
  STEP: Updating a LimitRange @ 01/29/24 21:32:29.804
  STEP: Verifying LimitRange updating is effective @ 01/29/24 21:32:29.81
  E0129 21:32:30.018145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:31.018831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 01/29/24 21:32:31.817
  STEP: Failing to create a Pod with more than max resources @ 01/29/24 21:32:31.825
  STEP: Deleting a LimitRange @ 01/29/24 21:32:31.83
  STEP: Verifying the LimitRange was deleted @ 01/29/24 21:32:31.837
  E0129 21:32:32.019107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:33.019645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:34.020122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:35.020600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:36.021142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:36.845: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 01/29/24 21:32:36.845
  Jan 29 21:32:36.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-3031" for this suite. @ 01/29/24 21:32:36.861
• [7.242 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 01/29/24 21:32:36.87
  Jan 29 21:32:36.870: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:32:36.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:36.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:36.891
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:32:36.896
  E0129 21:32:37.022098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:38.022879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:39.023452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:40.024021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:32:40.92
  Jan 29 21:32:40.924: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-df2cb0f2-d30f-4f75-a796-d0502c747ff3 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:32:40.933
  Jan 29 21:32:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6689" for this suite. @ 01/29/24 21:32:40.956
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 01/29/24 21:32:40.965
  Jan 29 21:32:40.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:32:40.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:40.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:40.985
  STEP: Setting up server cert @ 01/29/24 21:32:41.007
  E0129 21:32:41.024805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:32:41.394
  STEP: Deploying the webhook pod @ 01/29/24 21:32:41.403
  STEP: Wait for the deployment to be ready @ 01/29/24 21:32:41.42
  Jan 29 21:32:41.431: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:32:42.025859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:43.026625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:43.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:32:44.027350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:45.027968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:45.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:32:46.029104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:47.030313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:47.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:32:48.030915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:49.031547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:49.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:32:50.031766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:51.032387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:51.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 32, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:32:52.033159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:53.033853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:32:53.452
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:32:53.469
  E0129 21:32:54.034140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:32:54.469: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/29/24 21:32:54.475
  STEP: create a pod @ 01/29/24 21:32:54.501
  E0129 21:32:55.034183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:56.034656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 01/29/24 21:32:56.519
  Jan 29 21:32:56.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=webhook-9309 attach --namespace=webhook-9309 to-be-attached-pod -i -c=container1'
  Jan 29 21:32:56.658: INFO: rc: 1
  Jan 29 21:32:56.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9309" for this suite. @ 01/29/24 21:32:56.709
  STEP: Destroying namespace "webhook-markers-6810" for this suite. @ 01/29/24 21:32:56.714
• [15.755 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 01/29/24 21:32:56.724
  Jan 29 21:32:56.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:32:56.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:56.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:56.742
  STEP: Starting the proxy @ 01/29/24 21:32:56.746
  Jan 29 21:32:56.746: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-8823 proxy --unix-socket=/tmp/kubectl-proxy-unix644845796/test'
  STEP: retrieving proxy /api/ output @ 01/29/24 21:32:56.819
  Jan 29 21:32:56.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8823" for this suite. @ 01/29/24 21:32:56.827
• [0.110 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 01/29/24 21:32:56.834
  Jan 29 21:32:56.835: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-watch @ 01/29/24 21:32:56.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:32:56.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:32:56.855
  Jan 29 21:32:56.860: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:32:57.034884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:58.035844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:32:59.036374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 01/29/24 21:32:59.415
  Jan 29 21:32:59.420: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:32:59Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:32:59Z]] name:name1 resourceVersion:21641 uid:d3704e67-1136-4e71-9807-78a51156d4bf] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:00.036791      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:01.037383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:02.037402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:03.038246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:04.038749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:05.039536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:06.040058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:07.041050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:08.041448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:09.041895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 01/29/24 21:33:09.421
  Jan 29 21:33:09.430: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:33:09Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:33:09Z]] name:name2 resourceVersion:21687 uid:550a44c5-6a1b-4197-982f-b3487ef4d083] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:10.042435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:11.043262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:12.043991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:13.044477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:14.045349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:15.045862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:16.046378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:17.047395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:18.047872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:19.048365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 01/29/24 21:33:19.432
  Jan 29 21:33:19.442: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:32:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:33:19Z]] name:name1 resourceVersion:21703 uid:d3704e67-1136-4e71-9807-78a51156d4bf] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:20.048860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:21.049427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:22.049852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:23.050214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:24.050797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:25.051387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:26.052023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:27.052985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:28.053438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:29.054017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 01/29/24 21:33:29.442
  Jan 29 21:33:29.452: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:33:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:33:29Z]] name:name2 resourceVersion:21719 uid:550a44c5-6a1b-4197-982f-b3487ef4d083] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:30.054694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:31.055204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:32.055336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:33.055908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:34.056418      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:35.056874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:36.057451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:37.058322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:38.058866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:39.059404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 01/29/24 21:33:39.453
  Jan 29 21:33:39.463: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:32:59Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:33:19Z]] name:name1 resourceVersion:21735 uid:d3704e67-1136-4e71-9807-78a51156d4bf] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:40.059749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:41.059792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:42.060008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:43.060586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:44.061087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:45.061635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:46.062116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:47.063026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:48.063524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:49.063974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 01/29/24 21:33:49.463
  Jan 29 21:33:49.474: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-29T21:33:09Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-29T21:33:29Z]] name:name2 resourceVersion:21750 uid:550a44c5-6a1b-4197-982f-b3487ef4d083] num:map[num1:9223372036854775807 num2:1000000]]}
  E0129 21:33:50.065094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:51.065657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:52.065986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:53.066526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:54.067148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:55.067757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:56.068349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:57.069460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:58.069977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:33:59.070535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:33:59.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7985" for this suite. @ 01/29/24 21:34:00.001
• [63.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 01/29/24 21:34:00.009
  Jan 29 21:34:00.009: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename pods @ 01/29/24 21:34:00.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:34:00.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:34:00.03
  STEP: creating the pod @ 01/29/24 21:34:00.035
  STEP: submitting the pod to kubernetes @ 01/29/24 21:34:00.035
  W0129 21:34:00.045801      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0129 21:34:00.071392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:01.071936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 01/29/24 21:34:02.059
  STEP: updating the pod @ 01/29/24 21:34:02.064
  E0129 21:34:02.072760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:34:02.581: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fcc1fa82-416a-497a-954e-16b93eae6e8e"
  E0129 21:34:03.073698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:04.074373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:05.074547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:06.075088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:34:06.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1310" for this suite. @ 01/29/24 21:34:06.607
• [6.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 01/29/24 21:34:06.618
  Jan 29 21:34:06.618: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-runtime @ 01/29/24 21:34:06.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:34:06.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:34:06.637
  STEP: create the container @ 01/29/24 21:34:06.642
  W0129 21:34:06.651526      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/29/24 21:34:06.651
  E0129 21:34:07.076206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:08.077082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:09.077253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 01/29/24 21:34:09.673
  STEP: the container should be terminated @ 01/29/24 21:34:09.677
  STEP: the termination message should be set @ 01/29/24 21:34:09.677
  Jan 29 21:34:09.677: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/29/24 21:34:09.677
  Jan 29 21:34:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8419" for this suite. @ 01/29/24 21:34:09.7
• [3.087 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 01/29/24 21:34:09.706
  Jan 29 21:34:09.706: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename cronjob @ 01/29/24 21:34:09.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:34:09.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:34:09.725
  STEP: Creating a cronjob @ 01/29/24 21:34:09.73
  STEP: Ensuring more than one job is running at a time @ 01/29/24 21:34:09.736
  E0129 21:34:10.077596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:11.078152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:12.078308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:13.078846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:14.079786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:15.080422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:16.080469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:17.081485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:18.082393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:19.082926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:20.083710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:21.084235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:22.085112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:23.085655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:24.086706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:25.087255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:26.087540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:27.088565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:28.088782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:29.089534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:30.090514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:31.091062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:32.091250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:33.092274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:34.093012      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:35.093812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:36.095023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:37.095943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:38.096440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:39.096865      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:40.096973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:41.097247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:42.097961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:43.098482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:44.098675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:45.098986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:46.100129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:47.100270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:48.101403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:49.101698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:50.102132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:51.102446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:52.103073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:53.103337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:54.104308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:55.104461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:56.104750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:57.104956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:58.105432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:34:59.106320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:00.106594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:01.106827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:02.107353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:03.107757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:04.108484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:05.109041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:06.110146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:07.111212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:08.111541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:09.111832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:10.112090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:11.112969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:12.113354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:13.113907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:14.114922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:15.115215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:16.115584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:17.116525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:18.117387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:19.117975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:20.119077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:21.119193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:22.119320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:23.119817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:24.120034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:25.120569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:26.121680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:27.122540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:28.123318      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:29.123799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:30.124093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:31.124973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:32.125025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:33.125312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:34.125611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:35.126432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:36.127400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:37.128569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:38.129426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:39.129677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:40.129939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:41.131453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:42.131501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:43.131958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:44.132015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:45.132506      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:46.133428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:47.134339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:48.135326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:49.135791      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:50.136604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:51.137136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:52.137311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:53.137653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:54.137827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:55.138453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:56.138518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:57.139604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:58.140482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:35:59.140971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:00.141722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:01.142242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 01/29/24 21:36:01.742
  STEP: Removing cronjob @ 01/29/24 21:36:01.749
  Jan 29 21:36:01.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6216" for this suite. @ 01/29/24 21:36:01.764
• [112.065 seconds]
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 01/29/24 21:36:01.771
  Jan 29 21:36:01.771: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replication-controller @ 01/29/24 21:36:01.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:01.792
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:01.796
  STEP: Creating ReplicationController "e2e-rc-n2czj" @ 01/29/24 21:36:01.801
  Jan 29 21:36:01.806: INFO: Get Replication Controller "e2e-rc-n2czj" to confirm replicas
  E0129 21:36:02.143308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:02.810: INFO: Get Replication Controller "e2e-rc-n2czj" to confirm replicas
  Jan 29 21:36:02.816: INFO: Found 1 replicas for "e2e-rc-n2czj" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-n2czj" @ 01/29/24 21:36:02.816
  STEP: Updating a scale subresource @ 01/29/24 21:36:02.82
  STEP: Verifying replicas where modified for replication controller "e2e-rc-n2czj" @ 01/29/24 21:36:02.828
  Jan 29 21:36:02.828: INFO: Get Replication Controller "e2e-rc-n2czj" to confirm replicas
  E0129 21:36:03.144216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:03.833: INFO: Get Replication Controller "e2e-rc-n2czj" to confirm replicas
  Jan 29 21:36:03.838: INFO: Found 2 replicas for "e2e-rc-n2czj" replication controller
  Jan 29 21:36:03.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8603" for this suite. @ 01/29/24 21:36:03.845
• [2.082 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 01/29/24 21:36:03.854
  Jan 29 21:36:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename hostport @ 01/29/24 21:36:03.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:03.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:03.876
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 01/29/24 21:36:03.886
  E0129 21:36:04.144845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:05.145251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.100.28 on the node which pod1 resides and expect scheduled @ 01/29/24 21:36:05.906
  E0129 21:36:06.146122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:07.146992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:08.147650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:09.147910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:10.148448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:11.148939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:12.149243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:13.149733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:14.150521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:15.151123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:16.152292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:17.152384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:18.153590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:19.154279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:20.154565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:21.154985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:22.155967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:23.156457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:24.157708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:25.158300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:26.159091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:27.160182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.100.28 but use UDP protocol on the node which pod2 resides @ 01/29/24 21:36:27.988
  E0129 21:36:28.161165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:29.161725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:30.162462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:31.162467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 01/29/24 21:36:32.033
  Jan 29 21:36:32.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.100.28 http://127.0.0.1:54323/hostname] Namespace:hostport-1447 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:32.033: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:32.035: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:32.035: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1447/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.100.28+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0129 21:36:32.162752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.100.28, port: 54323 @ 01/29/24 21:36:32.165
  Jan 29 21:36:32.165: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.100.28:54323/hostname] Namespace:hostport-1447 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:32.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:32.166: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:32.166: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1447/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.100.28%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.100.28, port: 54323 UDP @ 01/29/24 21:36:32.281
  Jan 29 21:36:32.282: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.100.28 54323] Namespace:hostport-1447 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:32.282: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:32.283: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:32.283: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-1447/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.100.28+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0129 21:36:33.164008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:34.164556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:35.165349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:36.165777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:37.166498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:37.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-1447" for this suite. @ 01/29/24 21:36:37.41
• [33.566 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 01/29/24 21:36:37.421
  Jan 29 21:36:37.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:36:37.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:37.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:37.442
  STEP: Creating a pod to test downward api env vars @ 01/29/24 21:36:37.447
  E0129 21:36:38.167746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:39.167792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:40.167998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:41.168535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:36:41.475
  Jan 29 21:36:41.479: INFO: Trying to get logs from node nodeb29 pod downward-api-a0e68c93-e761-4631-91d2-30d101032eec container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 21:36:41.506
  Jan 29 21:36:41.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8705" for this suite. @ 01/29/24 21:36:41.525
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 01/29/24 21:36:41.532
  Jan 29 21:36:41.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:36:41.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:41.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:41.55
  STEP: Creating configMap with name projected-configmap-test-volume-map-4394e983-b08f-4e70-88e6-04d9b51e369b @ 01/29/24 21:36:41.554
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:36:41.56
  E0129 21:36:42.169068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:43.169760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:44.170199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:45.170626      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:36:45.585
  Jan 29 21:36:45.589: INFO: Trying to get logs from node nodeb29 pod pod-projected-configmaps-5fee071e-2a68-4327-9778-d9ad48f24c28 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:36:45.599
  Jan 29 21:36:45.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5353" for this suite. @ 01/29/24 21:36:45.623
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 01/29/24 21:36:45.634
  Jan 29 21:36:45.634: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 01/29/24 21:36:45.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:45.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:45.654
  STEP: Setting up the test @ 01/29/24 21:36:45.658
  STEP: Creating hostNetwork=false pod @ 01/29/24 21:36:45.658
  E0129 21:36:46.171182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:47.171848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 01/29/24 21:36:47.682
  E0129 21:36:48.172055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:49.172473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 01/29/24 21:36:49.705
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 01/29/24 21:36:49.705
  Jan 29 21:36:49.705: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:49.705: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:49.706: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:49.706: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 29 21:36:49.815: INFO: Exec stderr: ""
  Jan 29 21:36:49.815: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:49.815: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:49.816: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:49.816: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 29 21:36:49.922: INFO: Exec stderr: ""
  Jan 29 21:36:49.922: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:49.923: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:49.923: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 29 21:36:50.027: INFO: Exec stderr: ""
  Jan 29 21:36:50.027: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.028: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.028: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 29 21:36:50.131: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 01/29/24 21:36:50.131
  Jan 29 21:36:50.131: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.132: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.132: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  E0129 21:36:50.172483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:50.230: INFO: Exec stderr: ""
  Jan 29 21:36:50.230: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.230: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.232: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.232: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jan 29 21:36:50.332: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 01/29/24 21:36:50.332
  Jan 29 21:36:50.332: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.333: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.334: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 29 21:36:50.448: INFO: Exec stderr: ""
  Jan 29 21:36:50.448: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.448: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.450: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.450: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 29 21:36:50.554: INFO: Exec stderr: ""
  Jan 29 21:36:50.554: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.554: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.555: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.555: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 29 21:36:50.661: INFO: Exec stderr: ""
  Jan 29 21:36:50.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-79 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 29 21:36:50.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  Jan 29 21:36:50.663: INFO: ExecWithOptions: Clientset creation
  Jan 29 21:36:50.663: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-79/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 29 21:36:50.762: INFO: Exec stderr: ""
  Jan 29 21:36:50.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-79" for this suite. @ 01/29/24 21:36:50.767
• [5.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 01/29/24 21:36:50.778
  Jan 29 21:36:50.778: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/29/24 21:36:50.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:50.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:50.795
  STEP: creating @ 01/29/24 21:36:50.798
  STEP: getting @ 01/29/24 21:36:50.813
  STEP: listing in namespace @ 01/29/24 21:36:50.815
  STEP: patching @ 01/29/24 21:36:50.818
  STEP: deleting @ 01/29/24 21:36:50.826
  Jan 29 21:36:50.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-4142" for this suite. @ 01/29/24 21:36:50.842
• [0.069 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 01/29/24 21:36:50.848
  Jan 29 21:36:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/29/24 21:36:50.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:50.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:50.864
  Jan 29 21:36:50.868: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:36:51.173369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:51.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4365" for this suite. @ 01/29/24 21:36:51.897
• [1.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 01/29/24 21:36:51.905
  Jan 29 21:36:51.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:36:51.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:51.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:51.924
  STEP: Creating configMap with name configmap-test-volume-map-e803bd89-c478-4e48-b104-79831ad26319 @ 01/29/24 21:36:51.929
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:36:51.933
  E0129 21:36:52.174361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:53.174837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:54.175854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:55.176357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:36:55.965
  Jan 29 21:36:55.971: INFO: Trying to get logs from node nodea08 pod pod-configmaps-2943c36e-aeaf-404a-ac55-ac01aa57cdeb container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:36:55.995
  Jan 29 21:36:56.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8885" for this suite. @ 01/29/24 21:36:56.013
• [4.112 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 01/29/24 21:36:56.018
  Jan 29 21:36:56.018: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:36:56.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:56.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:56.037
  STEP: create the deployment @ 01/29/24 21:36:56.041
  W0129 21:36:56.047396      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/29/24 21:36:56.047
  E0129 21:36:56.176995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 01/29/24 21:36:56.555
  STEP: wait for all rs to be garbage collected @ 01/29/24 21:36:56.562
  STEP: expected 0 rs, got 1 rs @ 01/29/24 21:36:56.57
  STEP: expected 0 pods, got 2 pods @ 01/29/24 21:36:56.573
  STEP: Gathering metrics @ 01/29/24 21:36:57.085
  E0129 21:36:57.178042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:36:57.231: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:36:57.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7284" for this suite. @ 01/29/24 21:36:57.236
• [1.223 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 01/29/24 21:36:57.242
  Jan 29 21:36:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:36:57.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:36:57.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:36:57.26
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:36:57.265
  E0129 21:36:58.179139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:36:59.179708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:00.179862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:01.180468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:37:01.288
  Jan 29 21:37:01.292: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-d68a7753-6258-47d1-b204-cf126cd4743e container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:37:01.301
  Jan 29 21:37:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9087" for this suite. @ 01/29/24 21:37:01.322
• [4.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 01/29/24 21:37:01.329
  Jan 29 21:37:01.329: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:37:01.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:37:01.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:37:01.348
  STEP: creating the pod with failed condition @ 01/29/24 21:37:01.352
  E0129 21:37:02.180571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:03.180651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:04.180683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:05.181201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:06.182167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:07.182630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:08.183141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:09.183781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:10.184353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:11.185061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:12.185747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:13.186329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:14.186581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:15.187201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:16.187352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:17.187738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:18.188409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:19.188982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:20.189402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:21.190166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:22.190462      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:23.191035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:24.191312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:25.191846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:26.192535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:27.193198      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:28.193281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:29.193767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:30.195020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:31.195597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:32.196629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:33.197219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:34.197326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:35.197819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:36.198789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:37.199919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:38.201206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:39.201763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:40.202267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:41.202788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:42.203738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:43.204117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:44.205243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:45.205771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:46.205944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:47.206449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:48.206969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:49.207442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:50.207954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:51.208560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:52.208852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:53.209370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:54.209880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:55.210766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:56.210860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:57.211216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:58.212359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:37:59.212958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:00.214099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:01.214618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:02.214706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:03.215231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:04.215605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:05.216134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:06.216647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:07.217689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:08.218310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:09.218875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:10.219021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:11.219557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:12.220075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:13.220600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:14.220994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:15.221581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:16.222332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:17.223389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:18.224136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:19.225106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:20.225613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:21.226217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:22.227279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:23.227756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:24.228856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:25.229457      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:26.229974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:27.230485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:28.230757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:29.231422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:30.232510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:31.232046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:32.232676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:33.233193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:34.234136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:35.234668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:36.234960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:37.235986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:38.236221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:39.236539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:40.236709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:41.237263      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:42.238144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:43.238634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:44.238758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:45.238926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:46.239941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:47.240891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:48.241746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:49.242311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:50.242565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:51.243132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:52.243623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:53.244222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:54.244497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:55.244703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:56.244947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:57.245918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:58.246918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:38:59.247428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:00.247972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:01.248503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 01/29/24 21:39:01.36
  Jan 29 21:39:01.879: INFO: Successfully updated pod "var-expansion-bb36c94b-2bb5-446f-a773-518a72af35d4"
  STEP: waiting for pod running @ 01/29/24 21:39:01.88
  E0129 21:39:02.249359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:03.250088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 01/29/24 21:39:03.891
  Jan 29 21:39:03.891: INFO: Deleting pod "var-expansion-bb36c94b-2bb5-446f-a773-518a72af35d4" in namespace "var-expansion-2089"
  Jan 29 21:39:03.898: INFO: Wait up to 5m0s for pod "var-expansion-bb36c94b-2bb5-446f-a773-518a72af35d4" to be fully deleted
  E0129 21:39:04.250928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:05.251527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:06.251993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:07.252232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:08.252944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:09.253508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:10.254270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:11.254819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:12.255802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:13.256356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:14.257336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:15.257893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:16.258680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:17.259823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:18.259794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:19.260314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:20.261097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:21.261704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:22.262359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:23.262902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:24.263326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:25.263815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:26.264152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:27.265182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:28.266026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:29.266466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:30.266642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:31.267209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:32.267766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:33.268332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:34.268897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:35.269400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:39:36.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2089" for this suite. @ 01/29/24 21:39:36.016
• [154.696 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 01/29/24 21:39:36.026
  Jan 29 21:39:36.026: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:39:36.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:36.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:36.046
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/29/24 21:39:36.051
  E0129 21:39:36.270243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:37.271309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:38.271543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:39.272073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:39:40.077
  Jan 29 21:39:40.081: INFO: Trying to get logs from node nodea08 pod pod-5e788b5a-b88a-4a60-a444-7ed7582ef335 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:39:40.108
  Jan 29 21:39:40.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-121" for this suite. @ 01/29/24 21:39:40.127
• [4.105 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 01/29/24 21:39:40.133
  Jan 29 21:39:40.133: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:39:40.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:40.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:40.153
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/29/24 21:39:40.157
  E0129 21:39:40.272602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:41.273478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:42.274332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:43.274810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:39:44.18
  Jan 29 21:39:44.185: INFO: Trying to get logs from node nodea08 pod pod-e855aa58-d669-4a49-b38a-fdb833ff3252 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:39:44.194
  Jan 29 21:39:44.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3524" for this suite. @ 01/29/24 21:39:44.217
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 01/29/24 21:39:44.225
  Jan 29 21:39:44.225: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename runtimeclass @ 01/29/24 21:39:44.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:44.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:44.244
  Jan 29 21:39:44.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3926" for this suite. @ 01/29/24 21:39:44.261
• [0.042 seconds]
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 01/29/24 21:39:44.267
  Jan 29 21:39:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename subjectreview @ 01/29/24 21:39:44.269
  E0129 21:39:44.275926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:44.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:44.285
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2235" @ 01/29/24 21:39:44.289
  Jan 29 21:39:44.294: INFO: saUsername: "system:serviceaccount:subjectreview-2235:e2e"
  Jan 29 21:39:44.294: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2235"}
  Jan 29 21:39:44.294: INFO: saUID: "20271ee2-41ea-4df4-82e5-1cecab134c50"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2235:e2e" @ 01/29/24 21:39:44.294
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2235:e2e" @ 01/29/24 21:39:44.295
  Jan 29 21:39:44.297: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2235:e2e" api 'list' configmaps in "subjectreview-2235" namespace @ 01/29/24 21:39:44.297
  Jan 29 21:39:44.300: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2235:e2e" @ 01/29/24 21:39:44.3
  Jan 29 21:39:44.303: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jan 29 21:39:44.303: INFO: LocalSubjectAccessReview has been verified
  Jan 29 21:39:44.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2235" for this suite. @ 01/29/24 21:39:44.309
• [0.049 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 01/29/24 21:39:44.317
  Jan 29 21:39:44.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 21:39:44.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:44.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:44.335
  STEP: Creating simple DaemonSet "daemon-set" @ 01/29/24 21:39:44.361
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 21:39:44.37
  Jan 29 21:39:44.375: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:39:44.378: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:39:44.378: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:39:45.276652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:39:45.385: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:39:45.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:39:45.390: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:39:46.276844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:39:46.386: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:39:46.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 21:39:46.391: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 01/29/24 21:39:46.395
  STEP: DeleteCollection of the DaemonSets @ 01/29/24 21:39:46.4
  STEP: Verify that ReplicaSets have been deleted @ 01/29/24 21:39:46.408
  Jan 29 21:39:46.425: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22916"},"items":null}

  Jan 29 21:39:46.431: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22919"},"items":[{"metadata":{"name":"daemon-set-bd4d5","generateName":"daemon-set-","namespace":"daemonsets-2652","uid":"a803512d-3334-432b-aed3-c552a0e5b0f8","resourceVersion":"22919","creationTimestamp":"2024-01-29T21:39:44Z","deletionTimestamp":"2024-01-29T21:40:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e7306692-4232-48a1-9b46-e092387ab687","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-29T21:39:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7306692-4232-48a1-9b46-e092387ab687\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-29T21:39:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-7454t","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-7454t","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodea08","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodea08"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:44Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:45Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:45Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:44Z"}],"hostIP":"192.168.100.28","podIP":"10.244.1.64","podIPs":[{"ip":"10.244.1.64"}],"startTime":"2024-01-29T21:39:44Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-29T21:39:45Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a4876ce18976f68b8f3499397941420351a914ea16e870cfe09c6ac3662380a9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-nr2rn","generateName":"daemon-set-","namespace":"daemonsets-2652","uid":"b269aaea-807c-4c2c-aa32-b5b2108c4ca4","resourceVersion":"22918","creationTimestamp":"2024-01-29T21:39:44Z","deletionTimestamp":"2024-01-29T21:40:16Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e7306692-4232-48a1-9b46-e092387ab687","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-29T21:39:44Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7306692-4232-48a1-9b46-e092387ab687\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-29T21:39:45Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.2.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ngzlq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ngzlq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"nodeb29","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["nodeb29"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:44Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:45Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:45Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-29T21:39:44Z"}],"hostIP":"192.168.100.129","podIP":"10.244.2.122","podIPs":[{"ip":"10.244.2.122"}],"startTime":"2024-01-29T21:39:44Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-29T21:39:45Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://741acfd9b8cabcaac3cdb9dc001c7555a7f5000056cc3845178cebe1ab26b7b7","started":true}],"qosClass":"BestEffort"}}]}

  Jan 29 21:39:46.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2652" for this suite. @ 01/29/24 21:39:46.445
• [2.133 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 01/29/24 21:39:46.451
  Jan 29 21:39:46.451: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:39:46.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:39:46.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:39:46.47
  STEP: create the rc @ 01/29/24 21:39:46.479
  W0129 21:39:46.483820      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0129 21:39:47.276999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:48.277222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:49.277866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:50.278217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:51.278856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:52.279071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 01/29/24 21:39:52.49
  STEP: wait for the rc to be deleted @ 01/29/24 21:39:52.499
  E0129 21:39:53.279872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:54.280439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:55.280992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:56.281879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:57.282994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 01/29/24 21:39:57.505
  E0129 21:39:58.283584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:39:59.284101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:00.284825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:01.285403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:02.286141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:03.286745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:04.287240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:05.287708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:06.288246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:07.289244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:08.289999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:09.290630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:10.291133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:11.291757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:12.292063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:13.292619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:14.293187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:15.293730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:16.294195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:17.295157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:18.295723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:19.296281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:20.296914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:21.297409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:22.298559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:23.299073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:24.299643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:25.300150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:26.300702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:27.301879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 01/29/24 21:40:27.527
  Jan 29 21:40:27.673: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:40:27.673: INFO: Deleting pod "simpletest.rc-285xw" in namespace "gc-2857"
  Jan 29 21:40:27.683: INFO: Deleting pod "simpletest.rc-29dq6" in namespace "gc-2857"
  Jan 29 21:40:27.694: INFO: Deleting pod "simpletest.rc-2dxr6" in namespace "gc-2857"
  Jan 29 21:40:27.703: INFO: Deleting pod "simpletest.rc-2mr8b" in namespace "gc-2857"
  Jan 29 21:40:27.713: INFO: Deleting pod "simpletest.rc-4ndqx" in namespace "gc-2857"
  Jan 29 21:40:27.726: INFO: Deleting pod "simpletest.rc-4ng4k" in namespace "gc-2857"
  Jan 29 21:40:27.736: INFO: Deleting pod "simpletest.rc-52pks" in namespace "gc-2857"
  Jan 29 21:40:27.748: INFO: Deleting pod "simpletest.rc-5qc5b" in namespace "gc-2857"
  Jan 29 21:40:27.757: INFO: Deleting pod "simpletest.rc-5smhf" in namespace "gc-2857"
  Jan 29 21:40:27.768: INFO: Deleting pod "simpletest.rc-62hxn" in namespace "gc-2857"
  Jan 29 21:40:27.779: INFO: Deleting pod "simpletest.rc-67mcl" in namespace "gc-2857"
  Jan 29 21:40:27.788: INFO: Deleting pod "simpletest.rc-6d9vv" in namespace "gc-2857"
  Jan 29 21:40:27.798: INFO: Deleting pod "simpletest.rc-6vqrp" in namespace "gc-2857"
  Jan 29 21:40:27.805: INFO: Deleting pod "simpletest.rc-7gz97" in namespace "gc-2857"
  Jan 29 21:40:27.813: INFO: Deleting pod "simpletest.rc-7hxnl" in namespace "gc-2857"
  Jan 29 21:40:27.820: INFO: Deleting pod "simpletest.rc-7kmmk" in namespace "gc-2857"
  Jan 29 21:40:27.830: INFO: Deleting pod "simpletest.rc-84fm6" in namespace "gc-2857"
  Jan 29 21:40:27.837: INFO: Deleting pod "simpletest.rc-88pcp" in namespace "gc-2857"
  Jan 29 21:40:27.847: INFO: Deleting pod "simpletest.rc-8gj25" in namespace "gc-2857"
  Jan 29 21:40:27.855: INFO: Deleting pod "simpletest.rc-8gnhw" in namespace "gc-2857"
  Jan 29 21:40:27.862: INFO: Deleting pod "simpletest.rc-8m6ft" in namespace "gc-2857"
  Jan 29 21:40:27.872: INFO: Deleting pod "simpletest.rc-8rwtm" in namespace "gc-2857"
  Jan 29 21:40:27.880: INFO: Deleting pod "simpletest.rc-8xb8m" in namespace "gc-2857"
  Jan 29 21:40:27.889: INFO: Deleting pod "simpletest.rc-9fts8" in namespace "gc-2857"
  Jan 29 21:40:27.899: INFO: Deleting pod "simpletest.rc-9mgqw" in namespace "gc-2857"
  Jan 29 21:40:27.907: INFO: Deleting pod "simpletest.rc-9qhd8" in namespace "gc-2857"
  Jan 29 21:40:27.915: INFO: Deleting pod "simpletest.rc-bbx8g" in namespace "gc-2857"
  Jan 29 21:40:27.928: INFO: Deleting pod "simpletest.rc-bbzwx" in namespace "gc-2857"
  Jan 29 21:40:27.937: INFO: Deleting pod "simpletest.rc-bczgf" in namespace "gc-2857"
  Jan 29 21:40:27.945: INFO: Deleting pod "simpletest.rc-bnz8b" in namespace "gc-2857"
  Jan 29 21:40:27.960: INFO: Deleting pod "simpletest.rc-bqbdt" in namespace "gc-2857"
  Jan 29 21:40:27.970: INFO: Deleting pod "simpletest.rc-bqq4g" in namespace "gc-2857"
  Jan 29 21:40:27.980: INFO: Deleting pod "simpletest.rc-brblp" in namespace "gc-2857"
  Jan 29 21:40:27.990: INFO: Deleting pod "simpletest.rc-bsn2f" in namespace "gc-2857"
  Jan 29 21:40:28.001: INFO: Deleting pod "simpletest.rc-bwstb" in namespace "gc-2857"
  Jan 29 21:40:28.010: INFO: Deleting pod "simpletest.rc-bxwbx" in namespace "gc-2857"
  Jan 29 21:40:28.019: INFO: Deleting pod "simpletest.rc-bzvhw" in namespace "gc-2857"
  Jan 29 21:40:28.034: INFO: Deleting pod "simpletest.rc-c4jwt" in namespace "gc-2857"
  Jan 29 21:40:28.046: INFO: Deleting pod "simpletest.rc-cssbd" in namespace "gc-2857"
  Jan 29 21:40:28.053: INFO: Deleting pod "simpletest.rc-ct7fw" in namespace "gc-2857"
  Jan 29 21:40:28.063: INFO: Deleting pod "simpletest.rc-cxrwx" in namespace "gc-2857"
  Jan 29 21:40:28.074: INFO: Deleting pod "simpletest.rc-dc4vm" in namespace "gc-2857"
  Jan 29 21:40:28.082: INFO: Deleting pod "simpletest.rc-dh8dr" in namespace "gc-2857"
  Jan 29 21:40:28.090: INFO: Deleting pod "simpletest.rc-dntsd" in namespace "gc-2857"
  Jan 29 21:40:28.099: INFO: Deleting pod "simpletest.rc-dw294" in namespace "gc-2857"
  Jan 29 21:40:28.110: INFO: Deleting pod "simpletest.rc-f97bd" in namespace "gc-2857"
  Jan 29 21:40:28.121: INFO: Deleting pod "simpletest.rc-fct2t" in namespace "gc-2857"
  Jan 29 21:40:28.135: INFO: Deleting pod "simpletest.rc-fg27g" in namespace "gc-2857"
  Jan 29 21:40:28.143: INFO: Deleting pod "simpletest.rc-fglc6" in namespace "gc-2857"
  Jan 29 21:40:28.152: INFO: Deleting pod "simpletest.rc-fsq4s" in namespace "gc-2857"
  Jan 29 21:40:28.163: INFO: Deleting pod "simpletest.rc-glkd4" in namespace "gc-2857"
  Jan 29 21:40:28.173: INFO: Deleting pod "simpletest.rc-gpxw2" in namespace "gc-2857"
  Jan 29 21:40:28.182: INFO: Deleting pod "simpletest.rc-h2cvh" in namespace "gc-2857"
  Jan 29 21:40:28.196: INFO: Deleting pod "simpletest.rc-jgvph" in namespace "gc-2857"
  Jan 29 21:40:28.204: INFO: Deleting pod "simpletest.rc-jw2zx" in namespace "gc-2857"
  Jan 29 21:40:28.214: INFO: Deleting pod "simpletest.rc-ktw58" in namespace "gc-2857"
  Jan 29 21:40:28.223: INFO: Deleting pod "simpletest.rc-l77rn" in namespace "gc-2857"
  Jan 29 21:40:28.231: INFO: Deleting pod "simpletest.rc-lfhsn" in namespace "gc-2857"
  Jan 29 21:40:28.240: INFO: Deleting pod "simpletest.rc-lv5sp" in namespace "gc-2857"
  Jan 29 21:40:28.249: INFO: Deleting pod "simpletest.rc-mdjmg" in namespace "gc-2857"
  Jan 29 21:40:28.259: INFO: Deleting pod "simpletest.rc-mppbg" in namespace "gc-2857"
  Jan 29 21:40:28.267: INFO: Deleting pod "simpletest.rc-mswz7" in namespace "gc-2857"
  E0129 21:40:28.302407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:28.318: INFO: Deleting pod "simpletest.rc-n6m9g" in namespace "gc-2857"
  Jan 29 21:40:28.373: INFO: Deleting pod "simpletest.rc-nc5kp" in namespace "gc-2857"
  Jan 29 21:40:28.418: INFO: Deleting pod "simpletest.rc-ng57f" in namespace "gc-2857"
  Jan 29 21:40:28.469: INFO: Deleting pod "simpletest.rc-pjldf" in namespace "gc-2857"
  Jan 29 21:40:28.521: INFO: Deleting pod "simpletest.rc-pz8cs" in namespace "gc-2857"
  Jan 29 21:40:28.565: INFO: Deleting pod "simpletest.rc-pzl9r" in namespace "gc-2857"
  Jan 29 21:40:28.616: INFO: Deleting pod "simpletest.rc-q24db" in namespace "gc-2857"
  Jan 29 21:40:28.668: INFO: Deleting pod "simpletest.rc-q4p5q" in namespace "gc-2857"
  Jan 29 21:40:28.719: INFO: Deleting pod "simpletest.rc-qm6x2" in namespace "gc-2857"
  Jan 29 21:40:28.767: INFO: Deleting pod "simpletest.rc-qmjcn" in namespace "gc-2857"
  Jan 29 21:40:28.816: INFO: Deleting pod "simpletest.rc-qpnjs" in namespace "gc-2857"
  Jan 29 21:40:28.868: INFO: Deleting pod "simpletest.rc-r6xdp" in namespace "gc-2857"
  Jan 29 21:40:28.917: INFO: Deleting pod "simpletest.rc-rb5nt" in namespace "gc-2857"
  Jan 29 21:40:28.967: INFO: Deleting pod "simpletest.rc-rjzqq" in namespace "gc-2857"
  Jan 29 21:40:29.016: INFO: Deleting pod "simpletest.rc-rq8bq" in namespace "gc-2857"
  Jan 29 21:40:29.066: INFO: Deleting pod "simpletest.rc-rrsk8" in namespace "gc-2857"
  Jan 29 21:40:29.118: INFO: Deleting pod "simpletest.rc-rvmnj" in namespace "gc-2857"
  Jan 29 21:40:29.166: INFO: Deleting pod "simpletest.rc-s2m5q" in namespace "gc-2857"
  Jan 29 21:40:29.216: INFO: Deleting pod "simpletest.rc-s78t4" in namespace "gc-2857"
  Jan 29 21:40:29.270: INFO: Deleting pod "simpletest.rc-sg9zk" in namespace "gc-2857"
  E0129 21:40:29.303015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:29.319: INFO: Deleting pod "simpletest.rc-sqgbh" in namespace "gc-2857"
  Jan 29 21:40:29.373: INFO: Deleting pod "simpletest.rc-sxj2l" in namespace "gc-2857"
  Jan 29 21:40:29.416: INFO: Deleting pod "simpletest.rc-t8w8r" in namespace "gc-2857"
  Jan 29 21:40:29.470: INFO: Deleting pod "simpletest.rc-v4kgx" in namespace "gc-2857"
  Jan 29 21:40:29.523: INFO: Deleting pod "simpletest.rc-v8l6j" in namespace "gc-2857"
  Jan 29 21:40:29.571: INFO: Deleting pod "simpletest.rc-vkzqq" in namespace "gc-2857"
  Jan 29 21:40:29.617: INFO: Deleting pod "simpletest.rc-vmdpv" in namespace "gc-2857"
  Jan 29 21:40:29.667: INFO: Deleting pod "simpletest.rc-vnsms" in namespace "gc-2857"
  Jan 29 21:40:29.719: INFO: Deleting pod "simpletest.rc-vwhqt" in namespace "gc-2857"
  Jan 29 21:40:29.769: INFO: Deleting pod "simpletest.rc-wxfn4" in namespace "gc-2857"
  Jan 29 21:40:29.817: INFO: Deleting pod "simpletest.rc-x27rf" in namespace "gc-2857"
  Jan 29 21:40:29.865: INFO: Deleting pod "simpletest.rc-x75mg" in namespace "gc-2857"
  Jan 29 21:40:29.915: INFO: Deleting pod "simpletest.rc-z4k8k" in namespace "gc-2857"
  Jan 29 21:40:29.967: INFO: Deleting pod "simpletest.rc-z8vxz" in namespace "gc-2857"
  Jan 29 21:40:30.020: INFO: Deleting pod "simpletest.rc-zcl9w" in namespace "gc-2857"
  Jan 29 21:40:30.067: INFO: Deleting pod "simpletest.rc-zhnwm" in namespace "gc-2857"
  Jan 29 21:40:30.118: INFO: Deleting pod "simpletest.rc-zqj78" in namespace "gc-2857"
  Jan 29 21:40:30.167: INFO: Deleting pod "simpletest.rc-zr7rc" in namespace "gc-2857"
  Jan 29 21:40:30.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2857" for this suite. @ 01/29/24 21:40:30.264
  E0129 21:40:30.304041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [43.863 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 01/29/24 21:40:30.315
  Jan 29 21:40:30.315: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename proxy @ 01/29/24 21:40:30.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:30.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:30.334
  Jan 29 21:40:30.338: INFO: Creating pod...
  E0129 21:40:31.304755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:32.304902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:32.356: INFO: Creating service...
  Jan 29 21:40:32.379: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/DELETE
  Jan 29 21:40:32.385: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 29 21:40:32.385: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/GET
  Jan 29 21:40:32.390: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 29 21:40:32.390: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/HEAD
  Jan 29 21:40:32.395: INFO: http.Client request:HEAD | StatusCode:200
  Jan 29 21:40:32.395: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/OPTIONS
  Jan 29 21:40:32.399: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 29 21:40:32.399: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/PATCH
  Jan 29 21:40:32.404: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 29 21:40:32.405: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/POST
  Jan 29 21:40:32.409: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 29 21:40:32.409: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/pods/agnhost/proxy/some/path/with/PUT
  Jan 29 21:40:32.414: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 29 21:40:32.414: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/DELETE
  Jan 29 21:40:32.420: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 29 21:40:32.420: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/GET
  Jan 29 21:40:32.425: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 29 21:40:32.425: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/HEAD
  Jan 29 21:40:32.430: INFO: http.Client request:HEAD | StatusCode:200
  Jan 29 21:40:32.430: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/OPTIONS
  Jan 29 21:40:32.434: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 29 21:40:32.435: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/PATCH
  Jan 29 21:40:32.440: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 29 21:40:32.441: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/POST
  Jan 29 21:40:32.446: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 29 21:40:32.446: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7288/services/test-service/proxy/some/path/with/PUT
  Jan 29 21:40:32.450: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 29 21:40:32.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7288" for this suite. @ 01/29/24 21:40:32.456
• [2.147 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 01/29/24 21:40:32.463
  Jan 29 21:40:32.464: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 21:40:32.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:32.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:32.481
  Jan 29 21:40:32.505: INFO: Create a RollingUpdate DaemonSet
  Jan 29 21:40:32.513: INFO: Check that daemon pods launch on every node of the cluster
  Jan 29 21:40:32.519: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:40:32.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:40:32.523: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:40:33.305789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:33.530: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:40:33.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 21:40:33.535: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  Jan 29 21:40:33.535: INFO: Update the DaemonSet to trigger a rollout
  Jan 29 21:40:33.547: INFO: Updating DaemonSet daemon-set
  E0129 21:40:34.306649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:35.307201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:35.563: INFO: Roll back the DaemonSet before rollout is complete
  Jan 29 21:40:35.572: INFO: Updating DaemonSet daemon-set
  Jan 29 21:40:35.572: INFO: Make sure DaemonSet rollback is complete
  Jan 29 21:40:35.576: INFO: Wrong image for pod: daemon-set-2wtlh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jan 29 21:40:35.576: INFO: Pod daemon-set-2wtlh is not available
  Jan 29 21:40:35.582: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0129 21:40:36.308007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:36.591: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0129 21:40:37.308754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:37.588: INFO: Pod daemon-set-kg6kz is not available
  Jan 29 21:40:37.592: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 21:40:37.6
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7987, will wait for the garbage collector to delete the pods @ 01/29/24 21:40:37.6
  Jan 29 21:40:37.661: INFO: Deleting DaemonSet.extensions daemon-set took: 7.164155ms
  Jan 29 21:40:37.762: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.789874ms
  E0129 21:40:38.309234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:39.310265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:39.365: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:40:39.365: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 21:40:39.369: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24955"},"items":null}

  Jan 29 21:40:39.372: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24955"},"items":null}

  Jan 29 21:40:39.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7987" for this suite. @ 01/29/24 21:40:39.388
• [6.930 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 01/29/24 21:40:39.396
  Jan 29 21:40:39.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:40:39.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:39.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:39.416
  STEP: validating cluster-info @ 01/29/24 21:40:39.42
  Jan 29 21:40:39.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-5981 cluster-info'
  Jan 29 21:40:39.524: INFO: stderr: ""
  Jan 29 21:40:39.524: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jan 29 21:40:39.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5981" for this suite. @ 01/29/24 21:40:39.529
• [0.141 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 01/29/24 21:40:39.537
  Jan 29 21:40:39.537: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:40:39.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:39.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:39.555
  STEP: creating secret secrets-5416/secret-test-517fffbb-810e-42c7-b96a-8f17d9504767 @ 01/29/24 21:40:39.56
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:40:39.564
  E0129 21:40:40.310864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:41.311566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:42.312463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:43.313690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:40:43.588
  Jan 29 21:40:43.592: INFO: Trying to get logs from node nodea08 pod pod-configmaps-434d0190-bfb8-444d-8242-3276a2f1e364 container env-test: <nil>
  STEP: delete the pod @ 01/29/24 21:40:43.602
  Jan 29 21:40:43.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5416" for this suite. @ 01/29/24 21:40:43.623
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 01/29/24 21:40:43.63
  Jan 29 21:40:43.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 21:40:43.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:43.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:43.652
  STEP: Creating a ResourceQuota @ 01/29/24 21:40:43.657
  STEP: Getting a ResourceQuota @ 01/29/24 21:40:43.662
  STEP: Listing all ResourceQuotas with LabelSelector @ 01/29/24 21:40:43.665
  STEP: Patching the ResourceQuota @ 01/29/24 21:40:43.668
  STEP: Deleting a Collection of ResourceQuotas @ 01/29/24 21:40:43.673
  STEP: Verifying the deleted ResourceQuota @ 01/29/24 21:40:43.682
  Jan 29 21:40:43.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3566" for this suite. @ 01/29/24 21:40:43.69
• [0.065 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 01/29/24 21:40:43.696
  Jan 29 21:40:43.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:40:43.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:43.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:43.715
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5631 @ 01/29/24 21:40:43.72
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/29/24 21:40:43.736
  STEP: creating service externalsvc in namespace services-5631 @ 01/29/24 21:40:43.736
  STEP: creating replication controller externalsvc in namespace services-5631 @ 01/29/24 21:40:43.752
  I0129 21:40:43.760848      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5631, replica count: 2
  E0129 21:40:44.314575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:45.314709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:46.315353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:40:46.812232      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 01/29/24 21:40:46.816
  Jan 29 21:40:46.841: INFO: Creating new exec pod
  E0129 21:40:47.315783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:48.316904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:48.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-5631 exec execpod79g7x -- /bin/sh -x -c nslookup nodeport-service.services-5631.svc.cluster.local'
  Jan 29 21:40:49.098: INFO: stderr: "+ nslookup nodeport-service.services-5631.svc.cluster.local\n"
  Jan 29 21:40:49.098: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-5631.svc.cluster.local\tcanonical name = externalsvc.services-5631.svc.cluster.local.\nName:\texternalsvc.services-5631.svc.cluster.local\nAddress: 10.104.236.250\n\n"
  Jan 29 21:40:49.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5631, will wait for the garbage collector to delete the pods @ 01/29/24 21:40:49.105
  Jan 29 21:40:49.166: INFO: Deleting ReplicationController externalsvc took: 6.877313ms
  Jan 29 21:40:49.268: INFO: Terminating ReplicationController externalsvc pods took: 101.199135ms
  E0129 21:40:49.317862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:50.318433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:51.318525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:51.489: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5631" for this suite. @ 01/29/24 21:40:51.497
• [7.807 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 01/29/24 21:40:51.505
  Jan 29 21:40:51.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:40:51.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:51.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:51.524
  STEP: Creating a job @ 01/29/24 21:40:51.528
  STEP: Ensuring active pods == parallelism @ 01/29/24 21:40:51.535
  E0129 21:40:52.319419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:53.319843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 01/29/24 21:40:53.542
  Jan 29 21:40:54.066: INFO: Successfully updated pod "adopt-release-7gsgt"
  STEP: Checking that the Job readopts the Pod @ 01/29/24 21:40:54.066
  E0129 21:40:54.320743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:55.321328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 01/29/24 21:40:56.077
  E0129 21:40:56.322092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:56.593: INFO: Successfully updated pod "adopt-release-7gsgt"
  STEP: Checking that the Job releases the Pod @ 01/29/24 21:40:56.593
  E0129 21:40:57.322512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:40:58.322896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:40:58.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-850" for this suite. @ 01/29/24 21:40:58.606
• [7.108 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 01/29/24 21:40:58.614
  Jan 29 21:40:58.614: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:40:58.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:40:58.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:40:58.634
  STEP: Setting up server cert @ 01/29/24 21:40:58.657
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:40:59.093
  STEP: Deploying the webhook pod @ 01/29/24 21:40:59.106
  STEP: Wait for the deployment to be ready @ 01/29/24 21:40:59.119
  Jan 29 21:40:59.129: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:40:59.323512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:00.324415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:41:01.143
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:41:01.16
  E0129 21:41:01.324971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:02.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/29/24 21:41:02.242
  STEP: Creating a configMap that should be mutated @ 01/29/24 21:41:02.265
  STEP: Deleting the collection of validation webhooks @ 01/29/24 21:41:02.316
  E0129 21:41:02.325509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a configMap that should not be mutated @ 01/29/24 21:41:02.356
  Jan 29 21:41:02.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-779" for this suite. @ 01/29/24 21:41:02.4
  STEP: Destroying namespace "webhook-markers-4581" for this suite. @ 01/29/24 21:41:02.406
• [3.798 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 01/29/24 21:41:02.412
  Jan 29 21:41:02.412: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:41:02.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:02.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:02.43
  STEP: create the rc @ 01/29/24 21:41:02.435
  W0129 21:41:02.441054      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0129 21:41:03.325742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:04.326614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:05.327192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:06.328187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:07.329814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 01/29/24 21:41:07.446
  STEP: wait for all pods to be garbage collected @ 01/29/24 21:41:07.452
  E0129 21:41:08.329751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:09.330229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:10.330808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:11.331350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:12.331790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 01/29/24 21:41:12.461
  Jan 29 21:41:12.576: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:41:12.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6326" for this suite. @ 01/29/24 21:41:12.582
• [10.176 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 01/29/24 21:41:12.589
  Jan 29 21:41:12.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 21:41:12.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:12.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:12.613
  STEP: Creating service test in namespace statefulset-1869 @ 01/29/24 21:41:12.617
  STEP: Creating statefulset ss in namespace statefulset-1869 @ 01/29/24 21:41:12.626
  Jan 29 21:41:12.635: INFO: Found 0 stateful pods, waiting for 1
  E0129 21:41:13.332933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:14.333445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:15.333821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:16.334037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:17.334781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:18.334949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:19.335673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:20.336212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:21.336729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:22.336753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:22.643: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 01/29/24 21:41:22.651
  STEP: Getting /status @ 01/29/24 21:41:22.669
  Jan 29 21:41:22.675: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 01/29/24 21:41:22.675
  Jan 29 21:41:22.686: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 01/29/24 21:41:22.686
  Jan 29 21:41:22.689: INFO: Observed &StatefulSet event: ADDED
  Jan 29 21:41:22.689: INFO: Found Statefulset ss in namespace statefulset-1869 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:41:22.689: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 01/29/24 21:41:22.689
  Jan 29 21:41:22.690: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 29 21:41:22.701: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 01/29/24 21:41:22.701
  Jan 29 21:41:22.704: INFO: Observed &StatefulSet event: ADDED
  Jan 29 21:41:22.704: INFO: Observed Statefulset ss in namespace statefulset-1869 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:41:22.704: INFO: Observed &StatefulSet event: MODIFIED
  Jan 29 21:41:22.704: INFO: Found Statefulset ss in namespace statefulset-1869 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jan 29 21:41:22.704: INFO: Deleting all statefulset in ns statefulset-1869
  Jan 29 21:41:22.708: INFO: Scaling statefulset ss to 0
  E0129 21:41:23.337224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:24.337804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:25.338352      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:26.338893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:27.338887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:28.339439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:29.340071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:30.341060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:31.341591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:32.341823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:32.735: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:41:32.740: INFO: Deleting statefulset ss
  Jan 29 21:41:32.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1869" for this suite. @ 01/29/24 21:41:32.759
• [20.175 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 01/29/24 21:41:32.765
  Jan 29 21:41:32.765: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 21:41:32.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:32.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:32.783
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 01/29/24 21:41:32.788
  E0129 21:41:33.342356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:34.342883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 01/29/24 21:41:34.807
  STEP: Then the orphan pod is adopted @ 01/29/24 21:41:34.813
  E0129 21:41:35.343382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 01/29/24 21:41:35.822
  Jan 29 21:41:35.827: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/29/24 21:41:35.841
  E0129 21:41:36.343892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:36.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-597" for this suite. @ 01/29/24 21:41:36.856
• [4.098 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 01/29/24 21:41:36.863
  Jan 29 21:41:36.863: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:41:36.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:36.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:36.885
  STEP: creating service nodeport-test with type=NodePort in namespace services-8501 @ 01/29/24 21:41:36.891
  STEP: creating replication controller nodeport-test in namespace services-8501 @ 01/29/24 21:41:36.911
  I0129 21:41:36.918627      23 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-8501, replica count: 2
  E0129 21:41:37.344747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:38.345051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:39.345599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:41:39.970329      23 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 21:41:39.970: INFO: Creating new exec pod
  E0129 21:41:40.345804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:41.346349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:42.347135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:42.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8501 exec execpod7bld5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jan 29 21:41:43.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jan 29 21:41:43.217: INFO: stdout: "nodeport-test-6gfnt"
  Jan 29 21:41:43.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8501 exec execpod7bld5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.183.49 80'
  E0129 21:41:43.347266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:43.423: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.183.49 80\nConnection to 10.103.183.49 80 port [tcp/http] succeeded!\n"
  Jan 29 21:41:43.424: INFO: stdout: ""
  E0129 21:41:44.347879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:44.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8501 exec execpod7bld5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.183.49 80'
  Jan 29 21:41:44.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.183.49 80\nConnection to 10.103.183.49 80 port [tcp/http] succeeded!\n"
  Jan 29 21:41:44.634: INFO: stdout: "nodeport-test-6gfnt"
  Jan 29 21:41:44.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8501 exec execpod7bld5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.28 30399'
  Jan 29 21:41:44.856: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.28 30399\nConnection to 192.168.100.28 30399 port [tcp/*] succeeded!\n"
  Jan 29 21:41:44.856: INFO: stdout: "nodeport-test-8xjc6"
  Jan 29 21:41:44.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-8501 exec execpod7bld5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.129 30399'
  Jan 29 21:41:45.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.129 30399\nConnection to 192.168.100.129 30399 port [tcp/*] succeeded!\n"
  Jan 29 21:41:45.077: INFO: stdout: "nodeport-test-8xjc6"
  Jan 29 21:41:45.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8501" for this suite. @ 01/29/24 21:41:45.084
• [8.227 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 01/29/24 21:41:45.09
  Jan 29 21:41:45.090: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:41:45.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:45.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:45.109
  STEP: Creating a pod to test emptydir volume type on node default medium @ 01/29/24 21:41:45.114
  E0129 21:41:45.349020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:46.349772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:47.349963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:48.351238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:41:49.136
  Jan 29 21:41:49.140: INFO: Trying to get logs from node nodea08 pod pod-b1826d8d-500f-48cc-980e-6778ef803d12 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:41:49.151
  Jan 29 21:41:49.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8411" for this suite. @ 01/29/24 21:41:49.175
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 01/29/24 21:41:49.187
  Jan 29 21:41:49.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename prestop @ 01/29/24 21:41:49.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:49.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:49.208
  STEP: Creating server pod server in namespace prestop-7100 @ 01/29/24 21:41:49.213
  STEP: Waiting for pods to come up. @ 01/29/24 21:41:49.226
  E0129 21:41:49.352071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:50.352887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-7100 @ 01/29/24 21:41:51.239
  E0129 21:41:51.353761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:52.354931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 01/29/24 21:41:53.259
  E0129 21:41:53.355763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:54.356310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:55.356822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:56.357493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:41:57.358191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:58.278: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jan 29 21:41:58.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 01/29/24 21:41:58.286
  STEP: Destroying namespace "prestop-7100" for this suite. @ 01/29/24 21:41:58.298
• [9.119 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 01/29/24 21:41:58.307
  Jan 29 21:41:58.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename aggregator @ 01/29/24 21:41:58.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:41:58.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:41:58.328
  Jan 29 21:41:58.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Registering the sample API server. @ 01/29/24 21:41:58.333
  E0129 21:41:58.359065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:41:58.730: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jan 29 21:41:58.763: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0129 21:41:59.359874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:00.360340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:00.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:01.361512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:02.361773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:02.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:03.363232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:04.363979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:04.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:05.364188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:06.364811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:06.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:07.365782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:08.365982      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:08.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:09.366432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:10.366930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:10.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:11.367972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:12.368121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:12.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:13.369176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:14.369656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:14.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:15.370775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:16.371321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:16.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:17.371538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:18.371818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:18.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:19.372460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:20.372904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:20.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:21.373355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:22.373658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:22.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 41, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:42:23.374100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:24.374794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:24.952: INFO: Waited 119.859686ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 01/29/24 21:42:25.023
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 01/29/24 21:42:25.027
  STEP: List APIServices @ 01/29/24 21:42:25.034
  Jan 29 21:42:25.041: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 01/29/24 21:42:25.041
  Jan 29 21:42:25.059: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 01/29/24 21:42:25.059
  Jan 29 21:42:25.073: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2024, time.January, 29, 21, 42, 24, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 01/29/24 21:42:25.074
  Jan 29 21:42:25.078: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2024-01-29 21:42:24 +0000 UTC Passed all checks passed}
  Jan 29 21:42:25.078: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:42:25.078: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 01/29/24 21:42:25.078
  Jan 29 21:42:25.090: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-909429474" @ 01/29/24 21:42:25.09
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 01/29/24 21:42:25.106
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 01/29/24 21:42:25.116
  STEP: Patch APIService Status @ 01/29/24 21:42:25.121
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 01/29/24 21:42:25.129
  Jan 29 21:42:25.134: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2024-01-29 21:42:24 +0000 UTC Passed all checks passed}
  Jan 29 21:42:25.134: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:42:25.134: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jan 29 21:42:25.134: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 01/29/24 21:42:25.134
  STEP: Confirm that the generated APIService has been deleted @ 01/29/24 21:42:25.141
  Jan 29 21:42:25.141: INFO: Requesting list of APIServices to confirm quantity
  Jan 29 21:42:25.146: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jan 29 21:42:25.146: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jan 29 21:42:25.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-3277" for this suite. @ 01/29/24 21:42:25.244
• [26.942 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 01/29/24 21:42:25.264
  Jan 29 21:42:25.264: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:42:25.265
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:42:25.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:42:25.288
  STEP: Creating secret with name projected-secret-test-1d9077cb-da0d-4e30-93c5-0db200ad06be @ 01/29/24 21:42:25.293
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:42:25.298
  E0129 21:42:25.375636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:26.376520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:27.377672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:28.377807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:42:29.323
  Jan 29 21:42:29.327: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-f8f99e68-c5df-404a-9f3a-def3864354f5 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:42:29.342
  Jan 29 21:42:29.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4937" for this suite. @ 01/29/24 21:42:29.368
• [4.109 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 01/29/24 21:42:29.374
  Jan 29 21:42:29.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:42:29.375
  E0129 21:42:29.378091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:42:29.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:42:29.391
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:42:29.396
  E0129 21:42:30.379233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:31.379749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:32.380604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:33.381609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:42:33.422
  Jan 29 21:42:33.426: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-1e3cb0a3-eb82-4924-8ec1-d39a5ef26636 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:42:33.435
  Jan 29 21:42:33.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3852" for this suite. @ 01/29/24 21:42:33.456
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 01/29/24 21:42:33.464
  Jan 29 21:42:33.464: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/29/24 21:42:33.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:42:33.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:42:33.482
  STEP: create the container to handle the HTTPGet hook request. @ 01/29/24 21:42:33.491
  E0129 21:42:34.382189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:35.383004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 01/29/24 21:42:35.516
  E0129 21:42:36.383126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:37.384166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 01/29/24 21:42:37.541
  STEP: delete the pod with lifecycle hook @ 01/29/24 21:42:37.55
  E0129 21:42:38.384722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:39.385287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:39.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5146" for this suite. @ 01/29/24 21:42:39.573
• [6.117 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 01/29/24 21:42:39.581
  Jan 29 21:42:39.581: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:42:39.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:42:39.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:42:39.605
  STEP: create the rc1 @ 01/29/24 21:42:39.615
  STEP: create the rc2 @ 01/29/24 21:42:39.621
  E0129 21:42:40.385988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:41.386552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:42.386831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:43.387436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:44.388062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:45.389033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 01/29/24 21:42:45.636
  STEP: delete the rc simpletest-rc-to-be-deleted @ 01/29/24 21:42:46.074
  STEP: wait for the rc to be deleted @ 01/29/24 21:42:46.08
  E0129 21:42:46.389407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:47.390557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:48.391088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:49.391683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:50.391928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:51.101: INFO: 71 pods remaining
  Jan 29 21:42:51.101: INFO: 71 pods has nil DeletionTimestamp
  Jan 29 21:42:51.101: INFO: 
  E0129 21:42:51.392556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:52.392783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:53.393192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:54.394264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:55.394750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 01/29/24 21:42:56.098
  Jan 29 21:42:56.233: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:42:56.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-26hvx" in namespace "gc-8447"
  Jan 29 21:42:56.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hh2w" in namespace "gc-8447"
  Jan 29 21:42:56.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sgws" in namespace "gc-8447"
  Jan 29 21:42:56.268: INFO: Deleting pod "simpletest-rc-to-be-deleted-5b4r5" in namespace "gc-8447"
  Jan 29 21:42:56.277: INFO: Deleting pod "simpletest-rc-to-be-deleted-5k2dd" in namespace "gc-8447"
  Jan 29 21:42:56.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-5swng" in namespace "gc-8447"
  Jan 29 21:42:56.295: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wbmv" in namespace "gc-8447"
  Jan 29 21:42:56.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-5zgzl" in namespace "gc-8447"
  Jan 29 21:42:56.312: INFO: Deleting pod "simpletest-rc-to-be-deleted-5znsm" in namespace "gc-8447"
  Jan 29 21:42:56.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-6k27t" in namespace "gc-8447"
  Jan 29 21:42:56.326: INFO: Deleting pod "simpletest-rc-to-be-deleted-6xcc8" in namespace "gc-8447"
  Jan 29 21:42:56.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-7jzjv" in namespace "gc-8447"
  Jan 29 21:42:56.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lnwl" in namespace "gc-8447"
  Jan 29 21:42:56.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-7n5j9" in namespace "gc-8447"
  Jan 29 21:42:56.374: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f2xd" in namespace "gc-8447"
  Jan 29 21:42:56.383: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hdf6" in namespace "gc-8447"
  Jan 29 21:42:56.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-8psv8" in namespace "gc-8447"
  E0129 21:42:56.396128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:42:56.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-94mgk" in namespace "gc-8447"
  Jan 29 21:42:56.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-9krr9" in namespace "gc-8447"
  Jan 29 21:42:56.414: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kzsf" in namespace "gc-8447"
  Jan 29 21:42:56.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pqwl" in namespace "gc-8447"
  Jan 29 21:42:56.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5lxn" in namespace "gc-8447"
  Jan 29 21:42:56.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4cbx" in namespace "gc-8447"
  Jan 29 21:42:56.455: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb5k5" in namespace "gc-8447"
  Jan 29 21:42:56.462: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgqr8" in namespace "gc-8447"
  Jan 29 21:42:56.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr7qs" in namespace "gc-8447"
  Jan 29 21:42:56.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5nfq" in namespace "gc-8447"
  Jan 29 21:42:56.493: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8sc2" in namespace "gc-8447"
  Jan 29 21:42:56.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-dckrc" in namespace "gc-8447"
  Jan 29 21:42:56.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddg9j" in namespace "gc-8447"
  Jan 29 21:42:56.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgjsx" in namespace "gc-8447"
  Jan 29 21:42:56.530: INFO: Deleting pod "simpletest-rc-to-be-deleted-dkwnl" in namespace "gc-8447"
  Jan 29 21:42:56.538: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmsfc" in namespace "gc-8447"
  Jan 29 21:42:56.547: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr664" in namespace "gc-8447"
  Jan 29 21:42:56.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffj5r" in namespace "gc-8447"
  Jan 29 21:42:56.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-fl294" in namespace "gc-8447"
  Jan 29 21:42:56.581: INFO: Deleting pod "simpletest-rc-to-be-deleted-fshgk" in namespace "gc-8447"
  Jan 29 21:42:56.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-g85gh" in namespace "gc-8447"
  Jan 29 21:42:56.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkhw4" in namespace "gc-8447"
  Jan 29 21:42:56.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-gv9bj" in namespace "gc-8447"
  Jan 29 21:42:56.623: INFO: Deleting pod "simpletest-rc-to-be-deleted-hrqv2" in namespace "gc-8447"
  Jan 29 21:42:56.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwrdh" in namespace "gc-8447"
  Jan 29 21:42:56.643: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt7c2" in namespace "gc-8447"
  Jan 29 21:42:56.651: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4f5q" in namespace "gc-8447"
  Jan 29 21:42:56.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-k88qt" in namespace "gc-8447"
  Jan 29 21:42:56.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-kkpsw" in namespace "gc-8447"
  Jan 29 21:42:56.684: INFO: Deleting pod "simpletest-rc-to-be-deleted-kp9p8" in namespace "gc-8447"
  Jan 29 21:42:56.693: INFO: Deleting pod "simpletest-rc-to-be-deleted-l9nhb" in namespace "gc-8447"
  Jan 29 21:42:56.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcmd5" in namespace "gc-8447"
  Jan 29 21:42:56.709: INFO: Deleting pod "simpletest-rc-to-be-deleted-lcpj4" in namespace "gc-8447"
  Jan 29 21:42:56.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8447" for this suite. @ 01/29/24 21:42:56.725
• [17.149 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 01/29/24 21:42:56.731
  Jan 29 21:42:56.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:42:56.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:42:56.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:42:56.749
  STEP: Creating a suspended job @ 01/29/24 21:42:56.755
  STEP: Patching the Job @ 01/29/24 21:42:56.761
  STEP: Watching for Job to be patched @ 01/29/24 21:42:56.78
  Jan 29 21:42:56.782: INFO: Event ADDED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jan 29 21:42:56.782: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jan 29 21:42:56.782: INFO: Event MODIFIED found for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 01/29/24 21:42:56.782
  STEP: Watching for Job to be updated @ 01/29/24 21:42:56.793
  Jan 29 21:42:56.795: INFO: Event MODIFIED found for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:42:56.795: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 01/29/24 21:42:56.795
  Jan 29 21:42:56.798: INFO: Job: e2e-jmqvk as labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk]
  STEP: Waiting for job to complete @ 01/29/24 21:42:56.798
  E0129 21:42:57.396890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:58.398054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:42:59.399230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:00.399795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:01.400496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:02.400620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:03.401421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:04.402016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:05.403065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:06.403294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 01/29/24 21:43:06.804
  STEP: Watching for Job to be deleted @ 01/29/24 21:43:06.813
  Jan 29 21:43:06.816: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event MODIFIED observed for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 29 21:43:06.817: INFO: Event DELETED found for Job e2e-jmqvk in namespace job-7481 with labels: map[e2e-jmqvk:patched e2e-job-label:e2e-jmqvk] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 01/29/24 21:43:06.817
  Jan 29 21:43:06.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7481" for this suite. @ 01/29/24 21:43:06.826
• [10.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 01/29/24 21:43:06.834
  Jan 29 21:43:06.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename tables @ 01/29/24 21:43:06.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:43:06.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:43:06.881
  Jan 29 21:43:06.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5655" for this suite. @ 01/29/24 21:43:06.893
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 01/29/24 21:43:06.902
  Jan 29 21:43:06.902: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 21:43:06.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:43:06.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:43:06.921
  STEP: Creating pod liveness-6429098e-7312-4fd3-b9f4-ef4684fc3309 in namespace container-probe-9742 @ 01/29/24 21:43:06.925
  E0129 21:43:07.403808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:08.404875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:08.943: INFO: Started pod liveness-6429098e-7312-4fd3-b9f4-ef4684fc3309 in namespace container-probe-9742
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 21:43:08.943
  Jan 29 21:43:08.947: INFO: Initial restart count of pod liveness-6429098e-7312-4fd3-b9f4-ef4684fc3309 is 0
  E0129 21:43:09.406153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:10.407227      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:11.407734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:12.408183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:13.408629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:14.409242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:15.410051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:16.410147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:17.410495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:18.411061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:19.411438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:20.412141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:21.412758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:22.412999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:23.413523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:24.414105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:25.414392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:26.414676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:27.415143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:28.415802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:29.015: INFO: Restart count of pod container-probe-9742/liveness-6429098e-7312-4fd3-b9f4-ef4684fc3309 is now 1 (20.068325469s elapsed)
  Jan 29 21:43:29.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:43:29.021
  STEP: Destroying namespace "container-probe-9742" for this suite. @ 01/29/24 21:43:29.035
• [22.141 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 01/29/24 21:43:29.043
  Jan 29 21:43:29.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 21:43:29.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:43:29.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:43:29.063
  Jan 29 21:43:29.084: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 01/29/24 21:43:29.09
  Jan 29 21:43:29.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:29.093: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 01/29/24 21:43:29.093
  Jan 29 21:43:29.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:29.112: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:43:29.416743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:30.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:30.119: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:43:30.417401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:31.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:43:31.119: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 01/29/24 21:43:31.123
  Jan 29 21:43:31.145: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:43:31.145: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0129 21:43:31.417705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:32.151: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:32.151: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 01/29/24 21:43:32.151
  Jan 29 21:43:32.170: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:32.170: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:43:32.418632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:33.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:33.177: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:43:33.419634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:34.175: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:43:34.175: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 21:43:34.182
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4638, will wait for the garbage collector to delete the pods @ 01/29/24 21:43:34.182
  Jan 29 21:43:34.242: INFO: Deleting DaemonSet.extensions daemon-set took: 6.082531ms
  Jan 29 21:43:34.343: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.796993ms
  E0129 21:43:34.419672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:35.420100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:43:35.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:43:35.848: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 21:43:35.852: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28109"},"items":null}

  Jan 29 21:43:35.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28109"},"items":null}

  Jan 29 21:43:35.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4638" for this suite. @ 01/29/24 21:43:35.886
• [6.849 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 01/29/24 21:43:35.892
  Jan 29 21:43:35.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename sched-preemption @ 01/29/24 21:43:35.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:43:35.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:43:35.909
  Jan 29 21:43:35.926: INFO: Waiting up to 1m0s for all nodes to be ready
  E0129 21:43:36.420508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:37.420739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:38.420871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:39.421530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:40.422097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:41.423108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:42.423729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:43.424288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:44.424719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:45.425177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:46.425345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:47.425538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:48.426192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:49.426864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:50.427582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:51.427647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:52.428326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:53.428892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:54.429118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:55.429798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:56.430281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:57.430511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:58.431071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:43:59.431665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:00.431831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:01.431956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:02.432466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:03.433034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:04.433950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:05.434508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:06.434759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:07.435922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:08.436431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:09.436937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:10.437089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:11.437375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:12.438245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:13.438834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:14.439730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:15.440326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:16.441258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:17.442478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:18.442550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:19.443130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:20.444345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:21.444454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:22.444620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:23.445240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:24.445528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:25.445999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:26.446700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:27.446958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:28.447562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:29.448128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:30.448416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:31.448951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:32.450106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:33.450620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:34.451736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:35.452293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:44:35.960: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/29/24 21:44:35.965
  Jan 29 21:44:35.994: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 29 21:44:35.999: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 29 21:44:36.017: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 29 21:44:36.024: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/29/24 21:44:36.024
  E0129 21:44:36.452613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:37.452713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 01/29/24 21:44:38.049
  E0129 21:44:38.453321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:39.454387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:44:40.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7761" for this suite. @ 01/29/24 21:44:40.154
• [64.274 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 01/29/24 21:44:40.167
  Jan 29 21:44:40.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:44:40.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:44:40.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:44:40.193
  STEP: Creating a pod to test downward api env vars @ 01/29/24 21:44:40.198
  E0129 21:44:40.454654      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:41.455716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:42.456593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:43.457112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:44:44.222
  Jan 29 21:44:44.226: INFO: Trying to get logs from node nodea08 pod downward-api-8bca6aec-4450-4dc6-8add-5219d64839a9 container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 21:44:44.255
  Jan 29 21:44:44.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-960" for this suite. @ 01/29/24 21:44:44.277
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 01/29/24 21:44:44.284
  Jan 29 21:44:44.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename endpointslice @ 01/29/24 21:44:44.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:44:44.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:44:44.303
  E0129 21:44:44.457683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:45.458450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:46.458941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:47.459856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:48.460320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 01/29/24 21:44:49.376
  E0129 21:44:49.460769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:50.461355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:51.461858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:52.462365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:53.462830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 01/29/24 21:44:54.385
  E0129 21:44:54.462911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:55.463507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:56.464117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:57.464185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:44:58.465157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 01/29/24 21:44:59.398
  E0129 21:44:59.466148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:00.466703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:01.467293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:02.467796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:03.468353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 01/29/24 21:45:04.408
  Jan 29 21:45:04.427: INFO: EndpointSlice for Service endpointslice-8076/example-named-port not found
  E0129 21:45:04.468434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:05.468937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:06.469466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:07.470596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:08.470890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:09.470979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:10.471571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:11.472177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:12.472414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:13.472945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:45:14.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8076" for this suite. @ 01/29/24 21:45:14.447
• [30.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 01/29/24 21:45:14.456
  Jan 29 21:45:14.456: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:45:14.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:14.472
  E0129 21:45:14.472928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:14.476
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:45:14.481
  E0129 21:45:15.473430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:16.474331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:17.475326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:18.475907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:45:18.507
  Jan 29 21:45:18.512: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-b48add03-44f8-4ac3-9b9a-5eef3f309687 container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:45:18.521
  Jan 29 21:45:18.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2191" for this suite. @ 01/29/24 21:45:18.547
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 01/29/24 21:45:18.56
  Jan 29 21:45:18.560: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/29/24 21:45:18.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:18.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:18.579
  STEP: creating @ 01/29/24 21:45:18.584
  STEP: getting @ 01/29/24 21:45:18.6
  STEP: listing @ 01/29/24 21:45:18.607
  STEP: deleting @ 01/29/24 21:45:18.611
  Jan 29 21:45:18.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5094" for this suite. @ 01/29/24 21:45:18.634
• [0.079 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 01/29/24 21:45:18.641
  Jan 29 21:45:18.641: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:45:18.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:18.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:18.66
  STEP: Creating configMap with name projected-configmap-test-volume-map-0aa1a8a7-741c-4fb2-8a01-b8ef3817bea0 @ 01/29/24 21:45:18.665
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:45:18.669
  E0129 21:45:19.476413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:20.477408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:21.477467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:22.477803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:45:22.698
  Jan 29 21:45:22.702: INFO: Trying to get logs from node nodeb29 pod pod-projected-configmaps-1a2f6ba8-25b0-4db9-82ce-df7b3d214cc3 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:45:22.727
  Jan 29 21:45:22.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-115" for this suite. @ 01/29/24 21:45:22.751
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 01/29/24 21:45:22.764
  Jan 29 21:45:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:45:22.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:22.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:22.786
  STEP: Creating a pod to test env composition @ 01/29/24 21:45:22.79
  E0129 21:45:23.477915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:24.478300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:25.478843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:26.479408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:45:26.816
  Jan 29 21:45:26.820: INFO: Trying to get logs from node nodea08 pod var-expansion-2b8ae54b-1c44-43b0-9653-fc9b68e60628 container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 21:45:26.833
  Jan 29 21:45:26.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2481" for this suite. @ 01/29/24 21:45:26.857
• [4.098 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 01/29/24 21:45:26.864
  Jan 29 21:45:26.864: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 21:45:26.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:26.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:26.885
  Jan 29 21:45:26.889: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:45:27.480400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:28.481331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:29.481887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:45:29.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5684" for this suite. @ 01/29/24 21:45:30.014
• [3.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 01/29/24 21:45:30.026
  Jan 29 21:45:30.026: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:45:30.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:30.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:30.045
  STEP: Setting up server cert @ 01/29/24 21:45:30.066
  E0129 21:45:30.483010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:45:30.488
  STEP: Deploying the webhook pod @ 01/29/24 21:45:30.499
  STEP: Wait for the deployment to be ready @ 01/29/24 21:45:30.511
  Jan 29 21:45:30.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:45:31.483661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:32.483804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:45:32.532
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:45:32.548
  E0129 21:45:33.484163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:45:33.549: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 01/29/24 21:45:33.554
  STEP: create a configmap that should be updated by the webhook @ 01/29/24 21:45:33.582
  Jan 29 21:45:33.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2995" for this suite. @ 01/29/24 21:45:33.647
  STEP: Destroying namespace "webhook-markers-6542" for this suite. @ 01/29/24 21:45:33.653
• [3.633 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 01/29/24 21:45:33.665
  Jan 29 21:45:33.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-runtime @ 01/29/24 21:45:33.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:33.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:33.683
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 01/29/24 21:45:33.696
  E0129 21:45:34.485381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:35.485835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:36.486110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:37.486555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:38.486815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:39.486954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:40.487769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:41.487840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:42.488131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:43.488444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:44.489071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:45.490249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:46.490262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:47.491314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:48.491662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:49.492430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:50.492559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:51.492903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 01/29/24 21:45:51.813
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 01/29/24 21:45:51.818
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 01/29/24 21:45:51.825
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 01/29/24 21:45:51.825
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 01/29/24 21:45:51.848
  E0129 21:45:52.494003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:53.494890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:54.495374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 01/29/24 21:45:54.87
  E0129 21:45:55.496166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 01/29/24 21:45:55.88
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 01/29/24 21:45:55.889
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 01/29/24 21:45:55.889
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 01/29/24 21:45:55.915
  E0129 21:45:56.496789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 01/29/24 21:45:56.926
  E0129 21:45:57.497795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:45:58.498696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 01/29/24 21:45:58.943
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 01/29/24 21:45:58.952
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 01/29/24 21:45:58.952
  Jan 29 21:45:58.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2630" for this suite. @ 01/29/24 21:45:58.983
• [25.324 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 01/29/24 21:45:58.989
  Jan 29 21:45:58.990: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 21:45:58.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:45:59.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:45:59.013
  STEP: Counting existing ResourceQuota @ 01/29/24 21:45:59.018
  E0129 21:45:59.498968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:00.498986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:01.499303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:02.500466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:03.501476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/29/24 21:46:04.022
  STEP: Ensuring resource quota status is calculated @ 01/29/24 21:46:04.027
  E0129 21:46:04.501687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:05.502342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 01/29/24 21:46:06.034
  STEP: Ensuring resource quota status captures replication controller creation @ 01/29/24 21:46:06.05
  E0129 21:46:06.502885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:07.503284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 01/29/24 21:46:08.056
  STEP: Ensuring resource quota status released usage @ 01/29/24 21:46:08.063
  E0129 21:46:08.503492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:09.503775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:10.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8564" for this suite. @ 01/29/24 21:46:10.074
• [11.094 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 01/29/24 21:46:10.085
  Jan 29 21:46:10.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:46:10.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:46:10.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:46:10.11
  STEP: starting the proxy server @ 01/29/24 21:46:10.115
  Jan 29 21:46:10.115: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-7283 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 01/29/24 21:46:10.189
  Jan 29 21:46:10.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7283" for this suite. @ 01/29/24 21:46:10.21
• [0.133 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 01/29/24 21:46:10.222
  Jan 29 21:46:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 21:46:10.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:46:10.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:46:10.241
  Jan 29 21:46:10.258: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0129 21:46:10.503933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:11.505016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:12.506121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:13.506690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:14.507340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:15.263: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 21:46:15.263
  STEP: Scaling up "test-rs" replicaset  @ 01/29/24 21:46:15.264
  Jan 29 21:46:15.272: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 01/29/24 21:46:15.273
  W0129 21:46:15.279915      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 29 21:46:15.282: INFO: observed ReplicaSet test-rs in namespace replicaset-1000 with ReadyReplicas 1, AvailableReplicas 1
  Jan 29 21:46:15.290: INFO: observed ReplicaSet test-rs in namespace replicaset-1000 with ReadyReplicas 1, AvailableReplicas 1
  Jan 29 21:46:15.300: INFO: observed ReplicaSet test-rs in namespace replicaset-1000 with ReadyReplicas 1, AvailableReplicas 1
  Jan 29 21:46:15.304: INFO: observed ReplicaSet test-rs in namespace replicaset-1000 with ReadyReplicas 1, AvailableReplicas 1
  E0129 21:46:15.508306      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:16.307: INFO: observed ReplicaSet test-rs in namespace replicaset-1000 with ReadyReplicas 2, AvailableReplicas 2
  Jan 29 21:46:16.454: INFO: observed Replicaset test-rs in namespace replicaset-1000 with ReadyReplicas 3 found true
  Jan 29 21:46:16.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1000" for this suite. @ 01/29/24 21:46:16.459
• [6.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 01/29/24 21:46:16.466
  Jan 29 21:46:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:46:16.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:46:16.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:46:16.487
  STEP: Creating configMap with name configmap-test-volume-ef6ad8c1-8057-48af-bcdf-7f2a30c2ad42 @ 01/29/24 21:46:16.491
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:46:16.496
  E0129 21:46:16.508911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:17.509780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:18.510310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:19.510995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:20.511567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:46:20.522
  Jan 29 21:46:20.527: INFO: Trying to get logs from node nodeb29 pod pod-configmaps-e5ecf282-0fc5-43b9-bf39-c535488a896f container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:46:20.536
  Jan 29 21:46:20.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9627" for this suite. @ 01/29/24 21:46:20.561
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 01/29/24 21:46:20.569
  Jan 29 21:46:20.569: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 21:46:20.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:46:20.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:46:20.589
  STEP: Creating service test in namespace statefulset-6633 @ 01/29/24 21:46:20.594
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 01/29/24 21:46:20.6
  STEP: Creating stateful set ss in namespace statefulset-6633 @ 01/29/24 21:46:20.606
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6633 @ 01/29/24 21:46:20.615
  Jan 29 21:46:20.618: INFO: Found 0 stateful pods, waiting for 1
  E0129 21:46:21.512485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:22.512738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:23.513181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:24.513938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:25.514553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:26.515014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:27.515813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:28.516246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:29.517125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:30.517693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:30.624: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 01/29/24 21:46:30.624
  Jan 29 21:46:30.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 21:46:30.866: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:46:30.866: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:46:30.866: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 21:46:30.870: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0129 21:46:31.518265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:32.518503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:33.519784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:34.520331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:35.520879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:36.521444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:37.521582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:38.522479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:39.522989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:40.523641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:40.876: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 21:46:40.876: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:46:40.894: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999587s
  E0129 21:46:41.524099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:41.900: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995803689s
  E0129 21:46:42.525076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:42.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989830025s
  E0129 21:46:43.526277      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:43.913: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983498916s
  E0129 21:46:44.526941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:44.919: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976560116s
  E0129 21:46:45.527243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:45.926: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.9704751s
  E0129 21:46:46.528203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:46.932: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963690567s
  E0129 21:46:47.529359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:47.940: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.957435964s
  E0129 21:46:48.530200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:48.947: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.949475365s
  E0129 21:46:49.531224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:46:49.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.54448ms
  E0129 21:46:50.532405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6633 @ 01/29/24 21:46:50.955
  Jan 29 21:46:50.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 21:46:51.168: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:46:51.168: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:46:51.168: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 21:46:51.173: INFO: Found 1 stateful pods, waiting for 3
  E0129 21:46:51.532974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:52.534044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:53.534621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:54.535507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:55.536166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:56.536644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:57.537818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:58.538348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:46:59.538899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:00.539513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:01.181: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:47:01.181: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:47:01.181: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 01/29/24 21:47:01.181
  STEP: Scale down will halt with unhealthy stateful pod @ 01/29/24 21:47:01.181
  Jan 29 21:47:01.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 21:47:01.409: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:47:01.409: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:47:01.409: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 21:47:01.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0129 21:47:01.540575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:01.635: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:47:01.635: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:47:01.635: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 21:47:01.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 29 21:47:01.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 29 21:47:01.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 29 21:47:01.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 29 21:47:01.871: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:47:01.875: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0129 21:47:02.541778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:03.541921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:04.542207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:05.542919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:06.543488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:07.543592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:08.543884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:09.544163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:10.544495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:11.545106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:11.889: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 21:47:11.889: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 21:47:11.889: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 29 21:47:11.910: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999596s
  E0129 21:47:12.545488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:12.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993047416s
  E0129 21:47:13.545942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:13.925: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986274837s
  E0129 21:47:14.546186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:14.932: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979008369s
  E0129 21:47:15.546632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:15.938: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972437448s
  E0129 21:47:16.547399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:16.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96563697s
  E0129 21:47:17.548407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:17.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.958620765s
  E0129 21:47:18.548974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:18.960: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.950834269s
  E0129 21:47:19.550108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:19.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.943892648s
  E0129 21:47:20.550432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:20.976: INFO: Verifying statefulset ss doesn't scale past 3 for another 935.390916ms
  E0129 21:47:21.550736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6633 @ 01/29/24 21:47:21.977
  Jan 29 21:47:21.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 21:47:22.221: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:47:22.221: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:47:22.221: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 21:47:22.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 29 21:47:22.426: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:47:22.426: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:47:22.426: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 21:47:22.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=statefulset-6633 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0129 21:47:22.551541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:22.629: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 29 21:47:22.629: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 29 21:47:22.629: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 29 21:47:22.629: INFO: Scaling statefulset ss to 0
  E0129 21:47:23.552309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:24.552781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:25.553305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:26.554405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:27.554543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:28.555052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:29.556079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:30.556565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:31.557065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:32.557293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 01/29/24 21:47:32.653
  Jan 29 21:47:32.653: INFO: Deleting all statefulset in ns statefulset-6633
  Jan 29 21:47:32.658: INFO: Scaling statefulset ss to 0
  Jan 29 21:47:32.671: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 29 21:47:32.674: INFO: Deleting statefulset ss
  Jan 29 21:47:32.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6633" for this suite. @ 01/29/24 21:47:32.703
• [72.140 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 01/29/24 21:47:32.71
  Jan 29 21:47:32.710: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 21:47:32.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:32.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:32.732
  Jan 29 21:47:32.737: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:47:33.557836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/29/24 21:47:34.135
  Jan 29 21:47:34.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-9210 --namespace=crd-publish-openapi-9210 create -f -'
  E0129 21:47:34.558029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:34.933: INFO: stderr: ""
  Jan 29 21:47:34.933: INFO: stdout: "e2e-test-crd-publish-openapi-7406-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 29 21:47:34.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-9210 --namespace=crd-publish-openapi-9210 delete e2e-test-crd-publish-openapi-7406-crds test-cr'
  Jan 29 21:47:35.035: INFO: stderr: ""
  Jan 29 21:47:35.035: INFO: stdout: "e2e-test-crd-publish-openapi-7406-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jan 29 21:47:35.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-9210 --namespace=crd-publish-openapi-9210 apply -f -'
  Jan 29 21:47:35.313: INFO: stderr: ""
  Jan 29 21:47:35.313: INFO: stdout: "e2e-test-crd-publish-openapi-7406-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 29 21:47:35.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-9210 --namespace=crd-publish-openapi-9210 delete e2e-test-crd-publish-openapi-7406-crds test-cr'
  Jan 29 21:47:35.420: INFO: stderr: ""
  Jan 29 21:47:35.420: INFO: stdout: "e2e-test-crd-publish-openapi-7406-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 01/29/24 21:47:35.42
  Jan 29 21:47:35.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-9210 explain e2e-test-crd-publish-openapi-7406-crds'
  E0129 21:47:35.558622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:35.693: INFO: stderr: ""
  Jan 29 21:47:35.693: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-7406-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0129 21:47:36.559402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:37.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9210" for this suite. @ 01/29/24 21:47:37.12
• [4.416 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 01/29/24 21:47:37.128
  Jan 29 21:47:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 21:47:37.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:37.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:37.148
  STEP: apply creating a deployment @ 01/29/24 21:47:37.153
  Jan 29 21:47:37.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-960" for this suite. @ 01/29/24 21:47:37.173
• [0.053 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 01/29/24 21:47:37.182
  Jan 29 21:47:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:47:37.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:37.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:37.199
  STEP: Creating secret with name secret-test-9c74fc5c-6072-4270-a307-91ae98cb7835 @ 01/29/24 21:47:37.204
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:47:37.209
  E0129 21:47:37.559760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:38.560747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:39.560780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:40.561344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:47:41.235
  Jan 29 21:47:41.239: INFO: Trying to get logs from node nodea08 pod pod-secrets-92d81ad3-e66b-4fc0-b358-843b63b398e4 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:47:41.266
  Jan 29 21:47:41.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1265" for this suite. @ 01/29/24 21:47:41.287
• [4.110 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 01/29/24 21:47:41.292
  Jan 29 21:47:41.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubelet-test @ 01/29/24 21:47:41.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:41.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:41.309
  E0129 21:47:41.562445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:42.562730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:43.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8272" for this suite. @ 01/29/24 21:47:43.354
• [2.070 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 01/29/24 21:47:43.363
  Jan 29 21:47:43.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 21:47:43.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:43.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:43.391
  Jan 29 21:47:43.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:47:43.563455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:44.564191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:45.564943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0129 21:47:45.960446      23 warnings.go:70] unknown field "alpha"
  W0129 21:47:45.960504      23 warnings.go:70] unknown field "beta"
  W0129 21:47:45.960516      23 warnings.go:70] unknown field "delta"
  W0129 21:47:45.960528      23 warnings.go:70] unknown field "epsilon"
  W0129 21:47:45.960539      23 warnings.go:70] unknown field "gamma"
  Jan 29 21:47:46.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-628" for this suite. @ 01/29/24 21:47:46.512
• [3.154 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 01/29/24 21:47:46.518
  Jan 29 21:47:46.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:47:46.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:47:46.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:47:46.536
  STEP: Creating a job @ 01/29/24 21:47:46.54
  STEP: Ensuring active pods == parallelism @ 01/29/24 21:47:46.546
  E0129 21:47:46.565292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:47.566044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 01/29/24 21:47:48.551
  STEP: deleting Job.batch foo in namespace job-3689, will wait for the garbage collector to delete the pods @ 01/29/24 21:47:48.552
  E0129 21:47:48.566829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:47:48.614: INFO: Deleting Job.batch foo took: 7.918245ms
  Jan 29 21:47:48.714: INFO: Terminating Job.batch foo pods took: 100.769974ms
  E0129 21:47:49.567717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:50.568249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:51.568412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:52.569229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:53.569622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:54.570752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:55.570923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:56.571672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:57.572656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:58.573120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:47:59.573531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:00.573946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:01.574558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:02.575525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:03.576497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:04.577486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:05.578050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:06.579035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:07.579868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:08.579896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:09.580079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:10.580729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:11.581701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:12.581945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:13.583113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:14.583625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:15.584196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:16.585390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:17.586181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:18.587120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:19.588049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 01/29/24 21:48:20.015
  Jan 29 21:48:20.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3689" for this suite. @ 01/29/24 21:48:20.028
• [33.518 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 01/29/24 21:48:20.036
  Jan 29 21:48:20.036: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename field-validation @ 01/29/24 21:48:20.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:20.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:20.054
  Jan 29 21:48:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:48:20.589179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:21.589920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:22.590236      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0129 21:48:22.624984      23 warnings.go:70] unknown field "alpha"
  W0129 21:48:22.625023      23 warnings.go:70] unknown field "beta"
  W0129 21:48:22.625031      23 warnings.go:70] unknown field "delta"
  W0129 21:48:22.625037      23 warnings.go:70] unknown field "epsilon"
  W0129 21:48:22.625044      23 warnings.go:70] unknown field "gamma"
  Jan 29 21:48:23.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9327" for this suite. @ 01/29/24 21:48:23.187
• [3.162 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 01/29/24 21:48:23.198
  Jan 29 21:48:23.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 21:48:23.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:23.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:23.218
  STEP: Creating simple DaemonSet "daemon-set" @ 01/29/24 21:48:23.244
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 21:48:23.251
  Jan 29 21:48:23.255: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:48:23.259: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:48:23.259: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:48:23.590199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:24.266: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:48:24.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:48:24.271: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:48:24.590916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:25.264: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:48:25.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 21:48:25.268: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 01/29/24 21:48:25.27
  Jan 29 21:48:25.275: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 01/29/24 21:48:25.275
  Jan 29 21:48:25.286: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 01/29/24 21:48:25.286
  Jan 29 21:48:25.290: INFO: Observed &DaemonSet event: ADDED
  Jan 29 21:48:25.290: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.290: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.290: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.290: INFO: Found daemon set daemon-set in namespace daemonsets-4767 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 29 21:48:25.290: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 01/29/24 21:48:25.29
  STEP: watching for the daemon set status to be patched @ 01/29/24 21:48:25.298
  Jan 29 21:48:25.302: INFO: Observed &DaemonSet event: ADDED
  Jan 29 21:48:25.302: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.302: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.303: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.303: INFO: Observed daemon set daemon-set in namespace daemonsets-4767 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 29 21:48:25.303: INFO: Observed &DaemonSet event: MODIFIED
  Jan 29 21:48:25.303: INFO: Found daemon set daemon-set in namespace daemonsets-4767 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jan 29 21:48:25.303: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 21:48:25.307
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4767, will wait for the garbage collector to delete the pods @ 01/29/24 21:48:25.307
  Jan 29 21:48:25.368: INFO: Deleting DaemonSet.extensions daemon-set took: 7.16505ms
  Jan 29 21:48:25.469: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.615296ms
  E0129 21:48:25.591383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:26.591516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:26.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:48:26.774: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 21:48:26.778: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29584"},"items":null}

  Jan 29 21:48:26.782: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29584"},"items":null}

  Jan 29 21:48:26.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4767" for this suite. @ 01/29/24 21:48:26.8
• [3.610 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 01/29/24 21:48:26.808
  Jan 29 21:48:26.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 21:48:26.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:26.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:26.827
  Jan 29 21:48:26.831: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jan 29 21:48:26.842: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0129 21:48:27.592190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:28.592384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:29.592909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:30.593450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:31.593988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:31.848: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/29/24 21:48:31.848
  Jan 29 21:48:31.848: INFO: Creating deployment "test-rolling-update-deployment"
  Jan 29 21:48:31.855: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jan 29 21:48:31.862: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0129 21:48:32.595022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:33.595610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:33.874: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jan 29 21:48:33.878: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jan 29 21:48:33.891: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3117  b942cc21-c058-4599-a6d8-1f61b2068958 29660 1 2024-01-29 21:48:31 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-29 21:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ebb098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-29 21:48:31 +0000 UTC,LastTransitionTime:2024-01-29 21:48:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2024-01-29 21:48:33 +0000 UTC,LastTransitionTime:2024-01-29 21:48:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 29 21:48:33.895: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3117  ec68dc25-ab56-464b-a415-406e2d6b9dea 29650 1 2024-01-29 21:48:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b942cc21-c058-4599-a6d8-1f61b2068958 0xc003ebb5d7 0xc003ebb5d8}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b942cc21-c058-4599-a6d8-1f61b2068958\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:48:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ebb688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:48:33.895: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jan 29 21:48:33.895: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3117  1f9dfa76-ddc6-4260-b8fc-d94b2cf5d9ce 29659 2 2024-01-29 21:48:26 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b942cc21-c058-4599-a6d8-1f61b2068958 0xc003ebb487 0xc003ebb488}] [] [{e2e.test Update apps/v1 2024-01-29 21:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:48:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b942cc21-c058-4599-a6d8-1f61b2068958\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:48:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003ebb548 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:48:33.900: INFO: Pod "test-rolling-update-deployment-656d657cd8-c4r4n" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-c4r4n test-rolling-update-deployment-656d657cd8- deployment-3117  d383ed6e-6643-4b0a-b2d8-3f71ac8dfc83 29649 0 2024-01-29 21:48:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 ec68dc25-ab56-464b-a415-406e2d6b9dea 0xc003ebbb27 0xc003ebbb28}] [] [{kube-controller-manager Update v1 2024-01-29 21:48:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec68dc25-ab56-464b-a415-406e2d6b9dea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:48:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ls2td,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ls2td,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:48:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:48:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:48:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:48:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.217,StartTime:2024-01-29 21:48:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:48:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://bf87ead23db2e62c0cbf2943c8a327d6471561bad8134b0cb3cae7aa654ad425,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.217,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 21:48:33.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3117" for this suite. @ 01/29/24 21:48:33.906
• [7.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 01/29/24 21:48:33.915
  Jan 29 21:48:33.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename kubectl @ 01/29/24 21:48:33.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:33.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:33.936
  Jan 29 21:48:33.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 create -f -'
  E0129 21:48:34.596260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:34.790: INFO: stderr: ""
  Jan 29 21:48:34.790: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jan 29 21:48:34.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 create -f -'
  Jan 29 21:48:35.158: INFO: stderr: ""
  Jan 29 21:48:35.158: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/29/24 21:48:35.158
  E0129 21:48:35.597221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:36.164: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 21:48:36.164: INFO: Found 1 / 1
  Jan 29 21:48:36.164: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 29 21:48:36.169: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 29 21:48:36.169: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 29 21:48:36.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 describe pod agnhost-primary-n8sbp'
  Jan 29 21:48:36.287: INFO: stderr: ""
  Jan 29 21:48:36.288: INFO: stdout: "Name:             agnhost-primary-n8sbp\nNamespace:        kubectl-1734\nPriority:         0\nService Account:  default\nNode:             nodea08/192.168.100.28\nStart Time:       Mon, 29 Jan 2024 21:48:34 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.218\nIPs:\n  IP:           10.244.1.218\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://2c6fc7b801477e86b528919c906e813f699b2ca3d1626f78e3d988cd8c4d0853\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 29 Jan 2024 21:48:35 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-67nl9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-67nl9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1734/agnhost-primary-n8sbp to nodea08\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jan 29 21:48:36.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 describe rc agnhost-primary'
  Jan 29 21:48:36.407: INFO: stderr: ""
  Jan 29 21:48:36.407: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1734\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-n8sbp\n"
  Jan 29 21:48:36.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 describe service agnhost-primary'
  Jan 29 21:48:36.517: INFO: stderr: ""
  Jan 29 21:48:36.517: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1734\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.110.166.103\nIPs:               10.110.166.103\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.218:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jan 29 21:48:36.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 describe node nodea08'
  E0129 21:48:36.597635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:36.665: INFO: stderr: ""
  Jan 29 21:48:36.665: INFO: stdout: "Name:               nodea08\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=nodea08\n                    kubernetes.io/os=linux\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"6e:54:f1:5f:de:fd\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.100.28\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 29 Jan 2024 20:10:26 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  nodea08\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 29 Jan 2024 21:48:28 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 29 Jan 2024 20:55:12 +0000   Mon, 29 Jan 2024 20:55:12 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Mon, 29 Jan 2024 21:47:26 +0000   Mon, 29 Jan 2024 20:10:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 29 Jan 2024 21:47:26 +0000   Mon, 29 Jan 2024 20:10:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 29 Jan 2024 21:47:26 +0000   Mon, 29 Jan 2024 20:10:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 29 Jan 2024 21:47:26 +0000   Mon, 29 Jan 2024 20:10:44 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.100.28\n  Hostname:    nodea08\nCapacity:\n  cpu:                    16\n  ephemeral-storage:      162406320Ki\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 65841692Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    16\n  ephemeral-storage:      149673664265\n  example.com/fakecpu:    1k\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 65739292Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 4c0f2602d29847879565ef7551967ef4\n  System UUID:                4c0f2602-d298-4787-9565-ef7551967ef4\n  Boot ID:                    e78eb774-f9b4-4387-938e-ed29cb959b16\n  Kernel Version:             5.15.0-87-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.18\n  Kubelet Version:            v1.27.6-ckp\n  Kube-Proxy Version:         v1.27.6-ckp\nPodCIDR:                      10.244.1.0/24\nPodCIDRs:                     10.244.1.0/24\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  deployment-3117             test-rolling-update-deployment-656d657cd8-c4r4n            0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  kube-flannel                kube-flannel-ds-s6bhq                                      100m (0%)     0 (0%)      50Mi (0%)        0 (0%)         53m\n  kube-system                 kube-proxy-5wdv6                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         98m\n  kubectl-1734                agnhost-primary-n8sbp                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         2s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  sonobuoy                    sonobuoy-e2e-job-4fcfaf908eac4396                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-c23a8e4483624fdb-7btqf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests   Limits\n  --------               --------   ------\n  cpu                    100m (0%)  0 (0%)\n  memory                 50Mi (0%)  0 (0%)\n  ephemeral-storage      0 (0%)     0 (0%)\n  hugepages-1Gi          0 (0%)     0 (0%)\n  hugepages-2Mi          0 (0%)     0 (0%)\n  example.com/fakecpu    0          0\n  scheduling.k8s.io/foo  0          0\nEvents:                  <none>\n"
  Jan 29 21:48:36.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=kubectl-1734 describe namespace kubectl-1734'
  Jan 29 21:48:36.770: INFO: stderr: ""
  Jan 29 21:48:36.770: INFO: stdout: "Name:         kubectl-1734\nLabels:       e2e-framework=kubectl\n              e2e-run=31722b4b-774a-433c-b403-fd73cf822c96\n              kubernetes.io/metadata.name=kubectl-1734\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jan 29 21:48:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1734" for this suite. @ 01/29/24 21:48:36.775
• [2.868 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 01/29/24 21:48:36.783
  Jan 29 21:48:36.783: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:48:36.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:36.798
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:36.802
  STEP: Creating configMap with name configmap-test-volume-map-d7b4d215-2ab1-4976-b83f-18c913cc56ba @ 01/29/24 21:48:36.806
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:48:36.811
  E0129 21:48:37.598058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:38.598950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:39.599149      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:40.599749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:48:40.838
  Jan 29 21:48:40.841: INFO: Trying to get logs from node nodeb29 pod pod-configmaps-2fd70d4e-e75a-4f21-97f8-6f708b756a1f container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:48:40.863
  Jan 29 21:48:40.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8091" for this suite. @ 01/29/24 21:48:40.884
• [4.107 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 01/29/24 21:48:40.89
  Jan 29 21:48:40.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:48:40.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:40.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:40.91
  E0129 21:48:41.599812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:42.600046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:42.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:48:42.938: INFO: Deleting pod "var-expansion-8d2a6e1f-1ff3-407d-92de-d71e3b5ba3cd" in namespace "var-expansion-3121"
  Jan 29 21:48:42.947: INFO: Wait up to 5m0s for pod "var-expansion-8d2a6e1f-1ff3-407d-92de-d71e3b5ba3cd" to be fully deleted
  E0129 21:48:43.600991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:44.601564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3121" for this suite. @ 01/29/24 21:48:44.957
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 01/29/24 21:48:44.965
  Jan 29 21:48:44.965: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 21:48:44.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:44.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:44.985
  STEP: Creating a ResourceQuota @ 01/29/24 21:48:44.99
  STEP: Getting a ResourceQuota @ 01/29/24 21:48:44.994
  STEP: Updating a ResourceQuota @ 01/29/24 21:48:44.998
  STEP: Verifying a ResourceQuota was modified @ 01/29/24 21:48:45.003
  STEP: Deleting a ResourceQuota @ 01/29/24 21:48:45.007
  STEP: Verifying the deleted ResourceQuota @ 01/29/24 21:48:45.012
  Jan 29 21:48:45.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1788" for this suite. @ 01/29/24 21:48:45.02
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 01/29/24 21:48:45.028
  Jan 29 21:48:45.028: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context @ 01/29/24 21:48:45.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:45.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:45.047
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/29/24 21:48:45.052
  E0129 21:48:45.601789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:46.602432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:47.603405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:48.604023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:48:49.079
  Jan 29 21:48:49.084: INFO: Trying to get logs from node nodea08 pod security-context-dbb51b55-39b8-400c-a44f-6921310d758c container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:48:49.093
  Jan 29 21:48:49.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-2315" for this suite. @ 01/29/24 21:48:49.117
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 01/29/24 21:48:49.127
  Jan 29 21:48:49.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename statefulset @ 01/29/24 21:48:49.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:48:49.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:48:49.146
  STEP: Creating service test in namespace statefulset-3132 @ 01/29/24 21:48:49.151
  Jan 29 21:48:49.165: INFO: Found 0 stateful pods, waiting for 1
  E0129 21:48:49.604766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:50.605282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:51.605823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:52.606373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:53.606893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:54.607403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:55.607958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:56.608490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:57.609652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:48:58.610242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:48:59.172: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 01/29/24 21:48:59.181
  W0129 21:48:59.193033      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 29 21:48:59.201: INFO: Found 1 stateful pods, waiting for 2
  E0129 21:48:59.611117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:00.611734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:01.612264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:02.612459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:03.612971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:04.613550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:05.614108      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:06.614610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:07.614825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:08.615384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:09.209: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 29 21:49:09.209: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 01/29/24 21:49:09.218
  STEP: Delete all of the StatefulSets @ 01/29/24 21:49:09.222
  STEP: Verify that StatefulSets have been deleted @ 01/29/24 21:49:09.231
  Jan 29 21:49:09.235: INFO: Deleting all statefulset in ns statefulset-3132
  Jan 29 21:49:09.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3132" for this suite. @ 01/29/24 21:49:09.255
• [20.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 01/29/24 21:49:09.266
  Jan 29 21:49:09.266: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename replicaset @ 01/29/24 21:49:09.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:09.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:09.286
  Jan 29 21:49:09.289: INFO: Creating ReplicaSet my-hostname-basic-12d05cf2-eaae-4bdb-acb1-ed5b6d68db95
  Jan 29 21:49:09.299: INFO: Pod name my-hostname-basic-12d05cf2-eaae-4bdb-acb1-ed5b6d68db95: Found 0 pods out of 1
  E0129 21:49:09.616537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:10.616643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:11.617289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:12.617609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:13.618052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:14.303: INFO: Pod name my-hostname-basic-12d05cf2-eaae-4bdb-acb1-ed5b6d68db95: Found 1 pods out of 1
  Jan 29 21:49:14.303: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-12d05cf2-eaae-4bdb-acb1-ed5b6d68db95" is running
  Jan 29 21:49:14.306: INFO: Pod "my-hostname-basic-12d05cf2-eaae-4bdb-acb1-ed5b6d68db95-2hsbv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 21:49:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 21:49:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 21:49:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-29 21:49:09 +0000 UTC Reason: Message:}])
  Jan 29 21:49:14.307: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/29/24 21:49:14.307
  Jan 29 21:49:14.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6188" for this suite. @ 01/29/24 21:49:14.324
• [5.064 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 01/29/24 21:49:14.331
  Jan 29 21:49:14.331: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:49:14.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:14.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:14.348
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/29/24 21:49:14.354
  E0129 21:49:14.618999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:15.619908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:16.620161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:17.620923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:49:18.377
  Jan 29 21:49:18.382: INFO: Trying to get logs from node nodea08 pod pod-7adfb8ff-f1fe-4a54-9f1e-6302aac142d5 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:49:18.39
  Jan 29 21:49:18.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6662" for this suite. @ 01/29/24 21:49:18.411
• [4.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 01/29/24 21:49:18.42
  Jan 29 21:49:18.420: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 21:49:18.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:18.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:18.443
  Jan 29 21:49:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:49:18.620935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:19.621287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/29/24 21:49:19.846
  Jan 29 21:49:19.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-1768 --namespace=crd-publish-openapi-1768 create -f -'
  Jan 29 21:49:20.577: INFO: stderr: ""
  Jan 29 21:49:20.577: INFO: stdout: "e2e-test-crd-publish-openapi-3064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 29 21:49:20.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-1768 --namespace=crd-publish-openapi-1768 delete e2e-test-crd-publish-openapi-3064-crds test-cr'
  E0129 21:49:20.622083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:20.677: INFO: stderr: ""
  Jan 29 21:49:20.677: INFO: stdout: "e2e-test-crd-publish-openapi-3064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jan 29 21:49:20.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-1768 --namespace=crd-publish-openapi-1768 apply -f -'
  Jan 29 21:49:20.957: INFO: stderr: ""
  Jan 29 21:49:20.957: INFO: stdout: "e2e-test-crd-publish-openapi-3064-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 29 21:49:20.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-1768 --namespace=crd-publish-openapi-1768 delete e2e-test-crd-publish-openapi-3064-crds test-cr'
  Jan 29 21:49:21.061: INFO: stderr: ""
  Jan 29 21:49:21.061: INFO: stdout: "e2e-test-crd-publish-openapi-3064-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/29/24 21:49:21.061
  Jan 29 21:49:21.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=crd-publish-openapi-1768 explain e2e-test-crd-publish-openapi-3064-crds'
  Jan 29 21:49:21.334: INFO: stderr: ""
  Jan 29 21:49:21.334: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3064-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0129 21:49:21.622399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:22.623499      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:22.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1768" for this suite. @ 01/29/24 21:49:22.801
• [4.390 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 01/29/24 21:49:22.811
  Jan 29 21:49:22.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:49:22.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:22.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:22.831
  STEP: Setting up server cert @ 01/29/24 21:49:22.854
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:49:23.288
  STEP: Deploying the webhook pod @ 01/29/24 21:49:23.297
  STEP: Wait for the deployment to be ready @ 01/29/24 21:49:23.309
  Jan 29 21:49:23.318: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:49:23.623568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:24.624322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:49:25.332
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:49:25.347
  E0129 21:49:25.624708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:26.347: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 29 21:49:26.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:49:26.624935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4489-crds.webhook.example.com via the AdmissionRegistration API @ 01/29/24 21:49:26.873
  STEP: Creating a custom resource while v1 is storage version @ 01/29/24 21:49:26.902
  E0129 21:49:27.625050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:28.625665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 01/29/24 21:49:28.958
  STEP: Patching the custom resource while v2 is storage version @ 01/29/24 21:49:28.992
  Jan 29 21:49:29.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6871" for this suite. @ 01/29/24 21:49:29.609
  STEP: Destroying namespace "webhook-markers-9488" for this suite. @ 01/29/24 21:49:29.617
• [6.813 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS  E0129 21:49:29.625883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 01/29/24 21:49:29.626
  Jan 29 21:49:29.626: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 21:49:29.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:29.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:29.645
  STEP: set up a multi version CRD @ 01/29/24 21:49:29.65
  Jan 29 21:49:29.651: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:49:30.625896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:31.626426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:32.626405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:33.627143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 01/29/24 21:49:34.054
  STEP: check the unserved version gets removed @ 01/29/24 21:49:34.08
  E0129 21:49:34.627936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 01/29/24 21:49:35.531
  E0129 21:49:35.628590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:36.629531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:37.630690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1953" for this suite. @ 01/29/24 21:49:38.587
• [8.967 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 01/29/24 21:49:38.595
  Jan 29 21:49:38.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename controllerrevisions @ 01/29/24 21:49:38.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:38.613
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:38.618
  E0129 21:49:38.631658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating DaemonSet "e2e-fg2hr-daemon-set" @ 01/29/24 21:49:38.639
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 21:49:38.646
  Jan 29 21:49:38.650: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:49:38.654: INFO: Number of nodes with available pods controlled by daemonset e2e-fg2hr-daemon-set: 0
  Jan 29 21:49:38.654: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:49:39.632231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:39.663: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:49:39.668: INFO: Number of nodes with available pods controlled by daemonset e2e-fg2hr-daemon-set: 0
  Jan 29 21:49:39.668: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:49:40.632526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:40.661: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:49:40.666: INFO: Number of nodes with available pods controlled by daemonset e2e-fg2hr-daemon-set: 2
  Jan 29 21:49:40.666: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-fg2hr-daemon-set
  STEP: Confirm DaemonSet "e2e-fg2hr-daemon-set" successfully created with "daemonset-name=e2e-fg2hr-daemon-set" label @ 01/29/24 21:49:40.67
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-fg2hr-daemon-set" @ 01/29/24 21:49:40.678
  Jan 29 21:49:40.682: INFO: Located ControllerRevision: "e2e-fg2hr-daemon-set-84877bc4bd"
  STEP: Patching ControllerRevision "e2e-fg2hr-daemon-set-84877bc4bd" @ 01/29/24 21:49:40.685
  Jan 29 21:49:40.695: INFO: e2e-fg2hr-daemon-set-84877bc4bd has been patched
  STEP: Create a new ControllerRevision @ 01/29/24 21:49:40.695
  Jan 29 21:49:40.701: INFO: Created ControllerRevision: e2e-fg2hr-daemon-set-557968766b
  STEP: Confirm that there are two ControllerRevisions @ 01/29/24 21:49:40.701
  Jan 29 21:49:40.701: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 29 21:49:40.704: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-fg2hr-daemon-set-84877bc4bd" @ 01/29/24 21:49:40.704
  STEP: Confirm that there is only one ControllerRevision @ 01/29/24 21:49:40.709
  Jan 29 21:49:40.709: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 29 21:49:40.713: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-fg2hr-daemon-set-557968766b" @ 01/29/24 21:49:40.716
  Jan 29 21:49:40.726: INFO: e2e-fg2hr-daemon-set-557968766b has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 01/29/24 21:49:40.726
  W0129 21:49:40.733310      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 01/29/24 21:49:40.733
  Jan 29 21:49:40.733: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0129 21:49:41.632684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:41.739: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 29 21:49:41.744: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-fg2hr-daemon-set-557968766b=updated" @ 01/29/24 21:49:41.744
  STEP: Confirm that there is only one ControllerRevision @ 01/29/24 21:49:41.752
  Jan 29 21:49:41.752: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 29 21:49:41.756: INFO: Found 1 ControllerRevisions
  Jan 29 21:49:41.759: INFO: ControllerRevision "e2e-fg2hr-daemon-set-7f5bd48599" has revision 3
  STEP: Deleting DaemonSet "e2e-fg2hr-daemon-set" @ 01/29/24 21:49:41.763
  STEP: deleting DaemonSet.extensions e2e-fg2hr-daemon-set in namespace controllerrevisions-6043, will wait for the garbage collector to delete the pods @ 01/29/24 21:49:41.763
  Jan 29 21:49:41.823: INFO: Deleting DaemonSet.extensions e2e-fg2hr-daemon-set took: 6.182121ms
  Jan 29 21:49:41.924: INFO: Terminating DaemonSet.extensions e2e-fg2hr-daemon-set pods took: 100.737451ms
  E0129 21:49:42.632765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:43.429: INFO: Number of nodes with available pods controlled by daemonset e2e-fg2hr-daemon-set: 0
  Jan 29 21:49:43.429: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-fg2hr-daemon-set
  Jan 29 21:49:43.433: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30265"},"items":null}

  Jan 29 21:49:43.436: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30265"},"items":null}

  Jan 29 21:49:43.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-6043" for this suite. @ 01/29/24 21:49:43.453
• [4.865 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 01/29/24 21:49:43.46
  Jan 29 21:49:43.460: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:49:43.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:43.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:43.478
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/29/24 21:49:43.483
  E0129 21:49:43.633463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:44.634541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:45.635589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:46.636143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:49:47.506
  Jan 29 21:49:47.511: INFO: Trying to get logs from node nodea08 pod pod-383b569a-1a0e-4418-9e34-14533ce40c7f container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:49:47.52
  Jan 29 21:49:47.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-822" for this suite. @ 01/29/24 21:49:47.542
• [4.088 seconds]
------------------------------
SSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 01/29/24 21:49:47.549
  Jan 29 21:49:47.549: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:49:47.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:47.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:47.568
  STEP: Creating configMap that has name configmap-test-emptyKey-e99a088f-1cbc-4717-97fe-a12efb1e3bab @ 01/29/24 21:49:47.573
  Jan 29 21:49:47.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6547" for this suite. @ 01/29/24 21:49:47.581
• [0.039 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 01/29/24 21:49:47.588
  Jan 29 21:49:47.588: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/29/24 21:49:47.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:47.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:47.607
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 01/29/24 21:49:47.612
  Jan 29 21:49:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:49:47.636422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:48.637018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:49.030: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:49:49.637780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:50.638910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:51.639066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:52.639466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:53.640232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:54.641548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:54.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9860" for this suite. @ 01/29/24 21:49:54.898
• [7.317 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 01/29/24 21:49:54.905
  Jan 29 21:49:54.905: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:49:54.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:49:54.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:49:54.926
  STEP: Setting up server cert @ 01/29/24 21:49:54.951
  E0129 21:49:55.642078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:49:55.735
  STEP: Deploying the webhook pod @ 01/29/24 21:49:55.748
  STEP: Wait for the deployment to be ready @ 01/29/24 21:49:55.762
  Jan 29 21:49:55.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:49:56.642593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:57.642663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:57.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:49:58.643555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:49:59.644116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:49:59.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:50:00.644381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:01.644895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:01.797: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:50:02.645494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:03.646084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:03.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:50:04.646995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:05.647624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:05.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 49, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:50:06.648303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:07.649365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:50:07.795
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:50:07.813
  E0129 21:50:08.649475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:08.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 01/29/24 21:50:08.819
  STEP: create a namespace for the webhook @ 01/29/24 21:50:08.845
  STEP: create a configmap should be unconditionally rejected by the webhook @ 01/29/24 21:50:08.863
  Jan 29 21:50:08.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9229" for this suite. @ 01/29/24 21:50:08.917
  STEP: Destroying namespace "webhook-markers-7102" for this suite. @ 01/29/24 21:50:08.924
  STEP: Destroying namespace "fail-closed-namespace-7745" for this suite. @ 01/29/24 21:50:08.931
• [14.030 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 01/29/24 21:50:08.936
  Jan 29 21:50:08.936: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename endpointslice @ 01/29/24 21:50:08.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:50:08.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:50:08.953
  STEP: getting /apis @ 01/29/24 21:50:08.958
  STEP: getting /apis/discovery.k8s.io @ 01/29/24 21:50:08.964
  STEP: getting /apis/discovery.k8s.iov1 @ 01/29/24 21:50:08.967
  STEP: creating @ 01/29/24 21:50:08.968
  STEP: getting @ 01/29/24 21:50:08.982
  STEP: listing @ 01/29/24 21:50:08.985
  STEP: watching @ 01/29/24 21:50:08.989
  Jan 29 21:50:08.989: INFO: starting watch
  STEP: cluster-wide listing @ 01/29/24 21:50:08.991
  STEP: cluster-wide watching @ 01/29/24 21:50:08.994
  Jan 29 21:50:08.994: INFO: starting watch
  STEP: patching @ 01/29/24 21:50:08.996
  STEP: updating @ 01/29/24 21:50:09.004
  Jan 29 21:50:09.013: INFO: waiting for watch events with expected annotations
  Jan 29 21:50:09.013: INFO: saw patched and updated annotations
  STEP: deleting @ 01/29/24 21:50:09.013
  STEP: deleting a collection @ 01/29/24 21:50:09.025
  Jan 29 21:50:09.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7334" for this suite. @ 01/29/24 21:50:09.04
• [0.109 seconds]
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 01/29/24 21:50:09.045
  Jan 29 21:50:09.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:50:09.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:50:09.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:50:09.063
  STEP: Creating a job @ 01/29/24 21:50:09.068
  STEP: Ensure pods equal to parallelism count is attached to the job @ 01/29/24 21:50:09.073
  E0129 21:50:09.650495      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:10.651563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 01/29/24 21:50:11.08
  STEP: updating /status @ 01/29/24 21:50:11.092
  STEP: get /status @ 01/29/24 21:50:11.125
  Jan 29 21:50:11.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8638" for this suite. @ 01/29/24 21:50:11.134
• [2.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 01/29/24 21:50:11.141
  Jan 29 21:50:11.141: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename endpointslice @ 01/29/24 21:50:11.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:50:11.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:50:11.161
  E0129 21:50:11.651883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:12.651931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:13.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9691" for this suite. @ 01/29/24 21:50:13.222
• [2.087 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 01/29/24 21:50:13.228
  Jan 29 21:50:13.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:50:13.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:50:13.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:50:13.25
  STEP: Creating configMap with name configmap-test-volume-map-ae1afb70-6e95-4539-9968-cbb45bc45eb4 @ 01/29/24 21:50:13.254
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:50:13.259
  E0129 21:50:13.652885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:14.652945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:15.653824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:16.654446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:50:17.284
  Jan 29 21:50:17.288: INFO: Trying to get logs from node nodeb29 pod pod-configmaps-d9513162-f89b-4327-9904-ba46bccfc022 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:50:17.311
  Jan 29 21:50:17.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2356" for this suite. @ 01/29/24 21:50:17.337
• [4.117 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 01/29/24 21:50:17.346
  Jan 29 21:50:17.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename container-probe @ 01/29/24 21:50:17.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:50:17.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:50:17.369
  STEP: Creating pod liveness-35901eb6-6623-48c2-afcf-f3f4a0250d8c in namespace container-probe-5552 @ 01/29/24 21:50:17.374
  E0129 21:50:17.654657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:18.654738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:50:19.396: INFO: Started pod liveness-35901eb6-6623-48c2-afcf-f3f4a0250d8c in namespace container-probe-5552
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/29/24 21:50:19.396
  Jan 29 21:50:19.400: INFO: Initial restart count of pod liveness-35901eb6-6623-48c2-afcf-f3f4a0250d8c is 0
  E0129 21:50:19.655230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:20.655786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:21.656525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:22.656962      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:23.657882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:24.658459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:25.658816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:26.659330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:27.660097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:28.660621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:29.661593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:30.662182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:31.662821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:32.663133      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:33.663180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:34.663863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:35.664069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:36.664650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:37.665608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:38.666165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:39.666790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:40.667452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:41.668540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:42.668972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:43.670164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:44.670757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:45.671775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:46.672366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:47.672403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:48.672972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:49.673846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:50.674455      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:51.674959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:52.676071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:53.676083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:54.676633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:55.677152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:56.677355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:57.678082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:58.678704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:50:59.679846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:00.680513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:01.681157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:02.681749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:03.682500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:04.683076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:05.683659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:06.684233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:07.685090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:08.685634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:09.686252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:10.686865      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:11.687902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:12.688182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:13.688745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:14.689651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:15.690476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:16.691295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:17.692049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:18.692677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:19.693371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:20.693955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:21.695033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:22.695308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:23.696380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:24.697010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:25.698054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:26.698683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:27.698846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:28.699358      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:29.700463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:30.701104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:31.701081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:32.701450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:33.701928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:34.702563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:35.703717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:36.704284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:37.705148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:38.705747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:39.706381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:40.707059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:41.707185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:42.708228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:43.708643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:44.709192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:45.709998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:46.710586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:47.711553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:48.712266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:49.712896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:50.713404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:51.714010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:52.714315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:53.715179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:54.715790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:55.716330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:56.716834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:57.717604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:58.718088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:51:59.718157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:00.718624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:01.719284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:02.719582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:03.719577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:04.720039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:05.720934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:06.721432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:07.721686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:08.722391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:09.722938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:10.723910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:11.724403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:12.725184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:13.725692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:14.726249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:15.726811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:16.727038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:17.727891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:18.728954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:19.729510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:20.729602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:21.730153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:22.730986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:23.731591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:24.732025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:25.732497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:26.733105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:27.733991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:28.734610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:29.735106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:30.735554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:31.736115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:32.736690      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:33.737182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:34.738143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:35.738694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:36.738929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:37.739733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:38.740331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:39.740898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:40.741240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:41.742093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:42.743204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:43.743751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:44.744604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:45.745102      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:46.745360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:47.745541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:48.746391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:49.746901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:50.746932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:51.747487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:52.748040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:53.748592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:54.748823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:55.749029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:56.749341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:57.750294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:58.750467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:52:59.750848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:00.751536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:01.752034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:02.752159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:03.752686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:04.753604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:05.754125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:06.754490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:07.754722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:08.755011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:09.755498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:10.756484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:11.756941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:12.757677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:13.758215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:14.758551      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:15.758961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:16.759832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:17.759992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:18.760268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:19.760463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:20.761094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:21.761617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:22.762622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:23.763156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:24.763351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:25.763663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:26.763898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:27.765060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:28.765287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:29.765443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:30.765742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:31.766036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:32.766906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:33.767406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:34.767876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:35.768139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:36.768842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:37.769061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:38.769667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:39.770209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:40.770958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:41.771544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:42.771836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:43.772566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:44.773049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:45.773071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:46.773469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:47.774676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:48.775508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:49.776049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:50.776393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:51.776893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:52.777233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:53.777801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:54.778751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:55.780355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:56.781095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:57.782915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:58.782766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:53:59.783287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:00.784065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:01.784550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:02.785641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:03.786180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:04.787223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:05.787743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:06.788288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:07.789114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:08.790268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:09.790920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:10.791178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:11.791730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:12.792144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:13.792808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:14.792995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:15.793421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:16.794142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:17.795052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:18.795286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:19.795816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:20.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:54:20.15
  STEP: Destroying namespace "container-probe-5552" for this suite. @ 01/29/24 21:54:20.164
• [242.827 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 01/29/24 21:54:20.178
  Jan 29 21:54:20.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename deployment @ 01/29/24 21:54:20.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:54:20.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:54:20.199
  STEP: creating a Deployment @ 01/29/24 21:54:20.207
  Jan 29 21:54:20.208: INFO: Creating simple deployment test-deployment-9j57m
  Jan 29 21:54:20.222: INFO: deployment "test-deployment-9j57m" doesn't have the required revision set
  E0129 21:54:20.796768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:21.797046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 01/29/24 21:54:22.242
  Jan 29 21:54:22.249: INFO: Deployment test-deployment-9j57m has Conditions: [{Available True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9j57m-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 01/29/24 21:54:22.249
  Jan 29 21:54:22.263: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 21, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 21, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 20, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-9j57m-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 01/29/24 21:54:22.263
  Jan 29 21:54:22.267: INFO: Observed &Deployment event: ADDED
  Jan 29 21:54:22.267: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9j57m-5994cf9475"}
  Jan 29 21:54:22.268: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.268: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9j57m-5994cf9475"}
  Jan 29 21:54:22.268: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 29 21:54:22.268: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.268: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 29 21:54:22.268: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9j57m-5994cf9475" is progressing.}
  Jan 29 21:54:22.269: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.269: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 29 21:54:22.269: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9j57m-5994cf9475" has successfully progressed.}
  Jan 29 21:54:22.269: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.269: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 29 21:54:22.269: INFO: Observed Deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9j57m-5994cf9475" has successfully progressed.}
  Jan 29 21:54:22.269: INFO: Found Deployment test-deployment-9j57m in namespace deployment-2690 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:54:22.269: INFO: Deployment test-deployment-9j57m has an updated status
  STEP: patching the Statefulset Status @ 01/29/24 21:54:22.269
  Jan 29 21:54:22.269: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 29 21:54:22.282: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 01/29/24 21:54:22.282
  Jan 29 21:54:22.285: INFO: Observed &Deployment event: ADDED
  Jan 29 21:54:22.285: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9j57m-5994cf9475"}
  Jan 29 21:54:22.285: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.285: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-9j57m-5994cf9475"}
  Jan 29 21:54:22.286: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 29 21:54:22.286: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.286: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 29 21:54:22.286: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:20 +0000 UTC 2024-01-29 21:54:20 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-9j57m-5994cf9475" is progressing.}
  Jan 29 21:54:22.286: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.286: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 29 21:54:22.286: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9j57m-5994cf9475" has successfully progressed.}
  Jan 29 21:54:22.287: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.287: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:21 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 29 21:54:22.287: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-29 21:54:21 +0000 UTC 2024-01-29 21:54:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-9j57m-5994cf9475" has successfully progressed.}
  Jan 29 21:54:22.287: INFO: Observed deployment test-deployment-9j57m in namespace deployment-2690 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 29 21:54:22.287: INFO: Observed &Deployment event: MODIFIED
  Jan 29 21:54:22.287: INFO: Found deployment test-deployment-9j57m in namespace deployment-2690 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jan 29 21:54:22.287: INFO: Deployment test-deployment-9j57m has a patched status
  Jan 29 21:54:22.291: INFO: Deployment "test-deployment-9j57m":
  &Deployment{ObjectMeta:{test-deployment-9j57m  deployment-2690  62534580-66f7-41f7-909f-983496c58dbf 30997 1 2024-01-29 21:54:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-29 21:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-29 21:54:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-29 21:54:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004354c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-9j57m-5994cf9475",LastUpdateTime:2024-01-29 21:54:22 +0000 UTC,LastTransitionTime:2024-01-29 21:54:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 29 21:54:22.295: INFO: New ReplicaSet "test-deployment-9j57m-5994cf9475" of Deployment "test-deployment-9j57m":
  &ReplicaSet{ObjectMeta:{test-deployment-9j57m-5994cf9475  deployment-2690  276440bf-cdc4-4c03-abf7-118cb9b0a84e 30991 1 2024-01-29 21:54:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-9j57m 62534580-66f7-41f7-909f-983496c58dbf 0xc004355160 0xc004355161}] [] [{kube-controller-manager Update apps/v1 2024-01-29 21:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"62534580-66f7-41f7-909f-983496c58dbf\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-29 21:54:21 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004355208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 29 21:54:22.299: INFO: Pod "test-deployment-9j57m-5994cf9475-mp6nj" is available:
  &Pod{ObjectMeta:{test-deployment-9j57m-5994cf9475-mp6nj test-deployment-9j57m-5994cf9475- deployment-2690  e235b122-b3bd-4831-8503-2f706c0a6699 30990 0 2024-01-29 21:54:20 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-9j57m-5994cf9475 276440bf-cdc4-4c03-abf7-118cb9b0a84e 0xc004355630 0xc004355631}] [] [{kube-controller-manager Update v1 2024-01-29 21:54:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"276440bf-cdc4-4c03-abf7-118cb9b0a84e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-29 21:54:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7xh6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7xh6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:nodea08,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:54:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:54:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-29 21:54:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.100.28,PodIP:10.244.1.231,StartTime:2024-01-29 21:54:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-29 21:54:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1e8e0bd96323dcc50bb36613d2486c01a8cc58c619e16d419acc9960dc88054e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.231,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 29 21:54:22.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2690" for this suite. @ 01/29/24 21:54:22.304
• [2.132 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 01/29/24 21:54:22.31
  Jan 29 21:54:22.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:54:22.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:54:22.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:54:22.333
  STEP: Setting up server cert @ 01/29/24 21:54:22.357
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:54:22.754
  STEP: Deploying the webhook pod @ 01/29/24 21:54:22.765
  STEP: Wait for the deployment to be ready @ 01/29/24 21:54:22.779
  Jan 29 21:54:22.787: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:54:22.797710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:23.798188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:24.798705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:24.804: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:54:25.799872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:26.800513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:26.810: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:54:27.801043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:28.801628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:28.812: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:54:29.802625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:30.803197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:30.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:54:31.803602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:32.803813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:32.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 29, 21, 54, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0129 21:54:33.804351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:34.804991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:54:34.81
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:54:34.824
  E0129 21:54:35.805150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:35.825: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 01/29/24 21:54:35.83
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/29/24 21:54:35.858
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 01/29/24 21:54:35.873
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/29/24 21:54:35.886
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 01/29/24 21:54:35.898
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/29/24 21:54:35.907
  Jan 29 21:54:35.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4391" for this suite. @ 01/29/24 21:54:35.956
  STEP: Destroying namespace "webhook-markers-2070" for this suite. @ 01/29/24 21:54:35.96
• [13.656 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 01/29/24 21:54:35.974
  Jan 29 21:54:35.974: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename daemonsets @ 01/29/24 21:54:35.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:54:35.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:54:35.992
  Jan 29 21:54:36.015: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/29/24 21:54:36.021
  Jan 29 21:54:36.027: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:54:36.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:54:36.030: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:54:36.805606      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:37.038: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:54:37.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:54:37.043: INFO: Node nodea08 is running 0 daemon pod, expected 1
  E0129 21:54:37.806746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:38.039: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:54:38.043: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 21:54:38.043: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 01/29/24 21:54:38.06
  STEP: Check that daemon pods images are updated. @ 01/29/24 21:54:38.075
  Jan 29 21:54:38.079: INFO: Wrong image for pod: daemon-set-5xfdf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 29 21:54:38.079: INFO: Wrong image for pod: daemon-set-xscn6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 29 21:54:38.085: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0129 21:54:38.807568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:39.093: INFO: Wrong image for pod: daemon-set-5xfdf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 29 21:54:39.093: INFO: Pod daemon-set-5zthh is not available
  Jan 29 21:54:39.098: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0129 21:54:39.808154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:40.091: INFO: Wrong image for pod: daemon-set-5xfdf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 29 21:54:40.091: INFO: Pod daemon-set-5zthh is not available
  Jan 29 21:54:40.096: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0129 21:54:40.809256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:41.089: INFO: Pod daemon-set-bht7h is not available
  Jan 29 21:54:41.093: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 01/29/24 21:54:41.093
  Jan 29 21:54:41.097: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:54:41.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 29 21:54:41.101: INFO: Node nodeb29 is running 0 daemon pod, expected 1
  E0129 21:54:41.810073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:42.108: INFO: DaemonSet pods can't tolerate node noded36 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 29 21:54:42.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 29 21:54:42.113: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/29/24 21:54:42.133
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5393, will wait for the garbage collector to delete the pods @ 01/29/24 21:54:42.133
  Jan 29 21:54:42.195: INFO: Deleting DaemonSet.extensions daemon-set took: 6.706378ms
  Jan 29 21:54:42.295: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.324541ms
  E0129 21:54:42.810764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:43.810923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:54:44.000: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 29 21:54:44.000: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 29 21:54:44.004: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"31213"},"items":null}

  Jan 29 21:54:44.010: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"31213"},"items":null}

  Jan 29 21:54:44.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5393" for this suite. @ 01/29/24 21:54:44.028
• [8.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 01/29/24 21:54:44.038
  Jan 29 21:54:44.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:54:44.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:54:44.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:54:44.059
  STEP: Creating configMap with name cm-test-opt-del-efd93e0c-e457-491d-8154-c70b94fc811a @ 01/29/24 21:54:44.07
  STEP: Creating configMap with name cm-test-opt-upd-5a1073ba-c2e1-4a22-9775-f5d1454674af @ 01/29/24 21:54:44.077
  STEP: Creating the pod @ 01/29/24 21:54:44.082
  E0129 21:54:44.811025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:45.811558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-efd93e0c-e457-491d-8154-c70b94fc811a @ 01/29/24 21:54:46.154
  STEP: Updating configmap cm-test-opt-upd-5a1073ba-c2e1-4a22-9775-f5d1454674af @ 01/29/24 21:54:46.159
  STEP: Creating configMap with name cm-test-opt-create-4fc266f5-96ea-4976-ae05-18d7ec0b8a56 @ 01/29/24 21:54:46.164
  STEP: waiting to observe update in volume @ 01/29/24 21:54:46.171
  E0129 21:54:46.811711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:47.811851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:48.812226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:49.813051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:50.814078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:51.814625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:52.815601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:53.816219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:54.816697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:55.817363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:56.818442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:57.819653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:58.819798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:54:59.820343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:00.820840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:01.821517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:02.822546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:03.823154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:04.824266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:05.824846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:06.825041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:07.825888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:08.827201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:09.827757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:10.828353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:11.828949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:12.830107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:13.831032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:14.831571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:15.832579      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:16.833181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:17.833437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:18.834487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:19.835037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:20.835308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:21.835868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:22.836949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:23.837505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:24.838409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:25.839023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:26.839335      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:27.839388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:28.839821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:29.839833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:30.840111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:31.840576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:32.841403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:33.841949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:34.842842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:35.843565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:36.844442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:37.844591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:38.845613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:39.846256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:40.847297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:41.847855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:42.848156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:43.848737      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:44.849789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:45.850367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:46.851151      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:47.852142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:48.853073      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:49.853631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:50.854452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:51.855071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:52.855175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:53.855456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:54.856464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:55.856945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:56.857033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:57.857366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:58.857841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:55:59.858463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:00.859527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:01.860048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:02.860081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:03.860633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:04.860847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:05.861452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:06.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3654" for this suite. @ 01/29/24 21:56:06.718
• [82.685 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 01/29/24 21:56:06.724
  Jan 29 21:56:06.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svc-latency @ 01/29/24 21:56:06.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:06.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:06.744
  Jan 29 21:56:06.748: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-679 @ 01/29/24 21:56:06.749
  I0129 21:56:06.755288      23 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-679, replica count: 1
  E0129 21:56:06.862319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:56:07.807100      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0129 21:56:07.862571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:56:08.808350      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0129 21:56:08.862710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:08.933: INFO: Created: latency-svc-x2d5w
  Jan 29 21:56:08.939: INFO: Got endpoints: latency-svc-x2d5w [30.522458ms]
  Jan 29 21:56:08.956: INFO: Created: latency-svc-s5dcz
  Jan 29 21:56:08.961: INFO: Got endpoints: latency-svc-s5dcz [21.966143ms]
  Jan 29 21:56:08.971: INFO: Created: latency-svc-7kzcd
  Jan 29 21:56:08.978: INFO: Got endpoints: latency-svc-7kzcd [37.888275ms]
  Jan 29 21:56:08.985: INFO: Created: latency-svc-thlmb
  Jan 29 21:56:08.989: INFO: Got endpoints: latency-svc-thlmb [49.128232ms]
  Jan 29 21:56:08.998: INFO: Created: latency-svc-h8sm6
  Jan 29 21:56:09.001: INFO: Got endpoints: latency-svc-h8sm6 [61.42862ms]
  Jan 29 21:56:09.010: INFO: Created: latency-svc-46j9h
  Jan 29 21:56:09.013: INFO: Got endpoints: latency-svc-46j9h [73.397416ms]
  Jan 29 21:56:09.021: INFO: Created: latency-svc-rzdks
  Jan 29 21:56:09.025: INFO: Got endpoints: latency-svc-rzdks [85.417691ms]
  Jan 29 21:56:09.035: INFO: Created: latency-svc-tjp4s
  Jan 29 21:56:09.038: INFO: Got endpoints: latency-svc-tjp4s [98.308025ms]
  Jan 29 21:56:09.044: INFO: Created: latency-svc-xmdsp
  Jan 29 21:56:09.049: INFO: Got endpoints: latency-svc-xmdsp [109.370136ms]
  Jan 29 21:56:09.061: INFO: Created: latency-svc-r57lc
  Jan 29 21:56:09.069: INFO: Got endpoints: latency-svc-r57lc [128.640363ms]
  Jan 29 21:56:09.074: INFO: Created: latency-svc-b9q5m
  Jan 29 21:56:09.076: INFO: Got endpoints: latency-svc-b9q5m [136.256617ms]
  Jan 29 21:56:09.086: INFO: Created: latency-svc-q8bcc
  Jan 29 21:56:09.092: INFO: Got endpoints: latency-svc-q8bcc [151.640521ms]
  Jan 29 21:56:09.101: INFO: Created: latency-svc-rznq2
  Jan 29 21:56:09.104: INFO: Got endpoints: latency-svc-rznq2 [163.982793ms]
  Jan 29 21:56:09.110: INFO: Created: latency-svc-sz42h
  Jan 29 21:56:09.114: INFO: Got endpoints: latency-svc-sz42h [173.853491ms]
  Jan 29 21:56:09.122: INFO: Created: latency-svc-vt5nx
  Jan 29 21:56:09.131: INFO: Got endpoints: latency-svc-vt5nx [191.617397ms]
  Jan 29 21:56:09.132: INFO: Created: latency-svc-q5ftm
  Jan 29 21:56:09.137: INFO: Got endpoints: latency-svc-q5ftm [197.648866ms]
  Jan 29 21:56:09.143: INFO: Created: latency-svc-vn2zq
  Jan 29 21:56:09.151: INFO: Got endpoints: latency-svc-vn2zq [189.131544ms]
  Jan 29 21:56:09.152: INFO: Created: latency-svc-lntdz
  Jan 29 21:56:09.159: INFO: Got endpoints: latency-svc-lntdz [181.287713ms]
  Jan 29 21:56:09.189: INFO: Created: latency-svc-rt5gh
  Jan 29 21:56:09.194: INFO: Got endpoints: latency-svc-rt5gh [205.152118ms]
  Jan 29 21:56:09.197: INFO: Created: latency-svc-jzxz5
  Jan 29 21:56:09.201: INFO: Got endpoints: latency-svc-jzxz5 [199.585137ms]
  Jan 29 21:56:09.203: INFO: Created: latency-svc-s8fq8
  Jan 29 21:56:09.210: INFO: Got endpoints: latency-svc-s8fq8 [196.385966ms]
  Jan 29 21:56:09.213: INFO: Created: latency-svc-fz5m8
  Jan 29 21:56:09.218: INFO: Got endpoints: latency-svc-fz5m8 [192.531783ms]
  Jan 29 21:56:09.221: INFO: Created: latency-svc-5h7ct
  Jan 29 21:56:09.225: INFO: Got endpoints: latency-svc-5h7ct [186.806041ms]
  Jan 29 21:56:09.233: INFO: Created: latency-svc-scmc4
  Jan 29 21:56:09.241: INFO: Got endpoints: latency-svc-scmc4 [191.329459ms]
  Jan 29 21:56:09.243: INFO: Created: latency-svc-mv2l9
  Jan 29 21:56:09.249: INFO: Got endpoints: latency-svc-mv2l9 [180.467521ms]
  Jan 29 21:56:09.254: INFO: Created: latency-svc-wmdv8
  Jan 29 21:56:09.258: INFO: Got endpoints: latency-svc-wmdv8 [181.65551ms]
  Jan 29 21:56:09.265: INFO: Created: latency-svc-bwfxn
  Jan 29 21:56:09.272: INFO: Got endpoints: latency-svc-bwfxn [180.350987ms]
  Jan 29 21:56:09.273: INFO: Created: latency-svc-xbn42
  Jan 29 21:56:09.284: INFO: Got endpoints: latency-svc-xbn42 [179.402286ms]
  Jan 29 21:56:09.286: INFO: Created: latency-svc-7r6xr
  Jan 29 21:56:09.292: INFO: Got endpoints: latency-svc-7r6xr [177.880845ms]
  Jan 29 21:56:09.293: INFO: Created: latency-svc-m4mj5
  Jan 29 21:56:09.301: INFO: Got endpoints: latency-svc-m4mj5 [169.712359ms]
  Jan 29 21:56:09.302: INFO: Created: latency-svc-dx4nr
  Jan 29 21:56:09.308: INFO: Got endpoints: latency-svc-dx4nr [171.283685ms]
  Jan 29 21:56:09.311: INFO: Created: latency-svc-r5qsw
  Jan 29 21:56:09.318: INFO: Got endpoints: latency-svc-r5qsw [167.602133ms]
  Jan 29 21:56:09.321: INFO: Created: latency-svc-xzzbk
  Jan 29 21:56:09.326: INFO: Got endpoints: latency-svc-xzzbk [166.732563ms]
  Jan 29 21:56:09.330: INFO: Created: latency-svc-7frkk
  Jan 29 21:56:09.336: INFO: Got endpoints: latency-svc-7frkk [142.318049ms]
  Jan 29 21:56:09.340: INFO: Created: latency-svc-l5wfn
  Jan 29 21:56:09.348: INFO: Got endpoints: latency-svc-l5wfn [147.197688ms]
  Jan 29 21:56:09.349: INFO: Created: latency-svc-prdpd
  Jan 29 21:56:09.360: INFO: Got endpoints: latency-svc-prdpd [150.08788ms]
  Jan 29 21:56:09.364: INFO: Created: latency-svc-skq6g
  Jan 29 21:56:09.370: INFO: Got endpoints: latency-svc-skq6g [152.524455ms]
  Jan 29 21:56:09.375: INFO: Created: latency-svc-lzzth
  Jan 29 21:56:09.378: INFO: Got endpoints: latency-svc-lzzth [153.10058ms]
  Jan 29 21:56:09.400: INFO: Created: latency-svc-pmt9q
  Jan 29 21:56:09.405: INFO: Got endpoints: latency-svc-pmt9q [164.67891ms]
  Jan 29 21:56:09.421: INFO: Created: latency-svc-8c8m2
  Jan 29 21:56:09.430: INFO: Created: latency-svc-zvq2p
  Jan 29 21:56:09.440: INFO: Got endpoints: latency-svc-8c8m2 [190.529755ms]
  Jan 29 21:56:09.441: INFO: Created: latency-svc-xfglk
  Jan 29 21:56:09.454: INFO: Created: latency-svc-8jp2v
  Jan 29 21:56:09.467: INFO: Created: latency-svc-snpgt
  Jan 29 21:56:09.476: INFO: Created: latency-svc-n7kvq
  Jan 29 21:56:09.485: INFO: Created: latency-svc-bkr82
  Jan 29 21:56:09.488: INFO: Got endpoints: latency-svc-zvq2p [230.377505ms]
  Jan 29 21:56:09.511: INFO: Created: latency-svc-6ljm7
  Jan 29 21:56:09.522: INFO: Created: latency-svc-gczjl
  Jan 29 21:56:09.534: INFO: Created: latency-svc-h9hp6
  Jan 29 21:56:09.539: INFO: Got endpoints: latency-svc-xfglk [266.58501ms]
  Jan 29 21:56:09.546: INFO: Created: latency-svc-2kz58
  Jan 29 21:56:09.559: INFO: Created: latency-svc-t7vff
  Jan 29 21:56:09.572: INFO: Created: latency-svc-84w9z
  Jan 29 21:56:09.592: INFO: Got endpoints: latency-svc-8jp2v [308.550262ms]
  Jan 29 21:56:09.626: INFO: Created: latency-svc-jrg64
  Jan 29 21:56:09.834: INFO: Got endpoints: latency-svc-6ljm7 [515.525146ms]
  Jan 29 21:56:09.835: INFO: Got endpoints: latency-svc-snpgt [542.831312ms]
  Jan 29 21:56:09.835: INFO: Got endpoints: latency-svc-bkr82 [526.819172ms]
  Jan 29 21:56:09.835: INFO: Got endpoints: latency-svc-n7kvq [534.108244ms]
  Jan 29 21:56:09.837: INFO: Created: latency-svc-5bvgk
  Jan 29 21:56:09.840: INFO: Got endpoints: latency-svc-gczjl [514.042845ms]
  Jan 29 21:56:09.855: INFO: Created: latency-svc-67g84
  E0129 21:56:09.863889      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:09.866: INFO: Created: latency-svc-ppk96
  Jan 29 21:56:09.877: INFO: Created: latency-svc-gklhm
  Jan 29 21:56:09.887: INFO: Created: latency-svc-hnqtp
  Jan 29 21:56:09.890: INFO: Got endpoints: latency-svc-h9hp6 [553.226373ms]
  Jan 29 21:56:09.899: INFO: Created: latency-svc-mcbdd
  Jan 29 21:56:09.910: INFO: Created: latency-svc-7lm8z
  Jan 29 21:56:09.922: INFO: Created: latency-svc-s68sw
  Jan 29 21:56:09.944: INFO: Got endpoints: latency-svc-2kz58 [596.133951ms]
  Jan 29 21:56:09.945: INFO: Created: latency-svc-fxqtd
  Jan 29 21:56:09.959: INFO: Created: latency-svc-b2bd6
  Jan 29 21:56:09.972: INFO: Created: latency-svc-znjkt
  Jan 29 21:56:09.987: INFO: Created: latency-svc-cvhb5
  Jan 29 21:56:09.993: INFO: Got endpoints: latency-svc-t7vff [632.679634ms]
  Jan 29 21:56:10.010: INFO: Created: latency-svc-hf57l
  Jan 29 21:56:10.041: INFO: Got endpoints: latency-svc-84w9z [670.205495ms]
  Jan 29 21:56:10.067: INFO: Created: latency-svc-jf8mr
  Jan 29 21:56:10.089: INFO: Got endpoints: latency-svc-jrg64 [710.754979ms]
  Jan 29 21:56:10.110: INFO: Created: latency-svc-7qpcp
  Jan 29 21:56:10.140: INFO: Got endpoints: latency-svc-5bvgk [734.783701ms]
  Jan 29 21:56:10.165: INFO: Created: latency-svc-kplx7
  Jan 29 21:56:10.190: INFO: Got endpoints: latency-svc-67g84 [750.310328ms]
  Jan 29 21:56:10.213: INFO: Created: latency-svc-zfr6q
  Jan 29 21:56:10.240: INFO: Got endpoints: latency-svc-ppk96 [751.262754ms]
  Jan 29 21:56:10.271: INFO: Created: latency-svc-8v7xq
  Jan 29 21:56:10.290: INFO: Got endpoints: latency-svc-gklhm [750.932887ms]
  Jan 29 21:56:10.314: INFO: Created: latency-svc-vwnwd
  Jan 29 21:56:10.339: INFO: Got endpoints: latency-svc-hnqtp [746.751197ms]
  Jan 29 21:56:10.357: INFO: Created: latency-svc-484hw
  Jan 29 21:56:10.389: INFO: Got endpoints: latency-svc-mcbdd [555.048312ms]
  Jan 29 21:56:10.408: INFO: Created: latency-svc-q222r
  Jan 29 21:56:10.439: INFO: Got endpoints: latency-svc-7lm8z [603.259738ms]
  Jan 29 21:56:10.459: INFO: Created: latency-svc-mhjgw
  Jan 29 21:56:10.490: INFO: Got endpoints: latency-svc-s68sw [655.469946ms]
  Jan 29 21:56:10.509: INFO: Created: latency-svc-n8cqr
  Jan 29 21:56:10.540: INFO: Got endpoints: latency-svc-fxqtd [703.88304ms]
  Jan 29 21:56:10.560: INFO: Created: latency-svc-tcr9n
  Jan 29 21:56:10.593: INFO: Got endpoints: latency-svc-b2bd6 [752.775609ms]
  Jan 29 21:56:10.615: INFO: Created: latency-svc-8crbw
  Jan 29 21:56:10.639: INFO: Got endpoints: latency-svc-znjkt [749.441073ms]
  Jan 29 21:56:10.662: INFO: Created: latency-svc-vblm4
  Jan 29 21:56:10.693: INFO: Got endpoints: latency-svc-cvhb5 [748.761445ms]
  Jan 29 21:56:10.733: INFO: Created: latency-svc-rwjk7
  Jan 29 21:56:10.738: INFO: Got endpoints: latency-svc-hf57l [745.108535ms]
  Jan 29 21:56:10.754: INFO: Created: latency-svc-pmwns
  Jan 29 21:56:10.792: INFO: Got endpoints: latency-svc-jf8mr [751.102692ms]
  Jan 29 21:56:10.808: INFO: Created: latency-svc-l8bqm
  Jan 29 21:56:10.840: INFO: Got endpoints: latency-svc-7qpcp [750.702777ms]
  Jan 29 21:56:10.855: INFO: Created: latency-svc-55cqb
  E0129 21:56:10.864903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:10.890: INFO: Got endpoints: latency-svc-kplx7 [749.626974ms]
  Jan 29 21:56:10.908: INFO: Created: latency-svc-77rvz
  Jan 29 21:56:10.941: INFO: Got endpoints: latency-svc-zfr6q [750.980372ms]
  Jan 29 21:56:10.957: INFO: Created: latency-svc-7wmw2
  Jan 29 21:56:10.989: INFO: Got endpoints: latency-svc-8v7xq [749.470643ms]
  Jan 29 21:56:11.007: INFO: Created: latency-svc-dj2mt
  Jan 29 21:56:11.039: INFO: Got endpoints: latency-svc-vwnwd [748.920432ms]
  Jan 29 21:56:11.057: INFO: Created: latency-svc-j97zw
  Jan 29 21:56:11.091: INFO: Got endpoints: latency-svc-484hw [752.062758ms]
  Jan 29 21:56:11.107: INFO: Created: latency-svc-kd4hv
  Jan 29 21:56:11.150: INFO: Got endpoints: latency-svc-q222r [760.83831ms]
  Jan 29 21:56:11.165: INFO: Created: latency-svc-zpjj2
  Jan 29 21:56:11.188: INFO: Got endpoints: latency-svc-mhjgw [749.753169ms]
  Jan 29 21:56:11.204: INFO: Created: latency-svc-jr5v2
  Jan 29 21:56:11.242: INFO: Got endpoints: latency-svc-n8cqr [751.648456ms]
  Jan 29 21:56:11.260: INFO: Created: latency-svc-9zjwd
  Jan 29 21:56:11.290: INFO: Got endpoints: latency-svc-tcr9n [750.703673ms]
  Jan 29 21:56:11.309: INFO: Created: latency-svc-xsnqw
  Jan 29 21:56:11.339: INFO: Got endpoints: latency-svc-8crbw [745.970964ms]
  Jan 29 21:56:11.356: INFO: Created: latency-svc-vq7wm
  Jan 29 21:56:11.391: INFO: Got endpoints: latency-svc-vblm4 [751.790734ms]
  Jan 29 21:56:11.407: INFO: Created: latency-svc-qcz4j
  Jan 29 21:56:11.438: INFO: Got endpoints: latency-svc-rwjk7 [744.802243ms]
  Jan 29 21:56:11.454: INFO: Created: latency-svc-grn2t
  Jan 29 21:56:11.489: INFO: Got endpoints: latency-svc-pmwns [750.864249ms]
  Jan 29 21:56:11.506: INFO: Created: latency-svc-vf9bs
  Jan 29 21:56:11.539: INFO: Got endpoints: latency-svc-l8bqm [746.948317ms]
  Jan 29 21:56:11.555: INFO: Created: latency-svc-cf75m
  Jan 29 21:56:11.590: INFO: Got endpoints: latency-svc-55cqb [750.677385ms]
  Jan 29 21:56:11.607: INFO: Created: latency-svc-rfrhs
  Jan 29 21:56:11.643: INFO: Got endpoints: latency-svc-77rvz [752.886479ms]
  Jan 29 21:56:11.661: INFO: Created: latency-svc-d2nbp
  Jan 29 21:56:11.693: INFO: Got endpoints: latency-svc-7wmw2 [751.465262ms]
  Jan 29 21:56:11.709: INFO: Created: latency-svc-8xvhk
  Jan 29 21:56:11.741: INFO: Got endpoints: latency-svc-dj2mt [751.972796ms]
  Jan 29 21:56:11.756: INFO: Created: latency-svc-ww4n9
  Jan 29 21:56:11.789: INFO: Got endpoints: latency-svc-j97zw [750.496827ms]
  Jan 29 21:56:11.809: INFO: Created: latency-svc-2lqgr
  Jan 29 21:56:11.838: INFO: Got endpoints: latency-svc-kd4hv [746.806276ms]
  Jan 29 21:56:11.852: INFO: Created: latency-svc-wsxq5
  E0129 21:56:11.865262      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:11.889: INFO: Got endpoints: latency-svc-zpjj2 [738.831396ms]
  Jan 29 21:56:11.909: INFO: Created: latency-svc-m44dn
  Jan 29 21:56:11.938: INFO: Got endpoints: latency-svc-jr5v2 [749.733865ms]
  Jan 29 21:56:11.957: INFO: Created: latency-svc-xqmbb
  Jan 29 21:56:11.988: INFO: Got endpoints: latency-svc-9zjwd [746.39482ms]
  Jan 29 21:56:12.004: INFO: Created: latency-svc-vc2lc
  Jan 29 21:56:12.041: INFO: Got endpoints: latency-svc-xsnqw [750.061401ms]
  Jan 29 21:56:12.056: INFO: Created: latency-svc-j45fv
  Jan 29 21:56:12.089: INFO: Got endpoints: latency-svc-vq7wm [749.537757ms]
  Jan 29 21:56:12.104: INFO: Created: latency-svc-m7xqk
  Jan 29 21:56:12.139: INFO: Got endpoints: latency-svc-qcz4j [747.889849ms]
  Jan 29 21:56:12.156: INFO: Created: latency-svc-2hh67
  Jan 29 21:56:12.189: INFO: Got endpoints: latency-svc-grn2t [750.441834ms]
  Jan 29 21:56:12.206: INFO: Created: latency-svc-vhcjn
  Jan 29 21:56:12.240: INFO: Got endpoints: latency-svc-vf9bs [750.986151ms]
  Jan 29 21:56:12.255: INFO: Created: latency-svc-wxg7f
  Jan 29 21:56:12.291: INFO: Got endpoints: latency-svc-cf75m [752.163688ms]
  Jan 29 21:56:12.307: INFO: Created: latency-svc-t2zsd
  Jan 29 21:56:12.339: INFO: Got endpoints: latency-svc-rfrhs [748.900574ms]
  Jan 29 21:56:12.356: INFO: Created: latency-svc-pvq8g
  Jan 29 21:56:12.388: INFO: Got endpoints: latency-svc-d2nbp [745.246552ms]
  Jan 29 21:56:12.485: INFO: Got endpoints: latency-svc-8xvhk [792.560185ms]
  Jan 29 21:56:12.488: INFO: Created: latency-svc-rh2tx
  Jan 29 21:56:12.491: INFO: Got endpoints: latency-svc-ww4n9 [749.466065ms]
  Jan 29 21:56:12.502: INFO: Created: latency-svc-nrntk
  Jan 29 21:56:12.512: INFO: Created: latency-svc-gjcfq
  Jan 29 21:56:12.541: INFO: Got endpoints: latency-svc-2lqgr [751.755221ms]
  Jan 29 21:56:12.558: INFO: Created: latency-svc-wc4mx
  Jan 29 21:56:12.596: INFO: Got endpoints: latency-svc-wsxq5 [757.523027ms]
  Jan 29 21:56:12.612: INFO: Created: latency-svc-k59xr
  Jan 29 21:56:12.641: INFO: Got endpoints: latency-svc-m44dn [752.155103ms]
  Jan 29 21:56:12.655: INFO: Created: latency-svc-n68lv
  Jan 29 21:56:12.691: INFO: Got endpoints: latency-svc-xqmbb [752.932433ms]
  Jan 29 21:56:12.720: INFO: Created: latency-svc-2lkvh
  Jan 29 21:56:12.739: INFO: Got endpoints: latency-svc-vc2lc [750.877751ms]
  Jan 29 21:56:12.757: INFO: Created: latency-svc-99268
  Jan 29 21:56:12.791: INFO: Got endpoints: latency-svc-j45fv [750.42716ms]
  Jan 29 21:56:12.815: INFO: Created: latency-svc-bbf7c
  Jan 29 21:56:12.839: INFO: Got endpoints: latency-svc-m7xqk [750.274539ms]
  Jan 29 21:56:12.860: INFO: Created: latency-svc-fcklg
  E0129 21:56:12.866123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:12.899: INFO: Got endpoints: latency-svc-2hh67 [759.569198ms]
  Jan 29 21:56:12.922: INFO: Created: latency-svc-ths8n
  Jan 29 21:56:12.941: INFO: Got endpoints: latency-svc-vhcjn [752.598035ms]
  Jan 29 21:56:12.957: INFO: Created: latency-svc-lggzg
  Jan 29 21:56:12.992: INFO: Got endpoints: latency-svc-wxg7f [752.211972ms]
  Jan 29 21:56:13.010: INFO: Created: latency-svc-g5shf
  Jan 29 21:56:13.040: INFO: Got endpoints: latency-svc-t2zsd [748.754273ms]
  Jan 29 21:56:13.059: INFO: Created: latency-svc-54vbb
  Jan 29 21:56:13.092: INFO: Got endpoints: latency-svc-pvq8g [752.446989ms]
  Jan 29 21:56:13.110: INFO: Created: latency-svc-htbqr
  Jan 29 21:56:13.139: INFO: Got endpoints: latency-svc-rh2tx [751.183109ms]
  Jan 29 21:56:13.156: INFO: Created: latency-svc-lzxkl
  Jan 29 21:56:13.192: INFO: Got endpoints: latency-svc-nrntk [706.731541ms]
  Jan 29 21:56:13.211: INFO: Created: latency-svc-xfgpj
  Jan 29 21:56:13.246: INFO: Got endpoints: latency-svc-gjcfq [755.343202ms]
  Jan 29 21:56:13.263: INFO: Created: latency-svc-r67pp
  Jan 29 21:56:13.289: INFO: Got endpoints: latency-svc-wc4mx [747.511556ms]
  Jan 29 21:56:13.307: INFO: Created: latency-svc-xgpb6
  Jan 29 21:56:13.342: INFO: Got endpoints: latency-svc-k59xr [746.028009ms]
  Jan 29 21:56:13.366: INFO: Created: latency-svc-lfzqc
  Jan 29 21:56:13.392: INFO: Got endpoints: latency-svc-n68lv [750.52546ms]
  Jan 29 21:56:13.437: INFO: Created: latency-svc-g62fh
  Jan 29 21:56:13.442: INFO: Got endpoints: latency-svc-2lkvh [750.566852ms]
  Jan 29 21:56:13.461: INFO: Created: latency-svc-zxgnn
  Jan 29 21:56:13.490: INFO: Got endpoints: latency-svc-99268 [751.100061ms]
  Jan 29 21:56:13.507: INFO: Created: latency-svc-brfzg
  Jan 29 21:56:13.538: INFO: Got endpoints: latency-svc-bbf7c [747.261605ms]
  Jan 29 21:56:13.554: INFO: Created: latency-svc-cjd98
  Jan 29 21:56:13.590: INFO: Got endpoints: latency-svc-fcklg [750.802024ms]
  Jan 29 21:56:13.608: INFO: Created: latency-svc-mspch
  Jan 29 21:56:13.640: INFO: Got endpoints: latency-svc-ths8n [741.519605ms]
  Jan 29 21:56:13.657: INFO: Created: latency-svc-wwkmx
  Jan 29 21:56:13.692: INFO: Got endpoints: latency-svc-lggzg [750.928923ms]
  Jan 29 21:56:13.710: INFO: Created: latency-svc-swqzd
  Jan 29 21:56:13.739: INFO: Got endpoints: latency-svc-g5shf [747.479957ms]
  Jan 29 21:56:13.758: INFO: Created: latency-svc-dcvcj
  Jan 29 21:56:13.793: INFO: Got endpoints: latency-svc-54vbb [753.054366ms]
  Jan 29 21:56:13.810: INFO: Created: latency-svc-tlzmn
  Jan 29 21:56:13.839: INFO: Got endpoints: latency-svc-htbqr [747.260554ms]
  Jan 29 21:56:13.856: INFO: Created: latency-svc-4grvz
  E0129 21:56:13.867230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:13.894: INFO: Got endpoints: latency-svc-lzxkl [754.5283ms]
  Jan 29 21:56:13.910: INFO: Created: latency-svc-sbhqh
  Jan 29 21:56:13.942: INFO: Got endpoints: latency-svc-xfgpj [749.860526ms]
  Jan 29 21:56:13.961: INFO: Created: latency-svc-p5z27
  Jan 29 21:56:13.990: INFO: Got endpoints: latency-svc-r67pp [743.957149ms]
  Jan 29 21:56:14.011: INFO: Created: latency-svc-vgcvs
  Jan 29 21:56:14.040: INFO: Got endpoints: latency-svc-xgpb6 [750.920668ms]
  Jan 29 21:56:14.058: INFO: Created: latency-svc-vm9gk
  Jan 29 21:56:14.091: INFO: Got endpoints: latency-svc-lfzqc [749.209137ms]
  Jan 29 21:56:14.111: INFO: Created: latency-svc-8gdct
  Jan 29 21:56:14.140: INFO: Got endpoints: latency-svc-g62fh [748.215713ms]
  Jan 29 21:56:14.159: INFO: Created: latency-svc-x4phr
  Jan 29 21:56:14.189: INFO: Got endpoints: latency-svc-zxgnn [747.248292ms]
  Jan 29 21:56:14.203: INFO: Created: latency-svc-5f9zp
  Jan 29 21:56:14.242: INFO: Got endpoints: latency-svc-brfzg [751.733449ms]
  Jan 29 21:56:14.265: INFO: Created: latency-svc-7gjt2
  Jan 29 21:56:14.290: INFO: Got endpoints: latency-svc-cjd98 [751.313261ms]
  Jan 29 21:56:14.307: INFO: Created: latency-svc-mbz7d
  Jan 29 21:56:14.340: INFO: Got endpoints: latency-svc-mspch [749.655813ms]
  Jan 29 21:56:14.360: INFO: Created: latency-svc-6v7k2
  Jan 29 21:56:14.389: INFO: Got endpoints: latency-svc-wwkmx [748.984699ms]
  Jan 29 21:56:14.404: INFO: Created: latency-svc-v68f9
  Jan 29 21:56:14.441: INFO: Got endpoints: latency-svc-swqzd [748.708809ms]
  Jan 29 21:56:14.457: INFO: Created: latency-svc-98xhd
  Jan 29 21:56:14.489: INFO: Got endpoints: latency-svc-dcvcj [748.950364ms]
  Jan 29 21:56:14.508: INFO: Created: latency-svc-v7dtj
  Jan 29 21:56:14.541: INFO: Got endpoints: latency-svc-tlzmn [747.930641ms]
  Jan 29 21:56:14.557: INFO: Created: latency-svc-8q22x
  Jan 29 21:56:14.590: INFO: Got endpoints: latency-svc-4grvz [750.38798ms]
  Jan 29 21:56:14.607: INFO: Created: latency-svc-nn4lv
  Jan 29 21:56:14.648: INFO: Got endpoints: latency-svc-sbhqh [753.644237ms]
  Jan 29 21:56:14.663: INFO: Created: latency-svc-l4vkv
  Jan 29 21:56:14.691: INFO: Got endpoints: latency-svc-p5z27 [748.981825ms]
  Jan 29 21:56:14.708: INFO: Created: latency-svc-kcc4m
  Jan 29 21:56:14.741: INFO: Got endpoints: latency-svc-vgcvs [750.388056ms]
  Jan 29 21:56:14.758: INFO: Created: latency-svc-bt46x
  Jan 29 21:56:14.792: INFO: Got endpoints: latency-svc-vm9gk [752.683645ms]
  Jan 29 21:56:14.807: INFO: Created: latency-svc-srsfg
  Jan 29 21:56:14.841: INFO: Got endpoints: latency-svc-8gdct [749.958529ms]
  Jan 29 21:56:14.865: INFO: Created: latency-svc-7zjwf
  E0129 21:56:14.867853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:14.888: INFO: Got endpoints: latency-svc-x4phr [748.54028ms]
  Jan 29 21:56:14.903: INFO: Created: latency-svc-7c8sj
  Jan 29 21:56:14.944: INFO: Got endpoints: latency-svc-5f9zp [754.321886ms]
  Jan 29 21:56:14.959: INFO: Created: latency-svc-4klfc
  Jan 29 21:56:14.990: INFO: Got endpoints: latency-svc-7gjt2 [748.004307ms]
  Jan 29 21:56:15.008: INFO: Created: latency-svc-gd6kg
  Jan 29 21:56:15.039: INFO: Got endpoints: latency-svc-mbz7d [749.613716ms]
  Jan 29 21:56:15.056: INFO: Created: latency-svc-t2kfr
  Jan 29 21:56:15.090: INFO: Got endpoints: latency-svc-6v7k2 [750.658568ms]
  Jan 29 21:56:15.105: INFO: Created: latency-svc-t8nqc
  Jan 29 21:56:15.140: INFO: Got endpoints: latency-svc-v68f9 [750.244045ms]
  Jan 29 21:56:15.156: INFO: Created: latency-svc-96w5t
  Jan 29 21:56:15.190: INFO: Got endpoints: latency-svc-98xhd [748.636524ms]
  Jan 29 21:56:15.204: INFO: Created: latency-svc-7sqg7
  Jan 29 21:56:15.239: INFO: Got endpoints: latency-svc-v7dtj [750.333655ms]
  Jan 29 21:56:15.254: INFO: Created: latency-svc-zbd65
  Jan 29 21:56:15.295: INFO: Got endpoints: latency-svc-8q22x [754.001425ms]
  Jan 29 21:56:15.314: INFO: Created: latency-svc-4c9fd
  Jan 29 21:56:15.341: INFO: Got endpoints: latency-svc-nn4lv [750.892252ms]
  Jan 29 21:56:15.358: INFO: Created: latency-svc-gq9tz
  Jan 29 21:56:15.390: INFO: Got endpoints: latency-svc-l4vkv [741.888935ms]
  Jan 29 21:56:15.408: INFO: Created: latency-svc-j47b6
  Jan 29 21:56:15.441: INFO: Got endpoints: latency-svc-kcc4m [750.206555ms]
  Jan 29 21:56:15.460: INFO: Created: latency-svc-dmzh2
  Jan 29 21:56:15.489: INFO: Got endpoints: latency-svc-bt46x [748.159457ms]
  Jan 29 21:56:15.505: INFO: Created: latency-svc-zjnqd
  Jan 29 21:56:15.540: INFO: Got endpoints: latency-svc-srsfg [747.146617ms]
  Jan 29 21:56:15.557: INFO: Created: latency-svc-6pkkn
  Jan 29 21:56:15.589: INFO: Got endpoints: latency-svc-7zjwf [748.289634ms]
  Jan 29 21:56:15.608: INFO: Created: latency-svc-hzzp5
  Jan 29 21:56:15.641: INFO: Got endpoints: latency-svc-7c8sj [752.162052ms]
  Jan 29 21:56:15.657: INFO: Created: latency-svc-5wlnn
  Jan 29 21:56:15.690: INFO: Got endpoints: latency-svc-4klfc [746.682481ms]
  Jan 29 21:56:15.708: INFO: Created: latency-svc-zj2r9
  Jan 29 21:56:15.739: INFO: Got endpoints: latency-svc-gd6kg [749.098255ms]
  Jan 29 21:56:15.758: INFO: Created: latency-svc-t7q9d
  Jan 29 21:56:15.791: INFO: Got endpoints: latency-svc-t2kfr [751.092568ms]
  Jan 29 21:56:15.811: INFO: Created: latency-svc-zt46s
  Jan 29 21:56:15.841: INFO: Got endpoints: latency-svc-t8nqc [750.40611ms]
  Jan 29 21:56:15.859: INFO: Created: latency-svc-4wdn9
  E0129 21:56:15.868807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:15.889: INFO: Got endpoints: latency-svc-96w5t [749.57744ms]
  Jan 29 21:56:15.910: INFO: Created: latency-svc-4gz4t
  Jan 29 21:56:15.950: INFO: Got endpoints: latency-svc-7sqg7 [760.02156ms]
  Jan 29 21:56:15.970: INFO: Created: latency-svc-8bkch
  Jan 29 21:56:15.993: INFO: Got endpoints: latency-svc-zbd65 [753.968187ms]
  Jan 29 21:56:16.038: INFO: Got endpoints: latency-svc-4c9fd [743.10965ms]
  Jan 29 21:56:16.062: INFO: Created: latency-svc-tx9md
  Jan 29 21:56:16.081: INFO: Created: latency-svc-frsv5
  Jan 29 21:56:16.090: INFO: Got endpoints: latency-svc-gq9tz [749.56094ms]
  Jan 29 21:56:16.157: INFO: Got endpoints: latency-svc-j47b6 [767.013569ms]
  Jan 29 21:56:16.168: INFO: Created: latency-svc-rb9v5
  Jan 29 21:56:16.198: INFO: Got endpoints: latency-svc-dmzh2 [757.01277ms]
  Jan 29 21:56:16.200: INFO: Created: latency-svc-7599b
  Jan 29 21:56:16.222: INFO: Created: latency-svc-hgknp
  Jan 29 21:56:16.241: INFO: Got endpoints: latency-svc-zjnqd [751.808164ms]
  Jan 29 21:56:16.257: INFO: Created: latency-svc-4wvgh
  Jan 29 21:56:16.289: INFO: Got endpoints: latency-svc-6pkkn [748.926183ms]
  Jan 29 21:56:16.331: INFO: Created: latency-svc-pkg75
  Jan 29 21:56:16.337: INFO: Got endpoints: latency-svc-hzzp5 [748.000154ms]
  Jan 29 21:56:16.351: INFO: Created: latency-svc-55nzc
  Jan 29 21:56:16.389: INFO: Got endpoints: latency-svc-5wlnn [748.073196ms]
  Jan 29 21:56:16.405: INFO: Created: latency-svc-klhjh
  Jan 29 21:56:16.439: INFO: Got endpoints: latency-svc-zj2r9 [748.917149ms]
  Jan 29 21:56:16.457: INFO: Created: latency-svc-qnh7f
  Jan 29 21:56:16.494: INFO: Got endpoints: latency-svc-t7q9d [753.946984ms]
  Jan 29 21:56:16.511: INFO: Created: latency-svc-rww6f
  Jan 29 21:56:16.540: INFO: Got endpoints: latency-svc-zt46s [748.883302ms]
  Jan 29 21:56:16.556: INFO: Created: latency-svc-b5xr2
  Jan 29 21:56:16.592: INFO: Got endpoints: latency-svc-4wdn9 [751.031912ms]
  Jan 29 21:56:16.612: INFO: Created: latency-svc-cdrbk
  Jan 29 21:56:16.640: INFO: Got endpoints: latency-svc-4gz4t [750.379969ms]
  Jan 29 21:56:16.658: INFO: Created: latency-svc-v6nvf
  Jan 29 21:56:16.689: INFO: Got endpoints: latency-svc-8bkch [739.562647ms]
  Jan 29 21:56:16.710: INFO: Created: latency-svc-9d7rb
  Jan 29 21:56:16.739: INFO: Got endpoints: latency-svc-tx9md [746.203166ms]
  Jan 29 21:56:16.756: INFO: Created: latency-svc-kksxv
  Jan 29 21:56:16.790: INFO: Got endpoints: latency-svc-frsv5 [751.459197ms]
  Jan 29 21:56:16.842: INFO: Got endpoints: latency-svc-rb9v5 [751.474566ms]
  E0129 21:56:16.869338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:16.890: INFO: Got endpoints: latency-svc-7599b [733.1434ms]
  Jan 29 21:56:16.943: INFO: Got endpoints: latency-svc-hgknp [744.05859ms]
  Jan 29 21:56:16.990: INFO: Got endpoints: latency-svc-4wvgh [749.030849ms]
  Jan 29 21:56:17.039: INFO: Got endpoints: latency-svc-pkg75 [750.08461ms]
  Jan 29 21:56:17.090: INFO: Got endpoints: latency-svc-55nzc [752.047589ms]
  Jan 29 21:56:17.138: INFO: Got endpoints: latency-svc-klhjh [748.885811ms]
  Jan 29 21:56:17.189: INFO: Got endpoints: latency-svc-qnh7f [749.620795ms]
  Jan 29 21:56:17.240: INFO: Got endpoints: latency-svc-rww6f [745.95596ms]
  Jan 29 21:56:17.290: INFO: Got endpoints: latency-svc-b5xr2 [750.761807ms]
  Jan 29 21:56:17.341: INFO: Got endpoints: latency-svc-cdrbk [749.167053ms]
  Jan 29 21:56:17.390: INFO: Got endpoints: latency-svc-v6nvf [750.246827ms]
  Jan 29 21:56:17.442: INFO: Got endpoints: latency-svc-9d7rb [752.065974ms]
  Jan 29 21:56:17.491: INFO: Got endpoints: latency-svc-kksxv [752.083349ms]
  Jan 29 21:56:17.491: INFO: Latencies: [21.966143ms 37.888275ms 49.128232ms 61.42862ms 73.397416ms 85.417691ms 98.308025ms 109.370136ms 128.640363ms 136.256617ms 142.318049ms 147.197688ms 150.08788ms 151.640521ms 152.524455ms 153.10058ms 163.982793ms 164.67891ms 166.732563ms 167.602133ms 169.712359ms 171.283685ms 173.853491ms 177.880845ms 179.402286ms 180.350987ms 180.467521ms 181.287713ms 181.65551ms 186.806041ms 189.131544ms 190.529755ms 191.329459ms 191.617397ms 192.531783ms 196.385966ms 197.648866ms 199.585137ms 205.152118ms 230.377505ms 266.58501ms 308.550262ms 514.042845ms 515.525146ms 526.819172ms 534.108244ms 542.831312ms 553.226373ms 555.048312ms 596.133951ms 603.259738ms 632.679634ms 655.469946ms 670.205495ms 703.88304ms 706.731541ms 710.754979ms 733.1434ms 734.783701ms 738.831396ms 739.562647ms 741.519605ms 741.888935ms 743.10965ms 743.957149ms 744.05859ms 744.802243ms 745.108535ms 745.246552ms 745.95596ms 745.970964ms 746.028009ms 746.203166ms 746.39482ms 746.682481ms 746.751197ms 746.806276ms 746.948317ms 747.146617ms 747.248292ms 747.260554ms 747.261605ms 747.479957ms 747.511556ms 747.889849ms 747.930641ms 748.000154ms 748.004307ms 748.073196ms 748.159457ms 748.215713ms 748.289634ms 748.54028ms 748.636524ms 748.708809ms 748.754273ms 748.761445ms 748.883302ms 748.885811ms 748.900574ms 748.917149ms 748.920432ms 748.926183ms 748.950364ms 748.981825ms 748.984699ms 749.030849ms 749.098255ms 749.167053ms 749.209137ms 749.441073ms 749.466065ms 749.470643ms 749.537757ms 749.56094ms 749.57744ms 749.613716ms 749.620795ms 749.626974ms 749.655813ms 749.733865ms 749.753169ms 749.860526ms 749.958529ms 750.061401ms 750.08461ms 750.206555ms 750.244045ms 750.246827ms 750.274539ms 750.310328ms 750.333655ms 750.379969ms 750.38798ms 750.388056ms 750.40611ms 750.42716ms 750.441834ms 750.496827ms 750.52546ms 750.566852ms 750.658568ms 750.677385ms 750.702777ms 750.703673ms 750.761807ms 750.802024ms 750.864249ms 750.877751ms 750.892252ms 750.920668ms 750.928923ms 750.932887ms 750.980372ms 750.986151ms 751.031912ms 751.092568ms 751.100061ms 751.102692ms 751.183109ms 751.262754ms 751.313261ms 751.459197ms 751.465262ms 751.474566ms 751.648456ms 751.733449ms 751.755221ms 751.790734ms 751.808164ms 751.972796ms 752.047589ms 752.062758ms 752.065974ms 752.083349ms 752.155103ms 752.162052ms 752.163688ms 752.211972ms 752.446989ms 752.598035ms 752.683645ms 752.775609ms 752.886479ms 752.932433ms 753.054366ms 753.644237ms 753.946984ms 753.968187ms 754.001425ms 754.321886ms 754.5283ms 755.343202ms 757.01277ms 757.523027ms 759.569198ms 760.02156ms 760.83831ms 767.013569ms 792.560185ms]
  Jan 29 21:56:17.492: INFO: 50 %ile: 748.917149ms
  Jan 29 21:56:17.492: INFO: 90 %ile: 752.598035ms
  Jan 29 21:56:17.492: INFO: 99 %ile: 767.013569ms
  Jan 29 21:56:17.492: INFO: Total sample count: 200
  Jan 29 21:56:17.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-679" for this suite. @ 01/29/24 21:56:17.501
• [10.783 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 01/29/24 21:56:17.508
  Jan 29 21:56:17.508: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:56:17.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:17.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:17.529
  Jan 29 21:56:17.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4757" for this suite. @ 01/29/24 21:56:17.582
• [0.080 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 01/29/24 21:56:17.589
  Jan 29 21:56:17.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:56:17.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:17.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:17.607
  STEP: Creating projection with secret that has name projected-secret-test-map-592c5812-aca3-420b-9e15-dba7379c9f46 @ 01/29/24 21:56:17.611
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:56:17.616
  E0129 21:56:17.869759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:18.870726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:19.870936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:20.871537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:56:21.641
  Jan 29 21:56:21.645: INFO: Trying to get logs from node nodea08 pod pod-projected-secrets-f788a878-2557-45b7-86e0-acbf7c1fc167 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:56:21.654
  Jan 29 21:56:21.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2557" for this suite. @ 01/29/24 21:56:21.676
• [4.096 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 01/29/24 21:56:21.685
  Jan 29 21:56:21.685: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename svcaccounts @ 01/29/24 21:56:21.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:21.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:21.704
  Jan 29 21:56:21.722: INFO: created pod
  E0129 21:56:21.871561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:22.872745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:23.873419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:24.873925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:56:25.735
  E0129 21:56:25.873983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:26.874456      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:27.875458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:28.876057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:29.876544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:30.877088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:31.877633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:32.877906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:33.878493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:34.879094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:35.879646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:36.880158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:37.881054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:38.881513      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:39.882284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:40.882769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:41.883153      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:42.883275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:43.883739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:44.884257      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:45.884833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:46.885413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:47.885536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:48.886091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:49.886607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:50.887147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:51.887705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:52.887922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:53.888435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:54.889042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:56:55.736: INFO: polling logs
  Jan 29 21:56:55.747: INFO: Pod logs: 
  I0129 21:56:22.422303       1 log.go:198] OK: Got token
  I0129 21:56:22.422366       1 log.go:198] validating with in-cluster discovery
  I0129 21:56:22.422807       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0129 21:56:22.422849       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7555:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706565981, NotBefore:1706565381, IssuedAt:1706565381, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7555", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"764a06cc-3e29-4371-8c82-58549816c073"}}}
  I0129 21:56:22.447782       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0129 21:56:22.458005       1 log.go:198] OK: Validated signature on JWT
  I0129 21:56:22.458130       1 log.go:198] OK: Got valid claims from token!
  I0129 21:56:22.458161       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7555:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1706565981, NotBefore:1706565381, IssuedAt:1706565381, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7555", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"764a06cc-3e29-4371-8c82-58549816c073"}}}

  Jan 29 21:56:55.747: INFO: completed pod
  Jan 29 21:56:55.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7555" for this suite. @ 01/29/24 21:56:55.76
• [34.081 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 01/29/24 21:56:55.769
  Jan 29 21:56:55.769: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename csistoragecapacity @ 01/29/24 21:56:55.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:55.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:55.791
  STEP: getting /apis @ 01/29/24 21:56:55.796
  STEP: getting /apis/storage.k8s.io @ 01/29/24 21:56:55.803
  STEP: getting /apis/storage.k8s.io/v1 @ 01/29/24 21:56:55.805
  STEP: creating @ 01/29/24 21:56:55.807
  STEP: watching @ 01/29/24 21:56:55.825
  Jan 29 21:56:55.825: INFO: starting watch
  STEP: getting @ 01/29/24 21:56:55.837
  STEP: listing in namespace @ 01/29/24 21:56:55.84
  STEP: listing across namespaces @ 01/29/24 21:56:55.844
  STEP: patching @ 01/29/24 21:56:55.848
  STEP: updating @ 01/29/24 21:56:55.854
  Jan 29 21:56:55.859: INFO: waiting for watch events with expected annotations in namespace
  Jan 29 21:56:55.859: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 01/29/24 21:56:55.859
  STEP: deleting a collection @ 01/29/24 21:56:55.874
  Jan 29 21:56:55.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0129 21:56:55.889645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "csistoragecapacity-6974" for this suite. @ 01/29/24 21:56:55.891
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 01/29/24 21:56:55.904
  Jan 29 21:56:55.904: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename gc @ 01/29/24 21:56:55.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:55.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:55.925
  STEP: create the deployment @ 01/29/24 21:56:55.929
  W0129 21:56:55.935164      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/29/24 21:56:55.935
  STEP: delete the deployment @ 01/29/24 21:56:56.449
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 01/29/24 21:56:56.456
  E0129 21:56:56.890608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 01/29/24 21:56:56.981
  Jan 29 21:56:57.146: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 29 21:56:57.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1037" for this suite. @ 01/29/24 21:56:57.152
• [1.255 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 01/29/24 21:56:57.16
  Jan 29 21:56:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename configmap @ 01/29/24 21:56:57.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:57.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:57.182
  STEP: Creating configMap with name configmap-test-upd-4024f1e5-aacb-4e84-9c42-2d641339a35e @ 01/29/24 21:56:57.191
  STEP: Creating the pod @ 01/29/24 21:56:57.196
  E0129 21:56:57.891812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:56:58.892811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 01/29/24 21:56:59.22
  STEP: Waiting for pod with binary data @ 01/29/24 21:56:59.231
  Jan 29 21:56:59.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6845" for this suite. @ 01/29/24 21:56:59.245
• [2.091 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 01/29/24 21:56:59.251
  Jan 29 21:56:59.251: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename security-context-test @ 01/29/24 21:56:59.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:56:59.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:56:59.275
  E0129 21:56:59.893326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:00.894275      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:01.894510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:02.894614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:57:03.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4032" for this suite. @ 01/29/24 21:57:03.31
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 01/29/24 21:57:03.319
  Jan 29 21:57:03.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:57:03.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:03.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:03.34
  STEP: Creating configMap with name projected-configmap-test-volume-map-225e9fb0-2425-4173-b67f-9ca52107adb2 @ 01/29/24 21:57:03.345
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:57:03.35
  E0129 21:57:03.895546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:04.896015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:05.896952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:06.897546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:57:07.377
  Jan 29 21:57:07.382: INFO: Trying to get logs from node nodea08 pod pod-projected-configmaps-316c4deb-e32b-460e-ad78-1a241e7ea980 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:57:07.391
  Jan 29 21:57:07.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1137" for this suite. @ 01/29/24 21:57:07.412
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 01/29/24 21:57:07.422
  Jan 29 21:57:07.422: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:57:07.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:07.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:07.445
  STEP: Creating a pod to test downward API volume plugin @ 01/29/24 21:57:07.451
  E0129 21:57:07.897806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:08.898765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:09.899832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:10.900414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:57:11.478
  Jan 29 21:57:11.482: INFO: Trying to get logs from node nodea08 pod downwardapi-volume-5b956ef7-1f14-4942-be68-ba86c10437fe container client-container: <nil>
  STEP: delete the pod @ 01/29/24 21:57:11.493
  Jan 29 21:57:11.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4448" for this suite. @ 01/29/24 21:57:11.517
• [4.103 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 01/29/24 21:57:11.525
  Jan 29 21:57:11.525: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename resourcequota @ 01/29/24 21:57:11.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:11.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:11.546
  STEP: Counting existing ResourceQuota @ 01/29/24 21:57:11.551
  E0129 21:57:11.901170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:12.902264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:13.902428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:14.903435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:15.904054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/29/24 21:57:16.556
  STEP: Ensuring resource quota status is calculated @ 01/29/24 21:57:16.562
  E0129 21:57:16.905208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:17.905586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 01/29/24 21:57:18.568
  STEP: Creating a NodePort Service @ 01/29/24 21:57:18.594
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 01/29/24 21:57:18.621
  STEP: Ensuring resource quota status captures service creation @ 01/29/24 21:57:18.653
  E0129 21:57:18.905720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:19.906023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 01/29/24 21:57:20.659
  STEP: Ensuring resource quota status released usage @ 01/29/24 21:57:20.704
  E0129 21:57:20.906993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:21.907655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:57:22.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8632" for this suite. @ 01/29/24 21:57:22.718
• [11.199 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 01/29/24 21:57:22.725
  Jan 29 21:57:22.725: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename job @ 01/29/24 21:57:22.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:22.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:22.749
  STEP: Creating Indexed job @ 01/29/24 21:57:22.754
  STEP: Ensuring job reaches completions @ 01/29/24 21:57:22.763
  E0129 21:57:22.907753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:23.908611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:24.909217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:25.909444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:26.909838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:27.909989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:28.910134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:29.910700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 01/29/24 21:57:30.769
  Jan 29 21:57:30.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-389" for this suite. @ 01/29/24 21:57:30.782
• [8.064 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 01/29/24 21:57:30.79
  Jan 29 21:57:30.790: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:57:30.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:30.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:30.811
  STEP: Creating a pod to test downward api env vars @ 01/29/24 21:57:30.816
  E0129 21:57:30.911361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:31.911868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:32.912927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:33.913564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:57:34.843
  Jan 29 21:57:34.848: INFO: Trying to get logs from node nodea08 pod downward-api-14337dca-0815-4cd2-b63a-8e1e7cef19be container dapi-container: <nil>
  STEP: delete the pod @ 01/29/24 21:57:34.856
  Jan 29 21:57:34.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4781" for this suite. @ 01/29/24 21:57:34.877
• [4.092 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 01/29/24 21:57:34.883
  Jan 29 21:57:34.883: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:57:34.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:34.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:34.899
  STEP: Creating secret with name secret-test-963a6997-9263-4e75-9cca-507cd3b36fff @ 01/29/24 21:57:34.904
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:57:34.908
  E0129 21:57:34.914039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:35.914393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:36.914641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:37.914816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:38.915447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:57:38.935
  Jan 29 21:57:38.939: INFO: Trying to get logs from node nodea08 pod pod-secrets-509bb432-e887-4e2d-9a13-afe2c23015f9 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:57:38.949
  Jan 29 21:57:38.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9725" for this suite. @ 01/29/24 21:57:38.973
• [4.098 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 01/29/24 21:57:38.982
  Jan 29 21:57:38.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename downward-api @ 01/29/24 21:57:38.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:38.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:39.001
  STEP: Creating the pod @ 01/29/24 21:57:39.006
  E0129 21:57:39.915756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:40.916500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:57:41.555: INFO: Successfully updated pod "labelsupdate7b5ee959-dea3-4856-b17b-042efcd4a7ce"
  E0129 21:57:41.916745      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:42.917271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:57:43.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3012" for this suite. @ 01/29/24 21:57:43.581
• [4.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 01/29/24 21:57:43.591
  Jan 29 21:57:43.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename secrets @ 01/29/24 21:57:43.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:43.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:43.612
  STEP: Creating secret with name secret-test-map-3f462ca3-d692-48cc-94e6-b8f940ac0063 @ 01/29/24 21:57:43.617
  STEP: Creating a pod to test consume secrets @ 01/29/24 21:57:43.623
  E0129 21:57:43.917861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:44.918658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:45.919065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:46.919610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:57:47.654
  Jan 29 21:57:47.658: INFO: Trying to get logs from node nodea08 pod pod-secrets-a32b02ba-8915-469e-ae06-977e8fe7203f container secret-volume-test: <nil>
  STEP: delete the pod @ 01/29/24 21:57:47.668
  Jan 29 21:57:47.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6313" for this suite. @ 01/29/24 21:57:47.692
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 01/29/24 21:57:47.7
  Jan 29 21:57:47.700: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename webhook @ 01/29/24 21:57:47.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:47.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:47.72
  STEP: Setting up server cert @ 01/29/24 21:57:47.744
  E0129 21:57:47.919960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/29/24 21:57:48.354
  STEP: Deploying the webhook pod @ 01/29/24 21:57:48.366
  STEP: Wait for the deployment to be ready @ 01/29/24 21:57:48.379
  Jan 29 21:57:48.389: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0129 21:57:48.920759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:49.921623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/29/24 21:57:50.405
  STEP: Verifying the service has paired with the endpoint @ 01/29/24 21:57:50.424
  E0129 21:57:50.922911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:57:51.425: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 29 21:57:51.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  E0129 21:57:51.923216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 01/29/24 21:57:51.945
  STEP: Creating a custom resource that should be denied by the webhook @ 01/29/24 21:57:51.971
  E0129 21:57:52.924339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:53.925146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 01/29/24 21:57:54.011
  STEP: Updating the custom resource with disallowed data should be denied @ 01/29/24 21:57:54.02
  STEP: Deleting the custom resource should be denied @ 01/29/24 21:57:54.033
  STEP: Remove the offending key and value from the custom resource data @ 01/29/24 21:57:54.041
  STEP: Deleting the updated custom resource should be successful @ 01/29/24 21:57:54.056
  Jan 29 21:57:54.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3638" for this suite. @ 01/29/24 21:57:54.625
  STEP: Destroying namespace "webhook-markers-3179" for this suite. @ 01/29/24 21:57:54.633
• [6.942 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 01/29/24 21:57:54.642
  Jan 29 21:57:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename dns @ 01/29/24 21:57:54.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:57:54.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:57:54.662
  STEP: Creating a test headless service @ 01/29/24 21:57:54.667
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8612 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8612;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8612 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8612;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8612.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8612.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8612.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8612.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8612.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8612.svc;check="$$(dig +notcp +noall +answer +search 216.119.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.119.216_udp@PTR;check="$$(dig +tcp +noall +answer +search 216.119.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.119.216_tcp@PTR;sleep 1; done
   @ 01/29/24 21:57:54.688
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8612 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8612;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8612 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8612;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8612.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8612.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8612.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8612.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8612.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8612.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8612.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8612.svc;check="$$(dig +notcp +noall +answer +search 216.119.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.119.216_udp@PTR;check="$$(dig +tcp +noall +answer +search 216.119.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.119.216_tcp@PTR;sleep 1; done
   @ 01/29/24 21:57:54.688
  STEP: creating a pod to probe DNS @ 01/29/24 21:57:54.688
  STEP: submitting the pod to kubernetes @ 01/29/24 21:57:54.688
  E0129 21:57:54.925368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:55.926481      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/29/24 21:57:56.709
  STEP: looking for the results for each expected name from probers @ 01/29/24 21:57:56.713
  Jan 29 21:57:56.720: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.725: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.729: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.734: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.739: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.743: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.748: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.754: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.779: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.784: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.790: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.795: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.800: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.805: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.834: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:57:56.858: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:57:56.927317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:57.928220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:58.928753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:57:59.929328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:00.929813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:01.868: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.875: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.879: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.884: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.889: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.895: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.900: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.905: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.929: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  E0129 21:58:01.929869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:01.934: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.939: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.944: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.949: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.953: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.958: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:01.983: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:58:02.930132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:03.930659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:04.931255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:05.931916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:06.867: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.872: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.882: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.887: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.891: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.896: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.900: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.905: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.928: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.932: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  E0129 21:58:06.932809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:06.937: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.942: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.946: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.951: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.956: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.961: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:06.980: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:58:07.933532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:08.933923      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:09.934426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:10.934913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:11.866: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.874: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.880: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.885: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.889: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.895: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.905: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.932: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  E0129 21:58:11.935773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:11.937: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.942: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.947: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.952: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.957: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.962: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.967: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:11.987: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:58:12.937120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:13.937699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:14.938379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:15.939044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:16.866: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.871: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.876: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.882: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.886: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.891: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.896: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.901: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.927: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.932: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.937: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  E0129 21:58:16.939234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:16.942: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.947: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.952: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.957: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.961: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:16.981: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:58:17.939928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:18.941013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:19.941552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:20.942105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:21.866: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.871: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.878: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.883: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.888: INFO: Unable to read wheezy_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.893: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.899: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.904: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.930: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.935: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.939: INFO: Unable to read jessie_udp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  E0129 21:58:21.942533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:21.945: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612 from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.950: INFO: Unable to read jessie_udp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.955: INFO: Unable to read jessie_tcp@dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.964: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc from pod dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb: the server could not find the requested resource (get pods dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb)
  Jan 29 21:58:21.985: INFO: Lookups using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8612 wheezy_tcp@dns-test-service.dns-8612 wheezy_udp@dns-test-service.dns-8612.svc wheezy_tcp@dns-test-service.dns-8612.svc wheezy_udp@_http._tcp.dns-test-service.dns-8612.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8612.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8612 jessie_tcp@dns-test-service.dns-8612 jessie_udp@dns-test-service.dns-8612.svc jessie_tcp@dns-test-service.dns-8612.svc jessie_udp@_http._tcp.dns-test-service.dns-8612.svc jessie_tcp@_http._tcp.dns-test-service.dns-8612.svc]

  E0129 21:58:22.942761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:23.943357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:24.943580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:25.943753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:26.944724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:26.986: INFO: DNS probes using dns-8612/dns-test-195274e5-073f-40a1-b11c-3c4aadea40eb succeeded

  Jan 29 21:58:26.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/29/24 21:58:26.992
  STEP: deleting the test service @ 01/29/24 21:58:27.009
  STEP: deleting the test headless service @ 01/29/24 21:58:27.031
  STEP: Destroying namespace "dns-8612" for this suite. @ 01/29/24 21:58:27.039
• [32.402 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 01/29/24 21:58:27.045
  Jan 29 21:58:27.045: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:58:27.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:27.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:27.065
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1991 @ 01/29/24 21:58:27.069
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/29/24 21:58:27.082
  STEP: creating service externalsvc in namespace services-1991 @ 01/29/24 21:58:27.082
  STEP: creating replication controller externalsvc in namespace services-1991 @ 01/29/24 21:58:27.099
  I0129 21:58:27.104966      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-1991, replica count: 2
  E0129 21:58:27.945044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:28.945191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:29.945718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:58:30.156431      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 01/29/24 21:58:30.161
  Jan 29 21:58:30.188: INFO: Creating new exec pod
  E0129 21:58:30.945856      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:31.946128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:32.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-1991 exec execpodjfdtn -- /bin/sh -x -c nslookup clusterip-service.services-1991.svc.cluster.local'
  Jan 29 21:58:32.469: INFO: stderr: "+ nslookup clusterip-service.services-1991.svc.cluster.local\n"
  Jan 29 21:58:32.469: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1991.svc.cluster.local\tcanonical name = externalsvc.services-1991.svc.cluster.local.\nName:\texternalsvc.services-1991.svc.cluster.local\nAddress: 10.111.95.255\n\n"
  Jan 29 21:58:32.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-1991, will wait for the garbage collector to delete the pods @ 01/29/24 21:58:32.475
  Jan 29 21:58:32.539: INFO: Deleting ReplicationController externalsvc took: 8.386329ms
  Jan 29 21:58:32.640: INFO: Terminating ReplicationController externalsvc pods took: 101.065016ms
  E0129 21:58:32.947251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:33.948168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:34.761: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-1991" for this suite. @ 01/29/24 21:58:34.77
• [7.731 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 01/29/24 21:58:34.779
  Jan 29 21:58:34.779: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename var-expansion @ 01/29/24 21:58:34.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:34.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:34.798
  E0129 21:58:34.949038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:35.950241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:36.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:58:36.830: INFO: Deleting pod "var-expansion-fdac2156-4558-4948-b9c1-d4e786d9fdec" in namespace "var-expansion-3533"
  Jan 29 21:58:36.839: INFO: Wait up to 5m0s for pod "var-expansion-fdac2156-4558-4948-b9c1-d4e786d9fdec" to be fully deleted
  E0129 21:58:36.950504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:37.950633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3533" for this suite. @ 01/29/24 21:58:38.849
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 01/29/24 21:58:38.859
  Jan 29 21:58:38.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:58:38.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:38.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:38.878
  STEP: creating service endpoint-test2 in namespace services-2814 @ 01/29/24 21:58:38.882
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2814 to expose endpoints map[] @ 01/29/24 21:58:38.897
  Jan 29 21:58:38.902: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0129 21:58:38.951448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:39.912: INFO: successfully validated that service endpoint-test2 in namespace services-2814 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2814 @ 01/29/24 21:58:39.913
  E0129 21:58:39.952250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:40.952798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2814 to expose endpoints map[pod1:[80]] @ 01/29/24 21:58:41.936
  Jan 29 21:58:41.950: INFO: successfully validated that service endpoint-test2 in namespace services-2814 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 01/29/24 21:58:41.95
  Jan 29 21:58:41.950: INFO: Creating new exec pod
  E0129 21:58:41.953591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:42.954741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:43.955368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:44.955643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:44.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jan 29 21:58:45.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:45.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:58:45.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.145.179 80'
  Jan 29 21:58:45.399: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.145.179 80\nConnection to 10.108.145.179 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:45.399: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-2814 @ 01/29/24 21:58:45.399
  E0129 21:58:45.956409      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:46.956908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2814 to expose endpoints map[pod1:[80] pod2:[80]] @ 01/29/24 21:58:47.422
  Jan 29 21:58:47.440: INFO: successfully validated that service endpoint-test2 in namespace services-2814 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 01/29/24 21:58:47.44
  E0129 21:58:47.958011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:48.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jan 29 21:58:48.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:48.652: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:58:48.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.145.179 80'
  Jan 29 21:58:48.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.145.179 80\nConnection to 10.108.145.179 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:48.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2814 @ 01/29/24 21:58:48.865
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2814 to expose endpoints map[pod2:[80]] @ 01/29/24 21:58:48.877
  Jan 29 21:58:48.902: INFO: successfully validated that service endpoint-test2 in namespace services-2814 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 01/29/24 21:58:48.902
  E0129 21:58:48.958911      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:49.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0129 21:58:49.959451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:58:50.125: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:50.125: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:58:50.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-2814 exec execpodz7b5m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.145.179 80'
  Jan 29 21:58:50.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.145.179 80\nConnection to 10.108.145.179 80 port [tcp/http] succeeded!\n"
  Jan 29 21:58:50.367: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-2814 @ 01/29/24 21:58:50.367
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2814 to expose endpoints map[] @ 01/29/24 21:58:50.378
  Jan 29 21:58:50.390: INFO: successfully validated that service endpoint-test2 in namespace services-2814 exposes endpoints map[]
  Jan 29 21:58:50.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2814" for this suite. @ 01/29/24 21:58:50.408
• [11.555 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 01/29/24 21:58:50.414
  Jan 29 21:58:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:58:50.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:50.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:50.433
  STEP: Creating configMap with name projected-configmap-test-volume-3079a029-0bee-40d1-90f7-c4c3ba36a8e3 @ 01/29/24 21:58:50.438
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:58:50.444
  E0129 21:58:50.960568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:51.961168      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:52.961775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:53.962384      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:58:54.468
  Jan 29 21:58:54.472: INFO: Trying to get logs from node nodea08 pod pod-projected-configmaps-73c3f478-226c-4aec-9227-ca302d701f2d container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:58:54.482
  Jan 29 21:58:54.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1575" for this suite. @ 01/29/24 21:58:54.504
• [4.097 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 01/29/24 21:58:54.513
  Jan 29 21:58:54.513: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename projected @ 01/29/24 21:58:54.515
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:54.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:54.539
  STEP: Creating configMap with name projected-configmap-test-volume-68b7c601-beee-4677-9ac3-2b8ce39641f8 @ 01/29/24 21:58:54.544
  STEP: Creating a pod to test consume configMaps @ 01/29/24 21:58:54.565
  E0129 21:58:54.962589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:55.963752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:56.964844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:57.965908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:58:58.59
  Jan 29 21:58:58.595: INFO: Trying to get logs from node nodea08 pod pod-projected-configmaps-37ddb3eb-c428-4c4d-a5ee-f08eb1a73084 container agnhost-container: <nil>
  STEP: delete the pod @ 01/29/24 21:58:58.604
  Jan 29 21:58:58.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9220" for this suite. @ 01/29/24 21:58:58.624
• [4.118 seconds]
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 01/29/24 21:58:58.631
  Jan 29 21:58:58.631: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename disruption @ 01/29/24 21:58:58.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:58:58.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:58:58.65
  STEP: creating the pdb @ 01/29/24 21:58:58.655
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:58:58.66
  E0129 21:58:58.966560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:58:59.967920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 01/29/24 21:59:00.669
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:59:00.68
  E0129 21:59:00.967839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:01.968416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 01/29/24 21:59:02.691
  STEP: Waiting for the pdb to be processed @ 01/29/24 21:59:02.704
  E0129 21:59:02.968759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:03.969340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 01/29/24 21:59:04.72
  Jan 29 21:59:04.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9077" for this suite. @ 01/29/24 21:59:04.729
• [6.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 01/29/24 21:59:04.737
  Jan 29 21:59:04.737: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename emptydir @ 01/29/24 21:59:04.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:59:04.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:59:04.761
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/29/24 21:59:04.766
  E0129 21:59:04.970294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:05.971311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:06.971604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:07.971775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/29/24 21:59:08.791
  Jan 29 21:59:08.796: INFO: Trying to get logs from node nodea08 pod pod-bd2153c2-8db5-4072-987c-d02f0398be70 container test-container: <nil>
  STEP: delete the pod @ 01/29/24 21:59:08.805
  Jan 29 21:59:08.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8561" for this suite. @ 01/29/24 21:59:08.828
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 01/29/24 21:59:08.837
  Jan 29 21:59:08.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename namespaces @ 01/29/24 21:59:08.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:59:08.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:59:08.857
  STEP: Read namespace status @ 01/29/24 21:59:08.862
  Jan 29 21:59:08.867: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 01/29/24 21:59:08.867
  Jan 29 21:59:08.873: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 01/29/24 21:59:08.873
  Jan 29 21:59:08.882: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jan 29 21:59:08.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4372" for this suite. @ 01/29/24 21:59:08.885
• [0.053 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 01/29/24 21:59:08.89
  Jan 29 21:59:08.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4102575633
  STEP: Building a namespace api object, basename services @ 01/29/24 21:59:08.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/29/24 21:59:08.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/29/24 21:59:08.908
  STEP: creating service in namespace services-4507 @ 01/29/24 21:59:08.912
  STEP: creating service affinity-nodeport in namespace services-4507 @ 01/29/24 21:59:08.912
  STEP: creating replication controller affinity-nodeport in namespace services-4507 @ 01/29/24 21:59:08.926
  I0129 21:59:08.934003      23 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4507, replica count: 3
  E0129 21:59:08.972363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:09.972930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:10.973632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:11.974300      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0129 21:59:11.984756      23 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 29 21:59:12.000: INFO: Creating new exec pod
  E0129 21:59:12.975718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:13.975441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:14.975655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:15.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0129 21:59:15.975846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:16.976003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:17.255: INFO: rc: 1
  Jan 29 21:59:17.255: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 affinity-nodeport 80
  nc: connect to affinity-nodeport port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0129 21:59:17.976738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:18.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0129 21:59:18.977336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:19.978564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:20.487: INFO: rc: 1
  Jan 29 21:59:20.487: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 affinity-nodeport 80
  nc: connect to affinity-nodeport port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0129 21:59:20.978692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:21.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  E0129 21:59:21.978755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:22.979534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:23.474: INFO: rc: 1
  Jan 29 21:59:23.474: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 affinity-nodeport 80
  nc: connect to affinity-nodeport port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0129 21:59:23.979596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:24.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jan 29 21:59:24.472: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jan 29 21:59:24.472: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:59:24.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.240.110 80'
  Jan 29 21:59:24.684: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.240.110 80\nConnection to 10.97.240.110 80 port [tcp/http] succeeded!\n"
  Jan 29 21:59:24.684: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:59:24.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.28 30892'
  Jan 29 21:59:24.892: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.28 30892\nConnection to 192.168.100.28 30892 port [tcp/*] succeeded!\n"
  Jan 29 21:59:24.892: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:59:24.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.100.129 30892'
  E0129 21:59:24.980007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 29 21:59:25.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.100.129 30892\nConnection to 192.168.100.129 30892 port [tcp/*] succeeded!\n"
  Jan 29 21:59:25.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 29 21:59:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4102575633 --namespace=services-4507 exec execpod-affinity98qfd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.100.28:30892/ ; done'
  Jan 29 21:59:25.456: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.100.28:30892/\n"
  Jan 29 21:59:25.457: INFO: stdout: "\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm\naffinity-nodeport-f9kpm"
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Received response from host: affinity-nodeport-f9kpm
  Jan 29 21:59:25.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 29 21:59:25.464: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4507, will wait for the garbage collector to delete the pods @ 01/29/24 21:59:25.479
  Jan 29 21:59:25.542: INFO: Deleting ReplicationController affinity-nodeport took: 7.814871ms
  Jan 29 21:59:25.643: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.986599ms
  E0129 21:59:25.980815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0129 21:59:26.981353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4507" for this suite. @ 01/29/24 21:59:27.766
• [18.881 seconds]
------------------------------
S
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jan 29 21:59:27.773: INFO: Running AfterSuite actions on node 1
  Jan 29 21:59:27.773: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.074 seconds]
------------------------------

Ran 378 of 7207 Specs in 6135.497 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h42m15.97067011s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

